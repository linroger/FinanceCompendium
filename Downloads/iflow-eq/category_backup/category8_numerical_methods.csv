title,equation_latex,main_category,notes
Binomial Tree Up Factor,u = exp(sigma * sqrt(delta t)),Numerical Methods - Option Pricing Methods,This equation calculates the up factor in a binomial tree. u represents the multiplicative factor for an up move, sigma is the volatility, and delta t is the time step. The up and down factors ensure the tree recombines and matches the volatility of the underlying asset.
Binomial Tree Down Factor,d = 1 / u = exp(-sigma * sqrt(delta t)),Numerical Methods - Option Pricing Methods,This equation calculates the down factor in a binomial tree. d represents the multiplicative factor for a down move, which is the reciprocal of the up factor u. The relationship d = 1/u ensures that an up move followed by a down move returns to the original price.
Binomial Tree Risk-Neutral Probability,p = (exp(r * delta t) - d) / (u - d),Numerical Methods - Option Pricing Methods,This equation calculates the risk-neutral probability of an up move in a binomial tree. p represents the probability, r is the risk-free rate, delta t is the time step, and u and d are the up and down factors. The risk-neutral probability ensures the expected return equals the risk-free rate.
Binomial Tree Option Price,Option = exp(-r * delta t) * (p * Option_up + (1 - p) * Option_down),Numerical Methods - Option Pricing Methods,This equation calculates the option price at each node using backward induction. Option represents the discounted expected value under risk-neutral probabilities, Option_up and Option_down are the option values at up and down nodes. Backward induction starts from the terminal payoff.
Cox-Ross-Rubinstein Parameters,u = exp(sigma * sqrt(delta t)), d = exp(-sigma * sqrt(delta t)), p = 0.5 * (exp(r * delta t) - d) / (u - d),Numerical Methods - Option Pricing Methods,These are the CRR binomial tree parameters. The CRR method is a specific parameterization that ensures the tree matches the volatility and converges to the continuous-time solution as delta t approaches zero. CRR is widely used for American option pricing.
Jarrow-Rudd Parameters,u = exp((r - 0.5 * sigma^2) * delta t + sigma * sqrt(delta t)), d = exp((r - 0.5 * sigma^2) * delta t - sigma * sqrt(delta t)), p = 0.5,Numerical Methods - Option Pricing Methods,These are the Jarrow-Rudd binomial tree parameters. The JR method ensures that the expected return equals the risk-free rate at each step and uses equal probabilities. JR provides better convergence for certain types of options.
Trinomial Tree Up Factor,u = exp(lambda * sigma * sqrt(delta t)),Numerical Methods - Option Pricing Methods,This equation calculates the up factor in a trinomial tree. u represents the multiplicative factor for an up move, lambda is a stretching parameter (typically > 1), sigma is the volatility, and delta t is the time step. Trinomial trees provide better accuracy than binomial trees.
Trinomial Tree Down Factor,d = exp(-lambda * sigma * sqrt(delta t)),Numerical Methods - Option Pricing Methods,This equation calculates the down factor in a trinomial tree. d represents the multiplicative factor for a down move. Trinomial trees have three branches (up, down, middle) instead of two, providing more accurate approximations to continuous-time processes.
Trinomial Tree Middle Factor,m = 1,Numerical Methods - Option Pricing Methods,This equation defines the middle factor in a trinomial tree. m represents the multiplicative factor for no change in the underlying price. The middle branch allows for the possibility of no movement in the underlying asset, which can improve accuracy.
Trinomial Tree Probabilities,p_u = (a - (b - c)) / (a - c), p_m = (b - a) / (a - c), p_d = 1 - p_u - p_m,Numerical Methods - Option Pricing Methods,These equations calculate the probabilities for up, middle, and down moves in a trinomial tree. The probabilities depend on the drift and volatility parameters and ensure that the expected return matches the risk-free rate. Trinomial trees are more accurate than binomial trees.
Finite Difference Explicit Scheme,V_{i,j+1} = a_j * V_{i-1,j} + b_j * V_{i,j} + c_j * V_{i+1,j},Numerical Methods - Option Pricing Methods,This equation defines the explicit finite difference scheme for option pricing. V_{i,j} represents the option value at grid point (i, j), and a_j, b_j, c_j are coefficients depending on the PDE parameters. The explicit scheme is simple but conditionally stable.
Finite Difference Implicit Scheme,a_j * V_{i-1,j+1} + b_j * V_{i,j+1} + c_j * V_{i+1,j+1} = V_{i,j},Numerical Methods - Option Pricing Methods,This equation defines the implicit finite difference scheme. Unlike the explicit scheme, the implicit scheme requires solving a system of linear equations at each time step. The implicit scheme is unconditionally stable but more computationally intensive.
Crank-Nicolson Scheme,0.5 * (a_j * V_{i-1,j+1} + b_j * V_{i,j+1} + c_j * V_{i+1,j+1}) + 0.5 * (a_j * V_{i-1,j} + b_j * V_{i,j} + c_j * V_{i+1,j}) = V_{i,j},Numerical Methods - Option Pricing Methods,This equation defines the Crank-Nicolson scheme, which averages the explicit and implicit schemes. Crank-Nicolson is second-order accurate in both time and space and is unconditionally stable. It is widely used for option pricing PDEs.
Monte Carlo Option Price,Option = exp(-r * T) * (1/N) * sum_{i=1}^N Payoff_i,Numerical Methods - Option Pricing Methods,This equation calculates the option price using Monte Carlo simulation. Option represents the discounted average payoff, r is the risk-free rate, T is the time to maturity, N is the number of simulation paths, and Payoff_i is the payoff of path i. Monte Carlo is flexible for path-dependent options.
Monte Carlo Standard Error,SE = sigma_payoff / sqrt(N),Numerical Methods - Option Pricing Methods,This equation calculates the standard error of the Monte Carlo estimate. SE represents the standard error, sigma_payoff is the standard deviation of payoffs, and N is the number of simulations. The standard error decreases as 1/sqrt(N), requiring 4x more simulations to halve the error.
Variance Reduction: Antithetic Variates,Var((X + X')/2) = 0.5 * (Var(X) + Var(X') + 2 * Cov(X, X')),Numerical Methods - Option Pricing Methods,This equation shows how antithetic variates reduce variance. X' is the antithetic variable (using -W_t instead of W_t). If Cov(X, X') < 0, the variance is reduced. Antithetic variates are a simple and effective variance reduction technique.
Variance Reduction: Control Variates,Var(X_cv) = Var(X) - 2 * Cov(X, Y) / Var(Y) + Var(Y) * Cov(X, Y)^2 / Var(Y)^2,Numerical Methods - Option Pricing Methods,This equation shows how control variates reduce variance. X is the original estimator, Y is the control variate with known mean, and X_cv = X - c*(Y - E[Y]) is the controlled estimator. Control variates use correlated variables with known means to reduce variance.
Quasi-Monte Carlo,Error = O((log N)^d / N),Numerical Methods - Option Pricing Methods,This equation gives the convergence rate of quasi-Monte Carlo methods. The error decreases as O((log N)^d / N) instead of O(1/sqrt(N)) for standard Monte Carlo, where d is the dimension. Quasi-Monte Carlo uses low-discrepancy sequences for faster convergence in low dimensions.
Longstaff-Schwartz Algorithm,Continuation Value = sum_{i=1}^M w_i * L_i(S_t),Numerical Methods - Option Pricing Methods,This equation defines the continuation value in the Longstaff-Schwartz algorithm for American options. Continuation Value represents the expected value of continuing rather than exercising, w_i are regression coefficients, and L_i(S_t) are basis functions. The algorithm uses least squares regression to approximate continuation values.
Least Squares Monte Carlo,LSM = argmin_b sum_{i=1}^M (Discounted Payoff_i - sum_{j=1}^K b_j * L_j(S_{t_i}))^2,Numerical Methods - Option Pricing Methods,This equation defines the least squares problem in LSM. LSM estimates the coefficients b_j to minimize the squared error between discounted payoffs and basis function values. LSM is the standard method for pricing American options with Monte Carlo.
Newton-Raphson Method,x_{n+1} = x_n - f(x_n) / f'(x_n),Numerical Methods - Optimization Techniques,This equation defines the Newton-Raphson method for root finding. x_{n+1} represents the next approximation, x_n is the current approximation, and f(x_n) and f'(x_n) are the function value and derivative. Newton-Raphson converges quadratically near the root but requires the derivative.
Bisection Method,x_{n+1} = (a_n + b_n) / 2,Numerical Methods - Optimization Techniques,This equation defines the bisection method for root finding. x_{n+1} is the midpoint of the interval [a_n, b_n], and the interval is halved at each iteration. Bisection is robust but converges linearly, requiring many iterations for high precision.
Implied Volatility Newton-Raphson,sigma_{n+1} = sigma_n - (C(sigma_n) - C_market) / vega(sigma_n),Numerical Methods - Optimization Techniques,This equation calculates implied volatility using Newton-Raphson. sigma_{n+1} is the updated volatility, C(sigma_n) is the option price with volatility sigma_n, C_market is the market price, and vega(sigma_n) is the option vega. Newton-Raphson converges quickly for smooth functions.
Brent's Method,Combines bisection, secant, and inverse quadratic interpolation,Numerical Methods - Optimization Techniques,This method combines the robustness of bisection with the speed of the secant method. Brent's method is the standard algorithm for root finding and implied volatility calculation in financial libraries. It guarantees convergence and is efficient.
Gradient Descent,x_{n+1} = x_n - alpha * nabla f(x_n),Numerical Methods - Optimization Techniques,This equation defines gradient descent for optimization. x_{n+1} represents the next iterate, x_n is the current point, alpha is the learning rate, and nabla f(x_n) is the gradient. Gradient descent is widely used for calibrating financial models.
Stochastic Gradient Descent,x_{n+1} = x_n - alpha_n * nabla f_i(x_n),Numerical Methods - Optimization Techniques,This equation defines SGD for optimization with large datasets. x_{n+1} represents the next iterate, alpha_n is the learning rate, and nabla f_i(x_n) is the gradient of a randomly selected data point. SGD is faster than gradient descent for large-scale problems.
Levenberg-Marquardt Algorithm,(J^T * J + lambda * diag(J^T * J)) * delta = J^T * (y - f(x)),Numerical Methods - Optimization Techniques,This equation defines the Levenberg-Marquardt algorithm for nonlinear least squares. J is the Jacobian, lambda is a damping parameter, delta is the step, and y - f(x) is the residual. LM interpolates between gradient descent and Gauss-Newton for robust convergence.
Calibration Objective Function,min_theta sum_{i=1}^N w_i * (Model Price_i(theta) - Market Price_i)^2,Numerical Methods - Optimization Techniques,This equation defines the calibration objective function. The goal is to find model parameters theta that minimize the weighted sum of squared errors between model prices and market prices. Calibration is essential for making models match market data.
Model Risk,MR = |Model Price - Market Price| / Market Price,Numerical Methods - Optimization Techniques,This equation calculates model risk as the percentage error between model and market prices. MR represents the model risk, which measures how well the model captures market prices. Model risk is important for model validation and risk management.
Backtesting,Hit Ratio = (Number of Correct Predictions) / (Total Predictions),Numerical Methods - Optimization Techniques,This equation calculates the hit ratio for backtesting a model. Hit Ratio represents the proportion of correct predictions. Backtesting validates model performance on historical data and is essential for model validation.
Kupiec Test,LR = -2 * ln((1 - alpha)^{N - x} * alpha^x / ((1 - p)^{N - x} * p^x)),Numerical Methods - Optimization Techniques,This equation defines the Kupiec test for VaR model validation. LR is the likelihood ratio statistic, alpha is the VaR confidence level, N is the number of observations, x is the number of VaR violations, and p = 1 - alpha is the expected violation rate. The Kupiec test checks if VaR violations are consistent with the confidence level.
Christoffersen Test,LR_cc = LR_uc + LR_ind,Numerical Methods - Optimization Techniques,This equation defines the Christoffersen test for VaR model validation. LR_cc is the combined likelihood ratio, LR_uc tests the unconditional coverage (Kupiec test), and LR_ind tests the independence of violations. The Christoffersen test is more comprehensive than the Kupiec test.
Bootstrap Method,theta* = f(X*) where X* is a bootstrap sample,Numerical Methods - Optimization Techniques,This equation defines the bootstrap method for estimating confidence intervals. theta* is the statistic calculated from the bootstrap sample X*, which is drawn with replacement from the original data. Bootstrap provides non-parametric confidence intervals without distributional assumptions.
Jackknife Method,theta_jackknife = (1/N) * sum_{i=1}^N theta_{(-i)},Numerical Methods - Optimization Techniques,This equation defines the jackknife method for bias and variance estimation. theta_jackknife is the jackknife estimate, and theta_{(-i)} is the statistic calculated with the i-th observation removed. Jackknife is a resampling method computationally cheaper than bootstrap.
Cross-Validation,CV = (1/K) * sum_{k=1}^K MSE_k,Numerical Methods - Optimization Techniques,This equation defines K-fold cross-validation. CV represents the cross-validation error, K is the number of folds, and MSE_k is the mean squared error on fold k. Cross-validation assesses model generalization and prevents overfitting.
Regularization,L2 = lambda * ||theta||^2, L1 = lambda * ||theta||_1,Numerical Methods - Optimization Techniques,These equations define L2 (ridge) and L1 (lasso) regularization. Regularization adds a penalty term to the objective function to prevent overfitting. L2 shrinks coefficients smoothly, while L1 promotes sparsity.
Akaike Information Criterion,AIC = 2k - 2 * ln(L),Numerical Methods - Optimization Techniques,This equation defines the AIC for model selection. AIC represents the Akaike Information Criterion, k is the number of parameters, and L is the likelihood. AIC balances model fit and complexity, penalizing additional parameters.
Bayesian Information Criterion,BIC = k * ln(N) - 2 * ln(L),Numerical Methods - Optimization Techniques,This equation defines the BIC for model selection. BIC represents the Bayesian Information Criterion, k is the number of parameters, N is the sample size, and L is the likelihood. BIC penalizes complexity more heavily than AIC for large N.
Maximum Likelihood Estimation,theta_MLE = argmax_theta L(theta | X),Numerical Methods - Optimization Techniques,This equation defines MLE. theta_MLE represents the parameter values that maximize the likelihood L(theta | X) given data X. MLE is a fundamental method for parameter estimation in statistical models.
Method of Moments,theta_MM solves E[X | theta] = sample_mean(X),Numerical Methods - Optimization Techniques,This equation defines the method of moments. theta_MM represents parameters that match theoretical moments to sample moments. Method of moments is simpler than MLE but less efficient for many models.
Expectation-Maximization,E-step: Q(theta | theta^{(t)}) = E[log L(theta | X, Z) | X, theta^{(t)}], M-step: theta^{(t+1)} = argmax_theta Q(theta | theta^{(t)}),Numerical Methods - Optimization Techniques,These equations define the EM algorithm for latent variable models. The E-step computes the expected log-likelihood given current parameters, and the M-step maximizes this expectation. EM is used for models with missing data or latent variables.
Kalman Filter,Predict: x_{t|t-1} = F * x_{t-1|t-1}, Update: x_{t|t} = x_{t|t-1} + K * (y_t - H * x_{t|t-1}),Numerical Methods - Optimization Techniques,These equations define the Kalman filter for state estimation. x_{t|t-1} is the predicted state, F is the state transition matrix, K is the Kalman gain, y_t is the observation, and H is the observation matrix. Kalman filter is used for time series filtering and estimation.
Particle Filter,weights_i proportional to likelihood(y_t | x_t^{(i)}),Numerical Methods - Optimization Techniques,This equation defines the particle filter (sequential Monte Carlo) for state estimation. weights_i are the importance weights for particle i, y_t is the observation, and x_t^{(i)} is the particle state. Particle filter handles nonlinear and non-Gaussian models.
Greeks by Finite Differences,Delta = (f(S + delta S) - f(S - delta S)) / (2 * delta S),Numerical Methods - Option Pricing Methods,This equation calculates Delta using central finite differences. Delta represents the sensitivity of the option price to the underlying price. Finite differences are used when analytical Greeks are not available. Central differences are more accurate than forward/backward differences.
Gamma by Finite Differences,Gamma = (f(S + delta S) - 2 * f(S) + f(S - delta S)) / (delta S)^2,Numerical Methods - Option Pricing Methods,This equation calculates Gamma using finite differences. Gamma represents the second derivative of the option price with respect to the underlying price. Finite difference Gamma requires three option price evaluations.
Vega by Finite Differences,Vega = (f(S, sigma + delta sigma) - f(S, sigma - delta sigma)) / (2 * delta sigma),Numerical Methods - Option Pricing Methods,This equation calculates Vega using finite differences. Vega represents the sensitivity to volatility. Finite difference Vega is used when analytical Vega is not available, such as for exotic options.
Adaptive Mesh Refinement,Refine grid where solution changes rapidly,Numerical Methods - Option Pricing Methods,This technique refines the finite difference grid in regions where the solution has high gradients (e.g., near barriers or strike prices). AMR improves accuracy without excessive computational cost by concentrating grid points where needed.
Operator Splitting,Split PDE into simpler sub-problems,Numerical Methods - Option Pricing Methods,This technique splits a complex PDE into simpler sub-problems that can be solved sequentially. Operator splitting is used for multi-dimensional PDEs and PDEs with mixed derivatives, making them more tractable.
Spectral Methods,Expand solution in basis functions,Numerical Methods - Option Pricing Methods,This approach expands the solution in a basis of orthogonal functions (e.g., Chebyshev polynomials). Spectral methods can achieve exponential convergence for smooth solutions but are less flexible than finite difference methods for discontinuous payoffs.