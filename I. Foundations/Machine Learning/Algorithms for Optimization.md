---
title: Algorithms for Optimization
parent_directory: Machine Learning
primary_tags:
- optimization algorithms
- engineering design optimization
- mathematical optimization methods
- computational optimization techniques
- gradient based methods
- direct search methods
- stochastic optimization
secondary_tags:
- unconstrained optimization
- constrained optimization
- derivative free methods
- line search algorithms
- trust region methods
- multiobjective optimization
- surrogate model optimization
- uncertainty quantification
cssclasses: academia
---

# Algorithms for Optimization

SECOND EDITION

Mykel J. Kochenderfer

Tim A. Wheeler

The MIT Press

Cambridge, Massachusetts

London, England

This work is licensed under a Creative Commons "Attribution-NonCommercial-NoDerivatives 4.0 International" license.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/59b4e70231845e7756e399ba41a4c2358ccb789c4ac50484388bcaff84e558c2.jpg)

© 2025 Massachusetts Institute of Technology

Copyright in this monograph has been licensed exclusively to The MIT Press, http://mitpress.mit.edu. All inquiries regarding rights should be addressed to The MIT Press, Rights and Permissions Department.

This book was set in  $\mathrm{T_{E}X}$  Gyre Pagella by the authors in LATEX.

To our families.

# APPENDICES

A Julia 535

A.1 Types 535  
A.2 Functions 549  
A.3 Control Flow 552  
A.4 Packages 554

B Test Functions 555

B.1 Ackley's Function 555  
B.2 Booth's Function 556  
B.3 Branin Function 557  
B.4 Flower Function 558  
B.5 Michalewicz Function 559  
B.6 Rosenbrock's Banana Function 560  
B.7 Wheeler's Ridge 561  
B.8 Circle Function 562

C Mathematical Concepts 563

C.1 Asymptotic Notation 563  
C.2 Taylor Expansion 565  
C.3 Convexity 566  
C.4 Norms 569  
C.5 Matrix Calculus 569  
C.6 Positive Definiteness 572  
C.7 Matrix Decompositions 572  
C.8 Gaussian Distribution 576  
C.9 Gaussian Quadrature 576

References 581

Index 597

# Preface

This book provides a broad introduction to optimization with a focus on practical algorithms for the design of engineering systems. We cover a wide variety of optimization topics, introducing the underlying mathematical problem formulations and the algorithms for solving them. Figures, examples, and exercises are provided to convey the intuition behind the various approaches.

This text is intended for advanced undergraduates and graduate students as well as professionals. The book requires some mathematical maturity and assumes prior exposure to multivariable calculus, linear algebra, and probability concepts. Some review material is provided in the appendix. Disciplines where the book would be especially useful include mathematics, statistics, computer science, aerospace, electrical engineering, and operations research.

Fundamental to this textbook are the algorithms, which are all implemented in the Julia programming language. We have found the language to be ideal for specifying algorithms in human readable form. Permission is granted, free of charge, to use the code snippets associated with this book, subject to the condition that the source of the code is acknowledged. We anticipate that others may want to contribute translations of these algorithms to other programming languages. As translations become available, we will link to them from the book's webpage.

New in this second edition are three new chapters on duality, quadratic programming, and disciplined convex programming. The first new chapter covers the principles of duality and several of the core optimization algorithms that leverage its theory, including primal-dual methods, dual ascent, and the alternating direction method of multipliers. The previous edition had sections that mentioned duality, but dedicating a chapter to it allows for more focus on their theory and primal-dual algorithms. We are particularly excited to cover the alternating direction method of multipliers, an approach that has been the subject of

recent theoretical and practical advances. This algorithm is generally applicable to constrained optimization and can be robustly scaled to extremely large problems. We apply the alternating direction method of multipliers to a broad range of problems that generalize under its umbrella, including alternating projections, least absolute deviation, and basis pursuit.

The second new chapter covers quadratic programming. It mirrors our existing chapter on linear constrained optimization, now renamed to 'Linear Programming.' Linear programming refers to solving a very general class of linear-constrained optimization problems with linear objectives. Quadratic programming is also a very general class of linear-constrained optimization problems, but with quadratic objectives. This new chapter covers a series of algorithmic problem decompositions that successively transform a general quadratic program into a form amenable to efficient solution by iterative algorithms.

The third new chapter focuses on disciplined convex programming. Disciplined convex programming allows problems to be expressed in a natural manner, but allows for algorithms to verify problem convexity and reformulate input problems into a general structure amenable to efficient solution by a wide variety of solvers. Disciplined convex programming is a workhorse of modern convex optimization. This chapter covers the theory and algorithms to allow the reader to be better equipped to effectively use disciplined convex solvers, verify convexity, and exploit problem structure to efficiently solve large problems.

In addition to these new chapters, we have added additional methods, ranging from new ways to estimate gradients to multi-fidelity techniques. Based on feedback from multiple iterations of the course, we improved the clarity of the text to make the topics more accessible, expanded explanations, updated references, and streamlined the algorithms. There are now additional exercises, example applications, and figures.

MYKEL J. KOCHENDERFER

TIM A. WHEELER

Stanford, Calif.

February 9, 2025

Ancillary material is available on the book's webpage:

http://mitpress.mit.edu/algorithms-for-optimization

# Acknowledgments

This textbook has grown from a course on engineering design optimization taught at Stanford. We are grateful to the students and teaching assistants who have helped shape the course over the past five years. We are also indebted to the faculty who have taught variations of the course before in our department based on lecture notes from Joaquim Martins, Juan Alonso, Ilan Kroo, Dev Rajnarayan, and Jason Hicken. Many of the topics discussed in this textbook were inspired by their lecture notes.

The authors wish to thank the many individuals who have provided valuable feedback on early drafts of our manuscript, including Atharva Aalok, Mohamed Abdelaty, Atish Agarwala, Ross Alexander, Yasmine Alonso, Piergiorgio Alotto, Nancy Ammar, Grayson Armour, Dylan Asmar, David Ata, Rishi Bedi, Logan Bell, Felix Berkenkamp, Raunak Bhattacharyya, Hans Borchers, Maxime Bouton, Stephen Boyd, Ellis Brown, Abhishek Cauligi, Mo Chen, Zhengyu Chen, Raphael Chinchilla, Vince Chiu, Hanyou Chu, Anthony Corso, Nikhil Devanathan, Holly Dinkel, Jonathan Cox, Katherine Driggs-Campbell, Thai Duong, Hamza El-Saawy, Sofiane Ennadir, Daniel Fein, Kaijun Feng, Tamas Gal, Christopher Lazarus Garcia, Wouter Van Gijseghem, Michael Gobble, Robert Goedman, Jayesh Gupta, Aaron Havens, William Healy, Richard Hsieh, Sydney Hsu, Jeremy Huang, Zdenek Hurák, Luke Hyman, Masha Itkina, Arec Jamgochian, Bogumil Kamiński, Walker Kehoe, Mindaugas Kepalas, Shogo Kishimoto, Veronika Korneyeva, Erez Krimsky, Petr Krysl, Tim Lappe, Jessie Lauzon, Ruilin Li, Ye Li, Iblis Lin, Sean Lin, Edward Londner, Charles Lu, Miles Lubin, Marcus Luebke, Robert Lupton, Jacqueline Machesky, Ashe Magalhaes, Zouhair Mahboubi, Pranav Maheshwari, Yuki Matsuoka, Travis McGuire, Jeremy Morton, Robert Moss, Truong Minh Nhat, Longwu Ou, Santiago Padron, Jimin Park, Harsh Patel, Christian Peel, Derek Phillips, Brad Rafferty, Sidd Rao, Andreas Reschka, Alex Reynell, Stuart Rogers, Per Rutquist

Ryan Samuels, Orson Sandoval, Jeffrey Sarnoff, Chelsea Sidrane, Sumeet Singh, Cooper Shea, Nathan Stacey, Ethan Strijbosch, Anshrin Srivastava, Andre Tkacenko, Alex Toews, Ava Tolentino, Pamela Toman, Olivia Tomassetti, Rachael Tompa, Zacharia Tuten, Alexandros Tzikas, Raman Vikhu, Boris Vishnevsky, Yuri Vishnevsky, Julie Walker, Zijian Wang, Patrick Washington, Jacob West, Adam Wiktor, Brian Wu, John Wu, Sofia Wyetzner, Esen Yel, Brandon Yeung, Anil Yildiz, Robert Young, Javier Yu, Andrea Zanette, Remy Zawislak, and Tarek Zougari. In addition, it has been a pleasure working with Marie Lufkin Lee and Christine Bridget Savage from the MIT Press in preparing this manuscript for publication.

The style of this book was inspired by Edward Tufte. Among other stylistic elements, we adopted his wide margins and use of small multiples. In fact, the typesetting of this book is heavily based on the Tufte-LaTeX package by Kevin Godby, Bil Kleb, and Bill Wood. We were also inspired by the clarity of the textbooks by Donald Knuth and Stephen Boyd.

Over the past few years, we have benefited from discussions with the core Julia developers, including Jeff Bezanson, Stefan Karpinski, and Viral Shah. We have also benefited from the various open source packages on which this textbook depends (see appendix A.4). The typesetting of the code is done with the help of pythonex, which is maintained by Geoffrey Poore. Plotting is handled by pgfplots, which is maintained by Christian Feuersanger. The book's color scheme was adapted from the Monokai theme by Jon Skinner of Sublime Text. For plots, we use the viridis colormap defined by Stefan van der Walt and Nathaniel Smith.

# 1 Introduction

Many disciplines involve optimization at their core. In physics, systems naturally tend toward their lowest energy state subject to physical laws. In business, corporations aim to maximize shareholder value. In biology, fitter organisms are more likely to survive. This book will focus on optimization from an engineering perspective, where the objective is to design a system that optimizes a set of metrics subject to constraints. The system could be a complex physical system like an aircraft, or it could be a simple structure such as a bicycle frame. The system might not even be physical; for example, we might be interested in designing a control system for an automated vehicle or a computer vision system that detects whether an image of a tumor biopsy is cancerous. We want these systems to perform as well as possible. Depending on the application, relevant metrics might include efficiency, safety, and accuracy. Constraints on the design might include cost, weight, and structural soundness.

This book is about the algorithms, or computational processes, for optimization. Given some representation of the system design, such as a set of numbers encoding the geometry of an airfoil, these algorithms will tell us how to search the space of possible designs with the aim of finding the best one. Depending on the application, this search may involve running physical experiments, such as wind tunnel tests, or it might involve evaluating an analytical expression or running computer simulations. We will discuss computational approaches for addressing a variety of challenges, such as how to search high-dimensional spaces, handling problems where there are multiple competing objectives, and accommodating uncertainty in the metrics.

# 1.1 A History

We will begin our discussion of the history of algorithms for optimization<sup>1</sup> with the ancient Greek philosophers. Pythagoras of Samos (569-475 BCE), the developer of the Pythagorean theorem, claimed that "the principles of mathematics were the principles of all things,"<sup>2</sup> popularizing the idea that mathematics could model the world. Both Plato (427-347 BCE) and Aristotle (384-322 BCE) used reasoning for the purpose of societal optimization.<sup>3</sup> They contemplated the best style of human life, which involves the optimization of both individual lifestyle and functioning of the state. Aristotelian logic was an early formal process—an algorithm—by which deductions can be made.

Optimization of mathematical abstractions also dates back millennia. Euclid of Alexandria (325-265 BCE) solved early optimization problems in geometry, including how to find the shortest and longest lines from a point to the circumference of a circle. He also showed that a square is the rectangle with the maximum area for a fixed perimeter. $^{4}$  The Greek mathematician Zenodorus (200-140 BCE) studied Dido's problem, shown in figure 1.1. Others demonstrated that nature seems to optimize. Heron of Alexandria (10-75 CE) showed that light travels between points through the path of shortest length. Pappus of Alexandria (290-350 CE), among his many contributions to optimization, argued that the hexagon repeated in honeycomb is the optimal regular polygon for storing honey; its hexagonal structure uses the least material to create a lattice of cells over a plane. $^{5}$

Central to the study of optimization is the use of algebra, which is the study of the rules for manipulating mathematical symbols. Algebra is credited to the Persian mathematician al-Khwarizmi (790-850 CE) with the treatise "Kitab al-jabr wal-muqābala," or "The Compendious Book on Calculation by Completion and Balancing." Algebra had the advantage of using Hindu-Arabic numerals, including the use of zero in base notation. The word al'jabr is Arabic for restoration and is the source for the Western word algebra. The term algorithm comes from algo-rimi, the Latin translation and pronunciation of al-Khwarizmi's name.

Optimization problems are often posed as a search in a space defined by a set of coordinates. Use of coordinates comes from René Descartes (1596-1650), who used two numbers to describe a point on a two-dimensional plane. His insight linked algebra, with its analytic equations, to the descriptive and visual field of geometry. His work also included a method for finding the tangent to any curve whose equation is known. Tangents are useful in identifying the minima and

This discussion is not meant to be comprehensive. A more detailed history is provided by X.-S. Yang, "A Brief History of Optimization," in Engineering Optimization. Wiley, 2010, pp. 3-10.  
2 Aristotle, Metaphysics, trans. by W.D.Ross.350 BCE,Book I,Part 5. 3 See discussion by S.Kiranyaz,T. Ince, and M.Gabbouj, Multidimensional Particle Swarm Optimization for Machine Learning and Pattern Recognition.Springer,2014,Section 2.1.

sea

Carthage

Figure 1.1. Queen Dido, founder of Carthage, was granted as much land as she could enclose with a bullhide thong. She made a semicircle with each end of the thong against the Mediterranean Sea, thus enclosing the maximum possible area. This problem is mentioned in Virgil's Aeneid (19 BCE).

4 See books III and VI of Euclid, The Elements, trans. by D.E.Joyce. 300 BCE.  
5 T.C. Hales, "The Honeycomb Conjecture," Discrete & Computational Geometry, vol. 25, pp. 1-22, 2001.

$^6$  R. Descartes, "La Géométrie," in Discours de la Méthode. 1637.

maxima of functions. Pierre de Fermat (1601-1665) began solving for where the derivative is zero to identify potentially optimal points.

The concept of calculus, or the study of continuous change, plays an important role in our discussion of optimization. Modern calculus stems from the developments of Gottfried Wilhelm Leibniz (1646-1716) and Sir Isaac Newton (1642-1727). Differential calculus is particularly important in guiding optimization, especially in high-dimensional spaces. Joseph-Louis Lagrange (1736-1813) later unified tools from calculus and algebra to solve constrained optimization problems, where we want to optimize some objective subject to a set of constraints. He introduced the method of Lagrange multipliers, which transforms a constrained optimization problem into an unconstrained one by combining the objective function and constraints into a single function known as the Lagrangian. By taking derivatives of this function and solving the resulting system of equations, he linked the continuous methods of calculus with the algebraic solution of equations, laying the groundwork for modern optimization techniques.

The mid-twentieth century saw the rise of the electronic computer, spurring interest in numerical algorithms for optimization. The ease of calculations allowed optimization to be applied to much larger problems in a variety of domains. One of the major breakthroughs came with the introduction of linear programming, which is an optimization problem with a linear objective function and linear constraints. Leonid Kantorovich (1912-1986) presented a formulation for linear programming and an algorithm to solve it.[9] It was applied to optimal resource allocation problems during World War II. George Dantzig (1914-2005) developed the simplex algorithm, which represented a significant advance in solving linear programs efficiently.[10] Richard Bellman (1920-1984) developed the notion of dynamic programming, which is a commonly used method for optimally solving complex problems by breaking them down into simpler problems.[11] Dynamic programming has been used extensively for optimal control. This textbook outlines many of the key algorithms developed for digital computers that have been used for various engineering design optimization problems.

Decades of advances in large scale computation have resulted in innovative physical engineering designs as well as the design of artificially intelligent systems. The intelligence of these systems has been demonstrated in games such as chess, Jeopardy!, and Go. IBM's Deep Blue defeated the world chess champion Garry Kasparov in 1996 by optimizing moves by evaluating millions of positions. In 2011, IBM's Watson played Jeopardy! against former winners Brad Rutter and Ken

7 Derivatives and gradients are reviewed in the next chapter, along with many methods for computing them. Chapters 5 and 6 show how to use derivatives and gradients to direct the optimization process.

8 Chapter 10 introduces the Lagrangian and shows how Lagrange multipliers can be used to solve constrained optimization problems. These multipliers serve as a foundational concept for duality theory, which is covered in chapter 11.

9 L.V. Kantorovich, "A New Method of Solving Some Classes of Extremal Problems," Proceedings of the USSR Academy of Sciences, vol. 28, pp. 211-214, 1940.

10 The simplex algorithm will be covered in chapter 12.

11 Dynamic programming is reviewed in section 22.5. R. Bellman, "On the Theory of Dynamic Programming," Proceedings of the National Academy of Sciences of the United States of America, vol. 38, no. 8, pp. 716-719, 1952.

Jennings. Watson won the first place prize of \$1 million by optimizing its response with respect to probabilistic inferences about 200 million pages of structured and unstructured data. Since the competition, the system has evolved to assist in healthcare decisions and weather forecasting. In 2017, Google's AlphaGo defeated Ke Jie, the number one ranked Go player in the world. The system used neural networks with millions of parameters that were optimized from self-play and data from human games. The optimization of deep neural networks is fueling a major revolution in artificial intelligence that will likely continue.[12] When ChatGPT was released to the general public in 2022, it was recognized as representing a major milestone in general purpose artificial intelligence with its ability to generate natural language responses to text prompts, such as writing poetry or answering questions, by optimizing the parameters of a special type of deep neural network known as a transformer model with many billions of parameters.[13]

# 1.2 Optimization Process

A typical engineering design optimization process is shown in figure 1.2. $^{14}$  The role of the designer is to provide a problem specification that details the parameters, constants, objectives, and constraints that are to be achieved. The designer is responsible for crafting the problem and quantifying the merits of potential designs. The designer also typically supplies a baseline design or initial design point to the optimization algorithm.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5cd78ebac0bc4983a40c95c3dd818153e17e29bd5ea0696efc0fe4ea2f2feda9.jpg)  
Figure 1.2. The design optimization process. We seek to automate the optimization procedure highlighted in blue.

This book is about automating the process of refining the design to improve performance. An optimization algorithm is used to incrementally improve the design until it can no longer be improved or until the budgeted time or cost has been reached. The designer is responsible for analyzing the result of the optimiza

12 C. M. Bishop and H. Bishop, Deep Learning: Foundations and Concepts. Springer, 2024.  
13 T. Wu, S. He, J. Liu, S. Sun, K. Liu, Q.-L. Han, et al., "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development," IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 5, pp. 1122-1136, 2023. The transformer model was introduced by A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, et al., "Attention is All You Need," in Advances in Neural Information Processing Systems (NeurIPS), 2017.  
Further discussion of the design process in engineering is provided by J. Arora, Introduction to Optimum Design, 4th ed. Academic Press, 2016.

tion process to ensure its suitability for the final application. Misspecifications in the problem, poor baseline designs, and improperly implemented or unsuitable optimization algorithms can all lead to suboptimal or dangerous designs.

There are many advantages of an optimization approach to engineering design. The optimization process provides a systematic design procedure. If properly applied, optimization can help reduce the chance of human error in design. Sometimes design intuition can be misleading; it can be much better to optimize with respect to data. Optimization can speed the process of design, especially when a procedure can be written once and then be reapplied to other problems. Traditional engineering techniques are often visualized and reasoned about by humans in two or three dimensions. Modern optimization techniques, however, can be applied to problems with millions of variables and constraints.

There are also challenges associated with using optimization for design. We are generally limited in our computational resources and time, and so our algorithms have to be selective in how they explore the design space. Fundamentally, the optimization algorithms are limited by the designer's ability to specify the problem. In some cases, the optimization algorithm may exploit modeling errors or provide a solution that does not adequately solve the intended problem. When an algorithm results in an apparently optimal design that is counterintuitive, it can be difficult to interpret. Another limitation is that many optimization algorithms are not guaranteed to produce optimal designs.

# 1.3 Mathematical Formulation

An optimization problem can be formulated mathematically as follows:15

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \tag {1.1}
$$

$$
\text{subject} \quad \mathbf {x} \in \mathcal {X}
$$

Here,  $\mathbf{x}$  is a design point, which can be represented as a vector of values corresponding to different design variables. An  $n$ -dimensional design point is written  $[x_1,\ldots ,x_n]$ , where the  $i$ th design variable is denoted  $x_{i}$ . The elements in this vector can be adjusted to minimize the objective function  $f$ . Any value of  $\mathbf{x}$  in the feasible set  $\mathcal{X}$  that minimizes the objective function is called a solution or minimizer. A particular solution is written  $\mathbf{x}^*$ . Figure 1.3 shows a one-dimensional optimization problem.

15 We can convert maximization problems into minimization problems by simply negating the objective function. We will default to the convention of minimizing the objective in this textbook.  
As with the Julia programming language, square brackets with comma-separated entries are used to represent column vectors. Design points are column vectors.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6e0785bedcfbbef52f36f25c046e53d58d00bed6a9201896157b096bf6daf430.jpg)  
Figure 1.3. A one-dimensional optimization problem. Note that the minimum is merely the best in the feasible set—lower points may exist outside the feasible region.

The feasible set  $\mathcal{X}$  may be defined in terms of a set of constraints. Each constraint limits the set of possible solutions, and together the constraints define the feasible set  $\mathcal{X}$ . Feasible design points do not violate any constraints. For example, consider the following optimization problem:

$$
\underset{x_{1}, x_{2}} {\text{minimize}} f (x_{1}, x_{2})
$$

$$
\text{subject} x_{1} \geq 0 \tag {1.2}
$$

$$
x_{2} \geq 0
$$

$$
x_{1} + x_{2} \leq 1
$$

In this problem, the feasible set  $\mathcal{X} = \{\mathbf{x}\in \mathcal{X}\mid x_1\geq 0,x_2\geq 0,x_1 + x_2\leq 1\}$ . This set is plotted in figure 1.4.

Constraints are typically written as equalities or inequalities. If constraints involve strict inequalities, then the feasible set might not include the constraint boundary. A potential issue with not including the boundary is illustrated by

$$
\underset{x} {\text{minimize}} x \tag {1.3}
$$

$$
\begin{array}{l l} \text{subjectto} & x > 1 \end{array}
$$

The feasible set is shown in figure 1.5. The point  $x = 1$  produces values smaller than any  $x$  greater than 1, but  $x = 1$  is not feasible. We can pick any  $x$  arbitrarily close to, but greater than, 1, but no matter what we pick, we can always find an infinite number of values even closer to 1. We must conclude that the problem has no solution. To avoid such issues, it is often best to include the constraint boundary in the feasible set.

Modeling engineering problems within this mathematical formulation can be challenging. The way in which we formulate an optimization problem can make the solution process either easy or hard.[17] In some cases, a high fidelity representation of a problem will permit only approximate solutions. These approximate solutions may be acceptable for our particular application. Alternatively, we may want to pursue a lower fidelity representation that permits exact solutions. We will focus on the algorithmic aspects of optimization that arise after the problem has been properly formulated.[18]

Since this book discusses a wide variety of different optimization algorithms, one may wonder which algorithm is best. There are many ways to measure the performance of an optimization algorithm, ranging from theoretical asymptotic

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6ac0e3eabe49c01370b7c203a0abf711b4234839663426d631d9576dcb8b31a7.jpg)  
Figure 1.4. The feasible set  $\mathcal{X}$  associated with equation (1.2).

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6d0f68d2095f6e7cb4add9f51f5aefab62ab3033da04ea3882e3e24b9fa839c2.jpg)  
Figure 1.5. The problem in equation (1.3) has no solution because the constraint boundary is not feasible.

See discussion in S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.

18 Many texts provide examples of how to translate real-world optimization problems into optimization problems. See, for example, the following: R.K. Arora, Optimization: Algorithms and Applications. Chapman and Hall/CRC, 2015. A.D. Belegundu and T.R. Chandrupatla, Optimization Concepts and Applications in Engineering, 2nd ed. Cambridge University Press, 2011. A. Keane and P. Nair, Computational Approaches for Aerospace Design. Wiley, 2005. P.Y. Papalambros and D.J. Wilde, Principles of Optimal Design. Cambridge University Press, 2017.

complexity to empirical wall-clock time spent executing on a particular computer architecture. As elaborated by the no free lunch theorems, there is no reason to prefer one algorithm over another unless we make assumptions about the distribution over the space of possible objective functions. Although one algorithm may perform better than another algorithm on one distribution of problems, it will perform worse on another distribution of problems.[19] For many optimization algorithms to work effectively, there needs to be some regularity in the objective function, such as Lipschitz continuity or convexity, both topics that we will cover later. As we discuss different algorithms, we will outline their assumptions, the motivation for their mechanism, and their advantages and disadvantages.

# 1.4 Applications

The optimization algorithms discussed in this book have been applied to a wide variety of problems spanning many disciplines. This section discusses a few conceptual examples with real-world applications.

# 1.4.1 Aircraft Design

Aircraft design is a complex optimization problem that involves many disciplines, ranging from aerodynamics to structural mechanics. We can parameterize the design of the various components of an aircraft, such as the airfoil, with a set of design variables. For example, an airfoil can be parameterized by the thickness at various points along the chord length. Figure 1.6 shows a parameterization involving seven design variables. We want to search the space of design variables to find the values that minimize the drag of the airfoil while maintaining lift. In addition, we may have constraints such as the airfoil must have a certain minimum thickness so that it is structurally sound. To solve this optimization problem, we can use an optimization algorithm that searches the seven-dimensional design space to find an optimal airfoil shape that adheres to our constraints.

# 1.4.2 Deep Learning

Deep learning has been used to solve a wide variety of problems, such as image recognition, natural language processing, and game playing. In these applications, deep learning involves training a deep neural network to optimize an objective

19 The assumptions and results of the no free lunch theorems are provided by D.H. Wolpert and W.G. Macready, "No Free Lunch Theorems for Optimization," IEEE Transactions on Evolutionary Computation, vol. 1, no. 1, pp. 67-82, 1997.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b607ac5bf8063c25d18831d2197ea5b7954f6934808830ff9027db4323de7366.jpg)  
Figure 1.6. A symmetric airfoil parameterized by the design variables  $x_{1}, \ldots, x_{7}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2c77c04005a281aeb4505d4569cfd6065327d7263eca9964468d128f7997b40b.jpg)  
Figure 1.7. A deep neural network with two hidden layers. The input layer is shown in green, the hidden layers in blue, and the output layer in red. The parameters of the network are the weights on the connections between neurons and the associated biases added before the nonlinear activation function. In a home price estimation task, the inputs might correspond to the number of bedrooms, square footage, and year it was built, and the output would be the estimated price. The objective function would measure how well the network predicts the price of homes in the data set. A review of deep learning is provided by I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.

function that measures how well it does on a task, such as predicting the sale price of a home given a number of attributes of that home. A neural network is simply a collection of neurons that are connected to each other. Each neuron combines a set of inputs using a linear function using a set of weights and a bias and then applies a nonlinear activation function to the result. These neurons are typically organized in layers, with each layer of neurons producing outputs that are fed to the next layer of neurons. When there are many layers, the network is called deep. The weights and biases of the network are the design variables that are optimized to perform well on the training set. Most neural network representations used in practice lend themselves well to computing the gradient of the objective function with respect to the weights and biases. Gradient information is often used by neural network training algorithms to help guide the optimization process.

# 1.4.3 Statistical Modeling

An important area of statistics involves inference of probabilistic models from observed data. One common approach is to define a parameterized probability distribution and then optimize the parameters of the distribution to best fit the data. The Gaussian mixture model is a type of parametric model defined by a weighted sum of Gaussian distributions, where each component has its own mean and standard deviation. Figure 1.8 shows a Gaussian mixture model with two components. The parameters of the model are optimized to maximize the likelihood of the observed data subject to the constraints that the weights are non-negative and sum to one and that the standard deviations are positive. Although the optimal parameters of a single Gaussian distribution have a closed-form solution, finding the optimal parameters of a Gaussian mixture model generally requires an iterative optimization algorithm.

# 1.4.4 Financial Portfolio Construction

A major area of finance is portfolio construction. We have a budget and need to decide how to allocate our funds, such as to stocks, bonds, and other assets. The design variables correspond to what fraction of our budget we allocate to each asset, and they are constrained to be nonnegative and sum to one. The objective function might be to maximize the expected return of the portfolio while minimizing the risk. The risk of a portfolio is often measured by the variance of

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/11bf79abe19c3eb5128682dced3e5a817c08db67af4801fa237c30a8b6fd89a9.jpg)  
Figure 1.8. A Gaussian mixture model with two components. The red points correspond to observed data. The blue curve represents the probability density function  $p(x)$  of the mixture model. Optimizing the parameters of this density function is discussed by K.P. Murphy, Probabilistic Machine Learning: An Introduction. MIT Press, 2022.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/66c7546f3bf2b841c5b5c4232ec28bdd34bbdb58b078dd2c26ed89802291b4e8.jpg)  
Figure 1.9. A portfolio optimization problem with three stocks (green, purple, and red). The horizontal axis corresponds to how much we weight expected return versus risk, or the variance of the portfolio return. The vertical axis indicates what fraction of the budget is allocated to each stock in an optimal portfolio for a given risk-return weight. This kind of portfolio optimization problem is discussed in more detail in example 20.6.

the return. We are uncertain about the return of each asset, but we can estimate the mean and covariance of the returns from past data. We can compute a scalar objective function that weights together the expected return and the risk of the portfolio, and then use an algorithm to find the optimal allocation of funds.

# 1.4.5 Medical Radiotherapy

Medical radiotherapy involves treating cancer patients with radiation to kill cancer cells while minimizing damage to healthy tissue. The radiation from a particle accelerator is often delivered in the form of an external ionizing radiation beam, which passes through both the tumor and the surrounding healthy tissue as shown in figure 1.10. These beams can be positioned and shaped, and their intensity can be modulated. There are many ways to formulate radiotherapy planning as an optimization problem. One way is to have the design variables correspond to the positions, shapes, and intensities of the beams. The objective function to be minimized is the amount of radiation that is delivered to the healthy tissue through the treatment plan. The dosage of radiation to the tumor may be constrained to be above some threshold. Some formulations of this problem are convex, which means that the optimization problem can be solved exactly and efficiently.

# 1.4.6 Robotic Planning

Robotic planning involves finding a sequence of actions that a robot must take to achieve a goal. Figure 1.11 shows a simple example of a robot planning problem where the robot must navigate from a start position to a goal position while avoiding obstacles in a grid. The robot may go up, down, left, or right. The objective is to minimize the number of moves the robot makes to reach the goal. In contrast with the previous examples in this section, the design space is discrete. The robot can only move in one of four directions, and the design variables are the sequence of moves that the robot makes. Dynamic programming is a common optimization technique used to solve this type of problem efficiently.

# 1.4.7 Industrial Job Shop Scheduling

A classic problem in industrial operations research is job shop scheduling, where we need to allocate resources to jobs to minimize the total time it takes to process

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6080e74ac5031370628e41b6c6d0e7a7b7eee468a45807adb0ffe019c43e7d63.jpg)  
Figure 1.10. A medical radiotherapy problem. Shown is a cross section of the patient. The green blobs represent healthy organs, and the red blob represents a tumor. The blue wedge is a radiation beam. An optimization approach to treatment planning is presented by A. Fu, B. Ungun, L. Xing, and S. Boyd, "A Convex Optimization Approach to Radiation Treatment Planning with Dose Constraints," Optimization and Engineering, vol. 20, pp. 277-300, 2019.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/99807b7e0f7b22cf48c6fc4f5dfc0b083b8e7cabdc9a52851a25022e8d28c0f4.jpg)  
Figure 1.11. A robot planning problem. The robot starts at the black square square and must reach the green square with the minimum number of steps. The robot can move up, down, left, or right, but it cannot move diagonally. The design variables are the sequence of moves that the robot makes. The red squares are obstacles that the robot must avoid. Section 22.5 discusses how dynamic programming can be used to solve this type of problem.

all jobs. We have a set of jobs and a set of machines, and each job must be processed on different machines in a specific order. The time required for each job on each machine is known ahead of time, and we want to find the order in which to process the jobs on the machines to minimize the total time to process all jobs. Figure 1.12 shows an example of a job shop schedule with four machines and three jobs. There is no known polynomial-time algorithm to solve this problem exactly, but there are many heuristics and optimization algorithms that can be used to find good solutions.

# 1.5 Minima

When minimizing  $f$ , we wish to find a global minimizer, a value of  $x$  for which  $f(x)$  is minimized. A function may have at most one global minimum, but it may have multiple global minimizers. Unfortunately, it is generally difficult to prove that a given candidate point is at a global minimum. Often, the best we can do is check whether it is at a local minimum. When  $f$  is a univariate function, a point  $x^*$  is at a local minimum (or is a local minimizer) if there exists a  $\delta > 0$  such that  $f(x^*) \leq f(x)$  for all  $x$  with  $|x - x^*| < \delta$ . When  $f$  is a multivariate function, this definition generalizes to there being a  $\delta > 0$  such that  $f(\mathbf{x}^*) \leq f(\mathbf{x})$  whenever  $\| \mathbf{x} - \mathbf{x}^* \| < \delta$ .

Figure 1.13 shows two types of local minima: strong local minima and weak local minima. A strong local minimizer, also known as a strict local minimizer, is a point that uniquely minimizes  $f$  within a neighborhood. In other words,  $x^{*}$  is a strict local minimizer if there exists a  $\delta > 0$  such that  $f(x^{*}) < f(x)$  whenever  $x^{*} \neq x$  and  $|x - x^{*}| < \delta$ . In the multivariate context, this generalizes to there being a  $\delta > 0$  such that  $f(\mathbf{x}^{*}) < f(\mathbf{x})$  whenever  $\mathbf{x}^{*} \neq \mathbf{x}$  and  $\| \mathbf{x} - \mathbf{x}^{*} \| < \delta$ . A weak local minimizer is a local minimizer that is not a strong local minimizer.

# 1.6 Optimality Conditions

In cases where the objective function is twice differentiable (at least at the minimizer),[20] we can establish conditions for optimality. Our discussion in this section assumes that the problem is unconstrained. Conditions for optimality in constrained problems are introduced in chapter 10. We will first discuss the conditions

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ecf2cf225d56a8ebdc096eda24abec4a3a86d6386255902920ec0e401cba43eb.jpg)  
Figure 1.12. An industrial job shop scheduling problem. There are four machines,  $m_{1},\ldots ,m_{4}$  and there are three jobs indicated by red, blue, and green. The objective is to minimize the total time it takes to process all jobs. There are many variations of this problem as reviewed by H. Xiong, S. Shi, D. Ren, and J. Hu, "A Survey of Job Shop Scheduling Problem: The Types and Models," Computers and Operations Research, vol. 142, p. 105731, 2022.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/71130c5b6da2a14b07b8eb30e6d4441c8ea355a64458495e9c70f6092441bafe.jpg)  
Figure 1.13. Examples of different types of minima.

20 Differentiation is reviewed in the next chapter.

for local minima in univariate functions, and then extend these conditions to multivariate functions, where there are multiple design variables.

# 1.6.1 Univariate Conditions

A design point is guaranteed to be at a strong local minimum if the local derivative is zero and the second derivative is positive:

1.  $f^{\prime}(x^{*}) = 0$  
2.  $f^{\prime \prime}(x^{*}) > 0$

A zero derivative ensures that shifting the point by small values does not significantly affect the function value. A positive second derivative ensures that the zero first derivative occurs at the bottom of a bowl.[21]

A point can also be at a local minimum if it has a zero derivative and the second derivative is merely nonnegative:

1.  $f'(x^*) = 0$ , the first-order necessary condition<sup>22</sup>  
2.  $f''(x^*) \geq 0$ , the second-order necessary condition

These conditions are referred to as necessary because all local minima obey these two rules. Unfortunately, not all points with a zero derivative and a zero second derivative are local minima, as demonstrated in figure 1.14.

The first-order necessary condition can be derived using the Taylor expansion $^{23}$  about our candidate point  $x^{*}$ :

$$
f \left(x^{*} + h\right) = f \left(x^{*}\right) + h f^{\prime} \left(x^{*}\right) + O \left(h^{2}\right) \tag {1.4}
$$

$$
f \left(x^{*} - h\right) = f \left(x^{*}\right) - h f^{\prime} \left(x^{*}\right) + O \left(h^{2}\right) \tag {1.5}
$$

$$
f \left(x^{*} + h\right) \geq f \left(x^{*}\right) \Longrightarrow h f^{\prime} \left(x^{*}\right) \geq 0 \tag {1.6}
$$

$$
\begin{array}{l} f \left(x^{*} - h\right) \geq f \left(x^{*}\right) \Longrightarrow h f^{\prime} \left(x^{*}\right) \leq 0 (1.7) \\ \Longrightarrow f^{\prime} \left(x^{*}\right) = 0 (1.8) \\ \end{array}
$$

where the asymptotic notation  $O(h^2)$  is reviewed in appendix C.

The second-order necessary condition can also be obtained from the Taylor expansion:

$$
f \left(x^{*} + h\right) = f \left(x^{*}\right) + \underbrace{h f^{\prime} \left(x^{*}\right)}_{= 0} + \frac{h^{2}}{2} f^{\prime \prime} \left(x^{*}\right) + O \left(h^{3}\right) \tag {1.9}
$$

21 If  $f'(x) = 0$  and  $f''(x) < 0$ , then  $x$  is a local maximum.  
22 A point that satisfies the first-order necessary condition is sometimes called a stationary point.  
23 The Taylor expansion is derived in appendix C.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a39f75c40fd5aa8bdf758a9d31234d8715dab8443353d8ed62d702b58b99b28a.jpg)  
second-order but not first-order

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b712bb95396e7940ffd748d303a0913e1657046e430c9448b5bfbb618311df97.jpg)  
first-order and second-order

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/32c9eec31ae44d1b19e648f2c79e763cf08848078a8411d90facef46763c5e85.jpg)  
first-order and second-order

We know that the first-order necessary condition must apply:

$$
f \left(x^{*} + h\right) \geq f \left(x^{*}\right) \Longrightarrow \frac{h^{2}}{2} f^{\prime \prime} \left(x^{*}\right) \geq 0 \tag {1.10}
$$

since  $h > 0$ . It follows that  $f''(x^*) \geq 0$  must hold for  $x^*$  to be at a local minimum.

# 1.6.2 Multivariate Conditions

We can generalize the necessary conditions for local minima to multivariate functions as follows:

1.  $\nabla f(\mathbf{x}) = \mathbf{0}$ , the first-order necessary condition  
2.  $\nabla^2 f(\mathbf{x})$  is positive semidefinite (for a review of this definition, see appendix C.6), the second-order necessary condition

The first-order necessary condition tells us that the function is not changing at  $x$ . Figure 1.15 shows examples of multivariate functions where the first-order necessary condition is satisfied. The second-order necessary condition tells us that  $x$  is in a bowl. Example 1.1 provides an example of a way to visualize such a function.

These conditions can be obtained from a simple analysis. In order for  $\mathbf{x}^*$  to be at a local minimum, it must be smaller than those values around it:

$$
f \left(\mathbf {x}^{*}\right) \leq f \left(\mathbf {x}^{*} + h \mathbf {y}\right) \quad \Leftrightarrow \quad f \left(\mathbf {x}^{*} + h \mathbf {y}\right) - f \left(\mathbf {x}^{*}\right) \geq 0 \tag {1.11}
$$

If we write the second-order approximation for  $f(\mathbf{x}^{*})$ , we get:

$$
f \left(\mathbf {x}^{*} + h \mathbf {y}\right) = f \left(\mathbf {x}^{*}\right) + h \nabla f \left(\mathbf {x}^{*}\right) ^{\top} \mathbf {y} + \frac{1}{2} h^{2} \mathbf {y}^{\top} \nabla^{2} f \left(\mathbf {x}^{*}\right) \mathbf {y} + O \left(h^{3}\right) \tag {1.12}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/15604a284d51371121b4ceda15f6d94831367a24e5bfeaa722ba0ea5e4262629.jpg)  
Figure 1.14. Examples of the necessary but insufficient conditions for strong local minima.

A hill. The gradient at the center is zero, but the Hessian is negative definite.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d5bb03c0bb99632bd7830cacbd59da8c8b309b4d9e5e98608bf93d9034327ac3.jpg)  
Figure 1.15. The three local regions where the gradient is zero.

A saddle. The gradient at the center is zero, but it is not a local minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c5b97bae427b25412794ee21a7da8e5d47ed64b25c19970b765d0dfc1db39bca.jpg)

A bowl. The gradient at the center is zero and the Hessian is positive definite. It is a local minimum.

Most of the examples in this book include one or two design variables to facilitate plotting, but many real-world problems have more. For two-dimensional problems, a function of the form  $f(x_{1},x_{2}) = y$  can be rendered in three-dimensional space. A contour plot is a visual representation of a three-dimensional surface obtained by plotting regions with constant  $y = f(\mathbf{x})$  values, known as contours, on a two-dimensional plot with axes indexed by  $x_{1}$  and  $x_{2}$ . The plots below show  $f(x_{1},x_{2}) = x_{1}^{2} - x_{2}^{2}$  as both a three-dimensional surface and a contour plot.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4218878437af9601fdd46b1b3008cea34380b79c2771189d3de1c744b3d2b476.jpg)  
Example 1.1. An example three-dimensional visualization and the associated contour plot.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4e728030bc61cab3487038c60acc163b426dcc66905b85676fcba1cf77a9dcf1.jpg)

We know that at a minimum, the first derivative must be zero, and we neglect the higher order terms. Rearranging, we get:

$$
\frac{1}{2} h^{2} \mathbf {y}^{\top} \nabla^{2} f \left(\mathbf {x}^{*}\right) \mathbf {y} = f \left(\mathbf {x}^{*} + h \mathbf {y}\right) - f \left(\mathbf {x}^{*}\right) \geq 0 \tag {1.13}
$$

This is the definition of a positive semidefinite matrix, and we recover the second-order necessary condition. Example 1.2 illustrates how these conditions can be applied to the Rosenbrock banana function.

Consider the Rosenbrock banana function,

$$
f (\mathbf {x}) = (1 - x_{1}) ^{2} + 5 (x_{2} - x_{1}^{2}) ^{2}
$$

Does the point  $[1, 1]$  satisfy the first-order and second-order necessary conditions?

The gradient is:

$$
\nabla f (\mathbf {x}) = \left[ \begin{array}{c} \frac{\partial f}{\partial x_{1}} \\ \frac{\partial f}{\partial x_{2}} \end{array} \right] = \left[ \begin{array}{c} 2 \big (10 x_{1}^{3} - 10 x_{1} x_{2} + x_{1} - 1 \big) \\ 10 (x_{2} - x_{1}^{2}) \end{array} \right]
$$

and the Hessian is:

$$
\nabla^{2} f (\mathbf {x}) = \left[ \begin{array}{c c} \frac{\partial^{2} f}{\partial x_{1} \partial x_{1}} & \frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} \\ \frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} & \frac{\partial^{2} f}{\partial x_{2} \partial x_{2}} \end{array} \right] = \left[ \begin{array}{c c} - 20 (x_{2} - x_{1}^{2}) + 40 x_{1}^{2} + 2 & - 20 x_{1} \\ - 20 x_{1} & 10 \end{array} \right]
$$

We compute  $\nabla f(1,1) = 0$ , so the first-order necessary condition is satisfied.

The Hessian at  $[1,1]$  is:

$$
\left[ \begin{array}{c c} 42 & - 20 \\ - 20 & 10 \end{array} \right]
$$

which is positive definite, so the second-order necessary condition is also satisfied.

While necessary for optimality, these conditions are not sufficient for optimality. For unconstrained optimization of a twice-differentiable function, a point is guaranteed to be at a strong local minimum if the first-order necessary condition is satisfied and  $\nabla^2 f(\mathbf{x})$  is positive definite. These conditions are collectively known as the second-order sufficient condition.

Example 1.2. Checking the first- and second-order necessary conditions of a point on the Rosenbrock function. The minimizer is indicated by the dot in the figure below.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cef500f70be638fe6197bc5923825f5fff7d69a238790ce47e079cc0360f3b91.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fba9489421da08c6e2c04d013b82d9b495e245c3426224cd174fcb81ea6a291f.jpg)

# 1.7 Overview

This section provides a brief overview of the chapters of this book. The conceptual dependencies between the chapters are outlined in figure 1.16.

Chapter 2 begins by discussing derivatives and their generalization to multiple dimensions. Derivatives are used in many algorithms to inform the choice of direction of the search for an optimum. Often derivatives are not known analytically, and so we discuss how to estimate them numerically and using automatic differentiation techniques.

Chapter 3 discusses bracketing, which involves identifying an interval in which a local minimum lies for a univariate function. Different bracketing algorithms use different schemes for successively shrinking the interval based on function evaluations. We can use knowledge about the function, such as its Lipschitz constant, to guide the bracketing process. These bracketing algorithms are often used as subroutines within the optimization algorithms discussed later in the text.

Chapter 4 introduces local descent as a general approach to optimizing multivariate functions. Local descent involves iteratively choosing a descent direction and then taking a step in that direction and repeating that process until convergence

or some termination condition is met. There are different schemes for choosing the step length. We will also discuss methods that adaptively restrict the step size to a region where there is confidence in the local model.

Chapter 5 builds upon the previous chapter, explaining how to use first-order information obtained through the gradient estimate as a local model to inform the descent direction. Simply stepping in the direction of steepest descent is often not the best strategy for finding a minimum. This chapter discusses a wide variety of different methods for using the past sequence of gradient estimates to better inform the search.

Chapter 6 shows how to use local models based on second-order approximations to inform local descent. These models are based on estimates of the Hessian of the objective function. The advantage of second-order approximations is that it can inform both the direction and step size.

Chapter 7 presents a collection of direct methods for finding optima that avoid using gradient information for informing the direction of search. We begin by discussing methods that iteratively perform line search along a set of directions. We then discuss pattern search methods that do not perform line search but rather perform evaluations some step size away from the current point along a set of directions. The step size is incrementally adapted as the search proceeds. Another method uses a simplex that adaptively expands and contracts as it traverses the design space in the apparent direction of improvement. Finally, we discuss a method motivated by Lipschitz continuity to increase resolution in areas deemed likely to contain the global minimum.

Chapter 8 introduces stochastic methods, where randomness is incorporated into the optimization process. We show how stochasticity can improve some of the algorithms discussed in earlier chapters, such as steepest descent and pattern search. Some of the methods involve incrementally traversing the search space, but others involve learning a probability distribution over the design space, assigning greater weight to regions that are more likely to contain an optimum.

Chapter 9 discusses population methods, where a collection of points is used to explore the design space. Having a large number of points distributed through the space can help reduce the risk of becoming stuck in a local minimum. Population methods generally rely upon stochasticity to encourage diversity in the population, and they can be combined with local descent methods.

Chapter 10 introduces the notion of constraints in optimization problems. In some cases, we can transform a problem with constraints into an unconstrained

optimization problem. In other cases, we might want to use the concept of Lagrange multipliers to define mathematical conditions for optimality. For general optimization problems, we can use penalty methods, which incorporate constraints into the optimization objective to penalize infeasibility. By moving the constraints into the objective, we can use the various unconstrained optimization methods discussed earlier. We also discuss methods for ensuring that, if we start with a feasible point, the search will remain feasible.

Chapter 11 discusses the concept of duality in optimization problems. Duality provides a way to transform a constrained optimization problem into another optimization problem called the dual problem that can be easier to solve than the original or primal problem and provides a lowerbound on the primal solution. For some classes of problems, the dual solution is the same as the primal solution. We discuss optimization algorithms that alternate between the primal and dual problems to find the optimal solution.

Chapter 12 makes the assumption that both the objective function and constraints are linear. Although linearity may appear to be a strong assumption, many engineering problems can be framed as linear constrained optimization problems. Several methods have been developed for exploiting this linear structure. This chapter focuses on the simplex algorithm, which is guaranteed to result in a global minimum.

Chapter 13 discusses the special case where the objective function is quadratic and the constraints are linear. We show how convex quadratic optimization problems can be transformed into what is known as a least squares problem. Unconstrained least squares problems can be solved exactly using the pseudoinverse. General least squares problems can be transformed into least squares problems with linear inequalities, which can then be solved by transforming them into nonnegative least squares problems.

Chapter 14 discusses the concept of disciplined convex programming. Convex problems are those with a convex objective function and constraints that define a convex feasible set. Convex optimization problems have a single global minimum, and many methods have been developed to efficiently compute a solution. We introduce the concept of a disciplined convex program, which is a convex optimization problem that can be written in such a way that automated methods can both verify that the problem is convex and automatically transcribe the problem into a canonical form.

Chapter 15 shows how to address the problem of multiobjective optimization, where we have multiple objectives that we are trying to optimize simultaneously. Engineering often involves a tradeoff between multiple objectives, and it is often unclear how to prioritize different objectives. We discuss how to transform multi-objective problems into scalar-valued objective functions so that we can use the algorithms discussed in earlier chapters. We also discuss algorithms for finding the set of design points that represent the best tradeoff between objectives.

Chapter 16 discusses how to create sampling plans consisting of points that cover the design space. Random sampling of the design space often does not provide adequate coverage. We discuss methods for ensuring uniform coverage along each design dimension and methods for measuring and optimizing the coverage of the space. In addition, we discuss quasi-random sequences that can also be used to generate sampling plans.

Chapter 17 explains how to build surrogate models of the objective function. Surrogate models are often used for problems where evaluating the objective function is very expensive. An optimization algorithm can then use evaluations of the surrogate model instead of the actual objective function to improve the design. The evaluations can come from historical data, perhaps obtained through the use of a sampling plan introduced in the previous chapter. We discuss different types of surrogate models, how to fit them to data, and how to identify a suitable surrogate model.

Chapter 18 introduces probabilistic surrogate models that allow us to quantify our confidence in the predictions of the models. This chapter focuses on a particular type of surrogate model called a Gaussian process. We show how to use Gaussian processes for prediction, how to incorporate gradient measurements and noise, and how to estimate some of the parameters governing the Gaussian process from data.

Chapter 19 shows how to use the probabilistic models from the previous chapter to guide surrogate optimization. The chapter outlines several techniques for choosing which design point to evaluate next. We also discuss how surrogate models can be used to optimize an objective measure in a safe manner.

Chapter 20 explains how to perform optimization under uncertainty, relaxing the assumption made in previous chapters that the objective function is a deterministic function of the design variables. We discuss different approaches for representing uncertainty, including set-based and probabilistic approaches, and explain how to transform the problem to provide robustness to uncertainty.

Chapter 21 outlines approaches to uncertainty propagation, where known input distributions are used to estimate statistical quantities associated with the output distribution. Understanding the output distribution of an objective function is important to optimization under uncertainty. We discuss a variety of approaches, some based on mathematical concepts such as Monte Carlo, the Taylor series approximation, orthogonal polynomials, and Gaussian processes. They differ in the assumptions they make and the quality of their estimates.

Chapter 22 shows how to approach problems where the design variables are constrained to be discrete. A common approach is to relax the assumption that the variables are discrete, but this can result in infeasible designs. Another approach involves incrementally adding linear constraints until the optimal point is discrete. We also discuss branch and bound along with dynamic programming approaches, both of which guarantee optimality. The chapter also mentions a population-based method that often scales to large design spaces but does not provide guarantees.

Chapter 23 discusses how to search design spaces consisting of expressions defined by a grammar. For many problems, the number of variables is unknown, such as in the optimization of graphical structures or computer programs. We outline several algorithms that account for the grammatical structure of the design space to make the search more efficient.

Chapter 24 explains how to approach multidisciplinary design optimization. Many engineering problems involve complicated interactions between several disciplines, and optimizing disciplines individually may not lead to an optimal solution. This chapter discusses a variety of techniques for taking advantage of the structure of multidisciplinary problems to reduce the effort required for finding good designs.

The appendices contain supplementary material. Appendix A begins with a short introduction to the Julia programming language, focusing on the concepts used to specify the algorithms listed in this book. Appendix B specifies a variety of test functions used for evaluating the performance of different algorithms. Appendix C covers mathematical concepts used in the derivation and analysis of the optimization methods discussed in this text.

# 1.8 Summary

- Optimization in engineering is the process of finding the best system design subject to a set of constraints.

- Algorithms for optimization can be applied to a wide variety of fields, ranging from aircraft design to deep learning.  
- Minima occur where the gradient is zero, but zero-gradient does not imply optimality.

# 1.9 Exercises

Exercise 1.1. Give an example of a function with a local minimum that is not a global minimum.

Solution:  $f(x) = x^{3} / 3 - x$  at  $x = 1$ .

Exercise 1.2. What is the minimum and minimizer of the function  $f(x) = x^3 - x$ ?

Solution: It does not have a minimum. The function is said to be unbounded below.

Exercise 1.3. Give an example of a function that is bounded below but has no global optima when the feasible set is the interval  $[0,1]$ .

Solution: Here is one example:

$$
f (x) = \left\{ \begin{array}{l l} | x | & \text{if } x \neq 0 \\ 1 & \text{otherwise} \end{array} \right.
$$

Exercise 1.4. What are the minima and minimizers of the function  $f(x) = \sin (x)$ ?

Solution: The minimum value is  $-1$ , which is obtained by the minimizers  $x = \frac{3}{2}\pi + 2\pi i$  for all integers  $i$ .

Exercise 1.5. Does the first-order condition  $f'(x) = 0$  hold when  $x$  is the optimal solution of a constrained problem?

Solution: No. Consider minimizing  $f(x) = x$ , subject to  $x \geq 1$ .

Exercise 1.6. How many minima does  $f(x, y) = x^2 + y$ , subject to  $x > y \geq 1$ , have?

Solution: The function  $f$  can be broken into two separate functions that depend only on their specific coordinate:

$$
f (x, y) = g (x) + h (y)
$$

where  $g(x) = x^2$  and  $h(y) = y$ . Both  $g$  and  $h$  strictly increase for  $x, y \geq 1$ . While  $h$  is minimized for  $y = 1$ , we can merely approach  $x \to 1$  due to the strict inequality  $x > y$ . Thus,  $f$  has no minima.

Exercise 1.7. How many inflection points does  $f(x) = x^3 - 10$  have?

Solution: An inflection point is a point on a curve where the sign of the curvature changes. When  $f$  is continuously twice-differentiable, a necessary condition for  $x$  to be an inflection point on  $f$  is that the second derivative is zero. In this case,  $f$  is continuously twice-differentiable. The second derivative is  $f''(x) = 6x$ , which is only zero at  $x = 0$ . A sufficient condition for  $x$  to be an inflection point is that the second derivative changes sign around  $x$ . That is,  $f''(x + \epsilon)$  and  $f''(x - \epsilon)$  for  $\epsilon \ll 1$  have opposite signs. This holds for  $x = 0$ , so it is an inflection point. There is thus only one inflection point on  $x^3 - 10$ .

# 2 Derivatives and Gradients

Optimization is concerned with finding the design point that minimizes (or maximizes) an objective function. Knowing how the value of a function changes as its input is varied is useful because it tells us in which direction we can move to improve on previous points. The change in the value of the function is measured by the derivative in one dimension and the gradient in multiple dimensions. This chapter briefly reviews some essential elements from calculus. $^{1}$

# 2.1 Derivatives

The derivative  $f'(x)$  of a function  $f$  of a single variable  $x$  is the rate at which the value of  $f$  changes at  $x$ . It is often visualized, as shown in figure 2.1, using the tangent line to the graph of the function at  $x$ . The value of the derivative equals the slope of the tangent line.

We can use the derivative to provide a linear approximation of the function near  $x$ :

$$
f (x + \Delta x) \approx f (x) + f^{\prime} (x) \Delta x \tag {2.1}
$$

The derivative is the ratio between the change in  $f$  and the change in  $x$  at the point  $x$ :

$$
f^{\prime} (x) = \frac{\Delta f (x)}{\Delta x} \tag {2.2}
$$

which is the change in  $f(x)$  divided by the change in  $x$  as the step becomes infinitesimally small as illustrated by figure 2.2.

The notation  $f'(x)$  can be attributed to Lagrange. We also use the notation created by Leibniz,

$$
f^{\prime} (x) \equiv \frac{d f (x)}{d x} \tag {2.3}
$$

For a more comprehensive review, see S.J. Colley, Vector Calculus, 4th ed. Pearson, 2011.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5c0d470c2e86a1db6887c76ff34b1746dba718da5f5a26fe21a41961e418da99.jpg)  
Figure 2.1. The function  $f$  is drawn in black and the tangent line to  $f(x)$  is drawn in blue. The derivative of  $f$  at  $x$  is the slope of the tangent line.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/14986eaf993e213c3d74b91687c33c7e46030bc21da080b0be67ad547a97eefd.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d13bb287ecb7d763572d51f31bad9bdcc6360a8861487d7abe2f694c1615b8fd.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c7832c448e54a41cbee69c72207457395a9521f54837d6e95635d554fedcc014.jpg)  
Figure 2.2. The tangent line is obtained by joining points with sufficiently small step differences.

which emphasizes the fact that the derivative is the ratio of the change in  $f$  to the change in  $x$  at the point  $x$ .

The limit equation defining the derivative can be presented in three different ways: the forward difference, the central difference, and the backward difference. Each method uses an infinitely small step size  $h$ :

$$
f^{\prime} (x) \equiv \underbrace{\lim_{h \rightarrow 0} \frac{f (x + h) - f (x)}{h}}_{\text{forwarddifference}} = \underbrace{\lim_{h \rightarrow 0} \frac{f (x + h / 2) - f (x - h / 2)}{h}}_{\text{centraldifference}} = \underbrace{\lim_{h \rightarrow 0} \frac{f (x) - f (x - h)}{h}}_{\text{backwarddifference}} \tag {2.4}
$$

If  $f$  can be represented symbolically, symbolic differentiation can often provide an exact analytic expression for  $f'$  by applying derivative rules from calculus. The analytic expression can then be evaluated at any point  $x$ . The process is illustrated in example 2.1.

The implementation details of symbolic differentiation is outside the scope of this text. Various software packages, such as SymEngine.jl in Julia and SymPy in Python, provide implementations. Here we use SymEngine.jl to compute the derivative of  $x^{2} + x / 2 - \sin (x) / x$ .

```julia
julia> using SymEngine  
julia> @vars x; # define x as a symbolic variable  
julia> f = x^2 + x/2 - sin(x)/x;  
julia> diff(f, x)  
1/2 + 2*x + sin(x)/x^2 - cos(x)/x
```

Example 2.1. Symbolic differentiation provides analytical derivatives.

# 2.2 Derivatives in Multiple Dimensions

The gradient is the generalization of the derivative to multivariate functions. It captures the local slope of the function, allowing us to predict the effect of taking a small step from a point in any direction. Recall that the derivative is the slope of the tangent line. The gradient points in the direction of steepest ascent of the tangent hyperplane as shown in figure 2.3. A hyperplane in an  $n$ -dimensional space is the set of points that satisfies

$$
w_{1} x_{1} + \dots + w_{n} x_{n} = b \tag {2.5}
$$

for some vector  $\mathbf{w}$  and scalar  $b$ . A hyperplane has  $n - 1$  dimensions.

The gradient of  $f$  at  $\mathbf{x}$  is written  $\nabla f(\mathbf{x})$  and is a vector. Each component of that vector is the partial derivative² of  $f$  with respect to that component:

$$
\nabla f (\mathbf {x}) = \left[ \frac{\partial f (\mathbf {x})}{\partial x_{1}}, \quad \frac{\partial f (\mathbf {x})}{\partial x_{2}}, \quad \dots , \quad \frac{\partial f (\mathbf {x})}{\partial x_{n}} \right] \tag {2.6}
$$

We use the convention that vectors written with commas are column vectors. For example, we have  $[a,b,c] = [abc]^\top$ . Example 2.2 shows how to compute the gradient of a function at a particular point.

Compute the gradient of  $f(\mathbf{x}) = x_{1}\sin (x_{2}) + 1$  at  $\mathbf{c} = [2,0]$ .

$$
f (\mathbf {x}) = x_{1} \sin (x_{2}) + 1
$$

$$
\nabla f (\mathbf {x}) = \left[ \frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}} \right] = [ \sin (x_{2}), x_{1} \cos (x_{2}) ]
$$

$$
\nabla f (\mathbf {c}) = [ 0, 2 ]
$$

The Hessian of a multivariate function is a matrix containing all of the second derivatives with respect to the input. The second derivatives capture information about the local curvature of the function.

$$
\nabla^{2} f (\mathbf {x}) = \left[ \begin{array}{c c c c} \frac{\partial^{2} f (\mathbf {x})}{\partial x_{1} \partial x_{1}} & \frac{\partial^{2} f (\mathbf {x})}{\partial x_{1} \partial x_{2}} & \dots & \frac{\partial^{2} f (\mathbf {x})}{\partial x_{1} \partial x_{n}} \\ & \vdots & & \\ \frac{\partial^{2} f (\mathbf {x})}{\partial x_{n} \partial x_{1}} & \frac{\partial^{2} f (\mathbf {x})}{\partial x_{n} \partial x_{2}} & \dots & \frac{\partial^{2} f (\mathbf {x})}{\partial x_{n} \partial x_{n}} \end{array} \right] \tag {2.7}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/21d4260e093d71c7de38511b9dcfacc0600564ec9570a2a1d1cc44f7f48db95d.jpg)  
Figure 2.3. Each component of the gradient defines a local tangent line. These tangent lines define the local tangent hyperplane. The gradient vector points in the direction of greatest increase.

2 The partial derivative of a function with respect to a variable is the derivative assuming all other input variables are held constant. It is denoted  $\partial f / \partial x$ .

Example 2.2. Computing the gradient at a particular point.

3 The Hessian is symmetric only if the second derivatives of  $f$  are all continuous in a neighborhood of the point at which it is being evaluated:

$$
\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} = \frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}
$$

The directional derivative  $\nabla_{\mathbf{s}}f(\mathbf{x})$  of a multivariate function  $f$  is the instantaneous rate of change of  $f(\mathbf{x})$  as  $\mathbf{x}$  is moved with velocity  $\mathbf{s}$ . The definition is closely related to the definition of a derivative of a univariate function:

4 Some texts require that s be a unit vector. See, for example, G.B. Thomas, Calculus and Analytic Geometry, 9th ed. Addison-Wesley, 1968.

$$
\nabla_{\mathbf {s}} f (\mathbf {x}) \equiv \underbrace{\lim_{h \rightarrow 0} \frac{f (\mathbf {x} + h \mathbf {s}) - f (\mathbf {x})}{h}}_{\text{forwarddifference}} = \underbrace{\lim_{h \rightarrow 0} \frac{f (\mathbf {x} + h \mathbf {s} / 2) - f (\mathbf {x} - h \mathbf {s} / 2)}{h}}_{\text{centraldifference}} = \underbrace{\lim_{h \rightarrow 0} \frac{f (\mathbf {x}) - f (\mathbf {x} - h \mathbf {s})}{h}}_{\text{backwarddifference}} \tag {2.8}
$$

The directional derivative can be computed using the gradient of the function:

$$
\nabla_{\mathbf {s}} f (\mathbf {x}) = \nabla f (\mathbf {x}) ^{\top} \mathbf {s} \tag {2.9}
$$

Another way to compute the directional derivative  $\nabla_{\mathbf{s}}f(\mathbf{x})$  is to define  $g(\alpha)\equiv f(\mathbf{x} + \alpha \mathbf{s})$  and then compute  $g^{\prime}(0)$ , as illustrated in example 2.3.

The directional derivative is highest in the gradient direction, and it is lowest in the direction opposite the gradient. This directional dependence arises from the dot product in the directional derivative's definition and from the fact that the gradient is a local tangent hyperplane.

We wish to compute the directional derivative of  $f(\mathbf{x}) = x_{1}x_{2}$  at  $\mathbf{x} = [1,0]$  in the direction  $\mathbf{s} = [-1, - 1]$ :

$$
\nabla f (\mathbf {x}) = \left[ \begin{array}{c c} \frac{\partial f}{\partial x_{1}}, & \frac{\partial f}{\partial x_{2}} \end{array} \right] = [ x_{2}, x_{1} ]
$$

$$
\nabla_{\mathbf {s}} f (\mathbf {x}) = \nabla f (\mathbf {x}) ^{\top} \mathbf {s} = \left[ \begin{array}{l l} 0 & 1 \end{array} \right] \left[ \begin{array}{l} - 1 \\ - 1 \end{array} \right] = - 1
$$

We can also compute the directional derivative as follows:

$$
g (\alpha) = f (\mathbf {x} + \alpha \mathbf {s}) = (1 - \alpha) (- \alpha) = \alpha^{2} - \alpha
$$

$$
g^{\prime} (\alpha) = 2 \alpha - 1
$$

$$
g^{\prime} (0) = - 1
$$

Example 2.3. Computing a directional derivative.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/156fe85380706a4c7b0611f3c30315b8a8cbbaf0f790507e3cf7e71a7e4d8f54.jpg)

# 2.3 Numerical Differentiation

The process of estimating derivatives numerically is referred to as numerical differentiation. Estimates can be derived in different ways from function evaluations. This section discusses finite difference methods and the complex step method.<sup>5</sup>

For a more comprehensive treatment of the topics discussed in the remainder of this chapter, see A. Griewank and A. Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, 2nd ed. SIAM, 2008.

# 2.3.1 Finite Difference Methods

As the name implies, finite difference methods compute the difference between two values that differ by a finite step size. They approximate the derivative definitions in equation (2.4) using small differences:

$$
f^{\prime} (x) \approx \underbrace{\frac{f (x + h) - f (x)}{h}}_{\text{forwarddifference}} \approx \underbrace{\frac{f (x + h / 2) - f (x - h / 2)}{h}}_{\text{centraldifference}} \approx \underbrace{\frac{f (x) - f (x - h)}{h}}_{\text{backwarddifference}} \tag {2.10}
$$

Mathematically, the smaller the step size  $h$ , the better the derivative estimate. Practically, values for  $h$  that are too small can result in numerical cancellation errors. This effect is shown later in figure 2.4. Algorithm 2.1 provides implementations for these methods.

$$
\begin{array}{l} \text{diff_{forward} (f , x ; h = 1 e - 9) = (f (x + h) - f (x)) / h} \\ \text{diff_{central} (f , x ; h = 1 e - 9) = (f (x + h / 2) - f (x - h / 2)) / h} \\ \text{diff_{backward} (f , x ; h = 1 e - 9) = (f (x) - f (x - h)) / h} \end{array}
$$

The finite difference methods can be derived using the Taylor expansion. We will derive the forward difference derivative estimate, beginning with the Taylor expansion of  $f$  about  $x$ :

$$
f (x + h) = f (x) + \frac{f^{\prime} (x)}{1 !} h + \frac{f^{\prime \prime} (x)}{2 !} h^{2} + \frac{f^{\prime \prime \prime} (x)}{3 !} h^{3} + \dots \tag {2.11}
$$

We can rearrange and solve for the first derivative:

$$
f^{\prime} (x) h = f (x + h) - f (x) - \frac{f^{\prime \prime} (x)}{2 !} h^{2} - \frac{f^{\prime \prime \prime} (x)}{3 !} h^{3} - \dots \tag {2.12}
$$

$$
f^{\prime} (x) = \frac{f (x + h) - f (x)}{h} - \frac{f^{\prime \prime} (x)}{2 !} h - \frac{f^{\prime \prime \prime} (x)}{3 !} h^{2} - \dots \tag {2.13}
$$

$$
f^{\prime} (x) \approx \frac{f (x + h) - f (x)}{h} \tag {2.14}
$$

The forward difference approximates the true derivative for small  $h$  with error dependent on  $\frac{f''(x)}{2!} h + \frac{f'''(x)}{3!} h^2 + \dots$ . This error term is  $O(h)$ , meaning the forward difference has linear error as  $h$  approaches zero.

The central difference method has an error term of  $O(h^2)$ . We can derive this error term using the Taylor expansion. The Taylor expansions about  $x$  for

Algorithm 2.1. Finite difference methods for estimating the derivative of a function  $f$  at  $x$  with finite difference  $h$ . The default step sizes are the square root or cube root of the machine precision for floating point values. These step sizes balance machine round-off error with step size error.

Asymptotic notation is covered in appendix C.

7 J. H. Mathews and K. D. Fink, Numerical Methods Using MATLAB, 4th ed. Pearson, 2004.

$f(x + h / 2)$  and  $f(x - h / 2)$  are:

$$
f (x + h / 2) = f (x) + f^{\prime} (x) \frac{h}{2} + \frac{f^{\prime \prime} (x)}{2 !} \left(\frac{h}{2}\right) ^{2} + \frac{f^{\prime \prime \prime} (x)}{3 !} \left(\frac{h}{2}\right) ^{3} + \dots \tag {2.15}
$$

$$
f (x - h / 2) = f (x) - f^{\prime} (x) \frac{h}{2} + \frac{f^{\prime \prime} (x)}{2 !} \left(\frac{h}{2}\right) ^{2} - \frac{f^{\prime \prime \prime} (x)}{3 !} \left(\frac{h}{2}\right) ^{3} + \dots \tag {2.16}
$$

Subtracting these expansions produces:

$$
f (x + h / 2) - f (x - h / 2) \approx 2 f^{\prime} (x) \frac{h}{2} + \frac{2}{3 !} f^{\prime \prime \prime} (x) \left(\frac{h}{2}\right) ^{3} \tag {2.17}
$$

We rearrange to obtain:

$$
f^{\prime} (x) \approx \frac{f (x + h / 2) - f (x - h / 2)}{h} - \frac{f^{\prime \prime \prime} (x) h^{2}}{24} \tag {2.18}
$$

which shows that the approximation has quadratic error.

# 2.3.2 Complex Step Method

We often run into the problem of needing to choose a step size  $h$  small enough to provide a good approximation but not too small so as to lead to numerical subtractive cancellation issues. The complex step method bypasses the effect of subtractive cancellation by using a single function evaluation. We evaluate the function once after taking a step in the imaginary direction. $^{8}$

The Taylor expansion for an imaginary step is:

$$
f (x + i h) = f (x) + i h f^{\prime} (x) - h^{2} \frac{f^{\prime \prime} (x)}{2 !} - i h^{3} \frac{f^{\prime \prime \prime} (x)}{3 !} + \dots \tag {2.19}
$$

Taking only the imaginary component of each side produces a derivative approximation:

$$
\begin{array}{l} \operatorname{Im} \left(f (x + i h)\right) = h f^{\prime} (x) - h^{3} \frac{f^{\prime \prime \prime} (x)}{3 !} + \dots (2.20) \\ \Rightarrow f^{\prime} (x) = \frac{\operatorname{Im} (f (x + i h))}{h} + h^{2} \frac{f^{\prime \prime \prime} (x)}{3 !} - \dots (2.21) \\ = \frac{\operatorname{Im} (f (x + i h))}{h} + O \left(h^{2}\right) \text{as} h \rightarrow 0 (2.22) \\ \end{array}
$$

8 J. R. R. A. Martins, P. Sturdza, and J.J. Alonso, "The Complex-Step Derivative Approximation," ACM Transactions on Mathematical Software, vol. 29, no. 3, pp. 245-262, 2003. Special care must be taken to ensure that the implementation of  $f$  properly supports complex numbers as input.

An implementation is provided by algorithm 2.2. The real part approximates  $f(x)$  to within  $O(h^2)$  as  $h \to 0$ :

$$
\begin{array}{l} \operatorname{Re} (f (x + i h)) = f (x) - h^{2} \frac{f^{\prime \prime} (x)}{2 !} + \dots (2.23) \\ \Rightarrow f (x) = \operatorname{Re} \left(f (x + i h)\right) + h^{2} \frac{f^{\prime \prime} (x)}{2 !} - \dots (2.24) \\ \end{array}
$$

Thus, we can evaluate both  $f(x)$  and  $f'(x)$  using a single evaluation of  $f$  with complex arguments. Example 2.4 shows the calculations involved for estimating the derivative of a function at a particular point. Algorithm 2.2 implements the complex step method. Figure 2.4 compares the numerical error of the complex step method to the forward and central difference methods as the step size is varied.

```python
diff_complex(f, x; h=1e-9) = imag(f(x + h*im)) / h
```

Consider  $f(x) = \sin (x^{2})$ . The function value at  $x = \pi / 2$  is approximately 0.624266 and the derivative is  $\pi \cos (\pi^2 / 4) \approx -2.45425$ . We can arrive at this using the complex step method:

```txt
julia>  $f = x\rightarrow \sin (x\wedge 2)$    
julia>  $v = f(\pi /2 + 0.001im)$    
julia> real(v)#f(x)   
0.6242698144866649   
julia>imag(v)/0.001#f'(x)   
-2.4542516170381785
```

Algorithm 2.2. The complex step method for estimating the derivative of a function  $f$  at  $x$  with finite difference  $h$ .

Example 2.4. The complex step method for estimating derivatives.

# 2.4 Automatic Differentiation

This section introduces algorithms for the numeric evaluation of derivatives of functions specified by a computer program. Key to these automatic differentiation techniques is the application of the chain rule:

$$
\frac{d}{d x} f (g (x)) = \frac{d}{d x} (f \circ g) (x) = \frac{d f}{d g} \frac{d g}{d x} \tag {2.25}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ef325153579d3b9489d425607660c7d6d7852826685ac77aa3d73b957e33ccd6.jpg)  
Figure 2.4. A comparison of the error in derivative estimate for the function  $\sin (x)$  at  $x = 1 / 2$  as the step size is varied. The linear error of the forward difference method and the quadratic error of the central difference and complex methods can be seen by the constant slopes on the right hand side. The complex step method avoids the subtractive cancellation error that occurs when differencing two function evaluations that are close together.

A program is composed of elementary operations like addition, subtraction, multiplication, and division.

Consider the function  $f(a, b) = \ln (ab + \max(a, 2))$ . If we want to compute the partial derivative with respect to  $a$  at a point, we need to apply the chain rule several times:

$$
\begin{array}{l} \frac{\partial f}{\partial a} = \frac{\partial}{\partial a} \ln (a b + \max (a, 2)) (2.26) \\ = \frac{1}{a b + \max (a , 2)} \frac{\partial}{\partial a} (a b + \max (a, 2)) (2.27) \\ = \frac{1}{a b + \operatorname* {m a x} (a , 2)} \left[ \frac{\partial (a b)}{\partial a} + \frac{\partial \operatorname* {m a x} (a , 2)}{\partial a} \right] (2.28) \\ = \frac{1}{a b + \max (a , 2)} \left[ \left(b \frac{\partial a}{\partial a} + a \frac{\partial b}{\partial a}\right) + \left((2 > a) \frac{\partial 2}{\partial a} + (2 <   a) \frac{\partial a}{\partial a}\right) \right] \quad (2. 29) \\ = \frac{1}{a b + \max (a , 2)} [ b + (2 <   a) ] (2.30) \\ \end{array}
$$

This process can be automated through the use of a computational graph. A computational graph represents a function where the nodes are operations and the edges are input-output relations. The leaf nodes of a computational graph

We adopt the convention that Boolean expressions like  $(2 < \alpha)$  are 1 if true and 0 if false.

are input variables or constants, and terminal nodes are values output by the function. A computational graph is shown in figure 2.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bd87e54d40f980cea9a94c22aaf4ebc0cb7b1496da6a8d74f1d5420f26acd0cb.jpg)  
Figure 2.5. The computational graph for  $\ln (ab + \max (a,2))$

There are two methods for automatically differentiating  $f$  using its computational graph. The forward accumulation method traverses the tree from inputs to outputs, whereas reverse accumulation requires a backwards pass through the graph.

# 2.4.1 Forward Accumulation

Forward accumulation will automatically differentiate a function using a single forward pass through the function's computational graph. The method is equivalent to iteratively expanding the chain rule of the inner operation:

$$
\frac{d f}{d x} = \frac{d f}{d c_{4}} \frac{d c_{4}}{d x} = \frac{d f}{d c_{4}} \left(\frac{d c_{4}}{d c_{3}} \frac{d c_{3}}{d x}\right) = \frac{d f}{d c_{4}} \left(\frac{d c_{4}}{d c_{3}} \left(\frac{d c_{3}}{d c_{2}} \frac{d c_{2}}{d x} + \frac{d c_{3}}{d c_{1}} \frac{d c_{1}}{d x}\right)\right) \tag {2.31}
$$

To illustrate forward accumulation, we apply it to the example function  $f(a,b) = \ln (ab + \max (a,2))$  to calculate the partial derivative at  $a = 3$ ,  $b = 2$  with respect to  $a$ .

1. The procedure starts at the graph's source nodes consisting of the function inputs and any constant values. For each of these nodes, we note both the value and the partial derivative with respect to our target variable, as shown in figure 2.6.

2. Next we proceed down the tree, one node at a time, choosing as our next node one whose inputs have already been computed. We can compute the value by passing through the previous nodes' values, and we can compute the local partial derivative with respect to  $a$  using both the previous nodes' values and their partial derivatives.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/76bf541f70e6e3ce065ae9917ecefe33b714f894d15158429077633455e07b62.jpg)  
Figure 2.6. The computational graph for  $\ln (ab + \max (a,2))$  after forward accumulation is applied to calculate  $\partial f / \partial a$  with  $a = 3$  and  $b = 2$ . For compactness in this figure, we use dot notation or Newton's notation for derivatives. For example, if it is clear that we are taking the derivative with respect to  $a$ , we can write  $\partial b / \partial a$  as  $\dot{b}$ . Because  $\partial a / \partial a = 1$  and  $\partial a / \partial b = 0$ , we set  $\dot{a} = 1$  and  $\dot{b} = 0$  in this graph.

We end up with the correct result,  $f(3,2) = \ln 9$  and  $\frac{\partial f}{\partial a} = 1 / 3$ . This was done using one pass through the computational graph.

This process can be conveniently automated by a computer using a programming language which has overridden each operation to produce both the value and its derivative. Such pairs are called dual numbers.

Dual numbers can be expressed mathematically by including the abstract quantity  $\epsilon$ , where  $\epsilon^2$  is defined to be 0. Like a complex number, a dual number is written  $a + b\epsilon$ , where  $a$  and  $b$  are both real values. In  $a + b\epsilon$ ,  $a$  is the real part and  $b\epsilon$  is the dual part. We have:

$$
(a + b \epsilon) + (c + d \epsilon) = (a + c) + (b + d) \epsilon \tag {2.32}
$$

$$
(a + b \epsilon) \times (c + d \epsilon) = (a c) + (a d + b c) \epsilon \tag {2.33}
$$

In fact, by passing a dual number into any smooth function  $f$ , we get the evaluation and its derivative. We can show this using the Taylor series:

$$
f (x) = \sum_{k = 0}^{\infty} \frac{f^{(k)} (a)}{k !} (x - a) ^{k} \tag {2.34}
$$

$$
\begin{array}{l} f (a + b \epsilon) = \sum_{k = 0}^{\infty} \frac{f^{(k)} (a)}{k !} (a + b \epsilon - a) ^{k} (2.35) \\ = \sum_{k = 0}^{\infty} \frac{f^{(k)} (a) b^{k} \epsilon^{k}}{k !} (2.36) \\ = f (a) + b f^{\prime} (a) \epsilon + \epsilon^{2} \sum_{k = 2}^{\infty} \frac{f^{(k)} (a) b^{k}}{k !} \epsilon^{(k - 2)} (2.37) \\ = f (a) + b f^{\prime} (a) \epsilon (2.38) \\ \end{array}
$$

Example 2.5 shows an implementation.

# 2.4.2 Reverse Accumulation

Forward accumulation requires  $n$  passes in order to compute an  $n$ -dimensional gradient. Reverse accumulation<sup>10</sup> requires only a single run in order to compute a complete gradient but requires two passes through the graph: a forward pass during which necessary intermediate values are computed and a backward pass which computes the gradient. Reverse accumulation is often preferred over forward accumulation when gradients are needed, though care must be taken on memory-constrained systems when the computational graph is very large.<sup>11</sup>

Like forward accumulation, reverse accumulation will compute the partial derivative with respect to the chosen target variable but iteratively substitutes the outer function instead:

$$
\frac{d f}{d x} = \frac{d f}{d c_{4}} \frac{d c_{4}}{d x} = \left(\frac{d f}{d c_{3}} \frac{d c_{3}}{d c_{4}}\right) \frac{d c_{4}}{d x} = \left(\left(\frac{d f}{d c_{2}} \frac{d c_{2}}{d c_{3}} + \frac{d f}{d c_{1}} \frac{d c_{1}}{d c_{3}}\right) \frac{d c_{3}}{d c_{4}}\right) \frac{d c_{4}}{d x} \tag {2.39}
$$

This process is the reverse pass, the evaluation of which requires intermediate values that are obtained during a forward pass.

Reverse accumulation can be implemented through operation overloading $^{12}$  in a similar manner to the way dual numbers are used to implement forward accumulation. Two functions must be implemented for each fundamental operation: a forward operation that overloads the operation to store local gradient information

10 S. Linnainmaa, "The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors," M.S. thesis, University of Helsinki, 1970.  
11 Reverse accumulation is central to the backpropagation algorithm used to train neural networks. D.E. Rumelhart, G.E. Hinton, and R.J. Williams, "Learning Representations by Back-Propagating Errors," Nature, vol. 323, pp. 533-536, 1986.

12 Operation overloading refers to providing implementations for common operations such as  $+, -, \text{or} =$  for custom variable types. Overloading is discussed in appendix A.2.5.

Dual numbers can be implemented by defining a struct Dual that contains two fields, the value  $v$  and the derivative  $\partial$ .

```txt
struct Dual v # real part  $\partial$  # dual part end
```

We must then implement methods for each of the base operations required. These methods take in dual numbers and produce new dual numbers using that operation's chain rule logic.

```txt
Base.  $\vdots +$  (a::Dual, b::Dual)  $=$  Dual(a.v + b.v, a.  $\partial +$  b.  $\partial$  )   
Base.  $\vdots *$  (a::Dual, b::Dual)  $=$  Dual(a.v \* b.v, a.v\*b.  $\partial +$  b.v\*a.  $\partial$  ）   
Base.log(a::Dual)  $=$  Dual(log(a.v), a.  $\partial /$  a.v)   
function Base.max(a::Dual, b::Dual)   
 $\begin{array}{rl} & {\mathbf{v} = \max (\mathbf{a}.\mathbf{v},\mathbf{b}.\mathbf{v})}\\ & {\partial = \mathbf{a}.\mathbf{v} > \mathbf{b}.\mathbf{v}\text{?}\mathbf{a}.\partial :\mathbf{a}.\mathbf{v} <   \mathbf{b}.\mathbf{v}\text{?}\mathbf{b}.\partial :\mathbf{NaN}}\\ & {\mathrm{return~Dual}(v,\partial)} \end{array}$    
end   
function Base.max(a::Dual, b::Int)   
 $\begin{array}{rl} & {\mathbf{v} = \max (\mathbf{a}.v,b)}\\ & {\partial = \mathbf{a}.v > \mathbf{b}\text{?}\mathbf{a}.\partial :\mathbf{a}.v <   \mathbf{b}\text{?}\mathbf{0}\text{:}\mathbf{NaN}}\\ & {\mathrm{return~Dual}(v,\partial)} \end{array}$    
end
```

The ForwardDiff.jl package supports an extensive set of mathematical operations and additionally provides gradients and Hessians.

```julia
julia> using ForwardDiff  
julia> a = ForwardDiff.Dual(3,1);  
julia> b = ForwardDiff.Dual(2,0);  
julia> log(a*b + max(a,2))  
Dual{Nothing} (2.1972245773362196, 0.3333333333333333)
```

Example 2.5. An implementation of dual numbers that allows for automatic forward accumulation. The package DualNumbers.jl provides comprehensive coverage of many additional base operations.

during the forward pass and a backward operation that uses the information to propagate the gradient backwards. Packages like Tensorflow $^{13}$  or Zygote.jl can automatically construct the computational graph and the associated forward and backwards pass operations. Example 2.6 shows how Zygote.jl can be used.

The Zygote.jl package provides automatic differentiation in the form of reverse-accumulation. Here the gradient function is used to automatically generate the backwards pass through the source code of f to obtain the gradient.

```julia
julia> import Zygote: gradient  
julia> f(a, b) = log(a*b + max(a, 2));  
julia> gradient(f, 3.0, 2.0)  
(0.333333333333333, 0.333333333333333)
```

$^{13}$  Tensorflow is an open source software library for numerical computation using data flow graphs and is often used for deep learning applications. It may be obtained from tensorflow.org.

Example 2.6. Automatic differentiation using the Zygote.jl package. We find that the gradient at [3,2] is  $[1/3,1/3]$ .

# 2.5 Regression Gradient

Instead of estimating the gradient at  $\mathbf{x}$  by using a finite difference method along each coordinate axis, we can use linear regression<sup>14</sup> to estimate the gradient from the results of random perturbations from  $\mathbf{x}$ .<sup>15</sup> More design perturbations will tend to produce better gradient estimates. This method is particularly useful when the objective function is noisy<sup>16</sup> because the regression helps smooth out the noise in the evaluations when producing a gradient estimate.

Given a dataset of  $m$  perturbations and their function evaluations

$$
\left(\Delta \mathbf {x}^{(1)}, f (\mathbf {x} + \Delta \mathbf {x}^{(1)})\right), \left(\Delta \mathbf {x}^{(2)}, f (\mathbf {x} + \Delta \mathbf {x}^{(2)})\right), \dots , \left(\Delta \mathbf {x}^{(m)}, f (\mathbf {x} + \Delta \mathbf {x}^{(m)})\right) \tag {2.40}
$$

we seek the gradient  $\mathbf{g}$  that is consistent with the first-order Taylor expansion:

$$
\hat {f} (\mathbf {x} + \Delta \mathbf {x}) = f (\mathbf {x}) + \mathbf {g}^{\top} \Delta \mathbf {x} \tag {2.41}
$$

We define the gradient with the closest match to be the one that minimizes the squared distance:

$$
\underset{\mathbf {g}} {\operatorname{minimize}} \sum_{i = 1}^{m} \left(f (\mathbf {x} + \Delta \mathbf {x}^{(i)}) - \left(f (\mathbf {x}) + \mathbf {g}^{\top} \Delta \mathbf {x}^{(i)}\right)\right) ^{2} \tag {2.42}
$$

14 Linear regression is covered in section 17.2.  
15 This general approach is sometimes referred to as simultaneous perturbation stochastic approximation by J.C.Spall, Introduction to Stochastic Search and Optimization.Wiley, 2003. The general connection to linear regression is provided by J. Peters and S.Schaal, Reinforcement Learning of Motor Skills with Policy Gradients," Neural Networks, vol.21,no.4pp.682-697,2008.  
16 Problems with noisy objective functions are covered in chapter 20.

We can solve this minimization exactly. To do so, we rewrite the optimization problem in matrix form, where we define

$$
\Delta \mathbf {X} = \left[ \begin{array}{c} \left(\Delta \mathbf {x}^{(1)}\right) ^{\top} \\ \vdots \\ \left(\Delta \mathbf {x}^{(m)}\right) ^{\top} \end{array} \right] \tag {2.43}
$$

$$
\Delta \mathbf {f} = \left[ f (\mathbf {x} + \Delta \mathbf {x}^{(1)}) - f (\mathbf {x}), \dots , f (\mathbf {x} + \Delta \mathbf {x}^{(m)}) - f (\mathbf {x}) \right] \tag {2.44}
$$

The optimization problem can then be written as

$$
\underset{\mathbf {g}} {\operatorname{minimize}} \left\| \Delta \mathbf {X} \mathbf {g} - \Delta \mathbf {f} \right\| ^{2} \tag {2.45}
$$

This is a linear regression problem that can be solved exactly by finding the pseudoinverse of  $\Delta \mathbf{X}$  and multiplying by  $\Delta \mathbf{f}$ :<sup>17</sup>

$$
\mathbf {g} = \Delta \mathbf {X}^{+} \Delta \mathbf {f} \tag {2.46}
$$

Algorithm 2.3 provides an implementation of this approach in which the perturbations are drawn uniformly from a hypersphere with radius  $\delta$ . Figure 2.7 illustrates this approach on a simple two-dimensional function.

```julia
function regression_gradient(f, x, m, δ)  
fx = f(x)  
n = length(x)  
ΔX = stack(δ .* normalize(Δx) for Δx in eachrowrand(n, m))  
Δf = [f(x + Δx) - fx for Δx in eachrow(ΔX)]  
return ΔX \ Δf  
end
```

17 Here,  $\mathbf{X}^{+}$  denotes the pseudoinverse of  $\mathbf{X}$ , which is discussed in more detail in section 13.2. The function pinv in Julia computes the pseudoinverse of a matrix. Instead of using the pseudoinverse, we could also use the least squares solver built into Julia (discussed in appendix A.1.5). Using the backslash operator, we can write the solution as  $\Delta \mathbf{X} \setminus \Delta \mathbf{f}$  instead of pinv  $(\Delta \mathbf{X}) * \Delta \mathbf{f}$ .

Algorithm 2.3. A method for estimating the gradient of a function  $f$  at  $x$  using finite differences with multiple samples. Perturbation vectors are generated by normalizing  $m$  normally distributed samples and scaling by a perturbation scalar  $\delta$ .

# 2.6 Simultaneous Perturbation Stochastic Gradient Approximation

The simultaneous perturbation stochastic gradient approximation (SPSA),<sup>18</sup> can produce a stochastic estimate of the gradient using as few as two function evaluations, which is significantly more efficient than finite differences for large problems. The method is particularly useful when there are a large number of design variables, such as in deep learning applications where there may be many billions of parameters.

18 J.C. Spall, "Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation," IEEE Transactions on Automatic Control, vol. 37, pp. 332-341, 1992.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2d415a40926b68faccb5c85a0cf9e226376248766a5949c68f2ea6bdd5471815.jpg)  
Figure 2.7. Using the regression gradient method to estimate the gradient of the function  $f(\mathbf{x}) = \cos(x_1) / \sin(x_2)$ , evaluated at  $\mathbf{x} = [1.5, 2.5]$  with  $m = 20$  samples.

This approach approximates the gradient using directional derivatives of  $f$ . A single directional derivative is obtained using a central difference with a perturbation  $\mathbf{z}$  drawn from a zero-mean unit Gaussian distribution and a scalar  $\delta$ :

$$
\nabla_{\mathbf {z}} f (\mathbf {x}) \approx \frac{f (\mathbf {x} + \delta \mathbf {z}) - f (\mathbf {x} - \delta \mathbf {z})}{2 \delta} \tag {2.47}
$$

A single-sample SPSA estimate is this perturbation scaled by this directional derivative:

$$
\nabla f (\mathbf {x}) \approx (\nabla_{\mathbf {z}} f (\mathbf {x})) \mathbf {z} \tag {2.48}
$$

Averaging many samples can improve the gradient estimate, as done in algorithm 2.4. However, the sample count is typically left quite small or even set to 1 even though the gradient estimate is noisy in the interest of computational efficiency. When used in conjunction with a gradient-based optimization algorithm, the noisy gradient estimate can still provide a useful direction to search for a solution.

# 2.7 Summary

- Derivatives are useful in optimization because they provide information about how to change a given point in order to improve the objective function.

```julia
function simultaneous_perturbation_gradient(x, f,  $\delta$ , m)  
n = length(x)  
 $\nabla =$  zeros(n)  
for i in 1:m  
    z = randn(n)  
     $\nabla += ((f(x + \delta * z) - f(x - \delta * z)). / (2\delta)) * z$   
end  
return  $\nabla . / m$   
end
```

- For multivariate functions, various derivative-based concepts are useful for directing the search for an optimum, including the gradient, the Hessian, and the directional derivative.  
- One approach to numerical differentiation includes finite difference approximations.  
- The complex step method can eliminate the effect of subtractive cancellation error when taking small steps, resulting in high quality gradient estimates.  
- Analytic differentiation methods include forward and reverse accumulation on computational graphs.  
- The regression gradient uses linear regression to find the best estimate of the gradient with respect to a set of sampled perturbations and their function evaluations.  
- Simultaneous perturbation stochastic gradient approximation allows for the estimation of gradients for very large objective functions.

# 2.8 Exercises

Exercise 2.1. Adopt the forward difference method to approximate the Hessian of  $f(\mathbf{x})$  using its gradient,  $\nabla f(\mathbf{x})$ .

Solution: An entry of the Hessian can be computed using the forward difference method:

$$
H_{i j} = \frac{\partial^{2} f (\mathbf {x})}{\partial x_{i} \partial x_{j}} \approx \frac{\nabla f (\mathbf {x} + h \mathbf {e}_{j}) _{i} - \nabla f (\mathbf {x}) _{i}}{h}
$$

We can thus approximate the  $j$ th column of the Hessian using:

$$
\mathbf {H}_{. j} \approx \frac{\nabla f (\mathbf {x} + h \mathbf {e}_{j}) - \nabla f (x)}{h}
$$

Algorithm 2.4. A method for estimating the gradient of a function  $f$  at  $x$  in-place by averaging  $m$  directional derivatives via the simultaneous perturbation gradient approximation. Perturbations are zero-mean with standard deviation  $\delta$ .

where  $\mathbf{e}_j$  is the  $j$ th basis vector with its  $j$ th component equal to one and all other entries are zero.

This procedure can be repeated for each column of the Hessian.

Exercise 2.2. What is a drawback of the central difference method over other finite difference methods if we already know  $f(\mathbf{x})$ ?

Solution: It requires two evaluations of the objective function.

Exercise 2.3. Compute the gradient of  $f(x) = \ln x + e^{x} + \frac{1}{x}$  for a point  $x$  close to zero. What term dominates in the expression?

Solution:  $f'(x) = \frac{1}{x} + e^x - \frac{1}{x^2}$ . When  $x$  is close to zero, we find that  $x < 1$ . Hence  $\frac{1}{x} > 1$ , and finally  $\frac{1}{x^2} > \frac{1}{x} > 0$ , so  $-\frac{1}{x^2}$  dominates.

Exercise 2.4. Suppose  $f(x)$  is a real-valued function that is also defined for complex inputs. If  $f(3 + ih) = 2 + 4ih$ , what is  $f'(3)$ ?

Solution: From the complex step method, we have  $f'(x) \approx \mathrm{Im}(2 + 4ih) / h = 4h / h = 4$ .

Exercise 2.5. Draw the computational graph for  $f(x,y) = \sin (x + y^2)$ . Use the computational graph with forward accumulation to compute  $\frac{\partial f}{\partial y}$  at  $(x,y) = (1,1)$ . Label the intermediate values and partial derivatives as they are propagated through the graph.

Solution: See the picture below:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b126ecca9600a81abf70fe4a9df7c1db6a14ee862a546ff4283f5ce42dd41b8a.jpg)

Exercise 2.6. Combine the forward and backward difference methods to obtain a difference method for estimating the second-order derivative of a function  $f$  at  $x$  using three function evaluations.

Solution: The second-order derivative can be approximated using the central difference on the first-order derivative:

$$
f^{\prime \prime} (x) \approx \frac{f^{\prime} (x + h / 2) - f^{\prime} (x - h / 2)}{h}
$$

for small values of  $h$ .

Substituting in the forward and backwards different estimates of  $f^{\prime}(x + h / 2)$  and  $f^{\prime}(x - h / 2)$  yields:

$$
\begin{array}{l} f^{\prime \prime} (x) \approx \frac{\frac{f (x + h / 2 + h / 2) - f (x + h / 2 - h / 2)}{h} - \frac{f (x - h / 2 + h / 2) - f (x - h / 2 - h / 2)}{h}}{h} \\ = \frac{f (x + h) - f (x)}{h^{2}} - \frac{f (x) - f (x - h)}{h^{2}} \\ = \frac{f (x + h) - 2 f (x) + f (x - h)}{h^{2}} \\ \end{array}
$$

Exercise 2.7. The forward difference derivative approximation evaluates the objective function at  $x$  and  $x + h$ , and divides by  $h$ . Computers use floating point numbers with a finite number of bits. As such, adding  $h$  to  $x$  can produce a floating point number that is not exactly  $x + h$ . This difference can lead to inaccuracies.

Plot the absolute relative error of the derivative estimate on  $f(x) = \exp (x)$  for  $x = 0, x = 100$ , and  $x = 600$  for both the forward difference method introduced in the text and a modification that ensures that the step size we divide by is consistent with the difference between  $x$  and  $x + h$ :

$$
f^{\prime} (x) \approx \frac{f (x + h) - f (x)}{(x + h) - x}
$$

Does this modification make a difference?

# Solution:

The modification can make a difference for very small step sizes and for large differences in objective function values:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/71bd94cfc6cd5e64c9bb7132ee9d9eefb7b310cc920d95efc84ca37715f26f17.jpg)

# 3 Bracketing

This chapter presents a variety of bracketing methods for univariate functions, or functions involving a single variable. Bracketing is the process of identifying an interval in which a local minimum lies and then successively shrinking the interval. For many functions, derivative information can be helpful in directing the search for an optimum, but, for some functions, this information may not be available or might not exist. This chapter outlines a wide variety of approaches that leverage different assumptions. Later chapters that consider multivariate optimization will build upon the concepts introduced here.

# 3.1 Unimodality

Several of the algorithms presented in this chapter assume unimodality of the objective function. A unimodal function  $f$  is one where there is a unique  $x^{*}$ , such that  $f$  is monotonically decreasing for  $x \leq x^{*}$  and monotonically increasing for  $x \geq x^{*}$ . It follows from this definition that the unique global minimum is at  $x^{*}$ , and there are no other local minima. A multimodal function is one with multiple peaks and valleys.

Given a unimodal function, we can bracket an interval  $[a,c]$  containing the global minimum if we can find three points  $a < b < c$ , such that  $f(a) > f(b) < f(c)$ . Figure 3.1 shows an example.

# 3.2 Finding an Initial Bracket

When optimizing a function, we often start by first bracketing an interval containing a local minimum. We then successively reduce the size of the bracketed interval to converge on the local minimum. A simple procedure (algorithm 3.1)

It is perhaps more conventional to define unimodal functions in the opposite sense, such that there is a unique global maximum rather than a minimum. However, in this text, we try to minimize functions, and so we use the definition in this paragraph.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b0dfdf4354afb40de671ed229e846b7c83d9530a76aceefa30de85ec37469cf1.jpg)  
Figure 3.1. Three points shown bracketing a minimum.

can be used to find an initial bracket. Starting at a given point, we take a step in the positive direction. The distance we take is a hyperparameter to this algorithm,[2] but the algorithm provided defaults it to  $10^{-2}$ . We then search in the negative direction to find a new point that exceeds the lowest point. With each step, we expand the step size by some factor, which is another hyperparameter to this algorithm that is often set to 2. An example is shown in figure 3.2. Functions without local minima, such as  $\exp(x)$ , cannot be bracketed and will cause bracketimum to fail.

```txt
function bracket_minimum(f, x=0; s=1e-2, k=2.0)  
a, ya = x, f(x)  
b, yb = a + s, f(a + s)  
if yb > ya  
    a, b = b, a  
    ya, yb = yb, ya  
    s = -s  
end  
while true  
    c, yc = b + s, f(b + s)  
    if yc > yb  
        return a < c ? (a, c) : (c, a)  
    end  
    a, ya, b, yb = b, yb, c, yc  
    s *= k  
end
```

2 A hyperparameter is a parameter that governs the function of an algorithm. It can be set by an expert or tuned using an optimization algorithm. Many of the algorithms in this text have hyperparameters. We often provide default values suggested in the literature. The success of an algorithm can be sensitive to the choice of hyperparameter.

Algorithm 3.1. An algorithm for bracketing an interval in which a local minimum must exist. It takes as input a univariate function  $f$  and starting position  $x$ , which defaults to 0. The starting step size  $s$  and the expansion factor  $k$  can be specified. It returns a tuple containing the new interval  $[a, b]$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/96e719fae0d5c8b07deceba81f3acf2ed094a6fd8128fd27336c8a4a942bb588.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e98924e52bdfee4fc464de4ce75c010ffaee5cb6f3147bc3b01f36bd15186fd1.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3289f2f8444a801413b96846c6aa77ab70c8169530254c059ebf1aafe4f91b38.jpg)  
Figure 3.2. An example of running bracket_minimum on a function. The method reverses direction between the first and second iteration and then expands until a minimum is bracketed in the fourth iteration.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5b2e5610b0da19caffd07c87624cf4d976e6dbb51718087028f475e43eea6077.jpg)

# 3.3 Fibonacci Search

Suppose we have a unimodal  $f$  bracketed by the interval  $[a, b]$ . Given a limit on the number of times we can query the objective function, Fibonacci search (algorithm 3.2) is guaranteed to maximally shrink the bracketed interval.

Suppose we can query  $f$  only twice. If we query  $f$  on the one-third and two-thirds points on the interval, then we are guaranteed to remove one-third of our interval, regardless of  $f$ , as shown in figure 3.3.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f215e933ac58527a87b98da298fdbf39714c8ea4310a3bb1316755b134bd5d49.jpg)  
Figure 3.3. Our initial guess for two queries will remove one-third of the initial interval.

We can guarantee a tighter bracket by moving our guesses toward the center. In the limit as  $\epsilon \to 0$ , we are guaranteed to shrink our interval by a factor of two as shown in figure 3.4.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ba61c9603c67127f40828b002845a4796ee7e348cd3684a528b126ff47bf1b95.jpg)  
Figure 3.4. The most we can guarantee to shrink our interval is by just under a factor of two.

With three queries, we can shrink the interval by a factor of three. We first query  $f$  on the one-third and two-thirds points on the interval, discard one-third of the interval, and then sample just next to the better sample as shown in figure 3.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/872007b855c178eaecc4fe0c0ecf9cd3fc3b560935c4abb1767dfc5e742f47d2.jpg)  
Figure 3.5. With three queries we can shrink the domain by a factor of three. The third query is made based on the result of the first two queries.  
Figure 3.6. For  $n$  queries we are guaranteed to shrink our interval by a factor of  $F_{n + 1}$ . The length of every interval constructed during Fibonacci search can be expressed in terms of the final interval times a Fibonacci number. If the final, smallest interval has length  $I_{n}$ , then the second smallest interval has length  $I_{n - 1} = F_3I_n$ , the third smallest interval has length  $I_{n - 2} = F_4I_n$ , and so forth.

For  $n$  queries, the interval lengths are related to the Fibonacci sequence: 1, 1, 2, 3, 5, 8, and so forth. The first two terms are one, and the following terms are always the sum of the previous two:

$$
F_{n} = \left\{ \begin{array}{l l} 1 & \text{if } n \leq 2 \\ F_{n - 1} + F_{n - 2} & \text{otherwise} \end{array} \right. \tag {3.1}
$$

Figure 3.6 shows the relationship between the intervals. Example 3.1 walks through an application to a univariate function.

$$
\begin{array}{l} I_{1} = I_{2} + I_{3} = 8 I_{5} \\ \begin{array}{c c c c} & & & I_{2} = I_{3} + I_{4} = 5 I_{5} \end{array} \\ \begin{array}{c c c c} \hline & & & I_{3} = I_{4} + I_{5} = 3 I_{5} \\ \hline \end{array} \\ \| I_{4} = 2 I_{5} \\ - \mid I_{5} \\ \end{array}
$$

The Fibonacci sequence can be determined analytically using Binet's formula:

$$
F_{n} = \frac{\varphi^{n} - (1 - \varphi) ^{n}}{\sqrt{5}}, \tag {3.2}
$$

where  $\varphi = (1 + \sqrt{5}) / 2\approx 1.61803$  is the golden ratio.

The ratio between successive values in the Fibonacci sequence is:

$$
\frac{F_{n}}{F_{n - 1}} = \varphi \frac{1 - s^{n}}{1 - s^{n - 1}} \tag {3.3}
$$

where  $s = (1 - \sqrt{5}) / (1 + \sqrt{5}) \approx -0.382$ .

```julia
function fibonacci_search(f, a, b, n;  $\epsilon = 0.01$  )  
s = (1-√5)/(1+√5)  
 $\rho = 1 / (\varphi*(1-s^{\wedge}(n+1))/(1-s^{\wedge}n))$   
r = (1-ρ)*a + ρ*b # right sample point  
yr, n = f(r), n-1  
while n > 0  
    if n > 1  
        l = ρ*a + (1-ρ)*b  
    else  
        l = ε*a + (1-ε)*r  
    end  
     $\rho = 1 / (\varphi*(1-s^{\wedge}(n+1))/(1-s^{\wedge}n))$   
yl, n = f(l), n-1  
    if yl < yr  
        r, b, yr = l, r, yl  
    else  
        a, b = b, l  
    end  
end  
return a < b ? (a, b) : (b, a)  
end
```

Algorithm 3.2. Fibonacci search to be run on univariate function  $f$ , with bracketing interval  $[a, b]$ , for  $n > 1$  function evaluations. It returns the new interval  $(a, b)$ . The optional parameter  $\epsilon$  controls the lowest-level interval. The golden ratio  $\varphi$  is defined in Base.MathConstants.jl.

# 3.4 Golden Section Search

If we take the limit for large  $n$ , we see that the ratio between successive values of the Fibonacci sequence approaches the golden ratio:

$$
\lim_{n \rightarrow \infty} \frac{F_{n}}{F_{n - 1}} = \varphi \tag {3.4}
$$

Golden section search (algorithm 3.3) uses the golden ratio to approximate Fibonacci search. Figure 3.7 shows the relationship between the intervals. Figures 3.8 and 3.9 compare Fibonacci search with golden section search on unimodal and non-unimodal functions, respectively.

Consider using Fibonacci search with five function evaluations to minimize  $f(x) = \exp (x - 2) - x$  over the interval  $[a,b] = [-2,6]$ . The first two function evaluations are made at  $\frac{F_5}{F_6}$  and  $1 - \frac{F_5}{F_6}$ , along the length of the initial bracketing interval:

$$
f (x^{(1)}) = f \left(a + (b - a) \left(1 - \frac{F_{5}}{F_{6}}\right)\right) = f (1) = - 0. 63 2
$$

$$
f (x^{(2)}) = f \left(a + (b - a) \frac{F_{5}}{F_{6}}\right) = f (3) = - 0. 28 2
$$

The evaluation at  $x^{(1)}$  is lower, yielding the new interval  $[a,b] = [-2,3]$ . Two evaluations are needed for the next interval split:

$$
x_{\mathrm{left}} = a + (b - a) \left(1 - \frac{F_{4}}{F_{5}}\right) = 0
$$

$$
x_{\mathrm{right}} = a + (b - a) \frac{F_{4}}{F_{5}} = 1
$$

A third function evaluation is thus made at  $x_{\mathrm{left}}$ , as  $x_{\mathrm{right}}$  has already been evaluated:

$$
f (x^{(3)}) = f (0) = 0. 13 5
$$

The evaluation at  $x^{(1)}$  is lower, yielding the new interval  $[a,b] = [0,3]$ . Two evaluations are needed for the next interval split:

$$
x_{\text{left}} = a + (b - a) \left(1 - \frac{F_{3}}{F_{4}}\right) = 1
$$

$$
x_{\mathrm{right}} = a + (b - a) \frac{F_{3}}{F_{4}} = 2
$$

A fourth functional evaluation is thus made at  $x_{\mathrm{right}}$ , as  $x_{\mathrm{left}}$  has already been evaluated:

$$
f (x^{(4)}) = f (2) = - 1
$$

The new interval is  $[a, b] = [1, 3]$ . A final evaluation is made just next to the center of the interval at  $2 + \epsilon$ , and it is found to have a slightly higher value than  $f(2)$ . The final interval is  $[1, 2 + \epsilon]$ .

Example 3.1. Using Fibonacci search with five function evaluations to optimize a univariate function.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b7d3ead0d320176d43fe53d14b968ef6f4a0283a6daac996a31544a4f8b2e73e.jpg)  
Figure 3.7. For  $n$  queries of a univariate function we are guaranteed to shrink a bracketing interval by a factor of  $\varphi^{n - 1}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f49a7761f8858c8e446d097cf40b6c6bc2eb255975e6fa4d247c09d689a62515.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/94bd55304821afe5fa96ea089e5cf8d70f4bbb80dfbf2a2e18594e20338ba431.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/df1f428e64aa204d4d9c11002aa0a4c8a2c3fecb96dd56bc652ce908388da3db.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a28edafd8fe7e4d3c59ea4badd5c412da6e1d105ce54af5ec5c102ac515ae8cd.jpg)

```txt
function golden_section_search(f, a, b, n)  
 $\rho = \varphi - 1$ $d = \rho * b + (1 - \rho)*a$ $yd = f(d)$   
for  $i = 1 : n-1$ $\begin{array}{l} c = \rho*a + (1 - \rho)*b \\ yc = f(c) \\ if yc < yd \\ b, d, yd = d, c, yc \\ else \\ a, b = b, c \\ end \end{array}$   
return  $a < b ? (a, b) : (b, a)$   
end
```

Algorithm 3.3. Golden section search to be run on a univariate function  $f$ , with bracketing interval  $[a, b]$ , for  $n > 1$  function evaluations. It returns the new interval (a, b). Julia already has the golden ratio  $\varphi$  defined. Guaranteeing convergence to within  $\epsilon$  requires  $n = (b - a) / (\epsilon \ln \varphi)$  iterations.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0fe0809ddb93629109fc9d4a8ae066dfafaa28b5e1b00fa339d48098a2622cef.jpg)  
Fibonacci Search

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/722fd11be51073a446f662ffd8f8cc0150ffd834bdd6b4a48ccd5bba9cc60c97.jpg)  
Golden Section Search  
Figure 3.8. Fibonacci and golden section search on a unimodal function.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7d8420b7eb41d3ae98e5fb5700cba17e976475e4d47cb42a5ac4def0834971a9.jpg)  
Fibonacci Search

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b7cd01d5b202b3bcb95bcddd8a5bf48faafd846ee88ecfaca9ce5199ff4fcf39.jpg)  
Golden Section Search  
Figure 3.9. Fibonacci and golden section search on a nonunimodal function. Search is not guaranteed to find a global minimum.

# 3.5 Quadratic Fit Search

Quadratic fit search leverages our ability to analytically solve for the minimum of a quadratic function. Many local minima look quadratic when we zoom in close enough. Quadratic fit search iteratively fits a quadratic function to three bracketing points, solves for the minimum, chooses a new set of bracketing points, and repeats as shown in figure 3.10.

Given bracketing points  $a < b < c$ , we wish to find the coefficients  $p_1, p_2,$  and  $p_3$  for the quadratic function  $q$  that goes through  $(a, y_a), (b, y_b),$  and  $(c, y_c)$ :

$$
q (x) = p_{1} + p_{2} x + p_{3} x^{2} \tag {3.5}
$$

$$
y_{a} = p_{1} + p_{2} a + p_{3} a^{2} \tag {3.6}
$$

$$
y_{b} = p_{1} + p_{2} b + p_{3} b^{2} \tag {3.7}
$$

$$
y_{c} = p_{1} + p_{2} c + p_{3} c^{2} \tag {3.8}
$$

In matrix form, we have

$$
\left[ \begin{array}{l} y_{a} \\ y_{b} \\ y_{c} \end{array} \right] = \left[ \begin{array}{l l l} 1 & a & a^{2} \\ 1 & b & b^{2} \\ 1 & c & c^{2} \end{array} \right] \left[ \begin{array}{l} p_{1} \\ p_{2} \\ p_{3} \end{array} \right] \tag {3.9}
$$

We can solve for the coefficients through matrix inversion:

$$
\left[ \begin{array}{l} p_{1} \\ p_{2} \\ p_{3} \end{array} \right] = \left[ \begin{array}{c c c} 1 & a & a^{2} \\ 1 & b & b^{2} \\ 1 & c & c^{2} \end{array} \right] ^{- 1} \left[ \begin{array}{l} y_{a} \\ y_{b} \\ y_{c} \end{array} \right] \tag {3.10}
$$

Our quadratic function is then

$$
q (x) = y_{a} \frac{(x - b) (x - c)}{(a - b) (a - c)} + y_{b} \frac{(x - a) (x - c)}{(b - a) (b - c)} + y_{c} \frac{(x - a) (x - b)}{(c - a) (c - b)} \tag {3.11}
$$

We can solve for the unique minimum by finding where the derivative is zero:

$$
x^{*} = \frac{1}{2} \frac{y_{a} \left(b^{2} - c^{2}\right) + y_{b} \left(c^{2} - a^{2}\right) + y_{c} \left(a^{2} - b^{2}\right)}{y_{a} (b - c) + y_{b} (c - a) + y_{c} (a - b)} \tag {3.12}
$$

Quadratic fit search is typically faster than golden section search. It may need safeguards for cases where the next point is very close to other points. A basic implementation is provided in algorithm 3.4. Figure 3.11 shows several iterations of the algorithm.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3b5d6fb0585ca9cf4fff96c156ec498977d7c34cbded458e7a3867a74ace6766.jpg)  
Figure 3.10. Quadratic fit search fits a quadratic function to three bracketing points (black dots) and uses the analytic minimum (blue dot) to determine the next set of bracketing points.

```julia
function quadratic_fit_search(f, a, b, c, n)  
ya, yb, yc = f(a), f(b), f(c)  
for i in 1:n-3  
x = 0.5*(ya*(b^2-c^2) + yb*(c^2-a^2) + yc*(a^2-b^2)) / (ya*(b-c) + yb*(c-a) + yc*(a-b))  
yx = f(x)  
if x > b  
    if yx > yb  
        c, yc = x, yx  
    else  
        a, ya, b, yb = b, yb, x, yx  
end  
elseif x < b  
    if yx > yb  
        a, ya = x, yx  
    else  
        c, yc, b, yb = b, yb, x, yx  
end  
end  
return (a, b, c)  
end
```

Algorithm 3.4. Quadratic fit search to be run on univariate function  $f$ , with bracketing interval  $[a, c]$  with  $a < b < c$ . The method will run for  $n$  function evaluations. It returns the new bracketing values as a tuple,  $(a, b, c)$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3008169ce409a977274175e023c2d94cd79a861ab4de4ce6c1882b99a3c98c05.jpg)  
Figure 3.11. Four iterations of the quadratic fit method.

# 3.6 Shubert-Piyavskii Method

In contrast with previous methods in this chapter, the Shubert-Piyavskii method<sup>3</sup> is a global optimization method over a domain  $[a,b]$ , meaning it is guaranteed to converge on the global minimum of a function irrespective of any local minima or whether the function is unimodal. A basic implementation is provided by algorithm 3.5.

The Shubert-Piyavskii method requires that the function be Lipschitz continuous, meaning that it is continuous and there is an upper bound on the magnitude of its derivative. A function  $f$  is Lipschitz continuous on  $[a,b]$  if there exists an  $\ell > 0$  such that:

$$
\left| f (x) - f (y) \right| \leq \ell | x - y | \text{for all} x, y \in [ a, b ] \tag {3.13}
$$

Intuitively,  $\ell$  is as large as the largest unsigned instantaneous rate of change the function attains on  $[a,b]$ . Given a point  $(x_0,f(x_0))$ , we know that the lines  $f(x_0) - \ell (x - x_0)$  for  $x > x_0$  and  $f(x_0) + \ell (x - x_0)$  for  $x < x_0$  form a lower bound of  $f$ .

The Shubert-Piyavskii method iteratively builds a tighter and tighter lower bound on the function. Given a valid Lipschitz constant  $\ell$ , the algorithm begins by sampling the midpoint,  $x^{(1)} = (a + b) / 2$ . A sawtooth lower bound is constructed using lines of slope  $\pm \ell$  from this point. These lines will always lie below  $f$  if  $\ell$  is a valid Lipschitz constant as shown in figure 3.12.

Figure 3.12. The first iteration of the Shubert-Piyavskii method.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c9b1ab2ddc07dea18e82d64f85ac827a09106f2c80285548e9e312671116b960.jpg)  
$f(x)$  
—lower bound

3 S. Piyavskii, "An Algorithm for Finding the Absolute Extremum of a Function," USSR Computational Mathematics and Mathematical Physics, vol. 12, no. 4, pp. 57-67, 1972. B. O. Shubert, "A Sequential Method Seeking the Global Maximum of a Function," SIAM Journal on Numerical Analysis, vol. 9, no. 3, pp. 379-388, 1972.  
4 This property is named for the German mathematician Rudolf Lipschitz (1832-1903).  
5 We can extend the definition of Lipschitz continuity to multivariate functions, where  $\mathbf{x}$  and  $\mathbf{y}$  are vectors and the absolute value is replaced by any vector norm.

Upper vertices in the sawtooth correspond to sampled points. Lower vertices correspond to intersections between the Lipschitz lines originating from each sampled point. Further iterations find the minimum point in the sawtooth, evaluate the function at that  $x$  value, and then use the result to update the sawtooth. Figure 3.13 illustrates this process.

Figure 3.13. Updating the lower bound involves sampling a new point and intersecting the new lines with the existing sawtooth.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3c6ac58905ae3c8d389d8a6d8c70ae19c32df203292ac8fa57936a5aee26ba0a.jpg)  
$f(x)$  
—lower bound

The algorithm is typically stopped when the difference in height between the minimum sawtooth value and the function evaluation at that point is less than a given tolerance  $\epsilon$ . For the minimum peak  $(x^{(n)}, y^{(n)})$  and function evaluation  $f(x^{(n)})$ , we thus terminate if  $f(x^{(n)}) - y^{(n)} < \epsilon$ .

The regions in which the minimum could lie can be computed using this update information. For every peak, an uncertainty region can be computed according to:

$$
\left[ x^{(i)} - \frac{1}{\ell} \left(y_{\min } - y^{(i)}\right), x^{(i)} + \frac{1}{\ell} \left(y_{\min } - y^{(i)}\right) \right] \tag {3.14}
$$

for each sawtooth lower vertex  $(x^{(i)},y^{(i)})$  and the minimum sawtooth upper vertex  $(x_{\min},y_{\min})$ . A point will contribute an uncertainty region only if  $y^{(i)} < y_{\min}$ . The minimum is located in one of these peak uncertainty regions.

The main drawback of the Shubert-Piyavskii method is that it requires knowing a valid Lipschitz constant. Large Lipschitz constants will result in poor lower bounds. Figure 3.14 shows several iterations of the Shubert-Piyavskii method.

```julia
function shubert_piyavskii(f, a, b, l, e)  
x1 = (a+b)/2  
pts = [(x=x1, y=f(x1))]  
 $\Delta = \text{Inf}$   
while  $\Delta > \epsilon$   
best = (i=0, x=0.0, y=Inf)  
# consider leftmost point  
y = pts[1].y - l*(pts[1].x-a)  
if y < best.y  
best = (i=1, x=a, y=y)  
end  
# consider rightmost point  
y = pts[end].y - l*(b-pts[end].x)  
if y < best.y  
best = (i=length(pts)+1, x=b, y=y)  
end  
# consider interior points  
for i in 2:length(pts)  
A, B = pts[i-1], pts[i]  
t = ((A.y - B.y) - l*(A.x - B.x)) / (2l)  
P = (x=A.x + t, y=A.y - t*l)  
if P.y < best.y  
best = (i=i, x=P.x, y=P.y)  
end  
end  
insert!(pts, best.i, (x=best.x, y=f(best.x)))  
 $\Delta = \text{pts[best.i].y -best.y}$   
end  
return pts[argmin(P.y for P in pts)]  
end
```

Algorithm 3.5. The Shubert-Piyavskii method to be run on univariate function  $f$ , with bracketing interval  $a < b$  and Lipschitz constant  $l$ . The algorithm runs until the update is less than the tolerance  $\epsilon$ . The method returns the best point, which includes its  $x$  location and  $y$  value.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0b47f69d4dbb9209d7dc9e351a2c0b41f4594c05f35f503d59868515efb341f8.jpg)  
Figure 3.14. Nine iterations of the Shubert-Piyavskii method proceeding left to right and top to bottom. The blue lines are uncertainty regions in which the global minimum could lie.

# 3.7 Bisection Method

The bisection method (algorithm 3.6) can be used to find roots of a function, or points where the function is zero. Such root-finding methods can be used for optimization by applying them to the derivative of the objective, locating where  $f'(x) = 0$ . In general, we must ensure that the resulting points are indeed local minima.

The bisection method maintains a bracket  $[a, b]$  in which at least one root is known to exist. If  $f$  is continuous on  $[a, b]$ , and there is some  $y \in [f(a), f(b)]$ , then the intermediate value theorem stipulates that there exists at least one  $x \in [a, b]$ , such that  $f(x) = y$  as shown in figure 3.15. It follows that a bracket  $[a, b]$  is guaranteed to contain a zero if  $f(a)$  and  $f(b)$  have opposite signs.

The bisection method cuts the bracketed region in half with every iteration. The midpoint  $(a + b) / 2$  is evaluated, and the new bracket is formed from the midpoint and whichever side that continues to bracket a zero. We can terminate immediately if the midpoint evaluates to zero. Otherwise we can terminate after a fixed number of iterations. Figure 3.16 shows four iterations of the bisection method. This method is guaranteed to converge within  $\epsilon$  of  $x^{*}$  within  $\lg \left( \frac{|b - a|}{\epsilon} \right)$  iterations, where  $\lg$  denotes the base 2 logarithm.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0e9bfc606263a96138ba13d4b5c69af08881ee7f85e955e6022c9660563ae4a9.jpg)

Root-finding algorithms like the bisection method require starting intervals  $[a, b]$  on opposite sides of a zero. That is,  $\mathrm{sign}(f'(a)) \neq \mathrm{sign}(f'(b))$ , or equivalently,  $f'(a)f'(b) \leq 0$ . Algorithm 3.7 provides a method for automatically determining such an interval. It starts with a guess interval  $[a, b]$ . So long as the interval is invalid, its width is increased by a constant factor. Doubling the interval size is a common choice. This method will not always succeed as shown in figure 3.17. Functions that have two nearby roots can be missed, causing the interval to infinitely increase without termination.

The Brent-Dekker method is an extension of the bisection method. It is a root-finding algorithm that combines elements of the secant method (section 6.2) and inverse quadratic interpolation. It has reliable and fast convergence properties, and

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/759dcb0f1b8121476aac3dcc02082f9ce91f10c124996d9e49151a79c0e68d9a.jpg)  
Figure 3.15. A horizontal line drawn from any  $y \in [f(a), f(b)]$  must intersect the graph at least once.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2a0c615a6728c39095309c4e2fcb845702ea7fb90f1337e10d959b286e1c9724.jpg)  
Figure 3.16. Four iterations of the bisection method. The horizontal line corresponds to  $f^{\prime}(x) = 0$ . Note that multiple roots exist within the initial bracket.  
Figure 3.17. A bracketing method initialized such that it straddles the two roots in this figure will expand forever, never to find a sign change. Also, if the initial interval is between the two roots, doubling the interval can cause both ends of the interval to simultaneously pass the two roots.

```matlab
function bisection(f', a, b, e)  
if a > b; a, b = b, a; end # ensure a < b  
ya, yb = f'(a), f'(b)  
if ya == 0; b = a; end  
if yb == 0; a = b; end  
while b - a > e  
x = (a + b) / 2  
y = f'(x)  
if y == 0  
a, b = x, x  
elseif sign(y) == sign(ya)  
a = x  
else  
b = x  
end  
return (a, b)  
end
```

Algorithm 3.6. The bisection algorithm, where  $f'$  is the derivative of the univariate function we seek to optimize. We have  $a < b$  that bracket a zero of  $f'$ . The interval width tolerance is  $\epsilon$ . Calling bisection returns the new bracketed interval  $[a,b]$  as a tuple.

The prime character ' is not an apostrophe. Thus,  $f'$  is a variable name rather than a transposed vector  $f$ . The symbol can be created by typing \prime and hitting tab.

```matlab
function bracket_sign_change(f', a, b; k=2)  
if a > b; a, b = b, a; end # ensure a < b  
center, half_width = (b+a)/2, (b-a)/2  
while f'(a)*f'(b) > 0  
    half_width *= k  
    a = center - half_width  
    b = center + half_width  
end  
return (a, b)  
end
```

Algorithm 3.7. An algorithm for finding an interval in which a sign change occurs. The inputs are the real-valued function  $f'$  defined on the real numbers, and starting interval  $[a,b]$ . It returns the new interval as a tuple by expanding the interval width until there is a sign change between the function evaluated at the interval bounds. The expansion factor  $k$  defaults to 2.

it is the univariate optimization algorithm of choice in many popular numerical optimization packages. $^6$

# 3.8 Summary

- Many optimization methods shrink a bracketing interval, including Fibonacci search, golden section search, and quadratic fit search.  
- The Shubert-Piyavskii method outputs a set of bracketed intervals containing the global minima, given the Lipschitz constant.  
- Root-finding methods like the bisection method can be used to find where the derivative of a function is zero.

# 3.9 Exercises

Exercise 3.1. Give an example of a problem when Fibonacci search is preferred over the bisection method.

Solution: Fibonacci search is preferred when derivatives are not available.

Exercise 3.2. What is a drawback of the Shubert-Piyavskii method?

Solution: The Shubert-Piyavskii method needs the Lipschitz constant, which may not be known.

Exercise 3.3. Give an example of a nontrivial function where quadratic fit search would identify the minimum correctly once the function values at three distinct points are available.

Solution:  $f(x) = x^{2}$ . Since the function is quadratic, after three evaluations, the quadratic model will represent this function exactly.

Exercise 3.4. Suppose we have  $f(x) = x^{2} / 2 - x$ . Apply the bisection method to find an interval containing the minimizer of  $f$  starting with the interval [0, 1000]. Execute three steps of the algorithm.

Solution: We can use the bisection method to find the roots of  $f'(x) = x - 1$ . After the first update, we have [0, 500]. Then, [0, 250]. Finally, [0, 125].

Exercise 3.5. Suppose we have a function  $f(x) = (x + 2)^{2}$  on the interval [0,1]. Is 2 a valid Lipschitz constant for  $f$  on that interval?

The details of this algorithm can be found in R. P. Brent, Algorithms for Minimization Without Derivatives. Prentice Hall, 1973. The algorithm is an extension of the work by T.J. Dekker, "Finding a Zero by Means of Successive Linear Interpolation," in Constructive Aspects of the Fundamental Theorem of Algebra, B. Dejon and P. Henrici, eds., Interscience, 1969.

Solution: No, the Lipschitz constant must bound the derivative everywhere on the interval, and  $f^{\prime}(1) = 2(1 + 2) = 6$ .

Exercise 3.6. Suppose we have a unimodal function defined on the interval [1,32]. After three function evaluations of our choice, will we be able to narrow the optimum to an interval of at most length 10? Why or why not?

Solution: No. The best you can do is use Fibonacci Search and shrink the uncertainty by a factor of 3; that is, to  $(32 - 1) / 3 = 10^{\frac{1}{3}}$ .

# 4 Local Descent

Up to this point, we have focused on optimization involving a single design variable. This chapter introduces a general approach to optimization involving multivariate functions, or functions with more than one variable. The focus of this chapter is on how to use local models to incrementally improve a design point until some convergence criterion is met. We begin by discussing methods that, at each iteration, choose a descent direction based on a local model and then choose a step size. We then discuss methods that restrict the step to be within a region where the local model is believed to be valid. This chapter concludes with a discussion of convergence conditions. The next two chapters will discuss how to use first- and second-order models built from gradient or Hessian information.

# 4.1 Descent Direction Iteration

A common approach to optimization is to incrementally improve a design point  $\mathbf{x}$  by taking a step that minimizes the objective value based on a local model. The local model may be obtained, for example, from a first- or second-order Taylor approximation. Optimization algorithms that follow this general approach are referred to as descent direction methods. They start with a design point  $\mathbf{x}^{(1)}$  and then generate a sequence of points, sometimes called iterates, to converge to a local minimum.<sup>1</sup>

The iterative descent direction procedure involves the following steps:

1. Check whether  $\mathbf{x}^{(k)}$  satisfies the termination conditions. If it does, terminate; otherwise proceed to the next step.  
2. Determine the descent direction  $\mathbf{d}^{(k)}$  using local information such as the gradient or Hessian. Some algorithms assume  $\| \mathbf{d}^{(k)}\| = 1$ , but others do not.

1 The choice of  $\mathbf{x}^{(1)}$  can affect the success of the algorithm in finding a minimum. Domain knowledge is often used to choose a reasonable value. When that is not available, we can search over the design space using the techniques that will be covered in chapter 16. In some cases, we might have a solution from a previous optimization run on either the same or related problem that we can use as a starting point. This prior solution can be used to warm start the optimization algorithm.

3. Determine the step factor  $\alpha^{(k)}$ . A step factor is sometimes referred to as a learning rate, especially in the context of machine learning applications. Some algorithms attempt to optimize the step factor to maximally decrease  $f$ .  
4. Compute the next design point according to:

$$
\mathbf {x}^{(k + 1)} \leftarrow \mathbf {x}^{(k)} + \alpha^{(k)} \mathbf {d}^{(k)} \tag {4.1}
$$

There are many different optimization methods, each with their own ways of determining  $\alpha$  and  $\mathbf{d}$ . They typically follow a similar iterative structure, as implemented in algorithm 4.1.

```julia
abstract type DescentMethod end  
function iterated_descent(M::DescentMethod, f,  $\nabla f$ , x, k_max)  
init!(M, f,  $\nabla f$ , x)  
for k in 1:k_max  
    x = step!(M, f,  $\nabla f$ , x)  
end  
return x  
end
```

# 4.2 Step Factors

The step factor  $\alpha^{(k)}$  influences the size of the step taken in the descent direction. The step size at iteration  $k$  is the distance from  $\mathbf{x}^{(k)}$  to  $\mathbf{x}^{(k + 1)}$ . If  $\| \mathbf{d}^{(k)}\| = 1$ , then the step size is the same as the step factor. Otherwise, the step size is  $\alpha^{(k)}\|\mathbf{d}^{(k)}\|$ .

Some algorithms use a fixed step factor. Large steps will tend to result in faster convergence but risk overshooting the minimum. Smaller steps tend to be more stable but can result in slower convergence.

Alternatively, we can decay the step factor over time, with the intuition that larger steps are taken early in the optimization process to quickly arrive in the general proximity of a solution and smaller steps are later taken to refine the solution to a true minimum. One common decay scheme is to multiply the step factor by a constant  $\gamma$  at each iteration, where  $0 < \gamma \leq 1$ :

$$
\alpha^{(k)} \leftarrow \gamma \alpha^{(k - 1)} \tag {4.2}
$$

2 T. Hastie, R. Tibshirani, and J. H. Friedman, The Elements of Statistical Learning, 2nd ed. Springer, 2017.

Algorithm 4.1. An iterated descent method for minimizing  $f$  starting at design  $x$  using information from the gradient  $\nabla f$ . Other methods might additionally use the Hessian,  $\mathsf{H}$ . This implementation operates on an abstract DescentMethod object that should support both an init! call for initialization and a step! implementation for executing a single descent step. It executes  $k_{\max}$  iterations. Other termination conditions are presented in section 4.6.

This scheme ensures that the step factor decreases over time, which can help the algorithm converge to a minimum. Smaller values for  $\gamma$  result in faster decay of the step factor, resulting in a faster shift from large steps in design space to smaller, fine-tuning steps.

# 4.3 Line Search

Instead of using a fixed or decaying step factor, we can use line search to directly optimize the step factor to minimize the objective function given a descent direction  $\mathbf{d}$ :

$$
\underset{\alpha} {\text{minimize}} f (\mathbf {x} + \alpha \mathbf {d}) \tag {4.3}
$$

Line search is a univariate optimization problem, a class of problems covered in chapter 3.4 To inform the search, we can use the derivative of the line search objective, which is simply the directional derivative along  $\mathbf{d}$  at  $\mathbf{x} + \alpha \mathbf{d}$ . Line search is demonstrated in example 4.1 and implemented in algorithm 4.2.

```prolog
function line_search(f, x, d)  
    objective =  $\alpha \rightarrow f(x + \alpha * d)$   
    a, b = bracket_minimum(objective)  
     $\alpha =$  minimize(objective, a, b)  
    return x +  $\alpha * d$   
end
```

One disadvantage of conducting a line search at each step is the computational cost of optimizing  $\alpha$  to a high degree of precision. Instead, it is common to quickly find a reasonable value and then move on, selecting  $\mathbf{x}^{(k + 1)}$ , and then picking a new direction  $\mathbf{d}^{(k + 1)}$ .

# 4.4 Approximate Line Search

It is often more computationally efficient to perform more iterations of a descent method than to do exact line search at each iteration, especially if the function and derivative calculations are expensive. Many of the methods discussed so far can benefit from using approximate line search to find a suitable step size with a small number of evaluations. This section discusses methods for searching for a step size that satisfies certain criteria known as the Wolfe conditions.53 The convergence of various descent algorithms are discussed in detail by J. Nocedal and S.J. Wright, Numerical Optimization, 2nd ed. Springer, 2006.

4 The Brent-Dekker method, mentioned in the previous chapter, is a commonly used univariate optimization method.

Algorithm 4.2. A method for conducting a line search, which finds the optimal step factor along a descent direction  $\mathsf{d}$  from design point  $\mathbf{x}$  to minimize function  $f$ . The minimize function can be implemented using a univariate optimization algorithm such as the Brent-Dekker method.

5 These conditions are named for the American mathematician Philip Wolfe (1927-2016). P. Wolfe, "Convergence Conditions for Ascent Methods," SIAM Review, vol. 11, no. 2, pp. 226-235, 1969. P. Wolfe, "Convergence Conditions for Ascent Methods. II: Some Corrections," SIAM Review, vol. 13, no. 2, pp. 185-188, 1971.

Consider conducting a line search on  $f(x_{1},x_{2},x_{3}) = \sin (x_{1}x_{2}) + \exp (x_{2} + x_{3}) - x_{3}$  from  $\mathbf{x} = [1,2,3]$  in the direction  $\mathbf{d} = [0, - 1, - 1]$ . The corresponding optimization problem is:

$$
\underset{\alpha} {\text{minimize}} \sin ((1 + 0 \alpha) (2 - \alpha)) + \exp ((2 - \alpha) + (3 - \alpha)) - (3 - \alpha)
$$

which simplifies to:

$$
\underset{\alpha} {\text{minimize}} \sin (2 - \alpha) + \exp (5 - 2 \alpha) + \alpha - 3
$$

The minimum is at  $\alpha \approx 3.127$  with  $\mathbf{x} \approx [1, -1.126, -0.126]$ .

Example 4.1. Line search used to minimize a function along a descent direction.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/89a7fdf0a87349d68dae8b20182800943db36689b803143229887618c4a0a6fd.jpg)

# 4.4.1 Sufficient Decrease

The first Wolfe condition requires that the step factor  $\alpha$  cause a sufficient decrease in the objective function value. If the descent direction is  $\mathbf{d}^{(k)}$  at step  $k$ , then a first order approximation of the objective function at  $\mathbf{x}^{(k + 1)} = \mathbf{x}^{(k)} + \alpha \mathbf{d}^{(k)}$  is given by:

$$
f \left(\mathbf {x}^{(k + 1)}\right) = f \left(\mathbf {x}^{(k)}\right) + \alpha \nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k)}\right) \tag {4.4}
$$

Since we have a valid descent direction, the directional derivative  $\nabla_{\mathbf{d}^{(k)}}f(\mathbf{x}^{(k)})$  is negative. Hence, if we take a sufficiently small step size, the objective function value at the next design point will be less than the objective function value at the current design point.

Although we could choose a step factor that causes any decrease in the objective function value, it is often beneficial to require a certain amount of decrease related to what could be expected by a first-order approximation. We can define this amount as a fraction  $\beta \in [0,1]$  of the decrease predicted by the first-order approximation, resulting in the condition:

$$
f \left(\mathbf {x}^{(k + 1)}\right) \leq f \left(\mathbf {x}^{(k)}\right) + \beta \alpha \nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k)}\right) \tag {4.5}
$$

Figure 4.1 illustrates this condition. If  $\beta = 0$ , then any decrease is acceptable. If  $\beta = 1$ , then the decrease has to be at least as much as what would be predicted by a first-order approximation.

To find an  $\alpha$  that meets this condition, we can start with a large value and decrease it by a constant factor until the sufficient decrease condition is satisfied.

This condition is sometimes referred to as the Armijo condition.

7 A typical value for  $\beta$  is  $10^{-4}$

Figure 4.1. The sufficient decrease condition, the first Wolfe condition, can always be satisfied by a sufficiently small step size along a descent direction.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0024ce1cf8a30fac8599bc06a9d01a05832c4d6a4e1ad7314aac474e683bd6ff.jpg)  
Also known as Armijo line search. L. Armijo, "Minimization of Functions Having Lipschitz Continuous First Partial Derivatives," Pacific Journal of Mathematics, vol. 16, no. 1, pp. 1-3, 1966.

This algorithm is known as backtracking line search because of how it backtracks along the descent direction. Backtracking line search is shown in figure 4.2 and implemented in algorithm 4.3. We walk through the procedure in example 4.2.

```matlab
function backtracking_line_search(f,  $\nabla f$  ,x,d,  $\alpha$  .  $p = 0.5$  ，  $\beta = 1e - 4$  y,g=f(x),Vf(x) while  $f(x + \alpha *d) > y + \beta *\alpha *(g\cdot d)$ $\alpha * = p$  end return a   
end
```

# 4.4.2 Curvature Condition

The second Wolfe condition, called the curvature condition, requires the directional derivative at the next iterate to be shallower (less negative):

$$
\nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k + 1)}\right) \geq \sigma \nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k)}\right) \tag {4.6}
$$

where the parameter  $\sigma$  controls how shallow the next directional derivative must be. The intuition is that we should probably take a larger step if we are still descending steeply so that we can get closer to a point where the first-order necessary condition for optimality is satisfied.[9]

An alternative to the curvature condition is the strong curvature condition, which is a more restrictive criterion in that the slope is also required not to be too positive:

$$
\left| \nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k + 1)}\right) \right| \leq - \sigma \nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k)}\right) \tag {4·7}
$$

Algorithm 4.3. The backtracking line search algorithm, which takes objective function  $f$ , its gradient  $\nabla f$ , the current design point  $x$ , a descent direction  $d$ , and the maximum step size  $\alpha$ . We can optionally specify the reduction factor  $p$  and the first Wolfe condition parameter  $\beta$ . Note that the cdot character - aliases to the dot function such that  $a \cdot b$  is equivalent to  $\mathrm{dot}(a, b)$ . The symbol can be created by typing  $\backslash$  cdot and hitting tab.

9 It is common to set  $\beta <  \sigma <  1$  A typical value for  $\sigma$  is O.1 when used with the conjugate gradient method and O.9 when used with Newton's method. The conjugate gradient method is introduced in section 5.2, and Newton's method is introduced in section 6.1.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3efd70bcafcb93fda4d50e7c986adae10728617adca1d40ada98943cb804c0ec.jpg)  
Figure 4.2. Backtracking line search used on the Rosenbrock function (appendix B.6). The black lines show the seven iterations taken by the descent method and the red lines show the points considered during each line search.

Figure 4.3 illustrates this condition.

Together, the sufficient decrease condition and the curvature condition form the Wolfe conditions. The sufficient decrease condition with the strong curvature condition form the strong Wolfe conditions. We can prove that step sizes that satisfy either set of Wolfe conditions always exist. Furthermore, aside from adversarial cases such as happening to land at a saddle point, repeated line searches that always satisfy either Wolfe conditions are guaranteed to converge to a local minimum.[10]

Satisfying the strong Wolfe conditions requires a more complicated algorithm called strong backtracking line search (algorithm 4.4). The method operates in two phases. The first phase, the bracketing phase, tests successively larger step sizes to bracket an interval  $[\alpha^{(k-1)}, \alpha^{(k)}]$  guaranteed to contain step lengths satisfying the Wolfe conditions.

An interval guaranteed to contain step lengths satisfying the Wolfe conditions is found when one of the following conditions hold:

$$
f (\mathbf {x} + \alpha \mathbf {d}) \geq f (\mathbf {x}) \tag {4.8}
$$

$$
f (\mathbf {x} + \alpha \mathbf {d}) > f (\mathbf {x}) + \beta \alpha \nabla_{\mathbf {d}} f (\mathbf {x}) \tag {4.9}
$$

$$
\nabla f (\mathbf {x} + \alpha \mathbf {d}) \geq \mathbf {0} \tag {4.10}
$$

10 These proofs stipulate that the gradient always exists, is Lipschitz continuous, and that  $f$  is bounded below. For the original proofs, see P. Wolfe, "Convergence Conditions for Ascent Methods," SIAM Review, vol. 11, no. 2, pp. 226-235, 1969.

11J. Nocedal and S.J. Wright, Numerical Optimization, 2nd ed. Springer, 2006.

Consider approximate line search on  $f(x_{1},x_{2}) = x_{1}^{2} + x_{1}x_{2} + x_{2}^{2}$  from  $\mathbf{x} = [1,2]$  in the direction  $\mathbf{d} = [-1, - 1]$ , using a maximum step size of 10, a reduction factor of 0.5, and a first Wolfe condition parameter of  $\beta = 10^{-4}$ .

We check whether the maximum step size satisfies the first Wolfe condition, where the gradient at  $\mathbf{x}$  is  $\mathbf{g} = [4,5]$ :

$$
\begin{array}{l} f (\mathbf {x} + \alpha \mathbf {d}) \leq f (\mathbf {x}) + \beta \alpha (\mathbf {g}^{\top} \mathbf {d}) \\ f ([ 1, 2 ] + 10 \cdot [ - 1, - 1 ]) \leq 7 + 10^{- 4} \cdot 10 \cdot [ 4, 5 ] ^{\top} [ - 1, - 1 ] \\ 21 7 \leq 6. 99 1 \\ \end{array}
$$

Because it is not satisfied, the step size is multiplied by 0.5 to obtain 5, and the first Wolfe condition is checked again:

$$
\begin{array}{l} f ([ 1, 2 ] + 5 \cdot [ - 1, - 1 ]) \leq 7 + 10^{- 4} \cdot 5 \cdot [ 4, 5 ] ^{\top} [ - 1, - 1 ] \\ 37 \leq 6. 99 6 \\ \end{array}
$$

Because it is not satisfied, the step size is multiplied by 0.5 to obtain 2.5, and the first Wolfe condition is checked again:

$$
\begin{array}{l} f ([ 1, 2 ] + 2. 5 \cdot [ - 1, - 1 ]) \leq 7 + 10^{- 4} \cdot 2. 5 \cdot [ 4, 5 ] ^{\top} [ - 1, - 1 ] \\ 3. 25 \leq 6. 99 8 \\ \end{array}
$$

The first Wolfe condition is satisfied, and we terminate the line search.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dbc9ee0b5f9d85a796bba22a5bd02595d83229cf81e42e95811327d6f00c5002.jpg)  
Figure 4.3. Regions where the strong curvature condition is satisfied.

Example 4.2. An example of backtracking line search, an approximate line search method.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9a695e8442616608432ae8441c14832505df9aca8ef31133173104410c6497fd.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6721e37b54ee258b9e0db2f90edef499841a0666dd06e70ce4a233a89b01bb12.jpg)  
Figure 4.4. The interval  $[0, \alpha]$  is guaranteed to bracket an interval containing a step length satisfying the strong Wolfe conditions when any of these three conditions is true.

Satisfying equation (4.9) is equivalent to violating the first Wolfe condition, thereby ensuring that shrinking the step length will guarantee a satisfactory step length. Similarly, equation (4.8) and equation (4.10) guarantee that the descent step has overshot a local minimum, and the region between must therefore contain a satisfactory step length.

Figure 4.4 shows where each bracketing condition is true for an example line search. The figure shows bracket intervals  $[0, \alpha]$ , whereas strong backtracking line search successively increases the step length to obtain a bracketing interval  $[\alpha^{(k-1)}, \alpha^{(k)}]$ .

In the zoom phase, we shrink the interval to find a step size satisfying the strong Wolfe conditions. The shrinking can be done using the bisection method (section 3.7), updating the interval boundaries according to the same interval conditions. This process is shown in figure 4.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1ddbc04add5d10c8d58141974f212800f8ea4b0bb7141d8fa4db8b3c7a6cb91c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dd87f66f6521f092f776263526fdf0314b59cb12054710150ccc123ad1852a24.jpg)  
Figure 4.5. The first phase of strong backtracking line search is used to bracket an interval. In this case, the condition  $\nabla f(\mathbf{x} + \alpha^{(4)}\mathbf{d})\geq \mathbf{0}$  is triggered, causing the bracketing interval to be  $[\alpha^{(3)},\alpha^{(4)}]$ . The zoom phase searches within this interval using bisection to find a suitable step factor, resulting in  $\alpha^{(5)}$ .

```matlab
function strong_backtracking(f,  $\nabla f$  , x, d;  $\alpha = 1$ $\beta = 1e - 4$ $\sigma = 0.1$  y0,g0,y prev,  $\alpha_{-}$  prev  $=$  f(x),  $\nabla f(x)\cdot d$  ,NaN,0 alo,ahi  $=$  NaN,NaN   
#bracket phase while true  $y = f(x + \alpha *d)$  if  $y > y0 + \beta *\alpha *\mathbf{g}0||$  (!isnan(yprev)&&y≥yprev) alo, ahi  $=$  a prevail, a break end g  $=$ $\nabla f(x + \alpha *d)\cdot d$  if abs(g)  $\leq -\sigma *g0$  return a elseif  $g\geq 0$  alo,ahi  $=$  a,a prevail break end y prevail,  $\alpha_{-}$  prev,  $\alpha = y$  ,a,2a   
end   
#zoom phase ylo  $=$  f(x + a1o*d) while true  $\alpha = (\alpha \text{lo} +\alpha \text{hi}) / 2$  y  $= f(x + \alpha *d)$  if  $y > y0 + \beta *\alpha *\mathbf{g}0||$  y≥ylo ahi  $=$  a else g  $=\nabla f(x+\alpha*d)\cdot d$  if abs(g)  $\leq -\sigma *g0$  return a elseif g*(ahi - a1o)  $\geq 0$  ahi  $=$  a1o end a1o  $=\alpha$    
end
```

Algorithm 4.4. Strong backtracking approximate line search for satisfying the strong Wolfe conditions. It takes as input the objective function  $f$ , the gradient function  $\nabla f$ , the design point  $x$  and direction  $d$  from which line search is conducted, an initial step size  $\alpha$ , and the Wolfe condition parameters  $\beta$  and  $\sigma$ . The algorithm's bracket phase first brackets an interval containing a step size that satisfies the strong Wolfe conditions. It then reduces this bracketed interval in the zoom phase until a suitable step size is found. We interpolate with bisection, but other schemes can be used.

# 4.5 Trust Region Methods

Descent methods can place too much trust in their first- or second-order information, which can result in excessively large steps or premature convergence. A trust region<sup>12</sup> is the local area of the design space where the local model is believed to be reliable. A trust region method, or restricted step method, maintains a local model of the trust region that both limits the step taken by traditional line search and predicts the improvement associated with taking the step. If the improvement closely matches the predicted value, the trust region is expanded. If the improvement deviates from the predicted value, the trust region is contracted.<sup>13</sup> Figure 4.6 shows a design point centered within a circular trust region.

Trust region methods first choose the maximum step size and then the step direction, which is in contrast with line search methods that first choose a step direction and then optimize the step size. A trust region approach finds the next step by minimizing a model of the objective function  $\hat{f}$  over a trust region centered on the current design point  $\mathbf{x}$ . An example of  $\hat{f}$  is a second-order Taylor approximation (see appendix C.2). The radius of the trust region,  $\delta$ , is expanded and contracted based on how well the model predicts function evaluations. The next design point  $\mathbf{x}'$  is obtained by solving:

$$
\underset{\mathbf {x}^{\prime}} {\text{minimize}} \quad \hat {f} \left(\mathbf {x}^{\prime}\right) \tag {4.11}
$$

$$
\text{subject} \quad \| \mathbf {x} - \mathbf {x}^{\prime} \| \leq \delta
$$

where the trust region is defined by the positive radius  $\delta$  and a vector norm. $^{14}$  The equation above is a constrained optimization problem, which is covered in chapter 10.

The trust region radius  $\delta$  is expanded or contracted based on the local model's predictive performance. Trust region methods compare the predicted improvement  $\Delta y_{\mathrm{pred}} = f(\mathbf{x}) - \hat{f} (\mathbf{x}^{\prime})$  to the actual improvement  $\Delta y_{\mathrm{act}} = f(\mathbf{x}) - f(\mathbf{x}^{\prime})$ :

$$
\eta = \frac{\text{actualimprovement}}{\text{predictedimprovement}} = \frac{f (\mathbf {x}) - f \left(\mathbf {x}^{\prime}\right)}{f (\mathbf {x}) - \hat {f} \left(\mathbf {x}^{\prime}\right)} \tag {4.12}
$$

The ratio  $\eta$  is close to 1 when the predicted step size matches the actual step size. If the ratio is too small, such as below a threshold  $\eta_{1}$ , then the improvement is considered sufficiently less than expected, and the trust region radius is scaled down by a factor  $\gamma_{1} < 1$ . If the ratio is sufficiently large, such as above a threshold

12 K. Levenberg, "A Method for the Solution of Certain Non-Linear Problems in Least Squares," Quarterly of Applied Mathematics, vol. 2, no. 2, pp. 164-168, 1944.  
13 A recent review of trust region methods is provided by Y.X. Yuan, "Recent Advances in Trust Region Algorithms," Mathematical Programming, vol. 151, no. 1, pp. 249-281, 2015.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3ddc49407cf18dbfb43fba65e4941df08bef689d4055f7d080e63d6e368c9a0f.jpg)  
Figure 4.6. Trust region methods constrain the next step to lie within a local region. The trusted region is expanded and contracted based on the predictive performance of models of the objective function.

14 There are a variety of methods for solving equation (4.11) efficiently. For an overview of the trust region method applied to quadratic models, see D.C. Sorensen, "Newton's Method with a Model Trust Region Modification," SIAM Journal on Numerical Analysis, vol. 19, no. 2, pp. 409-426, 1982.

$\eta_{2}$ , then our prediction is considered accurate, and the trust region radius is scaled up by a factor  $\gamma_{2} > 1$ . Algorithm 4.5 provides an implementation and figure 4.7 demonstrates the optimization procedure.

```julia
mutable struct TrustRegionDescent  $\vDash$  : DescentMethod   
 $\delta$  # trust region radius   
 $\eta 1$  # improvement ratio down-scale threshold   
 $\eta 2$  # improvement ratio up-scale threshold   
y1 # down-scale multiplier   
y2 # up-scale multiplier   
end   
function step!(M::TrustRegionDescent, f,  $\nabla f$  , H, x)   
 $\delta$  , n1, n2, y1, y2 = M.8, M.n1, M.n2, M.y1, M.y2   
 $x^{\prime}$ $y^\prime =$  solve_trust_region_subproblem(  $\nabla f$  , H, x,  $\delta$  )   
 $\eta = (f(x) - f(x')) / (f(x) - y')$  # improvement ratio   
if  $\eta <  \eta 1$    
M.8  $\ast = \gamma 1$  # scale down trust region   
else if  $\eta >\eta 2$    
M.8  $\ast = \gamma 2$  # scale up trust region   
end   
return x' # accept new point   
end   
return x   
end   
using Convex, ECOS   
function solve_trust_region_subproblem(  $\nabla f$  , H, x0,  $\delta$  )   
 $x =$  Variable(length(x0))   
 $f = \nabla f(x0)\cdot (x - x0) +$  quadform(x-x0, H(x0))/2   
p = minimize(f, norm(x-x0) ≤  $\delta$  )   
solve!(p, ECOS.Optimizer)   
return (x.value, p optval)   
end
```

Algorithm 4.5. The trust region descent method, where  $f$  is the objective function,  $\nabla f$  produces the derivative,  $H$  produces the Hessian,  $x$  is an initial design point, and  $k_{\text{max}}$  is the number of iterations. The optional parameters  $n1$  and  $n2$  determine when the trust region radius  $\delta$  is increased or decreased, and  $\gamma 1$  and  $\gamma 2$  control the magnitude of the change. An implementation for solve_trust_region_subproblem must be provided that solves equation (4.11). We have provided an example implementation that uses a second-order Taylor approximation about  $x0$  with a circular trust region that assumes the Hessian is positive definite.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1d466a857b2794948902e3a456611ead2328f66fc5e647d2650d82ebc715953a.jpg)  
Figure 4.7. Trust region optimization used on the Rosenbrock function (appendix B.6).

Trust regions need not be circular. In some cases, certain directions may have higher trust than others. A norm can be constructed to produce elliptical regions as shown in figure 4.8:

$$
\left\| \mathbf {x} - \mathbf {x}_{0} \right\| _{\mathbf {E}} = \left(\mathbf {x} - \mathbf {x}_{0}\right) ^{\top} \mathbf {E} \left(\mathbf {x} - \mathbf {x}_{0}\right) \tag {4.13}
$$

The ellipse matrix  $\mathbf{E}$  can be updated with each descent iteration, which can involve more complicated adjustments than scaling the trusted region.[15]

# 4.6 Termination Conditions

There are four common termination conditions for descent direction methods:

- Maximum iterations. We may want to terminate when the number of iterations  $k$  exceeds some threshold  $k_{\mathrm{max}}$ . Alternatively, we might want to terminate once a maximum amount of elapsed time is exceeded.

$$
k > k_{\max } \tag {4.14}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a9493010a4e445e8cf478fd14253a44a355d3be8bbc9f3f596f51005d94d23a5.jpg)  
Figure 4.8. Trust region optimization can be adaptive to use elliptical trust regions. In this case, the trust region is elongated in the direction of the blue arrow and contracted in the direction of the red arrow.

15 Additional detail is provided by J. Nocedal and S.J. Wright, "Trust-Region Methods," in Numerical Optimization. Springer, 2006, pp. 66-100.

- Absolute improvement. This termination condition looks at the change in the function value over subsequent steps. If the change is smaller than a given threshold, it will terminate:

$$
f \left(\mathbf {x}^{(k)}\right) - f \left(\mathbf {x}^{(k + 1)}\right) <   \epsilon_{a} \tag {4.15}
$$

- Relative improvement. This termination condition also looks at the change in function value but uses the step factor relative to the current function value:

$$
f \left(\mathbf {x}^{(k)}\right) - f \left(\mathbf {x}^{(k + 1)}\right) <   \epsilon_{r} | f \left(\mathbf {x}^{(k)}\right) | \tag {4.16}
$$

- Gradient magnitude. We can also terminate based on the magnitude of the gradient:

$$
\| \nabla f (\mathbf {x}^{(k + 1)}) \| <   \epsilon_{g} \tag {4.17}
$$

In cases where multiple local minima are likely to exist, it can be beneficial to incorporate random restarts after our termination conditions are met where we restart our local descent method from randomly selected initial points.

# 4.7 Summary

- Descent direction methods incrementally descend toward a local optimum.  
- Univariate optimization can be applied during line search.  
- Approximate line search can be used to identify appropriate descent step sizes.  
- Trust region methods constrain the step to lie within a local region that expands or contracts based on predictive accuracy.  
- Termination conditions for descent methods can be based on criteria such as the change in the objective function value or magnitude of the gradient.

# 4.8 Exercises

Exercise 4.1. Why is it important to have more than one termination condition?

Solution: Consider running a descent method on  $f(x) = 1 / x$  for  $x > 0$ . The minimum does not exist and the descent method will forever proceed in the positive  $x$  direction with ever-increasing step sizes. Thus, only relying on a step-size termination condition would cause the method to run forever. Also terminating based on gradient magnitude would cause it to terminate.

A descent method applied to  $f(x) = -x$  will also forever proceed in the positive  $x$  direction. The function is unbounded below, so neither a step-size termination condition nor a gradient magnitude termination condition would trigger. It is common to include an additional termination condition to limit the number of iterations.

Exercise 4.2. The first Wolfe condition requires

$$
f \left(\mathbf {x}^{(k)} + \alpha \mathbf {d}^{(k)}\right) \leq f \left(\mathbf {x}^{(k)}\right) + \beta \alpha \nabla_{\mathbf {d}^{(k)}} f \left(\mathbf {x}^{(k)}\right)
$$

What is the maximum step length  $\alpha$  that satisfies this condition, given that  $f(\mathbf{x}) = 5 + x_1^2 + x_2^2$ ,  $\mathbf{x}^{(k)} = [-1, -1]$ ,  $\mathbf{d} = [1,0]$ , and  $\beta = 10^{-4}$ ?

Solution: Applying the first Wolfe condition to our objective function yields  $6 + (-1 + \alpha)^2 \leq 7 - 2\alpha \cdot 10^{-4}$ , which can be simplified to  $\alpha^2 - 2\alpha + 2 \cdot 10^{-4}\alpha \leq 0$ . This equation can be solved to obtain  $\alpha \leq 2(1 - 10^{-4})$ . Thus, the maximum step length is  $\alpha = 1.9998$ .

Exercise 4.3. Second-order methods, covered in chapter 6, use curvature information to perform updates or calculate descent directions. These methods often benefit from positive curvature. How would the Wolfe conditions help line search produce iterates with positive curvature?

Solution: The first Wolfe condition does not directly influence curvature. The second Wolfe condition ensures that the chosen step results in a shallower successor. This directly relates to positive curvature, as positive curvature is the increase of the directional derivative in a given direction.

# 5 First-Order Methods

The previous chapter introduced the general concept of descent direction methods. This chapter discusses a variety of algorithms that use first-order methods to select the appropriate descent direction. First-order methods rely on gradient information to help direct the search for a minimum, which can be obtained using methods outlined in chapter 2.

# 5.1 Gradient Descent

The gradient descent method uses the gradient to select the next descent direction  $\mathbf{d}$ . For convenience, we define the gradient at the  $k$ th iterate  $\mathbf{x}^{(k)}$  to be

$$
\mathbf {g}^{(k)} = \nabla f (\mathbf {x}^{(k)}) \tag {5.1}
$$

The motivation for gradient descent comes from the first-order Taylor series approximation about our current iterate  $\mathbf{x}^{(k)}$ :

$$
f \left(\mathbf {x}^{(k)} + \alpha \mathbf {d}\right) \approx f \left(\mathbf {x}^{(k)}\right) + \alpha \mathbf {d}^{\top} \mathbf {g}^{(k)} \tag {5.2}
$$

We can choose the direction  $\mathbf{d}$  that minimizes this first-order approximation subject to the constraint that  $\| \mathbf{d} \| = 1$ . The direction that minimizes this first-order approximation is the direction of steepest descent, which is simply the direction opposite the gradient:

$$
\mathbf {d}^{(k)} = - \frac{\mathbf {g}^{(k)}}{\| \mathbf {g}^{(k)} \|} \tag {5.3}
$$

Some implementations of gradient descent do not normalize the descent direction. In that case, the step factor  $\alpha$  does not correspond to the step length.

1 We assume the  $L_{2}$  norm here, but steepest descent with respect to other norms correspond to other optimization methods. For example, the  $L_{1}$  norm leads to the coordinate descent method. See Section 9.4 of S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.

Following the direction of steepest descent is guaranteed to lead to improvement, provided that the objective function is smooth, the step factor  $\alpha$  is sufficiently small, and we are not already at a point where the gradient is zero. Our intuition might suggest that we cannot do better than going in the direction of steepest descent, but this is not the case, even when we optimize the step size. Choosing step sizes that maximally decrease  $f$  produces jagged search paths. The next descent direction will always be orthogonal to the current direction, as shown in figure 5.1.

We can show that the next direction is orthogonal to the current direction as follows. If we optimize the step size at each step, we have

$$
\alpha^{(k)} = \underset{\alpha} {\arg \min } f \left(\mathbf {x}^{(k)} + \alpha \mathbf {d}^{(k)}\right) \tag {5.4}
$$

The optimization above implies that the directional derivative equals zero. Using equation (2.9), we have

$$
\nabla f \left(\mathbf {x}^{(k)} + \alpha \mathbf {d}^{(k)}\right) ^{\top} \mathbf {d}^{(k)} = 0 \tag {5.5}
$$

We know

$$
\mathbf {d}^{(k + 1)} = - \frac{\nabla f \left(\mathbf {x}^{(k)} + \alpha \mathbf {d}^{(k)}\right)}{\| \nabla f \left(\mathbf {x}^{(k)} + \alpha \mathbf {d}^{(k)}\right) \|} \tag {5.6}
$$

Hence,

$$
\mathbf {d}^{(k + 1) \top} \mathbf {d}^{(k)} = 0 \tag {5.7}
$$

which means that  $\mathbf{d}^{(k + 1)}$  and  $\mathbf{d}^{(k)}$  are orthogonal.

Narrow valleys aligned with a descent direction are not an issue. When narrow valleys are not aligned with the descent direction, many steps must be taken in order to make progress along the valley's floor as shown in figure 5.1. An implementation of gradient descent is provided by algorithm 5.1.

```julia
struct GradientDescent <: DescentMethod
     $\alpha$  # step factor
end
init!(M::GradientDescent, f,  $\nabla f$ , x) = M
function step!(M::GradientDescent, f,  $\nabla f$ , x)
     $\alpha$ , g = M.x,  $\nabla f(x)$ 
return x -  $\alpha * g$ 
end
```

2 A point where the gradient is zero is called a stationary point.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0c49dcd30c67b9bd0909bb851d0d8862d12838c1afda3a3b81d7c8086a1b0c22.jpg)  
Figure 5.1. Gradient descent can result in zig-zagging in narrow canyons as shown on the Rosenbrock function (appendix B.6).

Algorithm 5.1. The gradient descent method, which follows the direction of gradient descent with a fixed step factor. The step! function produces the next iterate whereas the init function does nothing.

# 5.2 Conjugate Gradient

Gradient descent can perform poorly in narrow valleys. The conjugate gradient method overcomes this issue by borrowing inspiration from methods for optimizing quadratic functions:

$$
\underset{\mathbf {x}} {\operatorname{minimize}} f (\mathbf {x}) = \frac{1}{2} \mathbf {x}^{\top} \mathbf {A x} + \mathbf {b}^{\top} \mathbf {x} + c \tag {5.8}
$$

where  $\mathbf{A}$  is symmetric and positive definite, and thus  $f$  has a unique local minimum (section 1.6.2).

The conjugate gradient method can optimize  $n$ -dimensional quadratic functions in  $n$  steps as shown in figure 5.2. Its directions are mutually conjugate with respect to  $\mathbf{A}$ :

$$
\mathbf {d}^{(i) \top} \mathbf {A} \mathbf {d}^{(j)} = 0 \text{for all} i \neq j \tag {5.9}
$$

The mutually conjugate vectors are the basis vectors of  $\mathbf{A}$ . They are generally not orthogonal to one another.

The successive conjugate directions are computed using gradient information and the previous descent direction. The algorithm starts with the direction of steepest descent:

$$
\mathbf {d}^{(1)} = - \mathbf {g}^{(1)} \tag {5.10}
$$

We then use line search to find the next design point. For quadratic functions, the step factor  $\alpha$  can be computed exactly (example 5.1). The update is then:

$$
\mathbf {x}^{(2)} = \mathbf {x}^{(1)} + \alpha^{(1)} \mathbf {d}^{(1)} \tag {5.11}
$$

Subsequent iterations choose  $\mathbf{d}^{(k + 1)}$  based on the current gradient and a contribution from the previous descent direction:

$$
\mathbf {d}^{(k)} = - \mathbf {g}^{(k)} + \beta^{(k)} \mathbf {d}^{(k - 1)} \tag {5.12}
$$

for scalar parameter  $\beta$ . Larger values of  $\beta$  indicate that the previous descent direction contributes more strongly.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/85c718c291d9d363df97687f19b6d389da8dc0dd326bf323a776e496c5ebda0a.jpg)  
Figure 5.2. Conjugate gradient descent converges in  $n$  steps when applied to an  $n$ -dimensional quadratic function.

Suppose we want to derive the optimal step factor for a line search on a quadratic function:

$$
\underset{\alpha} {\text{minimize}} f (\mathbf {x} + \alpha \mathbf {d})
$$

We can compute the derivative with respect to  $\alpha$ :

$$
\begin{array}{l} \frac{\partial f (\mathbf {x} + \alpha \mathbf {d})}{\partial \alpha} = \frac{\partial}{\partial \alpha} \left[ \frac{1}{2} (\mathbf {x} + \alpha \mathbf {d}) ^{\top} \mathbf {A} (\mathbf {x} + \alpha \mathbf {d}) + \mathbf {b}^{\top} (\mathbf {x} + \alpha \mathbf {d}) + c \right] \\ = \mathbf {d}^{\top} \mathbf {A} (\mathbf {x} + \alpha \mathbf {d}) + \mathbf {d}^{\top} \mathbf {b} \\ = \mathbf {d}^{\top} (\mathbf {A x} + \mathbf {b}) + \alpha \mathbf {d}^{\top} \mathbf {A d} \\ \end{array}
$$

Setting  $\frac{\partial f(\mathbf{x} + \alpha\mathbf{d})}{\partial\alpha} = 0$  results in:

$$
\alpha = - \frac{\mathbf {d}^{\top} (\mathbf {A x} + \mathbf {b})}{\mathbf {d}^{\top} \mathbf {A d}}
$$

Example 5.1. The optimal step factor for a line search on a quadratic function.

We can derive the best value for  $\beta$  for a known  $\mathbf{A}$ , using the fact that  $\mathbf{d}^{(k)}$  is conjugate to  $\mathbf{d}^{(k-1)}$ :

$$
\begin{array}{l} \mathbf {d}^{(k) \top} \mathbf {A d}^{(k - 1)} = 0 (5.13) \\ \Rightarrow \left(- \mathbf {g}^{(k)} + \beta^{(k)} \mathbf {d}^{(k - 1)}\right) ^{\top} \mathbf {A} \mathbf {d}^{(k - 1)} = 0 (5.14) \\ \Rightarrow - \mathbf {g}^{(k) ^{\top}} \mathbf {A d}^{(k - 1)} + \beta^{(k)} \mathbf {d}^{(k - 1) ^{\top}} \mathbf {A d}^{(k - 1)} = 0 (5.15) \\ \Rightarrow \beta^{(k)} = \frac{\mathbf {g}^{(k) ^{\top}} \mathbf {A d}^{(k - 1)}}{\mathbf {d}^{(k - 1) ^{\top}} \mathbf {A d}^{(k - 1)}} (5.16) \\ \end{array}
$$

The conjugate gradient method can be applied to nonquadratic functions as well. Smooth functions behave like quadratic functions close to a local minimum, and the conjugate gradient method will converge quickly in such regions. Unfortunately, we do not know the value of  $\mathbf{A}$  that best approximates  $f$  around  $\mathbf{x}^{(k)}$ . Several updates have been proposed for  $\beta^{(k)}$  that do not require knowing  $\mathbf{A}$  and tend to work well. One is the Fletcher-Reeves<sup>3</sup> update:

$$
\beta^{(k)} = \frac{\mathbf {g}^{(k) \top} \mathbf {g}^{(k)}}{\mathbf {g}^{(k - 1) \top} \mathbf {g}^{(k - 1)}} \tag {5.17}
$$

This update is equivalent to setting  $\beta^{(k)}$  to the ratio of the squared norm of the current gradient to the squared norm of the previous gradient.

4 E. Polak and G. Ribiere, "Note sur la Convergence de Méthodes de Directions Conjuguees," Revue Française d'informatique et de Recherche Opérationnelle, Série Rouge, vol. 3, no. 1, pp. 35-43, 1969.

An alternative to the Fletcher-Reeves update is the Polak-Ribiere4 update:

$$
\beta^{(k)} = \frac{\mathbf {g}^{(k) \top} \left(\mathbf {g}^{(k)} - \mathbf {g}^{(k - 1)}\right)}{\mathbf {g}^{(k - 1) \top} \mathbf {g}^{(k - 1)}} \tag {5.18}
$$

Convergence for the Polak-Ribiere method (algorithm 5.2) can be guaranteed if we modify it to allow for automatic resets:

$$
\beta \leftarrow \max (\beta , 0) \tag {5.19}
$$

Figure 5.3 shows an example search using this method.

```julia
mutable struct ConjugateGradientDescent  $\ll$  DescentMethod d#previous search direction g#previous gradient   
end   
function init!(M::ConjugateGradientDescent，f，Vf，x) M.g  $=$  Vf(x) M.d=-M.g return M   
end   
function step!(M::ConjugateGradientDescent，f，Vf，x) d,g=M.d,M.g  $\mathbf{g}^{\prime} = \nabla \mathbf{f}(\mathbf{x})$ $\beta = \max (0,g^{\prime}\bullet (g^{\prime} - g) / (g\bullet g))$ $\mathrm{d}' = -\mathrm{g}' + \beta *\mathrm{d}$  x' = line_search(f,x,d') M.d,M.g  $= \mathrm{d}^{\prime},\mathrm{g}^{\prime}$  return x'   
end
```

# 5.3 Momentum

Gradient descent will take a long time to traverse a nearly flat surface as shown in figure 5.4. Allowing momentum to accumulate is one way to speed progress. We can modify gradient descent to incorporate momentum. In addition to tracking the design iterate  $\mathbf{x}^{(k)}$ , we also track its associated velocity vector  $\mathbf{v}^{(k)}$ .

The momentum update equations are:

$$
\mathbf {v}^{(k + 1)} = \beta \mathbf {v}^{(k)} - \alpha \mathbf {g}^{(k)} \tag {5.20}
$$

$$
\mathbf {x}^{(k + 1)} = \mathbf {x}^{(k)} + \mathbf {v}^{(k + 1)} \tag {5.21}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/20c5fb19530dea78906fc36117ee81a832b705a84b82012cee61fe7d07d3de25.jpg)  
Figure 5.3. The conjugate gradient method with the Polak-Ribiere update. Gradient descent is shown in gray.

Algorithm 5.2. The conjugate gradient method with the Polak-Ribiere update, where  $\mathrm{d}$  is the previous search direction and  $\mathbf{g}$  is the previous gradient.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a6eeef4a5bd5bfa6dd252f779046efae61f2e6f9bcd063878fefc218f38c43a5.jpg)  
Figure 5.4. Regions that are nearly flat have gradients with small magnitudes and can thus require many iterations of gradient descent to traverse.

For  $\beta = 0$ , we recover gradient descent. Momentum can be interpreted as a ball rolling down a nearly horizontal incline. The ball naturally gathers momentum as gravity causes it to accelerate, just as the gradient causes momentum to accumulate in this descent method. Momentum descent is compared to gradient descent in figure 5.5. An implementation is provided in algorithm 5.3.

```julia
mutable struct Momentum <- DescentMethod
     $\alpha$  # step factor
     $\beta$  # momentum decay
     $\nu$  # momentum
end
function init!(M::Momentum, f,  $\nabla f$ , x)
    M.v = zeros(length(x))
    return M
end
function step!(M::Momentum, f,  $\nabla f$ , x)
     $\alpha$ ,  $\beta$ , v, g = M. $\alpha$ , M. $\beta$ , M.v,  $\nabla f(x)$ 
    v. =  $\beta * v - \alpha * g$ 
    return x + v
end
```

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1e37b66cb05ab8c858bf415463c5cf6a7a80bc954f889f641c99c4c9ba813c12.jpg)  
Figure 5.5. Gradient descent and the momentum method compared on the Rosenbrock function with  $b = 100$ ; see appendix B.6.

Algorithm 5.3. The momentum method for accelerated descent. The first line in step! makes copies of the scalars  $\alpha$  and  $\beta$ , but creates a reference to the vector  $\mathbf{v}$ . The line  $\mathbf{v} = \beta * \mathbf{v} - \alpha * \mathbf{g}$  modifies the original momentum vector in the struct M. See appendix A for a review of array assignment, referencing, and copying.

# 5.4 Nesterov Momentum

One issue of momentum is that the steps do not slow down enough at the bottom of a valley and tend to overshoot the valley floor. Nesterov momentum<sup>5</sup> modifies the momentum algorithm to use the gradient at the projected future position:

$$
\mathbf {v}^{(k + 1)} = \beta \mathbf {v}^{(k)} - \alpha \nabla f \left(\mathbf {x}^{(k)} + \beta \mathbf {v}^{(k)}\right) \tag {5.22}
$$

$$
\mathbf {x}^{(k + 1)} = \mathbf {x}^{(k)} + \mathbf {v}^{(k + 1)} \tag {5.23}
$$

The Nesterov momentum and momentum descent methods are compared in figure 5.6. An implementation is provided by algorithm 5.4.

# 5.5 AdaGrad

Momentum and Nesterov momentum update all components of  $\mathbf{x}$  with the same step factor. The adaptive gradient method, or AdaGrad, $^{6}$  adapts a step factor for each component of  $\mathbf{x}$ . AdaGrad updates parameters with smaller or infrequently

5 Y. Nesterov, "A Method of Solving a Convex Programming Problem with Convergence Rate  $O(1 / k^2)$ ," Soviet Mathematics Doklady, vol. 27, no. 2, pp. 543-547, 1983.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/befddd1cb89e8c102f4d7742df8974c9090bdd76e624fdc6927785f23fc427a7.jpg)  
Figure 5.6. The momentum and Nesterov momentum methods compared on the Rosenbrock function with  $b = 100$ .

$^{6}$  J. Duchi, E. Hazan, and Y. Singer, "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization," Journal of Machine Learning Research, vol. 12, pp. 2121-2159, 2011.

```julia
mutable struct NesterovMomentum <- DescentMethod
     $\alpha$  # step factor
     $\beta$  # momentum decay
     $\nu$  # momentum
end
function init!(M::NesterovMomentum, f,  $\nabla f$ , x)
    M.v = zeros(length(x))
    return M
end
function step!(M::NesterovMomentum, f,  $\nabla f$ , x)
     $\alpha$ ,  $\beta$ ,  $\nu$  = M. $\alpha$ , M. $\beta$ , M.v
     $\nu$  :=  $\beta * \nu - \alpha * \nabla f(x + \beta * \nu)$ 
    return x +  $\nu$ 
end
```

Algorithm 5.4. Nesterov's momentum method of accelerated descent.

large gradients more aggressively than parameters that are frequently subject to large gradients. $^7$

AdaGrad updates have the form:

$$
x_{i}^{(k + 1)} = x_{i}^{(k)} - \alpha_{\text{adagrad}, i}^{(k)} g_{i}^{(k)} \tag {5.24}
$$

with componentwise step factors:

$$
\alpha_{\text{adagrad}, i}^{(k)} = \frac{\alpha}{\epsilon + \sqrt{s_{i}^{(k)}}} \tag {5.25}
$$

In this equation,  $s_i^{(k)}$  is the sum of the squares of the partial derivatives, with respect to  $x_i$ , up to time step  $k$ ,

$$
s_{i}^{(k)} = \sum_{j = 1}^{k} \left(g_{i}^{(j)}\right) ^{2} \tag {5.26}
$$

and  $\epsilon$  is a small value, on the order of  $10^{-8}$ , to prevent division by zero. Components with larger partial derivatives will thus receive smaller step factors.

AdaGrad is far less sensitive to the baseline step factor  $\alpha$ , which is often set to a default value of 0.01. AdaGrad's primary weakness is that the components of  $\mathbf{s}$  are each strictly nondecreasing. The accumulated sum causes the effective step factor to decrease during training, often becoming infinitesimally small before convergence. An implementation is provided by algorithm 5.5.

7 AdaGrad is designed to work particularly well when the gradient is sparse. Many deep learning problems result in sparse gradients of the objective function as a result of some features occurring far less frequently than others.

```julia
mutable struct AdaGrad  $\ll$  : DescentMethod  $\alpha$  # baseline step factor  $\epsilon$  # small value s # sum of squared gradient   
end   
function init!(M::AdaGrad, f,  $\nabla f$  , x) M.s  $=$  zeros(length(x)) return M   
end   
function step!(M::AdaGrad, f,  $\nabla f$  , x)  $\alpha$  ,  $\epsilon$  ,s,g  $= M.\alpha$  ,M.e,M.s,  $\nabla f(x)$  S.  $+ =$  g.\*g return x -  $\alpha *\mathbf{g}$  / (sqrt.(s) .+ e)   
end
```

Algorithm 5.5. The AdaGrad accelerated descent method.

# 5.6 RMSProp

RMSProp (algorithm 5.6) $^{8}$  extends AdaGrad to avoid the effect of a monotonically decreasing step factor. RMSProp maintains a decaying average of squared gradients. This average is updated according to:

$$
\hat {\mathbf {s}}^{(k + 1)} = \gamma \hat {\mathbf {s}}^{(k)} + (1 - \gamma) \left(\mathbf {g}^{(k)} \odot \mathbf {g}^{(k)}\right) \tag {5.27}
$$

where the decay  $\gamma \in [0,1]$  is typically close to 0.9. The decaying average of past squared gradients can be substituted into RMSProp's update equation: $^{10}$

$$
x_{i}^{(k + 1)} = x_{i}^{(k)} - \frac{\alpha}{\epsilon + \sqrt{\hat {s}_{i}^{(k + 1)}}} g_{i}^{(k)} = x_{i}^{(k)} - \frac{\alpha}{\epsilon + \operatorname{RMS} (g_{i})} g_{i}^{(k)} \tag {5.28}
$$

$^{8}$  RMSProp is unpublished and comes from Lecture 6e of Geoff Hinton's Coursera class.  
9 The operation  $\mathbf{a} \odot \mathbf{b}$  is the element-wise product between vectors  $\mathbf{a}$  and  $\mathbf{b}$ .

10 The denominator is similar to the root mean square (RMS) of the gradient component. In this chapter we use  $\mathrm{RMS}(x)$  to refer to the decaying root mean square of the time series of  $x$ .

# 5.7 Adadelta

Adadelta (algorithm 5.7) $^{11}$  is another method for overcoming AdaGrad's monotonically decreasing step factor. After independently deriving the RMSProp update, the authors noticed that the units in the update equations for gradient descent, momentum, and AdaGrad do not match. To fix this, they use an exponentially decaying average of the square updates:

$$
x_{i}^{(k + 1)} = x_{i}^{(k)} - \frac{\operatorname{RMS} \left(\Delta x_{i}\right)}{\epsilon + \operatorname{RMS} \left(g_{i}\right)} g_{i}^{(k)} \tag {5.29}
$$

11 M.D. Zeiler, "ADADELTA: An Adaptive Learning Rate Method," 2012. arXiv: 1212.5701.

which eliminates the step factor parameter entirely.

```julia
mutable struct RMSProp  $\ll$  : DescentMethod   
 $\alpha$  # step factor   
 $\gamma$  # decay   
 $\epsilon$  # small value   
s # sum of squared gradient   
end   
function init!(M::RMSProp, f,  $\nabla f$  , x) M.s = zeros(length(x)) return M   
end   
function step!(M::RMSProp, f,  $\nabla f$  , x)  $\alpha ,\gamma ,\epsilon ,s,g = M.\alpha ,M.\gamma ,M.\epsilon ,M.s,\nabla f(x)$  s.  $= \gamma *\mathbf{s} + (1 - \gamma)*(g.*g)$  return x -  $\alpha *\mathbf{g}$  / (sqrt.(s).+e)   
end
```

Algorithm 5.6. The RMSProp accelerated descent method.

```julia
mutable struct Adadelta <- DescentMethod
    ys # gradient decay
    yx # update decay
    ε # small value
    s # sum of squared gradients
    u # sum of squared updates
end
function init!(M::Adadelta, f, ∇f, x)
    M.s = zeros(length(x))
    M.u = zeros(length(x))
    return M
end
function step!(M::Adadelta, f, ∇f, x)
    ys, yx, ε, s, u, g = M.ys, M.yx, M.ε, M.s, M.u, ∇f(x)
    s := ys*s + (1-ys)*g.*g
    Δx = - (sqrt(u) * ε) / (sqrt(s) * ε) .* g
    u := yx*u + (1-yx)*Δx.*Δx
    return x + Δx
end
```

Algorithm 5.7. The Adadelta accelerated descent method. The small constant  $\epsilon$  is added to the numerator as well to prevent progress from entirely decaying to zero and to start off the first iteration where  $\Delta x = 0$ .

# 5.8 Adam

The adaptive moment estimation method, or Adam, $^{12}$  also adapts step factors to each parameter (algorithm 5.8). It stores both an exponentially decaying squared gradient like RMSProp and Adadelta, but also an exponentially decaying gradient like momentum.

Initializing the gradient and squared gradient to zero introduces a bias. A bias correction step helps alleviate the issue.[13] The equations applied during each iteration for Adam are:

biased decaying momentum:  $\mathbf{v}^{(k + 1)} = \gamma_v\mathbf{v}^{(k)} + (1 - \gamma_v)\mathbf{g}^{(k)}$  (5.30)

biased decaying sq. gradient:  $\mathbf{s}^{(k + 1)} = \gamma_s\mathbf{s}^{(k)} + (1 - \gamma_s)\Big(\mathbf{g}^{(k)}\odot \mathbf{g}^{(k)}\Big)$  (5.31)

corrected decaying momentum:  $\hat{\mathbf{v}}^{(k + 1)} = \mathbf{v}^{(k + 1)} / (1 - \gamma_v^k)$  (5.32)

corrected decaying sq. gradient:  $\hat{\mathbf{s}}^{(k + 1)} = \mathbf{s}^{(k + 1)} / (1 - \gamma_s^k)$  (5.33)

next iterate:  $\mathbf{x}^{(k + 1)} = \mathbf{x}^{(k)} - \alpha \hat{\mathbf{v}}^{(k + 1)} / \left(\epsilon +\sqrt{\hat{\mathbf{s}}^{(k + 1)}}\right)$  (5.34)

# 5.9 Hypergradient Descent

The accelerated descent methods are either extremely sensitive to the step factor or go to great lengths to adapt the step factor during execution. The step factor dictates how sensitive the method is to the gradient signal. A rate that is too high or too low often drastically affects performance.

Hypergradient descent $^{14}$  was developed with the understanding that the derivative of the step factor should be useful for improving optimizer performance. A hypergradient is a derivative taken with respect to a hyperparameter. Hypergradient algorithms reduce the sensitivity to the hyperparameter, allowing it to adapt more quickly.

Hypergradient descent applies gradient descent to the step factor of an underlying descent method. The method requires the partial derivative of the objective function with respect to the step factor. For gradient descent, this partial derivative

12 D. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization," in International Conference on Learning Representations (ICLR), 2015.  
13 According to the original paper, good default settings are  $\alpha = 0.001$ ,  $\gamma_v = 0.9$ ,  $\gamma_s = 0.999$ , and  $\epsilon = 10^{-8}$ .

14 A.G. Baydin, R. Cornish, D.M. Rubio, M. Schmidt, and F. Wood, "Online Learning Rate Adaptation with Hypergradient Descent," in International Conference on Learning Representations (ICLR), 2018.

```julia
mutable struct Adam <: DescentMethod
     $\alpha$  # step factor
     $\gamma v$  # decay
     $\gamma s$  # decay
     $\epsilon$  # small value
    k # step counter
    v # 1st moment estimate
    s # 2nd moment estimate
end
function init!(M::Adam, f,  $\nabla f$ , x)
    M.k = 0
    M.v = zeros(length(x))
    M.s = zeros(length(x))
    return M
end
function step!(M::Adam, f,  $\nabla f$ , x)
     $\alpha$ ,  $\gamma v$ ,  $\gamma s$ ,  $\epsilon$ , k = M. $\alpha$ , M. $\gamma v$ , M. $\gamma s$ , M. $\epsilon$ , M.k
    s, v, g = M.s, M.v,  $\nabla f(x)$ 
    v. =  $\gamma v * v + (1 - \gamma v) * g$ 
    s. =  $\gamma s * s + (1 - \gamma s) * g$ .
    M.k = k += 1
    v_hat = v. / (1 -  $\gamma v^{\wedge}k$ )
    s_hat = s. / (1 -  $\gamma s^{\wedge}k$ )
    return x -  $\alpha * v_hat$  ./ (sqrt.(s_hat).+  $\epsilon$ )
end
```

Algorithm 5.8. The Adam accelerated descent method.

is:

$$
\begin{array}{l} \frac{\partial f \left(\mathbf {x}^{(k + 1)}\right)}{\partial \alpha^{(k)}} = \left(\mathbf {g}^{(k + 1)}\right) ^{\top} \frac{\partial}{\partial \alpha^{(k)}} \left(\mathbf {x}^{(k)} - \alpha^{(k)} \mathbf {g}^{(k)}\right) (5.35) \\ = \left(\mathbf {g}^{(k + 1)}\right) ^{\top} \left(- \mathbf {g}^{(k)}\right) (5.36) \\ \end{array}
$$

Computing the hypergradient thus requires keeping track of the last gradient. The resulting update rule is:

$$
\begin{array}{l} \alpha^{(k)} = \alpha^{(k - 1)} - \mu \frac{\partial f (\mathbf {x}^{(k)})}{\partial \alpha^{(k - 1)}} (5.37) \\ = \alpha^{(k - 1)} + \mu \left(\mathbf {g}^{(k)}\right) ^{\top} \mathbf {g}^{(k - 1)} (5.38) \\ \end{array}
$$

where  $\mu$  is the hypergradient step factor.

This derivation can be applied to any gradient-based descent method that follows equation (4.1). These methods are visualized in figure 5.7. Implementations are provided for the hypergradient versions of gradient descent (algorithm 5.9) and Nesterov momentum (algorithm 5.10).

```julia
mutable struct HyperGradientDescent  $\ll$  DescentMethod   
 $\alpha 0$  # initial step factor   
 $\mu$  # step factor for the step factor update   
 $\alpha$  # current step factor   
g prevail#previous gradient   
end   
function init!(M::HyperGradientDescent，f，Vf，x) M.α=M.αO M.g prevail=zeros(length(x)) return M   
end   
function step!(M::HyperGradientDescent，f，Vf，x)  $\alpha ,\mu ,g,g\_ prev = M.\alpha ,M.\mu ,\nabla f(x),M.g\_ prev$ $\alpha = \alpha +\mu *(g*g_{-}prev)$  M.g prevail,M.  $\alpha = g,\alpha$  return x -  $\alpha *\mathbf{g}$    
end
```

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4bd92aacb97a8fe2fd166abdbf4a0a6638b31908229892134bee4b2083d42cfa.jpg)  
Figure 5.7. Hypergradient versions of gradient descent and Nesterov momentum compared on the Rosenbrock function with  $b = 100$ ; see appendix B.6.

Algorithm 5.9. The hypergradient form of gradient descent.

```julia
mutable struct HyperNesterovMomentum <- DescentMethod
 $\alpha 0$  # initial step factor
 $\mu$  # step factor for the step factor update
 $\beta$  # momentum decay
 $\nu$  # momentum
 $\alpha$  # current step factor
hprev # previous gradient
end
function init!(M::HyperNesterovMomentum, f,  $\nabla f$ , x)
M. $\alpha = M.\alpha 0$  
M.v = zeros(length(x))
M.h Prev = zeros(length(x))
return M
end
function step!(M::HyperNesterovMomentum, f,  $\nabla f$ , x)
 $\alpha, \beta, \mu, \nu = M.\alpha, M.\beta, M.\mu, M.\nu$  
h, h Prev =  $\nabla f(x + \beta * \nu)$ , M.h Prev
 $\alpha += \mu * (h * h_{-} \text{prev})$  
v.  $= \beta * v - \alpha * h$  
M.h Prev, M. $\alpha = h$ ,  $\alpha$  
return x + v
end
```

Algorithm 5.10. The hypergradient form of the Nesterov momentum descent method.

# 5.10 Summary

Gradient descent follows the direction of steepest descent.  
- The conjugate gradient method can automatically adjust to local valleys.  
- Descent methods with momentum build up progress in favorable directions.  
- A wide variety of accelerated descent methods use special techniques to speed up descent.  
- Hypergradient descent applies gradient descent to the step factor of an underlying descent method.

# 5.11 Exercises

Exercise 5.1. Suppose we have  $f(\mathbf{x}) = x_1x_2^2$ . For  $\mathbf{x}^{(k)} = [1,2]$ , compute the normalized direction used for gradient descent.

Solution: The gradient is  $\nabla f = [x_2^2, 2x_1x_2]$ . At  $\mathbf{x}^{(k)} = [1, 2]$ , we get an unnormalized direction of steepest descent of  $\mathbf{d} = [-4, -4]$ , which is normalized to  $\mathbf{d} = \left[-\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}\right]$ .

Exercise 5.2. Compute the gradient of  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x} + \mathbf{b}^{\top}\mathbf{x}$  when  $\mathbf{A}$  is symmetric.

Solution:  $\nabla f(\mathbf{x}) = 2\mathbf{A}\mathbf{x} + \mathbf{b}$

Exercise 5.3. Apply gradient descent with a unit step factor to  $f(x) = x^4$  from a starting point of your choice. Compute two iterations.

Solution: The derivative is  $f'(x) = 4x^3$ . Starting from  $x^{(1)} = 1$ :

$$
f^{\prime} (1) = 4
$$

$$
\rightarrow x^{(2)} = 1 - 4 = - 3
$$

$$
f^{\prime} (- 3) = 4 \cdot (- 27) = - 10 8
$$

$$
\rightarrow x^{(3)} = - 3 + 10 8 = 10 5
$$

Exercise 5.4. Apply one step of gradient descent to  $f(x) = e^{x} + e^{-x}$  from  $x^{(1)} = 10$  with both a unit step factor and with exact line search.

Solution: We have  $f'(x) = e^x - e^{-x} \approx e^x$  for large  $x$ . Thus  $f'(x^{(1)}) \approx e^{10}$  and  $x^{(2)} \approx -e^{10}$ . If we apply an exact line search,  $x^{(2)} = 0$ . Thus, without a line search we are not guaranteed to reduce the value of the objective function.

Exercise 5.5. The conjugate gradient method can also be used to find a search direction  $\mathbf{d}$  when a local quadratic model of a function is available at the current point. With  $\mathbf{d}$  as search direction, let the model be

$$
q (\mathbf {d}) = \mathbf {d}^{\top} \mathbf {H} \mathbf {d} + \mathbf {b}^{\top} \mathbf {d} + \mathbf {c}
$$

for a symmetric matrix  $\mathbf{H}$ . What is the Hessian in this case? What is the gradient of  $q$  when  $\mathbf{d} = \mathbf{0}$ ? What can go wrong if the conjugate gradient method is applied to the quadratic model to get the search direction  $\mathbf{d}$ ?

Solution: The Hessian is  $2\mathbf{H}$ , and

$$
\nabla q (\mathbf {d}) = \left(\mathbf {H} + \mathbf {H}^{\top}\right) \mathbf {d} + \mathbf {b} = (2 \mathbf {H}) \mathbf {d} + \mathbf {b}
$$

The gradient is  $\mathbf{b}$  when  $\mathbf{d} = \mathbf{0}$ . The conjugate gradient method may diverge because  $\mathbf{H}$  is not guaranteed to be positive definite.

Exercise 5.6. How is Nesterov momentum an improvement over momentum?

Solution: Nesterov momentum looks at the point where you will be after the update to compute the update itself.

Exercise 5.7. In what way is the conjugate gradient method an improvement over steepest descent?

Solution: The conjugate gradient method implicitly reuses previous information about the function and thus may enjoy better convergence in practice.

Exercise 5.8. In conjugate gradient descent, what is the descent direction at the first iteration for the function  $f(x,y) = x^{2} + xy + y^{2} + 5$  when initialized at  $(x,y) = (1,1)$ ? What is the resulting point after two steps of the conjugate gradient method?

Solution: The conjugate gradient method initially follows the steepest descent direction. The gradient is

$$
\nabla f (x, y) = [ 2 x + y, 2 y + x ]
$$

which for  $(x,y) = (1,1)$  is [3,3]. The direction of steepest descent is opposite the gradient,  $\mathbf{d}^{(1)} = [-3, - 3]$ .

The Hessian is

$$
\left[ \begin{array}{c c} 2 & 1 \\ 1 & 2 \end{array} \right]
$$

Since the function is quadratic and the Hessian is positive definite, the conjugate gradient method converges in at most two steps. Thus, the resulting point after two steps is the optimum,  $(x,y) = (0,0)$ , where the gradient is zero.

Exercise 5.9. We have a polynomial function  $f$  such that  $f(\mathbf{x}) > 2$  for all  $\mathbf{x}$  in three-dimensional Euclidean space. Suppose we are using steepest descent with step lengths optimized at each step, and we want to find a local minimum of  $f$ . If our unnormalized descent direction is [1,2,3] at step  $k$ , is it possible for our unnormalized descent direction at step  $k + 1$  to be  $[0,0, -3]$ ? Why or why not?

Solution: No. If exact minimization is performed, then the descent directions between steps are orthogonal, but  $[1,2,3]^\top [0,0,-3] \neq 0$ .

# 6 Second-Order Methods

The previous chapter focused on optimization methods that involve first-order approximations of the objective function using the gradient. This chapter focuses on leveraging second-order approximations that use the second derivative in univariate optimization or the Hessian in multivariate optimization to direct the search. This additional information can help improve the local model used for informing the selection of directions and step lengths in descent algorithms.

# 6.1 Newton's Method

Knowing the function value and gradient for a design point can help determine the direction to travel, but this first-order information does not directly help determine how far to step to reach a local minimum. Second-order information, on the other hand, allows us to make a quadratic approximation of the objective function and approximate the right step size to reach a local minimum as shown in figure 6.1. As we have seen with quadratic fit search in chapter 3, we can analytically obtain the location where a quadratic approximation has a zero gradient. We can then use that location as the next iteration to approach a local minimum.

In univariate optimization, the quadratic approximation about a point  $x^{(k)}$  comes from the second-order Taylor expansion:

$$
q (x) = f \left(x^{(k)}\right) + \left(x - x^{(k)}\right) f^{\prime} \left(x^{(k)}\right) + \frac{\left(x - x^{(k)}\right) ^{2}}{2} f^{\prime \prime} \left(x^{(k)}\right) \tag {6.1}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bce27b800b9118f9f768f929d236e2f48195c833869c2782bf904ce3837079bc.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/38ae6967c383b2328ae9401d82b0738df248e80b985ba604db751a34665d5de6.jpg)  
Figure 6.1. A comparison of first-order and second-order approximations. Bowl-shaped quadratic approximations have unique locations where the derivative is zero.

Setting the derivative to zero and solving for the root yields the update equation for Newton's method:

$$
\frac{\partial}{\partial x} q (x) = f^{\prime} \left(x^{(k)}\right) + \left(x - x^{(k)}\right) f^{\prime \prime} \left(x^{(k)}\right) = 0 \tag {6.2}
$$

$$
x^{(k + 1)} = x^{(k)} - \frac{f^{\prime} \left(x^{(k)}\right)}{f^{\prime \prime} \left(x^{(k)}\right)} \tag {6.3}
$$

This update is shown in figure 6.2.

The update rule in Newton's method involves dividing by the second derivative. The update is undefined if the second derivative is zero, which occurs when the quadratic approximation is a line. Instability also occurs when the second derivative is very close to zero, in which case the next iterate will lie very far from the current design point, far from where the local quadratic approximation is valid. Poor local approximations can lead to poor performance with Newton's method. Figure 6.3 shows three kinds of failure cases.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bb27d7a6624bde40e6f2e201267373b72baabaa95b5accdd8064b8f0e4a94c6d.jpg)  
Oscillation

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2618b4f7c21e405fc250efeb3be0ec64337059434bd097fb19a9bc2072825ed1.jpg)  
Overshoot  
x

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3fe1ab7867a22f808fd476de8ee71f3edf500dc9e8dc34723a21f641bb67e1ec.jpg)  
Negative  $f^{\prime \prime}$  
x  
Figure 6.2. Newton's method can be interpreted as a root-finding method applied to  $f'$  that iteratively improves a univariate design point by taking the tangent line at  $(x, f'(x))$ , finding the intersection with the  $x$ -axis, and using that  $x$  value as the next design point.  
Figure 6.3. Examples of failure cases with Newton's method.

Newton's method does tend to converge quickly when in a bowl-like region that is sufficiently close to a local minimum. It has quadratic convergence, meaning the difference between the minimizer and the iterate is approximately squared with every iteration. This rate of convergence holds for Newton's method starting from  $x^{(1)}$  within a distance  $\delta \ll 1$  of a root  $x^*$  if for all  $x$  in the interval  $[x^* - \delta, x^* + \delta]$ :

-  $f''(x) \neq 0$ ,  
$f^{\prime \prime \prime}(x)$  is continuous, and  
$\frac{1}{2} \left| \frac{f'''(x^{(1)})}{f''(x^{(1)})} \right| < c \left| \frac{f'''(x^*)}{f''(x^*)} \right|$  for some  $c < \infty$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0e36b977deb55bfa63ce7475c8676e241bab5daa320ee1f0e55c5d761022a86d.jpg)  
x

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d19c42fde32af4e04df1d566f646e3c83b293a4b44be2eb5fd01f4f3484fc853.jpg)  
x

The final condition guards against overshoot.1

Newton's method can be extended to multivariate optimization (algorithm 6.1). The multivariate second-order Taylor expansion at  $\mathbf{x}^{(k)}$  is the quadratic:

$$
q (\mathbf {x}) = f \left(\mathbf {x}^{(k)}\right) + \left(\mathbf {g}^{(k)}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) + \frac{1}{2} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) ^{\top} \mathbf {H}^{(k)} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) \tag {6.4}
$$

where  $\mathbf{g}^{(k)}$  and  $\mathbf{H}^{(k)}$  are the gradient and Hessian at  $\mathbf{x}^{(k)}$ , respectively.

We evaluate the gradient and set it to zero:

$$
\nabla q (\mathbf {x}) = \mathbf {g}^{(k)} + \mathbf {H}^{(k)} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) = \mathbf {0} \tag {6.5}
$$

We then solve for the next iterate, thereby obtaining Newton's method in multivariate form:

$$
\mathbf {x}^{(k + 1)} = \mathbf {x}^{(k)} - \left(\mathbf {H}^{(k)}\right) ^{- 1} \mathbf {g}^{(k)} \tag {6.6}
$$

```python
struct NewtonsMethod <: DescentMethod end
step!(M::NewtonsMethod, f, \nabla f, H, x) = x - H(x) \nabla f(x)

If  $f$  is quadratic and its Hessian is positive definite, then the update converges to the global minimum in one step. For general functions, Newton's method is often terminated once  $x$  ceases to change by more than a given tolerance. Example 6.1 shows how Newton's method can be used to minimize a function.

Newton's method can also be used to supply a descent direction to line search or can be modified to use a step factor. Smaller steps toward the minimum or line searches along the descent direction can increase the method's robustness. The descent direction is:

$$
\mathbf {d}^{(k)} = - \left(\mathbf {H}^{(k)}\right) ^{- 1} \mathbf {g}^{(k)} \tag {6.7}
$$

# 6.2 Secant Method

Newton's method for univariate function minimization requires the first and second derivatives  $f'$  and  $f''$ . In many cases,  $f'$  is known but the second derivative is not. The secant method (algorithm 6.2) applies Newton's method using estimates of the second derivative and thus only requires  $f'$ . This property makes the secant method more convenient to use in practice.

1 The final condition enforces sufficient closeness, ensuring that the function is sufficiently approximated by the Taylor expansion. J. Stoer and R. Bulirsch, Introduction to Numerical Analysis, 3rd ed. Springer, 2002.

Algorithm 6.1. Newton's method, which uses a second-order approximation at  $x$  based on the Hessian of the objective function  $H$  and the objective function gradient  $\nabla f$ . The objective function input is not used.

Termination conditions for descent methods are given in chapter 4.

3 See chapter 5.

4 The descent direction given by Newton's method is similar to the natural gradient or covariant gradient. S. Amari, "Natural Gradient Works Efficiently in Learning," Neural Computation, vol. 10, no. 2, pp. 251-276, 1998.

With  $\mathbf{x}^{(1)} = [9,8]$ , we will use Newton's method to minimize Booth's function:

$$
f (\mathbf {x}) = (x_{1} + 2 x_{2} - 7) ^{2} + (2 x_{1} + x_{2} - 5) ^{2}
$$

The gradient of Booth's function is:

$$
\nabla f (\mathbf {x}) = [ 10 x_{1} + 8 x_{2} - 34, 8 x_{1} + 10 x_{2} - 38 ]
$$

The Hessian of Booth's function is:

$$
\mathbf {H} (\mathbf {x}) = \left[ \begin{array}{l l} 10 & 8 \\ 8 & 10 \end{array} \right]
$$

The first iteration of Newton's method yields:

$$
\begin{array}{l} \mathbf {x}^{(2)} = \mathbf {x}^{(1)} - \left(\mathbf {H}^{(1)}\right) ^{- 1} \mathbf {g}^{(1)} = \left[ \begin{array}{c} 9 \\ 8 \end{array} \right] - \left[ \begin{array}{c c} 10 & 8 \\ 8 & 10 \end{array} \right] ^{- 1} \left[ \begin{array}{c} 10 \cdot 9 + 8 \cdot 8 - 34 \\ 8 \cdot 9 + 10 \cdot 8 - 38 \end{array} \right] \\ = \left[ \begin{array}{l} 9 \\ 8 \end{array} \right] - \left[ \begin{array}{l l} 10 & 8 \\ 8 & 10 \end{array} \right] ^{- 1} \left[ \begin{array}{l} 12 0 \\ 11 4 \end{array} \right] = \left[ \begin{array}{l} 1 \\ 3 \end{array} \right] \\ \end{array}
$$

The gradient at  $\mathbf{x}^{(2)}$  is zero, so we have converged after a single iteration. The Hessian is positive definite everywhere, so  $\mathbf{x}^{(2)}$  is the global minimum.

Example 6.1. Newton's method used to minimize Booth's function; see appendix B.2.

The secant method uses the last two iterates to approximate the second derivative:

$$
f^{\prime \prime} (x^{(k)}) \approx \frac{f^{\prime} (x^{(k)}) - f^{\prime} (x^{(k - 1)})}{x^{(k)} - x^{(k - 1)}} \tag {6.8}
$$

This estimate is substituted into Newton's method:

$$
x^{(k + 1)} \leftarrow x^{(k)} - \frac{x^{(k)} - x^{(k - 1)}}{f^{\prime} \left(x^{(k)}\right) - f^{\prime} \left(x^{(k - 1)}\right)} f^{\prime} \left(x^{(k)}\right) \tag {6.9}
$$

The secant method requires an additional initial design point. It suffers from the same problems as Newton's method and may take more iterations to converge due to approximating the second derivative.

```julia
mutable struct SecantMethod  $\ll$  : DescentMethod   
 $\alpha$  # step factor, used in initialization   
xprev#previous design   
g prevail#previous gradient   
end   
function init!(M::SecantMethod，f，f'，x)   
 $\alpha = M.\alpha$    
M.x prevail  $= x + \alpha *f^{\prime}(x)$    
M.g prevail  $= f^{\prime}(M.x\_ prev)$    
return M   
end   
function step!(M::SecantMethod，f，f'，x)   
x prevail,g prevail  $= M.x\_ prev$  ，M.g prevail   
 $g = f^{\prime}(x)$ $x^{\prime} = x - (x - x_{-}\mathrm{prev}) / (g - g_{-}\mathrm{prev})*g$    
M.x prevail,M.g prevail  $= x$  ，g   
return x'   
end
```

Algorithm 6.2. The secant method for minimizing a univariate objective function  $f$  using only the first derivative  $f'$  and previous values. This init method uses a small negative gradient step with a step factor  $\alpha$  in order to initialize a previous value.

# 6.3 Levenberg-Marquardt Algorithm

The Levenberg-Marquardt algorithm<sup>5</sup> (algorithm 6.3) automatically interpolates between approximate Newton updates and gradient descent steps. Newton's method tends to perform well when a quadratic approximation is a good fit for our objective function, but will perform poorly when the fit is poor. A quadratic approximation for a smooth function will tend to be good sufficiently close to a local optimum, where the objective function is typically convex and bowl-like. In more linear regions, quadratic approximations will be poor, and in concave regions, quadratic approximations will have degenerate Hessians. In such cases it is often better to simply use a gradient descent step. These regimes are shown in figure 6.4.

The interpolating update rule is parameterized by a damping factor  $\delta$ :

$$
\mathbf {x}^{\prime} = \mathbf {x} - \left(\mathbf {H} + \delta \mathbf {I}\right) ^{- 1} \mathbf {g} \tag {6.10}
$$

When  $\delta$  is small, the update mimics Newton's method. When  $\delta$  is large, the update mimics gradient descent with a step factor  $\alpha \approx 1 / \delta$ .

We adjust the damping factor during descent based on whether our iterates improve the objective function value. In every iteration, the gradient and the approximate Hessian are calculated and used to evaluate a candidate next iterate. If the objective is better at the next iterate, it is accepted, and  $\delta$  can be decreased. If the objective is worse at the next iterate, it is rejected, the algorithm retains the current iterate, and  $\delta$  is increased.

The Levenberg-Marquardt update rule incorporates a slight adjustment to equation (6.10):

$$
\mathbf {x}^{\prime} = \mathbf {x} - \left(\mathbf {H} + \delta \operatorname{diag} (\mathbf {H})\right) ^{- 1} \mathbf {g} \tag {6.11}
$$

which incorporates the diagonal of the Hessian. This leverages information about the Hessian even when mimicking gradient descent, allowing iterates to move further in directions where the gradient is smaller.

If the Hessian is not invertible and  $\mathrm{diag}(\mathbf{H})$  has any negative entries, increasing the damping factor will not produce an invertible matrix. To mitigate this effect, we can take the component-wise maximum of these values with a small positive number, as done in the implementation. Figure 6.5 shows the Levenberg-Marquardt method on a test function.

5 This algorithm is named for American statisticians Kenneth Levenberg (1919-1973) and Donald Marquardt (1929-1997). It was originally published by Levenberg while working for the Frankford Army Arsenal. K. Levenberg, "A Method for the Solution of Certain Non-Linear Problems in Least Squares," Quarterly of Applied Mathematics, vol. 2, no. 2, pp. 164-168, 1944. It was later rediscovered by Marquardt while working for DuPont. D. W. Marquardt, "An Algorithm for Least-Squares Estimation of Nonlinear Parameters," Journal of the Society for Industrial and Applied Mathematics, vol. 11, no. 2, pp. 431-441, 1963. There were several other independent rediscoveries of this general method.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/81e55761bc2a03a6b5191df1909ba6cbdca902556f258a2e0fb9153988cd8041.jpg)  
Figure 6.4. The local curvature of the optimization function will determine the efficacy of Newton's method.

```julia
function levenberg-marquardt(f,  $\nabla f$  , H, x, 8, y_acc, y_rej, e) M = H(x) d = max.(diag(M), e) M +=  $\delta *\mathrm{Diagonal}(d)$ $x^{\prime} = x - M\backslash \nabla f(x)$  if  $f(x^{\prime}) <   f(x)$  # accept the new design and decrease damping return  $(x = x^{\prime}$  ,  $\delta = \delta *\gamma_{-}$  acc) end # reject the new design and increase damping return  $(x = x$  ,  $\delta = \delta *\gamma_{-}$  rej)   
end
```

Algorithm 6.3. The Levenberg-Marquardt algorithm for interpolating between standard gradient descent and Newton steps. It takes an objective function  $f$ , a gradient  $\nabla f$ , and a Hessian  $\mathsf{H}$  which are evaluated at a design  $x$ . The interpolation is controlled by a damping factor  $\delta > 0$ . This damping factor is increased or decreased by scaling with  $\gamma$  based on whether the next iterate is an improvement. Taking a component-wise max with a small positive scalar  $\epsilon$  ensures that the matrix can become invertible for a sufficiently large damping factor. Note that Julia will fall back to the pseudoinverse if  $\mathsf{M}$  is not invertible.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cc94123d532bb7e41694df74d7d3c07f5583cc176259f4f2caeed7c10dc8ea3b.jpg)

Figure 6.5. The Levenberg-Marquardt method run on Wheeler's Ridge (appendix B.7) using the update with the diagonal of the Hessian in equation (6.11), compared against the update without the diagonal given in equation (6.10). The methods start in the same location in a relatively flat region, and they must traverse that region to a more bowl-like region containing the minimum. We set  $\gamma_{acc} = 0.1$ ,  $\gamma_{rej} = 10.0$  and  $\epsilon = 1e - 6$ .  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/02badc9143ba7d24e047dc3f4d78268e33dc56d7c13d137211c42b2cd668f4ac.jpg)  
w/out diagonal  
with diagonal

# 6.4 Levenberg-Marquardt for Sum of Squares

The Levenberg-Marquardt algorithm was originally developed for problems involving sums of squares, for which we can derive an efficient outer product approximation of the Hessian. Consider an objective function:

$$
f (\mathbf {x}) = \sum_{i} f_{i} (\mathbf {x}) ^{2} \tag {6.12}
$$

The gradient of this objective is:

$$
\nabla f (\mathbf {x}) = 2 \sum_{i} f_{i} (\mathbf {x}) \nabla f_{i} (\mathbf {x}) \tag {6.13}
$$

We can form a linear approximation for each of the constituent objective functions at our current iterate  $\mathbf{x}^{(k)}$ :

$$
f_{i} (\mathbf {x}) \approx f_{i} \left(\mathbf {x}^{(k)}\right) + \nabla f_{i} \left(\mathbf {x}^{(k)}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) \tag {6.14}
$$

Substituting them into the gradient of the overall objective yields<sup>6</sup>

$$
\begin{array}{l} \nabla f (\mathbf {x}) \approx 2 \sum_{i} \left(f_{i} \left(\mathbf {x}^{(k)}\right) + \nabla f_{i} \left(\mathbf {x}^{(k)}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}^{(k)}\right)\right) \nabla f_{i} (\mathbf {x}) (6.15) \\ = 2 \sum_{i} f_{i} \left(\mathbf {x}^{(k)}\right) \nabla f_{i} \left(\mathbf {x}^{(k)}\right) + 2 \sum_{i} \nabla f_{i} \left(\mathbf {x}^{(k)}\right) \nabla f_{i} \left(\mathbf {x}^{(k)}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) (6.16) \\ = 2 \tilde {\mathbf {g}}^{(k)} + 2 \tilde {\mathbf {H}}^{(k)} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) (6.17) \\ \end{array}
$$

where we define

$$
\tilde {\mathbf {g}}^{(k)} = \sum_{i} f_{i} \left(\mathbf {x}^{(k)}\right) \nabla f_{i} \left(\mathbf {x}^{(k)}\right) \tag {6.18}
$$

$$
\tilde {\mathbf {H}}^{(k)} = \sum_{i} \nabla f_{i} \left(\mathbf {x}^{(k)}\right) \nabla f_{i} \left(\mathbf {x}^{(k)}\right) ^{\top} \tag {6.19}
$$

Setting this gradient to zero and solving for  $x$  yields the new iterate:

$$
\mathbf {x}^{(k + 1)} = \mathbf {x}^{(k)} - \left(\tilde {\mathbf {H}}^{(k)}\right) ^{- 1} \tilde {\mathbf {g}}^{(k)} \tag {6.20}
$$

This update matches Newton's method in equation (6.6), except the Hessian is approximated using the outer product of the gradients. Incorporating the damping factor produces the Levenberg-Marquardt update (equation (6.11)):

$$
\mathbf {x}^{(k + 1)} = \mathbf {x}^{(k)} - \left(\tilde {\mathbf {H}}^{(k)} + \delta \operatorname{diag} \left(\tilde {\mathbf {H}}^{(k)}\right)\right) ^{- 1} \tilde {\mathbf {g}}^{(k)} \tag {6.21}
$$

The estimates of the gradient and Hessian are implemented in algorithm 6.4.

This is because the gradient is equal everywhere in a first order approximation:

$$
\nabla f_{i} (\mathbf {x}) = \nabla f_{i} \left(\mathbf {x}^{(k)}\right)
$$

```julia
function sum_of_squares_descent_estimators(fs,  $\nabla$  fs, x)  
g = sum(f(x)* $\nabla$ f(x) for (f, $\nabla$ f) in zip(fs,  $\nabla$ fs))  
H = sum( $\nabla$ f(x)* $\nabla$ f(x) ' for  $\nabla$ f in  $\nabla$ fs)  
return (g, H)  
end
```

# 6.5 Quasi-Newton Methods

Just as the secant method approximates  $f''$  in the univariate case, quasi-Newton methods approximate the inverse Hessian. Quasi-Newton method updates have the form:

$$
\mathbf {x}^{(k + 1)} \leftarrow \mathbf {x}^{(k)} - \alpha^{(k)} \mathbf {Q}^{(k)} \mathbf {g}^{(k)} \tag {6.22}
$$

where  $\alpha^{(k)}$  is a scalar step factor and  $\mathbf{Q}^{(k)}$  approximates the inverse of the Hessian at  $\mathbf{x}^{(k)}$ .

These methods typically set  $\mathbf{Q}^{(1)}$  to the identity matrix, and they then apply updates to reflect information learned with each iteration. To simplify the equations for the various quasi-Newton methods, we define the following:

$$
\gamma^{(k + 1)} \equiv \mathbf {g}^{(k + 1)} - \mathbf {g}^{(k)} \tag {6.23}
$$

$$
\boldsymbol {\delta}^{(k + 1)} \equiv \mathbf {x}^{(k + 1)} - \mathbf {x}^{(k)} \tag {6.24}
$$

The Davidon-Fletcher-Powell (DFP) method (algorithm 6.5) uses:7

$$
\mathbf {Q} \leftarrow \mathbf {Q} - \frac{\mathbf {Q} \gamma \mathbf {\Upsilon}^{\top} \mathbf {Q}}{\gamma^{\top} \mathbf {Q} \gamma} + \frac{\delta \delta^{\top}}{\delta^{\top} \gamma} \tag {6.25}
$$

where all terms on the right hand side are evaluated at the same iteration.

The update for  $\mathbf{Q}$  in the DFP method keeps  $\mathbf{Q}$  symmetric and positive definite. If  $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^{\top}\mathbf{A}\mathbf{x} + \mathbf{b}^{\top}\mathbf{x} + c$ , then the inverse Hessian is  $\mathbf{A}^{-1}$ , which DFP approximates iteratively. For such quadratic functions, DFP converges in  $n$  steps with exact line searches, similar to the conjugate gradient method. For high-dimensional problems, storing and updating  $\mathbf{Q}$  can be significant compared to other methods like the conjugate gradient method.

An alternative to DFP, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method (algorithm 6.6), uses:8

$$
\mathbf {Q} \leftarrow \mathbf {Q} - \left(\frac{\delta \gamma^{\top} \mathbf {Q} + \mathbf {Q} \gamma \delta^{\top}}{\delta^{\top} \gamma}\right) + \left(1 + \frac{\gamma^{\top} \mathbf {Q} \gamma}{\delta^{\top} \gamma}\right) \frac{\delta \delta^{\top}}{\delta^{\top} \gamma} \tag {6.26}
$$

Algorithm 6.4. An algorithm for computing the gradient and outer product Hessian approximation for a sum-of-squares optimization problem. It takes as input a list of objective functions  $f_{s}$ , their gradients  $\nabla f_{s}$ , and a design  $x$ .

7 The original concept was presented in a technical report, W.C. Davidon, "Variable Metric Method for Minimization," Argonne National Laboratory, Tech. Rep. ANL-5990, 1959. It was later published: W.C. Davidon, "Variable Metric Method for Minimization," SIAM Journal on Optimization, vol. 1, no. 1, pp. 1-17, 1991. The method was modified by R. Fletcher and M.J.D. Powell, "A Rapidly Convergent Descent Method for Minimization," The Computer Journal, vol. 6, no. 2, pp. 163-168, 1963.

8 R. Fletcher, Practical Methods of Optimization, 2nd ed. Wiley, 1987.

```julia
mutable struct DFP  $\ll$  DescentMethod Q # approximate inverse Hessian   
end   
function init!(M::DFP, f,  $\nabla f$  , x) m = length(x) M.Q = Matrix(1.0I(m)) return M   
end   
function step!(M::DFP, f,  $\nabla f$  , x) Q,g  $=$  M.Q,  $\nabla f(x)$ $\mathbf{x}^{\prime} =$  line_search(f,x,-Q\*g) g'  $\equiv$ $\nabla f(x^{\prime})$ $\delta = x^{\prime} - x$  y=g'-g Q.=Q-Q\*\*y\*\*Q/(y\*\*Q\*y)+  $\delta *\delta ' / (\delta '\ast y)$  return x'   
end
```

Algorithm 6.5. The Davidon-Fletcher-Powell descent method.

```julia
mutable struct BFGS <- DescentMethod
Q # approximate inverse Hessian
end
function init!(M::BFGS, f,  $\nabla f$ , x)
m = length(x)
M.Q = Matrix(1.0I(m))
return M
end
function step!(M::BFGS, f,  $\nabla f$ , x)
Q, g = M.Q,  $\nabla f(x)$ 
x' = line_search(f, x, -Q*g)
g' =  $\nabla f(x')$ 
δ = x' - x
v = g' - g
Q := Q - (\delta * v' * Q + Q * v * δ') / (δ' * v) +
(1 + (v' * Q * v) / (δ' * v)) [1] * (δ * δ') / (δ' * v)
return x'
end
```

Algorithm 6.6. The Broyden-Fletcher-Goldfarb-Shanno descent method.

BFGS does better than DFP with approximate line search but still uses an  $n \times n$  dense matrix. For very large problems where space is a concern, the Limited-memory BFGS method (algorithm 6.7), or L-BFGS, can be used to approximate BFGS.9 L-BFGS stores the last  $m$  values for  $\delta$  and  $\gamma$  rather than the full inverse Hessian, where  $i = 1$  indexes the oldest value and  $i = m$  indexes the most recent.

The process for computing the descent direction  $\mathbf{d}$  at  $\mathbf{x}$  begins by computing  $\mathbf{q}^{(m)} = \nabla f(\mathbf{x})$ . The remaining vectors  $\mathbf{q}^{(i)}$  for  $i$  from  $m - 1$  down to 1 are computed using

$$
\mathbf {q}^{(i)} = \mathbf {q}^{(i + 1)} - \frac{\left(\boldsymbol {\delta}^{(i + 1)}\right) ^{\top} \mathbf {q}^{(i + 1)}}{\left(\boldsymbol {\gamma}^{(i + 1)}\right) ^{\top} \boldsymbol {\delta}^{(i + 1)}} \boldsymbol {\gamma}^{(i + 1)} \tag {6.27}
$$

These vectors are used to compute another  $m + 1$  vectors, starting with

$$
\mathbf {z}^{(0)} = \frac{\boldsymbol {\gamma}^{(m)} \odot \boldsymbol {\delta}^{(m)} \odot \mathbf {q}^{(m)}}{\left(\boldsymbol {\gamma}^{(m)}\right) ^{\top} \boldsymbol {\gamma}^{(m)}} \tag {6.28}
$$

and proceeding with  $\mathbf{z}^{(i)}$  for  $i$  from 1 to  $m$  according to

$$
\mathbf {z}^{(i)} = \mathbf {z}^{(i - 1)} + \boldsymbol {\delta}^{(i - 1)} \left(\frac{\left(\boldsymbol {\delta}^{(i - 1)}\right) ^{\top} \mathbf {q}^{(i - 1)}}{\left(\boldsymbol {\gamma}^{(i - 1)}\right) ^{\top} \boldsymbol {\delta}^{(i - 1)}} - \frac{\left(\boldsymbol {\gamma}^{(i - 1)}\right) ^{\top} \mathbf {z}^{(i - 1)}}{\left(\boldsymbol {\gamma}^{(i - 1)}\right) ^{\top} \boldsymbol {\delta}^{(i - 1)}}\right) \tag {6.29}
$$

The descent direction is  $\mathbf{d} = -\mathbf{z}^{(m)}$

For minimization, the inverse Hessian  $\mathbf{Q}$  must remain positive definite. The initial Hessian is often set to the diagonal of

$$
\mathbf {Q}^{(1)} = \frac{\boldsymbol {\gamma}^{(1)} \left(\boldsymbol {\delta}^{(1)}\right) ^{\top}}{\left(\boldsymbol {\gamma}^{(1)}\right) ^{\top} \boldsymbol {\gamma}^{(1)}} \tag {6.30}
$$

Computing the diagonal for the above expression and substituting the result into  $\mathbf{z}^{(1)} = \mathbf{Q}^{(1)}\mathbf{q}^{(1)}$  results in the equation for  $\mathbf{z}^{(1)}$ .

The quasi-Newton methods discussed in this section are compared in figure 6.6. They often perform quite similarly.

9J. Nocedal, "Updating QuasiNewton Matrices with Limited Storage," Mathematics of Computation, vol. 35, no. 151, pp. 773-782, 1980.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3379827666b0adc8d6cda71f2c4cc509cbdd297e3427c56dcb95636c467dae37.jpg)  
Figure 6.6. Several quasi-Newton methods compared on a 100-dimensional extension of the Rosenbrock function; see algorithm B.7. The limited-memory BFGS method underperforms relative to the other methods.

# 6.6 Summary

- Incorporating second-order information in descent methods often speeds convergence.  
- Newton's method is a root-finding method that leverages second-order information to quickly descend to a local minimum.  
- The secant method and quasi-Newton methods approximate Newton's method when the second-order information is not directly available.

# 6.7 Exercises

Exercise 6.1. What advantage does second-order information provide about convergence that first-order information lacks?

Solution: Second-order information can guarantee that one is at a local minimum, whereas a gradient of zero is necessary but insufficient to guarantee local optimality.

Exercise 6.2. When finding roots in one dimension, when would we use Newton's method instead of the bisection method?

```julia
mutable struct LimitedMemoryBFGS  $\ll$  : DescentMethod m # history size 8s # step differences ys # gradient changes qs # inverse Hessian contributions   
end   
function init!(M::LimitedMemoryBFGS, f,  $\nabla f$  , x) M.8s  $=$  [] M.ys  $=$  [] M.qs  $=$  [] return M   
end   
function step!(M::LimitedMemoryBFGS, f,  $\nabla f$  , x) 8s, ys, qs, g = M.8s, M.ys, M.qs,  $\nabla f(x)$  m = length(8s) if m > 0 q = g for i in m:-1:1 qs[i]  $=$  copy(q) q  $= = (\delta s[i]\cdot q) / (\gamma s[i]\cdot \delta s[i])\ast \gamma s[i]$  end z  $=$  (ys[m] .\* 8s[m] .\* q)/ (ys[m]  $\cdot$  ys[m]) for i in 1:m z += 8s[i]\* (8s[i]  $\cdot$  qs[i] - ys[i]  $\cdot$  z)/(ys[i]  $\cdot$  8s[i]) end x' = line_search(f, x, -z) else x' = line_search(f, x, -g) end g' =  $\nabla f(x^{\prime})$  push!(8s, x' - x); push!(ys, g' - g) push!(qs, zeros(length(x))) while length(8s) > M.m popfirst!(8s); popfirst!(ys); popfirst!(qs) end return x'   
end
```

Algorithm 6.7. The Limited-memory BFGS descent method, which avoids storing the approximate inverse Hessian. The parameter  $m$  determines the history size. The LimitedMemoryBFGS type also stores the step differences  $\delta s$ , the gradient changes  $\gamma s$ , and storage vectors  $q s$ .

Solution: We would prefer Newton's method if we start sufficiently close to the root and can compute derivatives analytically. Newton's method enjoys a better rate of convergence.

Exercise 6.3. Apply Newton's method to  $f(x) = x^{2}$  from a starting point of your choice. How many steps do we need to converge?

Solution:  $f^{\prime}(x) = 2x$ ,  $f^{\prime \prime}(x) = 2$ . Thus,  $x^{(2)} = x^{(1)} - 2x^{(1)} / 2 = 0$ ; that is, you converge in one step from any starting point.

Exercise 6.4. Apply Newton's method to  $f(x) = \frac{1}{2}\mathbf{x}^{\top}\mathbf{H}\mathbf{x}$  starting from  $\mathbf{x}^{(1)} = [1,1]$ . What have you observed? Use  $\mathbf{H}$  as follows:

$$
\mathbf {H} = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 10 00 \end{array} \right]
$$

Next, apply gradient descent to the same optimization problem by stepping with the unnormalized gradient. Do two steps of the algorithm. What have you observed? Finally, apply the conjugate gradient method. How many steps do you need to converge?

Solution: Since  $\nabla f(\mathbf{x}) = \mathbf{H}\mathbf{x},\nabla^{2}f(\mathbf{x}) = \mathbf{H},$  and  $\mathbf{H}$  is nonsingular, it follows that  $\mathbf{x}^{(2)} = \mathbf{x}^{(1)} - \mathbf{H}^{-1}\mathbf{H}\mathbf{x}^{(1)} = \mathbf{0}$ . That is, Newton's method converges in one step.

Gradient descent diverges:

$$
\begin{array}{l} \mathbf {x}^{(2)} = [ 1, 1 ] - [ 1, 10 00 ] = [ 0, - 99 9 ] \\ \mathbf {x}^{(3)} = [ 0, - 99 9 ] - [ 0, - 10 00 \cdot 99 9 ] = [ 0, 99 80 01 ] \\ \end{array}
$$

Conjugate gradient descent uses the same initial search direction as gradient descent and converges to the minimum in the second step because the optimization objective is quadratic.

Exercise 6.5. Compare Newton's method and the secant method on  $f(x) = x^{2} + x^{4}$ , with  $x^{(1)} = -3$  and  $x^{(0)} = -4$ . Run each method for 10 iterations. Make two plots:

1. Plot  $f$  vs. the iteration for each method.  
2. Plot  $f'$  vs.  $x$ . Overlay the progression of each method, drawing lines from  $(x^{(i)}, f'(x^{(i)}))$  to  $(x^{(i+1)}, 0)$  to  $(x^{(i+1)}, f'(x^{(i+1)}))$  for each transition.

What can we conclude about this comparison?

Solution: The left plot shows convergence for Newton's method approaching floating-point resolution within nine iterations. The secant method is slower to converge because it can merely approximate the derivative.

The right plot shows the projection of the exact and approximate tangent lines with respect to  $f'$  for each method. The secant method's tangent lines have a higher slope, and thus intersect the  $x$ -axis prematurely.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c68549b094bf488ae64a251dec3c06e680ab84ee192429b75c32f8aa45432cc2.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d1d90bf8b99b840347025582b5942b692abf9da6ad024a41898e39ef79b5a286.jpg)

Exercise 6.6. Give an example of a sequence of points  $x^{(1)}, x^{(2)}, \ldots$  and a function  $f$  such that  $f(x^{(1)}) > f(x^{(2)}) > \cdots$  and yet the sequence does not converge to a local minimum. Assume  $f$  is bounded from below.

Solution: Consider the sequence  $x^{(k + 1)} = x^{(k)} / 2$  starting from  $x^{(1)} = -1$  on the function  $f(x) = x^2 - x$ . Clearly the sequence converges to  $x = 0$ , the values for  $f(x)$  are decreasing, and yet the sequence does not converge to a minimizer.

Exercise 6.7. What is the advantage of a Quasi-Newton method over Newton's method?

Solution: It does not need computation or knowledge of the entries of the Hessian, and hence does not require solving a linear system at each iteration.

Exercise 6.8. Give an example where the BFGS update does not exist. What would you do in this case?

Solution: The BFGS update does not exist when  $\delta^\top \gamma \approx 0$ . In that case, simply skip the update.

Exercise 6.9. Suppose we have a function  $f(\mathbf{x}) = (x_{1} + 1)^{2} + (x_{2} + 3)^{2} + 4$ . If we start at the origin, what is the resulting point after one step of Newton's method?

Solution: The objective function is quadratic and can thus be minimized in one step. The gradient is  $\nabla f = [2(x_1 + 1), 2(x_2 + 3)]$ , which is zero at  $\mathbf{x}^* = [-1, -3]$ . The Hessian is positive definite, so  $\mathbf{x}^*$  is the minimum.

Exercise 6.10. Is the outer product approximation of the Hessian given in equation (6.19) always invertible?

Solution: No, the outer product approximation is not necessarily invertible. Consider an outer product formed with  $\mathbf{x} = [x_1, x_2]$ :

$$
\mathbf {H} = \mathbf {x x}^{\top} = \left[ \begin{array}{c c} x_{1}^{2} & x_{1} x_{2} \\ x_{1} x_{2} & x_{2}^{2} \end{array} \right]
$$

This matrix has a determinant  $x_1^2 x_2^2 - x_1 x_2 x_1 x_2 = 0$ , and is thus singular.

Exercise 6.11. In this problem we will derive the optimization problem from which the Davidon-Fletcher-Powell update is obtained. Start with a quadratic approximation at  $\mathbf{x}^{(k)}$ :

$$
f^{(k)} (\mathbf {x}) = y^{(k)} + \left(\mathbf {g}^{(k)}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) + \frac{1}{2} \left(\mathbf {x} - \mathbf {x}^{(k)}\right) ^{\top} \mathbf {H}^{(k)} \left(\mathbf {x} - \mathbf {x}^{(k)}\right)
$$

where  $y^{(k)}$ ,  $\mathbf{g}^{(k)}$ , and  $\mathbf{H}^{(k)}$  are the objective function value, the true gradient, and a positive definite Hessian approximation at  $\mathbf{x}^{(k)}$ .

The next iterate is chosen using line search to obtain:

$$
\mathbf {x}^{(k + 1)} \leftarrow \mathbf {x}^{(k)} - \alpha^{(k)} \left(\mathbf {H}^{(k)}\right) ^{- 1} \mathbf {g}^{(k)}
$$

We can construct a new quadratic approximation  $f^{(k + 1)}$  at  $\mathbf{x}^{(k + 1)}$ . The approximation should enforce that the local function evaluation is correct:

$$
f^{(k + 1)} \left(\mathbf {x}^{(k + 1)}\right) = y^{(k + 1)}
$$

and that the local gradient is correct:

$$
\nabla f^{(k + 1)} (\mathbf {x}^{(k + 1)}) = \mathbf {g}^{(k + 1)}
$$

and that the previous gradient is correct:

$$
\nabla f^{(k + 1)} (\mathbf {x}^{(k)}) = \mathbf {g}^{(k)}
$$

Show that updating the Hessian approximation to obtain  $\mathbf{H}^{(k + 1)}$  requires:10

$$
\mathbf {H}^{(k + 1)} \boldsymbol {\delta}^{(k + 1)} = \boldsymbol {\gamma}^{(k + 1)}
$$

Then, show that in order for  $\mathbf{H}^{(k + 1)}$  to be positive definite, we require:

$$
\left(\boldsymbol {\delta}^{(k + 1)}\right) ^{\top} \boldsymbol {\gamma}^{(k + 1)} > 0
$$

Finally, assuming that the curvature condition is enforced, explain why one then solves the following optimization problem to obtain  $\mathbf{H}^{(k + 1):12}$

$$
\underset{\mathbf {H}} {\text{minimize}} \quad \| \mathbf {H} - \mathbf {H}^{(k)} \|
$$

subject to  $\mathbf{H} = \mathbf{H}^{\top}$

$$
\mathbf {H} \boldsymbol {\delta}^{(k + 1)} = \boldsymbol {\gamma}^{(k + 1)}
$$

where  $\| \mathbf{H} - \mathbf{H}^{(k)}\|$  is a matrix norm that defines a distance between  $\mathbf{H}$  and  $\mathbf{H}^{(k)}$ .

This condition is called the secant equation. The vectors  $\delta$  and  $\gamma$  are defined in equation (6.23).

This condition is called the curvature condition. It can be enforced using the Wolfe conditions during line search.

12 The Davidon-Fletcher-Powell update is obtained by solving such an optimization problem to obtain an analytical solution and then finding the corresponding update equation for the inverse Hessian approximation.

Solution: The new approximation has the form

$$
f^{(k + 1)} (\mathbf {x}) = y^{(k + 1)} + \left(\mathbf {g}^{(k + 1)}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}^{(k + 1)}\right) + \frac{1}{2} \left(\mathbf {x} - \mathbf {x}^{(k + 1)}\right) ^{\top} \mathbf {H}^{(k + 1)} \left(\mathbf {x} - \mathbf {x}^{(k + 1)}\right)
$$

using the true function value and gradient at  $\mathbf{x}^{(k + 1)}$  but requires an updated Hessian  $\mathbf{H}^{(k + 1)}$ . This form automatically satisfies  $f^{(k + 1)}(\mathbf{x}^{(k + 1)}) = y^{(k + 1)}$  and  $\nabla f^{(k + 1)}(\mathbf{x}^{(k + 1)}) = \mathbf{g}^{(k + 1)}$ . We must select the new Hessian to satisfy the third condition:

$$
\begin{array}{l} \nabla f^{(k + 1)} (\mathbf {x}^{(k)}) = \mathbf {g}^{(k + 1)} + \mathbf {H}^{(k + 1)} \left(\mathbf {x}^{(k)} - \mathbf {x}^{(k + 1)}\right) \\ = \mathbf {g}^{(k + 1)} - \mathbf {H}^{(k + 1)} \left(\mathbf {x}^{(k + 1)} - \mathbf {x}^{(k)}\right) \\ = \mathbf {g}^{(k + 1)} - \mathbf {H}^{(k + 1)} \boldsymbol {\delta}^{(k + 1)} \\ = \mathbf {g}^{(k)} \\ \end{array}
$$

We can rearrange and substitute to obtain:

$$
\mathbf {H}^{(k + 1)} \boldsymbol {\delta}^{(k + 1)} = \boldsymbol {\gamma}^{(k + 1)}
$$

Recall that a matrix  $\mathbf{A}$  is positive definite if for every nonzero vector  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x} > 0$ . If we multiply the secant equation by  $\delta^{(k + 1)}$  we obtain the curvature condition:

$$
\left(\boldsymbol {\delta}^{(k + 1)}\right) ^{\top} \mathbf {H}^{(k + 1)} \boldsymbol {\delta}^{(k + 1)} = \left(\boldsymbol {\delta}^{(k + 1)}\right) ^{\top} \boldsymbol {\gamma}^{(k + 1)} > 0
$$

We seek a new positive definite matrix  $\mathbf{H}^{(k + 1)}$ . All positive definite matrices are symmetric, so calculating a new positive definite matrix requires specifying  $n(n + 1) / 2$  variables. The secant equation imposes  $n$  conditions on these variables, leading to an infinite number of solutions. In order to have a unique solution, we choose the positive definite matrix closest to  $\mathbf{H}^{(k)}$ . This objective leads to the desired optimization problem.

# 7 Direct Methods

Direct methods rely solely on the objective function  $f$ . These methods are also called zero-order, black box, pattern search, or derivative-free methods. Direct methods do not rely on derivative information to guide them toward a local minimum or identify when they have reached a local minimum. They use other criteria to choose the next search direction and to judge when they have converged.

# 7.1 Cyclic Coordinate Search

Cyclic coordinate search, also known as coordinate descent or taxicab search, simply alternates between coordinate directions for its line search. The search starts from an initial  $\mathbf{x}^{(1)}$  and optimizes the first input:

$$
\mathbf {x}_{1}^{(2)} = \underset{x_{1}} {\arg \min } f \left(x_{1}, x_{2}^{(1)}, x_{3}^{(1)}, \dots , x_{n}^{(1)}\right) \tag {7.1}
$$

Having solved this, it optimizes the next coordinate:

$$
\mathbf {x}_{2}^{(3)} = \underset{x_{2}} {\arg \min } f \left(x_{1}^{(2)}, x_{2}, x_{3}^{(2)}, \dots , x_{n}^{(2)}\right) \tag {7.2}
$$

This process is equivalent to doing a sequence of line searches along the set of  $n$  basis vectors, where the  $i$ th basis vector is all zero except for the  $i$ th component, which has value 1 (algorithm 7.1). For example, the third basis function in a four-dimensional space is  $\mathbf{e}^{(3)} = [0,0,1,0]$ . Figure 7.1 shows an example of a search through a two-dimensional space.

$$
\text{basis} (i, n) = [ k = = i ? 1. 0: 0. 0 \text{for} k \text{in} 1: n ]
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1cdafc7c5b6d9ea5fc7c7a74a6c11c7f025dd81954e1776291d340ba6564e400.jpg)  
Figure 7.1. Cyclic coordinate descent alternates between coordinate directions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/484b6ee4438b6be512c95d1de269029eadd9853f468523eb5aa2e45833790236.jpg)  
Figure 7.2. Above is an example of how cyclic coordinate search can get stuck. Moving in either of the coordinate directions will result only in increasing  $f$ , but moving diagonally, which is not allowed in cyclic coordinate search, can result in lowering  $f$ .

Algorithm 7.1. A function for constructing the  $i$ th basis vector of length  $n$ .

Like steepest descent, cyclic coordinate search is guaranteed either to improve or to remain the same with each iteration. No significant improvement after a full cycle over all coordinates indicates that the method has converged. Algorithm 7.2 provides an implementation. As figure 7.2 shows, cyclic coordinate search can fail to find even a local minimum.

```matlab
function cyclic_coordinates_descent(f, x, ε)  
 $\Delta$ , n = Inf, length(x)  
while abs( $\Delta$ ) > ε  
    x_orig = x  
    for i in 1 : n  
        d = basis(i, n)  
        x = line_search(f, x, d)  
    end  
     $\Delta$  = norm(x - x_orig)  
end  
return x
```

The method can be augmented with an acceleration step to help traverse diagonal valleys. For every full cycle starting with optimizing  $\mathbf{x}^{(1)}$  along  $\mathbf{e}^{(1)}$  and ending with  $\mathbf{x}^{(n + 1)}$  after optimizing along  $\mathbf{e}^{(n)}$ , an additional line search is conducted along the direction  $\mathbf{x}^{(n + 1)} - \mathbf{x}^{(1)}$ . An implementation is provided in algorithm 7.3 and an example search trajectory is shown in figure 7.3.

```julia
function cyclic Coordinate_descent_with acceleraion_step(f,x,ε)  $\Delta$  n  $=$  Inf,length(x) while abs(△）>ε x_orig=x for i in 1:n d  $=$  basis(i，n) x  $=$  line_searchf，x，d) end x  $=$  line_searchf，x，x-x_orig)# acceleration step  $\Delta =$  norm(x-x_orig)   
end return x   
end
```

Algorithm 7.2. The cyclic coordinate descent method takes as input the objective function  $f$  and a starting point  $x$ , and it runs until the step size over a full cycle is less than a given tolerance  $\epsilon$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/00db19f1e168be601f739d0f56b59c82215907986eb15650b39e1f3526e937d3.jpg)  
Figure 7.3. Adding the acceleration step to cyclic coordinate descent helps traverse valleys. Six steps are shown for both the original and accelerated versions.

Algorithm 7.3. The cyclic coordinate descent method with an acceleration step takes as input the objective function  $f$  and a starting point  $x$ , and it runs until the step size over a full cycle is less than a given tolerance  $\epsilon$ .

# 7.2 Powell's Method

Powell's method<sup>1</sup> can search in directions that are not orthogonal to each other. The method can automatically adjust for long, narrow valleys that might otherwise require a large number of iterations for cyclic coordinate descent or other methods that search in axis-aligned directions.

The algorithm maintains a list of search directions  $\mathbf{u}^{(1)},\ldots ,\mathbf{u}^{(n)}$ , which are initially the coordinate basis vectors,  $\mathbf{u}^{(i)} = \mathbf{e}^{(i)}$  for all  $i$ . Starting at  $\mathbf{x}^{(1)}$ , Powell's method conducts a line search for each search direction in succession, updating the design point each time:

$$
\mathbf {x}^{(i + 1)} \leftarrow \text{line -search} (f, \mathbf {x}^{(i)}, \mathbf {u}^{(i)}) \text{for} i \text{in} 1: n \tag {7.3}
$$

Next, all search directions are shifted down by one index, dropping the oldest search direction,  $\mathbf{u}^{(1)}$ :

$$
\mathbf {u}^{(i)} \leftarrow \mathbf {u}^{(i + 1)} \text{for} i \text{in} 1: n - 1 \tag {7.4}
$$

The last search direction is replaced with the direction from  $\mathbf{x}^{(1)}$  to  $\mathbf{x}^{(n + 1)}$ , which is the overall direction of progress over the last cycle:

$$
\mathbf {u}^{(n)} \leftarrow \mathbf {x}^{(n + 1)} - \mathbf {x}^{(1)} \tag {7.5}
$$

and another line search is conducted along the new direction to obtain a new  $\mathbf{x}^{(1)}$ . This process is repeated until convergence. Algorithm 7.4 provides an implementation. Figure 7.4 shows an example search trajectory.

Powell showed that for quadratic functions, after  $k$  full iterations the last  $k$  directions will be mutually conjugate. Recall that  $n$  line searches along mutually conjugate directions will optimize an  $n$ -dimensional quadratic function. Thus,  $n$  full iterations of Powell's method, totaling  $n(n + 1)$  line searches, will minimize a quadratic function.

The procedure of dropping the oldest search direction in favor of the overall direction of progress can lead the search directions to become linearly dependent. Without search vectors that are linearly independent, the search directions can no longer cover the full design space, and the method may not be able to find the minimum. This weakness can be mitigated by periodically resetting the search directions to the basis vectors.[2]

Powell's method was first introduced by M.J.D. Powell, "An Efficient Method for Finding the Minimum of a Function of Several Variables Without Calculating Derivatives," Computer Journal, vol. 7, no. 2, pp. 155-162, 1964. An overview is presented by W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery, Numerical Recipes in C: The Art of Scientific Computing, 2nd ed. Cambridge University Press, 1982.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4eab72ec6767c71d1ccbb579d5f9c0d4d2eab153742dad14eb86524055fce202.jpg)  
Figure 7.4. Powell's method starts the same as cyclic coordinate descent but iteratively learns conjugate directions.

One recommendation is to reset every  $n$  or  $n + 1$  iterations.

```matlab
function powell(f, x, e)  
 $\Delta, n = \text{Inf}, \text{length}(x)$   
U = [basis(i, n) for i in 1 : n]  
while  $\Delta > \epsilon$ $\mathbf{x}' = \mathbf{x}$   
for i in 1 : n  
d = U[i]  
 $\mathbf{x}' = \text{line_search}(\mathbf{f}, \mathbf{x}', d)$   
end  
for i in 1 : n-1  
U[i] = U[i+1]  
end  
U[n] = d =  $\mathbf{x}' - \mathbf{x}$ $\mathbf{x}' = \text{line_search}(\mathbf{f}, \mathbf{x}', d)$ $\Delta = \text{norm}(\mathbf{x}' - \mathbf{x})$ $\mathbf{x} = \mathbf{x}'$   
end  
return x  
end
```

Algorithm 7.4. Powell's method, which takes the objective function  $f$ , a starting point  $x$ , and a tolerance  $\epsilon$ .

# 7.3 Hooke-Jeeves

The Hooke-Jeeves method (algorithm 7.5) traverses the search space based on evaluations at small steps in each coordinate direction.<sup>3</sup> At every iteration, the Hooke-Jeeves method evaluates  $f(\mathbf{x})$  and  $f(\mathbf{x} \pm \alpha \mathbf{e}^{(i)})$  for a given step size  $\alpha$  in every coordinate direction from an anchoring point  $\mathbf{x}$ . It accepts any improvement it may find. If no improvements are found, it will decrease the step size. The process repeats until the step size is sufficiently small. Figure 7.5 shows a few iterations.

3 R. Hooke and T. A. Jeeves, "Direct Search Solution of Numerical and Statistical Problems," Journal of the ACM (JACM), vol. 8, no. 2, pp. 212-229, 1961.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0bbe9231bf010f13282a5e13697a8e28d8f7e1d58bb436e6c93afe9244df9969.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1bc59dc6236f374b1b7998e5c0c0da9ab2da84e34aa2a7958d5d63817ab48d98.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/31511b7ddf541668d3d2a5074c8da1de1c72b71aec1967ba3240932da7de0b29.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/210cb38a47a21e43d5fc85803c09733daf1400a71de966919a3f266e44cd9e63.jpg)

One step of the Hooke-Jeeves method requires  $2n$  function evaluations for an  $n$ -dimensional problem, which can be expensive for problems with many

Figure 7.5. The Hooke-Jeeves method, proceeding left to right. It begins with a large step size but then reduces it once it cannot improve by taking a step in any coordinate direction.

dimensions. The Hooke-Jeeves method is susceptible to local minima. The method has been proven to converge on certain classes of functions. $^4$

```matlab
function hooke_jeves(f, x,  $\alpha$ ,  $\epsilon$ ,  $\gamma = 0.5$ )  
y, n = f(x), length(x)  
while  $\alpha > \epsilon$   
improved = false  
best = (x=x, y=y)  
for i in 1 : n  
    for sgn in (-1,1)  
        x' = x + sgn *  $\alpha$ *basis(i, n)  
        y' = f(x')  
        if y' < best.y  
            best, improved = (x=x', y=y'), true  
        end  
    end  
    x, y = best  
    if !improved  
         $\alpha *= \gamma$   
    end  
end  
return x
```

4 E. D. Dolan, R. M. Lewis, and V. Torczon, "On the Local Convergence of Pattern Search," SIAM Journal on Optimization, vol. 14, no. 2, pp. 567-583, 2003.

Algorithm 7.5. The Hooke-Jeeves method, which takes the target function  $f$ , a starting point  $x$ , a starting step size  $\alpha$ , a tolerance  $\epsilon$ , and a step decay  $\nu$ . The method runs until the step size is less than  $\epsilon$  and the points sampled along the coordinate directions do not provide an improvement. Based on the implementation from A.F. Kaupe Jr., "AlGORITHM 178: Direct Search," Communications of the ACM, vol. 6, no. 6, pp. 313-314, 1963.

# 7.4 Generalized Pattern Search

In contrast with the Hooke-Jeeves method, which searches in the coordinate directions, generalized pattern search can search in arbitrary directions. A pattern  $\mathcal{P}$  can be constructed from a set of directions  $\mathcal{D}$  about an anchoring point  $\mathbf{x}$  with a step size  $\alpha$  according to:

$$
\mathcal {P} = \left\{\mathbf {x} + \alpha \mathbf {d} \text{for each} \mathbf {d} \text{in} \mathcal {D} \right\} \tag {7.6}
$$

The Hooke-Jeeves method uses  $2n$  directions for problems in  $n$  dimensions, but generalized pattern search can use as few as  $n + 1$ .

For generalized pattern search to converge to a local minimum, certain conditions must be met. The set of directions must be a positive spanning set, which means that we can construct any point in  $\mathbb{R}^n$  using a nonnegative linear combination of the directions in  $\mathcal{D}$ . A positive spanning set ensures that at least one of the directions is a descent direction from a location with a nonzero gradient.<sup>6</sup>

5C. Audet and J.E. Dennis Jr., "Mesh Adaptive Direct Search Algorithms for Constrained Optimization," SIAM Journal on Optimization, vol. 17, no. 1, pp. 188-217, 2006.  
6 Convergence guarantees for generalized pattern search require that all sampled points fall on a scaled lattice. Each direction must thus be a product  $\mathbf{d}^{(j)} = \mathbf{G}\mathbf{z}^{(j)}$  for a fixed nonsingular  $n\times n$  matrix  $\mathbf{G}$  and integer vector z.V.TorczonOn the Convergence of Pattern Search Algorithms, SiAM Journal of Optimization, vol. 7, no. 1, pp. 1-25, 1997.

We can determine whether a given set of directions  $\mathcal{D} = \{\mathbf{d}^{(1)},\mathbf{d}^{(2)},\dots ,\mathbf{d}^{(m)}\}$  in  $\mathbb{R}^n$  is a positive spanning set. First, we construct the matrix  $\mathbf{D}$  whose columns are the directions in  $\mathcal{D}$  (see figure 7.6). The set of directions  $\mathcal{D}$  is a positive spanning set if  $\mathbf{D}$  has full row rank and if  $\mathbf{D}\mathbf{x} = -\mathbf{D}\mathbf{1}$  with  $\mathbf{x}\geq \mathbf{0}$  has a solution. This optimization problem is identical to the initialization phase of a linear program, which is covered in chapter 12.

7 R.G. Regis, "On the Properties of Positive Spanning Sets and Positive Bases," Optimization and Engineering, vol. 17, no. 1, pp. 229-262, 2016.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c79df67c632e204176134f1c1d95923d0466e84bd41de76c99af3b659d58da67.jpg)  
only positively spans the cone

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f3313bb354d74f5a0027d7916b7f3bf49942c7c9ee3573d005994b37cbb996ea.jpg)  
only positively spans 1d space

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5e37e9fb8b18f4bea1c104814709b28bfb468631163d1056b25f9dffa71d17c4.jpg)  
positively spans  $\mathbb{R}^2$  
Figure 7.6. A valid pattern for generalized pattern search requires a positive spanning set. These directions are stored in the set  $\mathcal{D}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dfd6755589d9eb79e65474f262437813a76202cb5705c5dd99f4d46167b229c2.jpg)  
Figure 7.7. All previous points in generalized pattern search lie on a scaled lattice, or mesh. The lattice is not explicitly constructed and need not be axis-aligned.

```matlab
function generalized_pattern_search(f, x,  $\alpha$ , D,  $\epsilon$ , y=0.5)  
y, n = f(x), length(x)  
while  $\alpha > \epsilon$   
    improved = false  
for (i, d) in enumerate(D)  
    x' = x +  $\alpha * d$   
    y' = f(x')  
    if y' < y  
        x, y, improved = x', y', true  
        D = pushfirst!(deleteat!(D, i), d)  
    end  
end  
if!improved  
     $\alpha *= \gamma$   
end  
return x
```

Algorithm 7.6. Generalized pattern search, which takes the target function  $f$ , a starting point  $x$ , a starting step size  $\alpha$ , a set of search directions  $D$ , a tolerance  $\epsilon$ , and a step decay  $\gamma$ . The method runs until the step size is less than  $\epsilon$  and the points sampled along the coordinate directions do not provide an improvement.

The implementation of generalized pattern search in algorithm 7.6 contains additional enhancements over the original Hooke-Jeeves method. First, the implementation is opportunistic—as soon as an evaluation improves the current best design, it is accepted as the anchoring design point for the next iteration. Second, the implementation uses dynamic ordering to accelerate convergence—a direction that leads to an improvement is promoted to the beginning of the list of directions. Figure 7.7 shows a few iterations of the algorithm.

# 7.5 Nelder-Mead Simplex Method

The Nelder-Mead simplex method uses a simplex to traverse the space in search of a minimum. A simplex is a generalization of a tetrahedron to  $n$ -dimensional space. The Nelder-Mead method uses a series of rules that dictate how the simplex is updated based on evaluations of the objective function at its vertices. Like the Hooke-Jeeves method, the simplex can move around while roughly maintaining its size, and it can shrink as it approaches an optimum.

The simplex consists of the points  $\mathbf{x}^{(1)},\ldots ,\mathbf{x}^{(n + 1)}$ . Let  $\mathbf{x}_h$  be the vertex with the highest function value, let  $\mathbf{x}_s$  be the vertex with the second highest function value, and let  $\mathbf{x}_{\ell}$  be the vertex with the lowest function value. Let  $\bar{\mathbf{x}}$  be the mean

8 J. A. Nelder and R. Mead, "A Simplex Method for Function Minimization," The Computer Journal, vol. 7, no. 4, pp. 308-313, 1965. We incorporate the improvements in J. C. Lagarias, J. A. Reeds, M. H. Wright, and P. E. Wright, "Convergence Properties of the Nelder-Mead Simplex Method in Low Dimensions," SIAM Journal on Optimization, vol. 9, no. 1, pp. 112-147, 1998.

of all vertices except the highest point  $\mathbf{x}_h$ . For any design point  $\mathbf{x}_{\theta}$ , let  $y_{\theta} = f(\mathbf{x}_{\theta})$ . A single iteration then evaluates four simplex operations (figure 7.8):

Reflection.  $\mathbf{x}_r = \bar{\mathbf{x}} +\alpha (\bar{\mathbf{x}} -\mathbf{x}_h)$ , reflects the highest-valued point over the centroid. This typically moves the simplex from high regions toward lower regions. Here,  $\alpha >0$  and is typically set to 1.

Expansion.  $\mathbf{x}_e = \bar{\mathbf{x}} +\beta (\mathbf{x}_r - \bar{\mathbf{x}})$ , like reflection, but the reflected point is sent even further. This is done when the reflected point has an objective function value less than all points in the simplex. Here,  $\beta >\max (1,\alpha)$  and is typically set to 2.

Contraction.  $\mathbf{x}_c = \bar{\mathbf{x}} +\gamma (\mathbf{x}_h - \bar{\mathbf{x}})$ , the simplex is shrunk down by moving away from the worst point. It is parameterized by  $\gamma \in (0,1)$  which is typically set to 0.5.

Shrinkage. All points are moved toward the best point, typically halving the separation distance.

Figure 7.9 outlines the procedure that is implemented in algorithm 7.7. Figure 7.10 shows several iterations of the algorithm.

The convergence criterion for the Nelder-Mead simplex method is unlike Powell's method in that it considers the variation in the function values rather than the changes to the points in the design space. It compares the standard deviation of the sample  $y^{(1)}, \ldots, y^{(n+1)}$  to a tolerance  $\epsilon$ . This value is high for a simplex over a highly curved region, and it is low for a simplex over a flat region. A highly curved region indicates that there is still further optimization possible.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d977983b3c13bfb95fb326c03e9e417244eb0cff44806ccd489fbdfc96e14d4b.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6a0fa5d837b35adc46f2403e4f874a52b73ed6fffe2acd155f4ea1bf4bf39560.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/df5e3ec44cde7e9f80e2b78ded1b090a1a4b9b305763944cac007f544ca6a30b.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9507ce74f4db28af77e3155bcd5fe734e5ecfb54d87471644cfec400c9b2c620.jpg)  
Figure 7.8. The Nelder-Mead simplex operations visualized in two dimensions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/62f65be0eac4dd0c51feae4988b5c2f177ab7058287117c14019fb3499c4ac5c.jpg)  
Figure 7.9. Flowchart for the Nelder-Mead algorithm.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3786b55acb92db57c0a4ff15c0c3fa8c72d47f0069a58d105984a5c2d318ded0.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dcbefc1ebce58bbca1605c90c05420424ce84d98695933d1ca7a006b28576990.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5d88f19b4ce43ef21414bdbb686b604e125b1b87e82174853212e1950b9c3773.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/80c0cbb1d3d46b40270559762210e915df28f31c679e9fd3ccf7e49fa4595b86.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/856f1b9900f385e064558059bc67c04cc1db04a8015ba6a3bf4566df3f907659.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/66a9ea03e9757092a667569628ccd7b4780bc33fa182f57e2f6b6d3a79e7c18d.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5cdc32f56e44201c6a9e32672fa2bd7f60c250669655548c68519145346c77f5.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3bef378e95c0edaea28e219b7a77e9e6a01832632c666cbcdb43d34df48f2064.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5ad2c48aa2c44fe84c83c5b042b65e801db4d66ebaa31e57ded64c46cffe2b88.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d9586c857c82963fdb78f388dafe5f869013e0b7294d2fb0563ded7cc956c019.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7017a68b0d3140fac52288a087a329a509e7cad01b89c8d40ad79bbdc0ac5673.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cca68089ec454ebbe401ffc87fbfd43b4fc4086f2bc6fd39397b05e547d6e955.jpg)  
Figure 7.10. The Nelder-Mead method, proceeding left to right and top to bottom.

```julia
function nelder_mead(f, S,  $\epsilon$ ;  $\alpha = 1.0$ ,  $\beta = 2.0$ ,  $\gamma = 0.5$ )  
 $\Delta$ , y.arr = Inf, f.(S)  
while  $\Delta > \epsilon$   
p = sortperm(y.arr) # sort lowest to highest  
S, y.arr = S[p], y.arr[p]  
x1, y1 = S[1], y.arr[1] # lowest  
xh, yh = S(end], y.arr(end] # highest  
xs, ys = S(end-1], y.arr(end-1] # second-highest  
xm = mean(S[1:end-1]) # centroid  
xr = xm +  $\alpha * (xm - xh)$  # reflection point  
yr = f(xr)  
if yr < yl  
xe = xm +  $\beta * (xr - xm)$  # expansion point  
ye = f(xe)  
S[end], y.arr(end] = ye < yr ? (xe, ye) : (xr, yr)  
elseif yr ≥ ys  
if yr < yh  
xh, yh, S(end], y.arr(end] = xr, yr, xr, yr  
end  
xc = xm +  $\gamma * (xh - xm)$  # contraction point  
yc = f(xc)  
if yc > yh  
for i in 2 : length(y.arr)  
S[i] = (S[i] + xl)/2  
y.arr[i] = f(S[i])  
end  
else  
S[end], y.arr(end] = xc, yc  
end  
else  
S[end], y.arr(end] = xr, yr  
end  
 $\Delta = \text{std}(y\_arr, \text{corrected})$   
end  
return S[argmin(y Arr)]
```

Algorithm 7.7. The Nelder-Mead simplex method, which takes the objective function  $f$ , a starting simplex  $S$  consisting of a list of vectors, and a tolerance  $\epsilon$ . The Nelder-Mead parameters can be specified as well and default to recommended values.

# 7.6 Divided Rectangles

The divided rectangles algorithm, or DIRECT for DIvided RECTangles, incrementally refines a retangular partition of the design space as illustrated in figure 7.11. The refinement is driven by a heuristic that involves reasoning about potential Lipschitz constants.

To simplify the mathematics and to avoid oversensitivity to dimensions with larger domains, DIRECT first normalizes the search space to be the unit hypercube. If we are minimizing  $f(\mathbf{x})$  in the interval between lower and upper ranges  $\mathbf{a}$  and  $\mathbf{b}$ , DIRECT will instead minimize:

$$
g (\mathbf {x}) = f (\mathbf {x} \odot (\mathbf {b} - \mathbf {a}) + \mathbf {a}) \tag {7.7}
$$

After finding a minimizer  $\mathbf{x}^*$  of  $g$ , a minimizer of  $f$  is

$$
\mathbf {x}^{*} \odot (\mathbf {b} - \mathbf {a}) + \mathbf {a} \tag {7.8}
$$

DIRECT maintains a partition of this unit hypercube into hyperrectangular intervals. Each interval has a center  $\mathbf{c}^{(i)}$  and an associated objective function value  $f(\mathbf{c}^{(i)})$ . Each interval also has a radius  $r^{(i)}$ , which is the distance from the center to a vertex. Figure 7.12 shows such a partition and a plot of the intervals' objective function values at their centers with respect to their radii.

DIRECT begins every iteration by identifying intervals to be split with additional function evaluations. It splits the intervals by reasoning about possible Lipschitz constants. Given a Lipschitz constant  $\ell$ , the lowerbound for an interval is a circular cone extending downwards from its center  $\mathbf{c}^{(i)}$ :

$$
f (\mathbf {x}) \geq f \left(\mathbf {c}^{(i)}\right) - \ell \| \mathbf {x} - \mathbf {c}^{(i)} \| _{2} \tag {7.9}
$$

This lowerbound is constrained by the extents of the interval. Its lowest value is at its vertices, which are all at a distance  $r^{(i)}$  from the center with value  $f(\mathbf{c}^{(i)}) - \ell r^{(i)}$ . Figure 7.13 shows how this minimum value can be seen as the  $x$ -intercept of a line of slope  $\ell$  passing through the point  $(r^{(i)}, f(\mathbf{c}^{(i)}))$ .

Figure 7.14 shows how all such lines can be constructed to find the interval that produces the lowest lowerbound for a particular Lipschitz constant. That interval is selected for splitting.

9 D. R. Jones, C. D. Perttunen, and B. E. Stuckman, "Lipschitzian Optimization Without the Lipschitz Constant," Journal of Optimization Theory and Application, vol. 79, no. 1, pp. 157-181, 1993.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/41a54010ec9f73873dda9bf4cbd3abae406855d716aec4af886dad24608a5d48.jpg)  
Figure 7.11. The DIRECT method after 16 iterations on the Branin function (appendix B.3). The cells are much denser around the minima of the Branin function because the DIRECT method is designed to increase resolution in promising regions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fd33213558b1bb8f276f3574de6450c6b399188eeb0cfe89c189b976a124ea8c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0cd504b65c4a9615a61101c0853740cde5cc662e2d32b30f3f25628ea6c047df.jpg)  
Figure 7.12. The left plot shows the intervals for the DIRECT method after 5 iterations on the Branin function, appendix B.3. The right plot shows the interval objective function values versus their radii, which is useful for identifying intervals to split with further evaluations.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0b2423181ee409226bf5d49d4e23d76350ba77a7df20de82029e45e95c0c9b5e.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/297779e379457cd613cd5be88cf94bd5126ecd31edab17dd227639feda6e0a7a.jpg)  
Figure 7.13. The left plot shows the Lipschitz lowerbounds constructed for the DIRECT intervals using the Lipschitz constant  $\ell = 200$ , and highlights one interval. The right plot shows how the minimum value for the lowerbound within the highlighted interval is the same as the  $x$ -intercept for a line of slope  $\ell$  passing through that interval's  $(r, f(c))$  point.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f26917d73b3727fe08055deb8f1f4ab451ceb949fbfbc4225d4cf2a072269966.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c771e735bb2c904d835a9c1a440efd010d279c7adac9206d6efe8d3722d18a94.jpg)  
Figure 7.14. The left plot continues to show the Lipschitz lower bounds constructed for the DIRECT intervals using the Lipschitz constant  $\ell = 200$ , but now highlights the interval containing the lowest value. The right plot shows how the lowest lowerbound for a given Lipschitz constant is the one with the lowest  $x$ -intercept in the right-hand plot.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7b7b650380ffdd762e9c9ac26f18504d0842295a8ce3d5bcdab821c2d893af2a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2534e5c1b5772b2353f6e84acb412487b7431519b676c04bcd939a2c79049f73.jpg)  
Figure 7.15. The left plot shows the split intervals identified for this iteration of DIRECT on the Branin function. The right plot shows the lower-right convex hull formed by the points associated with these intervals in  $(r, f(\mathbf{c}))$  space.

The DIRECT method does not operate on just one value for the Lipschitz constant, but selects all intervals for which a Lipschitz constant exists such that their lower bounds have minimal value. These split intervals form a piecewise-linear boundary $^{10}$  along the lower-right of the  $(r,f(\mathfrak{c}))$  space, as shown in figure 7.15.

The selected intervals are split into thirds along the axis directions. The order in which we split an interval's dimensions matters, as shown in figure 7.16. DIRECT will choose a split order such that lower (better) function evaluations receive larger sub-rectangles, encouraging their selection for later splitting. When splitting a region without equal side lengths, only the longest dimensions are split (figure 7.17).

These points are the lower-right convex hull.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/806748137dc2eaacab2b05203eaf09dbb516d1b4c8dd3227d984892c165556e1.jpg)

The width in a given dimension depends on how many times that dimension has been split. Since DIRECT always splits axis directions by thirds, a dimension that has been split  $d$  times will have a width of  $3^{-d}$ . If we have  $n$  dimensions and track how many times each dimension of a given interval has been split in a vector  $\mathbf{d}$ , then the radius of that interval is

$$
r = \left\| \left[ \frac{1}{2 \cdot 3^{d_{1}}}, \dots , \frac{1}{2 \cdot 3^{d_{n}}} \right] \right\| _{2} \tag {7.10}
$$

DIRECT is implemented in algorithm 7.8. Iterations of DIRECT begin by finding all split intervals (algorithm 7.9).<sup>11</sup> It then splits all split intervals (algorithm 7.10). Two iterations of DIRECT in two dimensions are demonstrated in example 7.1.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/15ff1b3977fe7304920ac5f55b8ae19c6706c2c907dfdc901442f6669681cbb1.jpg)  
Figure 7.16. Interval splitting in multiple dimensions for DIRECT requires choosing an ordering for the split dimensions.  
Figure 7.17. DIRECT will only split the longest dimensions of intervals.

11 The algorithm only divides intervals larger than a minimum radius. This minimum radius prevents inefficient function evaluations very close to existing points.

```julia
struct DirectRectangle
    c # center point
    y # center point value
    d # number of divisions per dimension
    r # the radius of the interval
end
function direct(f, a, b, k_max, r_min)
    g = x → f(x .* (b - a) + a) # evaluate within unit hypercube
    n = length(a)
    c = fill(0.5, n)
    □s = [DirectRectangle(c, g(c), fill(0, n), sqrt(0.5^a))]
    c_best = c
    for k in 1 : k_max
        □s_split = direct_split_intervals(□s, r_min)
        setdiff!(□s, □s_split)
        for □_split in □s_split
            append!(□s, split_interval(□_split, g))
        end
        c_best = □s[findmin(□.y for □ in □s)[2]].c
    end
return c_best.*(b - a) + a # from unit hypercube
end
```

Algorithm 7.8. DIRECT, which takes the multidimensional objective function  $f$ , vector of lower bounds  $a$ , vector of upper bounds  $b$ , number of iterations  $k_{\text{max}}$ , and minimum interval radius  $r_{\text{min}}$ . It returns the best coordinate. DIRECT maintains a set of hyperrectangular intervals defined by the DirectRectangle structure.

```txt
function is_CCw(a, b, c) # is a  $\rightarrow$  b  $\rightarrow$  c counter-clockwise return a.r\*(b.y-c.y)-a.y\*(b.r-c.r)+(b.r*c.y-b.y*c.r) < 1e-6 end   
function direct_split_intervals(□s, r_min) hull  $=$  DirectRectangle[] # Sort the rects by increasing r, then by increasing y sort!(□s, by  $= \square \rightarrow (\square .r,\square .y)$ ) for in if length hull)  $\geq 1$  &&  $\square .r = =$  hull[end].r # Repeated r values cannot be improvements continue end if length hull)  $\geq 1$  &&  $\square .y\leq$  hull[end].y # Remove the last point if the new one is better pop!(hull) end if length hull)  $\geq 2$  && is_CCw(hull[end-1], hull[end],  $\square$  # Remove the last point if the new one is better pop!(hull) end push!(hull,  $\square$  ) end # Only split intervals larger than the minimum radius filter!  $(\square \rightarrow \square .r\geq r_{\min}$  , hull) return hull
```

Algorithm 7.9. A routine for obtaining the split intervals from a given list of DirectRectangles  $\square s$  and a minimum radius  $r_{\min}$ . The potentially optimal intervals form a lower-right convex hull in  $r$  and  $y$ .

```julia
function split_interval(□,g) c,n,d_min,d = □.c,length(□.c),minimum(□.d)，copy(□.d) dirs，δ=findall(d==d_min)，3.0^(-d_min-1) #Sample the objective function in all split directions,#and track the minimum value in each axis. Cs=[（c+8\*basis(i,n)，c-8\*basis(i,n))foriin dirs] Ys=[（g(C[1]），g(C[2]））forCinCs] minvals=[min(Y[1],Y[2])forYin Ys] #Split the axes in order by increasing minimum value. □s  $=$  DirectRectangle[] forjin sortperm(minvals) d［dir[s[j]]+=1#increment the number of splits C,Y,r=Cs[j],Ys[j],norm(0.5*3.0.^(-d)) push!(□s，DirectRectangle(C[1],Y[1],copy(d),r)) push!(□s，DirectRectangle(C[2],Y[2],copy(d),r)) end r=norm(0.5*3.0.^(-d)) push!(□s，DirectRectangle(c，□.y,d,r)) return s   
end
```

Algorithm 7.10. The routine for splitting an interval  $\square$ , where  $g$  is the objective function in the unit hypercube. It returns a list of the resulting smaller intervals.

We can use DIRECT to optimize the flower function over  $x_{1} \in [-1,3]$ ,  $x_{2} \in [-2,1]$ . The function is first normalized to the unit hypercube such that  $x_{1}', x_{2}' \in [0,1]$ :

$$
f (x_{1}^{\prime}, x_{2}^{\prime}) = \mathrm{flower} (4 x_{1}^{\prime} - 1, 3 x_{2}^{\prime} - 2)
$$

The objective function is sampled at  $[0.5, 0.5]$  to obtain 0.158. We have a single interval with center  $[0.5, 0.5]$  and side lengths  $[1, 1]$ . The interval is divided twice, first into thirds in  $x_1'$  and then the center interval is divided into thirds in  $x_2'$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/518253f9542ab313f40164fe990775a0308d32d544bfb9127e12532b76be9a23.jpg)

We now have five intervals:

<table><tr><td>interval</td><td>center</td><td>side lengths</td><td>vertex distance</td><td>center value</td></tr><tr><td>1</td><td>[1/6,3/6]</td><td>[1/3,1]</td><td>0.527</td><td>0.500</td></tr><tr><td>2</td><td>[5/6,3/6]</td><td>[1/3,1]</td><td>0.527</td><td>1.231</td></tr><tr><td>3</td><td>[3/6,3/6]</td><td>[1/3,1/3]</td><td>0.236</td><td>0.158</td></tr><tr><td>4</td><td>[3/6,1/6]</td><td>[1/3,1/3]</td><td>0.236</td><td>2.029</td></tr><tr><td>5</td><td>[3/6,5/6]</td><td>[1/3,1/3]</td><td>0.236</td><td>1.861</td></tr></table>

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/477906a9f8b176fffa51263c01c696942542011a1e153a1f53d072b75f5a5740.jpg)

We next split on the two intervals centered at  $[1/6, 3/6]$  and  $[3/6, 3/6]$ .

Example 7.1. The first two iterations of DIRECT on the flower function (appendix B.4).

# 7.7 Summary

- Direct methods rely solely on the objective function and do not use derivative information.  
- Cyclic coordinate search optimizes one coordinate direction at a time.  
Powell's method adapts search directions based on the direction of progress.  
- Hooke-Jeeves searches in each coordinate direction from the current point using a step size that is adapted over time.  
- Generalized pattern search is similar to Hooke-Jeeves, but it uses fewer search directions that positively span the design space.  
- The Nelder-Mead simplex method uses a simplex to search the design space, adaptively expanding and contracting the size of the simplex in response to evaluations of the objective function.  
- The divided rectangles algorithm uses a heuristic inspired by potential Lipschitz constants to iteratively refine a retangular partition of the design space.

# 7.8 Exercises

Exercise 7.1. Previous chapters covered methods that leverage the derivative to descend toward a minimum. Direct methods are able to use only zero-order information—evaluations of  $f$ . How many evaluations are needed to approximate the derivative and the Hessian of an  $n$ -dimensional objective function using finite difference methods? Why do you think it is important to have zero-order methods?

Solution: The derivative has  $n$  terms whereas the Hessian has  $n^2$  terms. Each derivative term requires two evaluations when using finite difference methods:  $f(\mathbf{x})$  and  $f(\mathbf{x} + h\mathbf{e}^{(i)})$ . Each Hessian term requires an additional evaluation when using finite difference methods:

$$
\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}} \approx \frac{f (\mathbf {x} + h \mathbf {e}^{(i)} + h \mathbf {e}^{(j)}) - f (\mathbf {x} + h \mathbf {e}^{(i)}) - f (\mathbf {x} + h \mathbf {e}^{(j)}) + f (x)}{h^{2}}
$$

Thus, to approximate the gradient, you need  $n + 1$  evaluations, and to approximate the Hessian you need on the order of  $n^2$  evaluations.

Approximating the Hessian is prohibitively expensive for large  $n$ . Direct methods can take comparatively more steps using  $n^2$  function evaluations, as direct methods need not estimate the derivative or Hessian at each step.

Exercise 7.2. Design an objective function and a starting point  $x_0$  such that Hooke-Jeeves will fail to attain a reduction in the objective function. You need to choose  $x_0$  such that it is not a local minimizer.

Solution: Consider minimizing  $f(x) = xy$  and  $x_0 = [0,0]$ . Proceeding in either canonical direction will not reduce the objective function, but  $x_0$  is clearly not a minimizer.

Exercise 7.3. Is the design point obtained using the Hooke-Jeeves method guaranteed to be within  $\epsilon$  of a local minimum?

Solution: At each iteration, the Hooke-Jeves method samples  $2n$  points along the coordinate directions with a step-size  $a$ . It stops when none of the points provides an improvement and the step size is no more than a given tolerance  $\epsilon$ . While this often causes the Hooke-Jeves method to stop when it has converged to within  $\epsilon$  of a local minimum, that need not be the case. For example, a valley can descend between two coordinate directions farther than  $\epsilon$  before arriving at a local minimum, and the Hooke-Jeves method would not detect it.

Exercise 7.4. Give an example of a concrete engineering problem where you may not be able to compute analytical derivatives.

Solution: Minimizing the drag of an airfoil subject to a minimum thickness (to preserve structural integrity). Evaluating the performance of the airfoil using computational fluid dynamics involves solving partial differential equations. Because the function is not known analytically, we are unlikely to have an analytical expression for the derivative.

Exercise 7.5. State a difference between the divided rectangles algorithm in one dimension and the Shubert-Piyavskii method.

Solution: The divided rectangles method samples at the center of the intervals and not where the bound derived from a known Lipschitz constant is lowest.

Exercise 7.6. Suppose our search algorithm has us transition from  $\mathbf{x}^{(k)} = [1,2,3,4]$  to  $\mathbf{x}^{(k + 1)} = [2,2,2,4]$ . Could our search algorithm be (a) cyclic coordinate search, (b) Powell's method, (c) both a and b, or (d) neither a nor b? Why?

Solution: It cannot be cyclic coordinate search since more than one component is changing. It can be Powell's method.

Exercise 7.7. The current implementation of direct_split_intervals in algorithm 7.9 iterates over all intervals. Knowing that many intervals will share the same radius, is there a data structure that could be used to make finding the splitting intervals more efficient?

Solution: Finding the split intervals requires finding those with points  $(f(\mathbf{c}), r)$  furthest to the bottom-right. If two intervals have the same radius  $r$ , then one with a higher objective funtion value cannot possibly lie in the bottom-right. Rather than storing the intervals in an array:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/49359b5a6694c79204709f2dca3d8f750261621754053df1adb06c130af66d87.jpg)

the recommended implementation for DIRECT maintains a separate  $y$ -based priority queue for each distinct value of  $r$ :

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3600f2710534110b5c13dfa4ddf43f8e719b6a0850293cf70929f4cd4e9635c5.jpg)

The priority queues are sorted in increasing order by radius. This approach allows for efficiently accessing and keeping track of the lowest interval for each radius.

# 8 Stochastic Methods

This chapter presents a variety of stochastic methods that use randomization strategically to help explore the design space for an optimum. Randomness can help escape local optima and increase the chances of finding a global optimum. Stochastic methods typically use pseudo-random number generators to ensure repeatability. $^{1}$  A large amount of randomness is generally ineffective because it prevents us from effectively using previous evaluation points to help guide the search. This chapter discusses a variety of ways to control the degree of randomness in our search.

# 8.1 Noisy Descent

Adding stochasticity to gradient descent can be beneficial in large nonlinear optimization problems. Saddle points, where the gradient is very close to zero, can cause descent methods to select step sizes that are too small to be useful. One approach is to add Gaussian noise to each descent step  $\mathbf{d}^{(k)}$ :

$$
\mathbf {x}^{(k + 1)} \leftarrow \mathbf {x}^{(k)} + \mathbf {d}^{(k)} + \boldsymbol {\epsilon}^{(k)} \tag {8.1}
$$

where  $\epsilon^{(k)}$  is zero-mean Gaussian noise with standard deviation  $\sigma$ . The amount of noise is typically reduced over time. The standard deviation of the noise is typically a decreasing sequence  $\sigma^{(k)}$  such as  $1 / k$ . Algorithm 8.1 provides an implementation of this method. Figure 8.1 compares descent with and without noise on a saddle function.

A common approach for training neural networks is stochastic gradient descent, which uses a noisy gradient approximation. In addition to helping traverse past saddle points, evaluating noisy gradients using randomly chosen subsets of the training data<sup>4</sup> is significantly less expensive computationally than calculating the true gradient at every iteration.

1 Although pseudo-random number generators produce numbers that appear random, they are actually a result of a deterministic process. Pseudo-random numbers can be produced through calls to the rand function. The process can be reset to an initial state using the seed! function from the Random.jl package.

2G. Hinton and S. Roweis, "Stochastic Neighbor Embedding," in Advances in Neural Information Processing Systems (NIPS), 2003.

3 The Hinton and Roweis paper used a fixed standard deviation for the first 3,500 iterations and set the standard deviation to zero thereafter.

4 These subsets are called batches.

```julia
mutable struct NoisyDescent  $\ll$  DescentMethod submethod # descent method to apply noise to  $\sigma$  #noise sequence k # iteration   
end   
function init!(M::NoisyDescent, f,  $\nabla f$  , x) init!(M.submethod, f,  $\nabla f$  , x) M.k = 1 return M   
end   
function step!(M::NoisyDescent, f,  $\nabla f$  , x) x  $=$  step!(M.submethod, f,  $\nabla f$  , x)  $\sigma = M.\sigma (M.k)$  x += o.\*randn(length(x)) M.k += 1 return x   
end
```

Algorithm 8.1. A noisy descent method, which augments another descent method with additive Gaussian noise. The method takes another DescentMethod submethod, a noise sequence  $\sigma$ , and stores the iteration count  $k$ .

Convergence guarantees for stochastic gradient descent require that the positive step sizes be chosen such that:

$$
\sum_{k = 1}^{\infty} \alpha^{(k)} = \infty \quad \sum_{k = 1}^{\infty} \left(\alpha^{(k)}\right) ^{2} <   \infty \tag {8.2}
$$

These conditions ensure that the step sizes decrease and allow the method to converge, but not too quickly so as to become stuck away from a local minimum.

# 8.2 Mesh Adaptive Direct Search

The generalized pattern search methods covered in section 7.4 restricted local exploration to a fixed set of directions. In contrast, mesh adaptive direct search uses random positive spanning directions. The mesh referred to in the name of this method consists of the points in the design space that are reachable by taking steps in these spanning directions.

The procedure used to sample positive spanning sets (see example 8.1) begins by constructing an initial linearly spanning set in the form of a lower triangular matrix  $\mathbf{L}$ . The diagonal terms in  $\mathbf{L}$  are sampled from  $\pm 1 / \sqrt{\alpha^{(k)}}$ , where  $\alpha^{(k)}$  is the step size at iteration  $k$ . The lower components of  $\mathbf{L}$  are sampled from

$$
\left\{- 1 / \sqrt{\alpha^{(k)}} + 1, - 1 / \sqrt{\alpha^{(k)}} + 2, \dots , 1 / \sqrt{\alpha^{(k)}} - 1 \right\} \tag {8.3}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0ef9a1feb2ae08f715838e406861499cf11d761e054ba4540b75cebb08942f5f.jpg)  
Figure 8.1. Adding stochasticity to a descent method helps with traversing saddle points such as  $f(\mathbf{x}) = x_1^2 - x_2^2$  shown here. Due to the initialization, the steepest descent method converges to the saddle point where the gradient is zero.

5 This section follows the lower triangular mesh adaptive direct search given by C. Audet and J. E. Dennis Jr., "Mesh Adaptive Direct Search Algorithms for Constrained Optimization," SIAM Journal on Optimization, vol. 17, no. 1, pp. 188-217, 2006.

Consider positive spanning sets constructed from the nonzero directions  $d_1, d_2 \in \{-1, 0, 1\}$ . There are 8 positive spanning sets with 3 elements that can be constructed from these directions:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4a29813e2b0dd75177a4bb45253372dfbe6419d1c4d74e09cb5cfae927dc0b97.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d14b633f8b44feee0fdcac5c8c58e448248c510794589955d94f005ba43b844d.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d3a53dcf8a1cd9a020fca59d53753e832201e96151b3696257fc88d2895ea17a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a9894fae61add049d6675d2b2d5681b91bb78b237380e891a30d3fb8e761c576.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3879dcc9c0b341948f9163dcaa034bf72410b94e1ba0fe238b8d0296bca10868.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9d678ed9b97bc365542f70b811333e8bf45c6af011d5651ae3f38ab256702974.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7a7c38ceb3fd70f28b085f9149fdabc9d4eaaf8dff1a1b2ccfc212ece8f6e986.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/59cd7de8c4b3063cac73b75df289862905ae1edc3b12f64d706eb4f4b171355e.jpg)

Example 8.1. Positive spanning sets for  $\mathbb{R}^2$ . Note that the lower triangular generation strategy can only generate the first two columns of spanning sets.

The rows and columns of  $\mathbf{L}$  are then randomly permuted to obtain a matrix  $\mathbf{D}$  whose columns correspond to  $n$  directions that linearly span  $\mathbb{R}^n$ . Each of these directions is inscribed inside a hypercube of side length  $2 / \sqrt{\alpha^{(k)}}$ .<sup>6</sup>

Two common methods for obtaining a positive spanning set from the linearly spanning set are to add one additional direction  $\mathbf{d}^{(n + 1)} = -\sum_{i = 1}^{n}\mathbf{d}^{(i)}$  and to add  $n$  additional directions  $\mathbf{d}^{(n + j)} = -\mathbf{d}^{(j)}$  for  $j$  in  $1:n$ . We use the first method in algorithm 8.2.

The step size  $\alpha$  starts at 1, is always a power of 4, and never exceeds 1. Using a power of 4 causes the maximum possible componentwise step size taken in each iteration to be scaled by a factor of 2, as the maximum componentwise step size  $\alpha / \sqrt{\alpha}$  has length  $4^m / \sqrt{4^m} = 2^m$  for integer  $m < 1$ . The step size is updated according to:

$$
\alpha^{(k + 1)} \leftarrow \left\{ \begin{array}{l l} \alpha^{(k)} / 4 & \text{ifnoimprovementwasfoundinthisiteration} \\ \min \left(1, 4 \alpha^{(k)}\right) & \text{otherwise} \end{array} \right. \tag {8.4}
$$

Mesh adaptive direct search is opportunistic and does not support dynamic ordering because, after a successful iteration, the new random pattern may not contain the previous successful direction. The algorithm queries a new design point along the accepted descent direction. If  $f(\mathbf{x}^{(k)} = \mathbf{x}^{(k - 1)} + \alpha \mathbf{d}) < f(\mathbf{x}^{(k - 1)})$ ,

6 For each direction,  $\| \mathbf{d}^{(i)}\|_{\infty} = 1 / \sqrt{\alpha^{(k)}}$

See section 7.4.

```julia
function rand_positive spanning_set(α, n)  
δ = round(Int, 1/sqrt(α))  
L = Matrix(Diagonal(δ*rand([1,-1], n)))  
for i in 1: n-1  
    for j in 1:i-1  
        L[i,j] = rand(-δ+1:δ-1)  
    end  
end  
D = L[randperm(n),:]  
D = D(:,randperm(n)]  
D = [D -sum(D, dims=2)]  
return [D(:,i] for i in 1: n+1]  
end
```

Algorithm 8.2. Randomly sampling a positive spanning set of  $n + 1$  directions according to mesh adaptive direct search with step size  $\alpha$  and number of dimensions n.

then the queried point is  $\mathbf{x}^{(k - 1)} + 4\alpha \mathbf{d} = \mathbf{x}^{(k)} + 3\alpha \mathbf{d}$ . The procedure is outlined in algorithm 8.3. Figure 8.2 illustrates how this algorithm explores the search space.

```txt
function mesh_adaptive_direct_search(f, x, e)  
 $\alpha, y, n = 1, f(x), \text{length}(x)$   
while  $\alpha > \epsilon$   
improved = false  
for (i,d) in enumerate(random_positive spanking_set( $\alpha$ , n))  
 $x' = x + \alpha * d$ $y' = f(x')$   
if  $y' < y$ $x, y, \text{improved} = x', y', \text{true}$ $x' = x + 3\alpha * d$ $y' = f(x')$   
if  $y' < y$ $x, y = x', y'$   
end  
break  
end  
end  
 $\alpha = \text{improved?min}(4\alpha, 1): \alpha / 4$   
end  
return x  
end
```

Algorithm 8.3. Mesh adaptive direct search for an objective function  $f$ , an initial design  $\mathbf{x}$ , and a tolerance  $\epsilon$ .

# 8.3 Memory-Efficient Zeroth-Order Optimization

Memory efficiency becomes a concern when optimizing very large designs, such as deep learning models with many billions of parameters that push the bound-

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1de7d2da3df94fa22f3d1641125cfdb213f7d383d969cb954f366d0e5764ad19.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d66655667a182dbca908f6f92cb6f3014c9ea54927ab8b7e4f4b44fb76599b7b.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/31ca416e2fa7456622598abf0f6534650f7409c34545ab7c17de33f4a3f75d5f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/255eeb3ee35c810d9d6b1f7b5f6ed088aafc5856e306b031f4f2a2f5cdcc8217.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/edc85cec357f1e92ab53f20ff13564caf89072211cb5fba444b65e28ffaeb20d.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8f2a023e64f4b3ae544fdaab96fc33f5cba157d575fc37a4151cbbfed9d845db.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/96b4382c305bbf445c02f7254e5106f970a22b5d6b2d799d84f6ee4af27afbd7.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d88c568babd298641e8ad40ac5aabfdbc270828ad3bff9ce790d65cf82a032cf.jpg)  
Figure 8.2. Mesh adaptive direct search proceeding left to right and top to bottom.

aries of memory storage. $^{8}$  Stochastic methods are not merely practical, but can be helpful because the exact computation of a gradient may be prohibitively expensive or infeasible. The memory-efficient zeroth-order optimizer (MeZO) computes a stochastic gradient step that can be estimated and applied in-place, using the same memory footprint used to store the design vector. This allows for the optimization of far larger problems. $^{9}$

This approach uses the simultaneous perturbation stochastic gradient approximation (section 2.6), which approximates the gradient using directional derivatives along randomly chosen directions. $^{10}$  In MeZO, the gradient is estimated and a step is applied in-place such that a separate gradient vector does not need to be allocated, thereby minimizing the memory footprint. The update is as follows:

$$
\mathbf {x}^{\prime} = \mathbf {x} - \alpha \frac{f (\mathbf {x} + \delta \mathbf {z}) - f (\mathbf {x} - \delta \mathbf {z})}{2 \delta} \mathbf {z} \tag {8.5}
$$

for a step factor  $\alpha$ , finite difference scalar  $\delta$ , and perturbation  $\mathbf{z}$  drawn from a zero-mean unit Gaussian distribution. Such an update is called a zero-order stochastic step.

The zero-order stochastic step is implemented in algorithm 8.4. We start by setting our pseudo-random number generator according to the seed, and then randomly perturb our design such that it contains  $\mathbf{x} + \delta \mathbf{z}$ . We then evaluate the objective function to get  $f(\mathbf{x} + \delta \mathbf{z})$ . We again reset the pseudo-random number

8 For example, the Llama 3.1 large language model has 405 billion parameters. A. Grattaftori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, et al., "The Llama 3 Herd of Models," 2024. arXiv: 2407.21783.  
In the context of training large language models, this algorithm has shown to lead to a  $12 \times$  memory reduction and a  $2 \times$  speed up in training time. S. Malladi, T. Gao, E. Nichani, A. Damian, J. Lee, D. Chen, et al., "Fine-Tuning Language Models with Just Forward Passes," in Advances in Neural Information Processing Systems (NeurIPS), 2023.  
10 The simultaneous perturbation stochastic gradient approximation can average over multiple sampled perturbations. It is common to use only a single sampled perurbation per step for MeZO.

generator to the same seed value, and again perturb the design, but this time by twice as much in the opposite direction. Our design will thereafter contain  $\mathbf{x} - \delta \mathbf{z}$ , and we can evaluate  $f(\mathbf{x} - \delta \mathbf{z})$ . We return our design to its initial value by again resetting to the seed and again perturbing by  $\delta \mathbf{z}$ . The step is applied using another perturbation of the appropriate magnitude.

```julia
function perturb_parameters!(x,  $\delta$  , seed) Random.seed!(seed) for i in eachindex(x) x[i] +=  $\delta *\mathrm{randn}()$  end   
end   
function zero_order_stochastic_step!(x,f,8,α,seed) # positive perturbation perturb_parameters!(x,δ,seed) y = f(x) # negative perturbation perturb_parameters!(x,-28,seed)  $\Delta y = y - f(x)$  # recover the original parameters perturb_parameters!(x,δ,seed) # apply a gradient step perturb_parameters!(x,- $\alpha * \Delta y / (28)$ , seed) return x   
end
```

# 8.4 Simulated Annealing

Simulated annealing<sup>11</sup> borrows inspiration from metallurgy.<sup>12</sup> Temperature is used to control the degree of stochasticity during the randomized search. The temperature starts high, allowing the process to freely move about the search space, with the hope that in this phase the process will find a good region with the best local minimum. The temperature is then slowly brought down, reducing the stochasticity and forcing the search to converge to a minimum. Simulated annealing is often used on functions with many local minima due to its ability to escape local minima.

Algorithm 8.4. A method for improving a design  $x$  in-place based on a single simultaneous perturbation stochastic gradient estimate, without allocating additional memory. Perturbations are zero-mean with standard deviation  $\delta$  using the provided seed. The method directly applies a step with step size  $\alpha$  to the parameters. Successive calls to this method should provide different random seeds.

11 S. Kirkpatrick, C.D. Gelatt Jr., and M.P. Vecchi, "Optimization by Simulated Annealing," Science, vol. 220, no. 4598, pp. 671-680, 1983.

12 Annealing is a process in which a material is heated and then cooled, making it more workable. When hot, the atoms in the material are more free to move around, and, through random motion, tend to settle into better positions. A slow cooling brings the material to an ordered, crystalline state. A fast, abrupt quenching causes defects because the material is forced to settle in its current condition.

At every iteration, a candidate transition from  $\mathbf{x}$  to  $\mathbf{x}'$  is sampled from a transition distribution  $T$  and is accepted with probability

$$
\left\{ \begin{array}{l l} 1 & \text{if } \Delta y \leq 0 \\ e^{- \Delta y / t} & \text{if } \Delta y > 0 \end{array} \right. \tag {8.6}
$$

where  $\Delta y = f(\mathbf{x}^{\prime}) - f(\mathbf{x})$  is the difference in the objective and  $t$  is the temperature. It is this acceptance probability, known as the Metropolis criterion, $^{13}$  that allows the algorithm to escape from local minima when the temperature is high.

The temperature parameter  $t$  controls the acceptance probability. An annealing schedule is used to slowly bring down the temperature as the algorithm progresses, as illustrated by figure 8.3. The temperature must be brought down to ensure convergence. If it is brought down too quickly, the search method may not cover the portion of the search space containing the global minimum.

It can be shown that a logarithmic annealing schedule of  $t^{(k)} = t^{(1)}\ln (2) / \ln (k + 1)$  for the  $k$ th iteration is guaranteed to asymptotically reach the global optimum under certain conditions,[14] but it can be very slow in practice. The exponential annealing schedule, which is more common, uses a simple decay factor  $t^{(k + 1)} = \gamma t^{(k)}$  for some  $\gamma \in (0,1)$ . Another common annealing schedule, fast annealing,[15] uses a temperature of  $t^{(k)} = \frac{t^{(1)}}{k}$ . A basic implementation of simulated annealing is provided by algorithm 8.5. Example 8.2 shows the effect of different transition distributions and annealing schedules have on the optimization process.

```matlab
function simulated_annealing(f, x, T, t, k_max)  
    y = f(x)  
    best = (x=x, y=y)  
    for k in 1: k_max  
        x' = x + rand(T)  
        y' = f(x')  
        Δy = y' - y  
        if Δy ≤ 0 || rand() < exp(-Δy/t(k))  
            x, y = x', y'  
        end  
        if y' < best.y  
            best = (x=x', y=y')  
        end  
    return best.x  
end
```

13 Named for the Greek-American physicist Nicholas Metropolis (1915-1999).

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f9e1d1741c612934a4a1327e08c970fa2184e6cdf5794e6dad163cf7c9e10fb6.jpg)  
Figure 8.3. Several annealing schedules commonly used in simulated annealing. The schedules have an initial temperature of 10.

14 B. Hajek, "Cooling Schedules for Optimal Annealing," Mathematics of Operations Research, vol. 13, no. 2, pp. 311-329, 1988.  
15 H. Szu and R. Hartley, "Fast Simulated Annealing," Physics Letters A, vol. 122, no. 3-4, pp. 157-162, 1987.

Algorithm 8.5. Simulated annealing, which takes as input an objective function  $f$ , an initial point  $x$ , a transition distribution  $T$ , an annealing schedule  $t$ , and the number of iterations  $k_{\max}$ .

We can use simulated annealing to optimize Ackley's function, appendix B.1. Ackley's function has many local minima, making it easy for gradient-based methods to get stuck.

Suppose we start at  $\mathbf{x}^{(1)} = [15, 15]$  and run 100 iterations. Below we show the distribution over iterations for multiple runs with different combinations of three zero-mean, diagonal covariance ( $\sigma \mathbf{I}$ ) Gaussian transition distributions, and three different temperature schedules  $t^{(k)} = t^{(1)} / k$ .

Example 8.2. Exploring the effect of distribution variance and temperature on the performance of simulated annealing. The blue regions indicate the  $5\%$  to  $95\%$  and  $25\%$  to  $75\%$  empirical Gaussian quantiles of the objective function value.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f3c05e69dfb6dbdee88e67d918770e634a1c948026affdd5266f694faba0664b.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a15983a577dee1abbabc04087ba07a47b175a73fd191d079817981dd1222c953.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/02d5f5f717adbfb90092ace102812d0c989dd4361bb0cbd1d742375b5e4844f5.jpg)

In this case, the spread of the transition distribution has the greatest impact on performance.

A more sophisticated algorithm was introduced by Corana, Marchesi, Martini, and Ridella in 1987 that allows for the step size to change during the search.16 Rather than using a fixed transition distribution, this adaptive simulated annealing method keeps track of a separate step size  $\mathbf{v}$  for each coordinate direction. For a given point  $\mathbf{x}$ , a cycle of random moves is performed in each coordinate direction  $i$  according to:

$$
\mathbf {x}^{\prime} = \mathbf {x} + r v_{i} \mathbf {e}_{i} \tag {8.7}
$$

where  $r$  is drawn uniformly at random from  $[-1, 1]$  and  $v_{i}$  is the maximum step size in the  $i$ th coordinate direction. Each new point is accepted according to the Metropolis criterion. The number of accepted points in each coordinate direction is stored in a vector  $\mathbf{a}$ .

After  $n_{s}$  cycles, the step sizes are adjusted with the aim to maintain an approximately equal number of accepted and rejected designs with an average acceptance rate near one-half. Rejecting too many moves is a waste of computational effort, while accepting too many moves indicates that the configuration is evolving too slowly because candidate points are too similar to the current location. The update formula used by Corana, Marchesi, Martini, and Ridella is:

$$
v_{i} = \left\{ \begin{array}{l l} v_{i} \left(1 + c_{i} \frac{a_{i} / n_{s} - 0.6}{0.4}\right) & \text{if } a_{i} > 0. 6 n_{s} \\ v_{i} \left(1 + c_{i} \frac{0.4 - a_{i} / n_{s}}{0.4}\right) ^{- 1} & \text{if } a_{i} <   0. 4 n_{s} \\ v_{i} & \text{otherwise} \end{array} \right. \tag {8.8}
$$

The  $c_{i}$  parameter controls the step variation along each direction and is typically set to 2 as shown in figure 8.4. Algorithm 8.6 implements this update. The temperature is reduced every  $n_t$  step adjustments.

```julia
function corana_update!(v, a, c, ns)  
for i in eachindex(v)  
ai, ci = a[i], c[i]  
if ai > 0.6ns  
v[i] *= (1 + ci*(ai/ns - 0.6)/0.4)  
elseif ai < 0.4ns  
v[i] /= (1 + ci*(0.4 - ai/ns)/0.4)  
end  
end  
return v  
end
```

16 A. Corana, M. Marchesi, C. Martini, and S. Ridella, "Minimizing Multimodal Functions of Continuous Variables with the 'Simulated Annealing' Algorithm," ACM Transactions on Mathematical Software, vol. 13, no. 3, pp. 262-280, 1987.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/be637ed9b7b1a867051072bfe6dc4780c95e8742e40883b1cdd43908f1d575db.jpg)  
Figure 8.4. The step multiplication factor as a function of acceptance rate for  $c = 2$ .

Algorithm 8.6. The update formula used by Corana, Marchesi, Martini, and Ridella in adaptive simulated annealing, where  $\nu$  is a vector of coordinate step sizes,  $a$  is a vector of the number of accepted steps in each coordinate direction,  $c$  is a vector of step scaling factors for each coordinate direction, and ns is the number of cycles before running the step size adjustment.

The process is terminated when the temperature sinks low enough such that improvement can no longer be expected. Termination occurs when the most recent function value is no farther than  $\epsilon$  from the previous  $n_{\epsilon}$  iterations and the best function value obtained over the course of execution. Algorithm 8.7 provides an implementation and the algorithm is visualized in figure 8.5.

# 8.5 Cross-Entropy Method

The cross-entropy method,[17] in contrast with the methods we have discussed so far in this chapter, maintains an explicit probability distribution over the design space.[18] This probability distribution, often called a proposal distribution, is used to propose new samples for the next iteration. At each iteration, we sample from the proposal distribution and then update the proposal distribution to fit a fixed number of best samples, known as elite samples. The aim at convergence is for the proposal distribution to focus on the global optima. Algorithm 8.8 provides an implementation.

The cross-entropy method requires choosing a family of distributions parameterized by  $\theta$ . One common choice is the family of multivariate normal distributions parameterized by a mean vector and a covariance matrix. The algorithm also requires us to specify the number of elite samples  $m_{\mathrm{elite}}$  to use when fitting the parameters for the next iteration.

Depending on the choice of distribution family, the process of fitting the distribution to the elite samples can be done analytically. In the case of the multivariate normal distribution, the parameters are updated according to the maximum likelihood estimate:

$$
\boldsymbol {\mu}^{(k + 1)} = \frac{1}{m_{\text{elite}}} \sum_{i = 1}^{m_{\text{elite}}} \mathbf {x}^{(i)} \tag {8.9}
$$

$$
\boldsymbol {\Sigma}^{(k + 1)} = \frac{1}{m_{\text{elite}}} \sum_{i = 1}^{m_{\text{elite}}} \left(\mathbf {x}^{(i)} - \boldsymbol {\mu}^{(k + 1)}\right) \left(\mathbf {x}^{(i)} - \boldsymbol {\mu}^{(k + 1)}\right) ^{\top} \tag {8.10}
$$

Example 8.3 applies the cross-entropy method to a simple function. Figure 8.6 shows several iterations on a more complex function. Example 8.4 shows the potential limitation of using a multivariate normal distribution for fitting elite samples.

17 R. Y. Rubinstein and D. P. Kroese, The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning. Springer, 2004.

18 The name of this method comes from the fact that the process of fitting the distribution involves minimizing cross-entropy, which is also called the Kullback-Leibler divergence. Under certain conditions, minimizing the cross-entropy corresponds to finding the maximum likelihood estimate of the parameters of the distribution.

```matlab
function adaptive_simulated_annealing(f, x, v, t, ε; ns=20, nε=4, nt=max(100,5length(x)), y=0.85, c=fill(2,length(x)))  
y = f(x)  
best = (x=x, y=y)  
y Arr, n, U = [], length(x), Uniform(-1.0,1.0)  
a, counts_cycles, counts_reset = zeros(n), 0, 0  
while true  
for i in 1:n  
    x' = x + basis(i, n)*rand(U)*v[i]  
    y' = f(x')  
    Δy = y' - y  
    if Δy < 0 || rand() < exp(-Δy/t)  
        x, y = x', y'  
        a[i] += 1  
        if y' < best.y; best = (x=x', y=y'); end  
    end  
end  
counts_cycles += 1  
counts_cycles ≥ ns || continue  
counts_cycles = 0  
corana_update!(v, a, c, ns)  
fill!(a, 0)  
counts Resets += 1  
counts Resets ≥ nt || continue  
t *= y  
counts Resets = 0  
push!(y Arr, y)  
if !(length(y Arr) > nε && y Arr[end] - best.y ≤ ε && all(abs(y Arr[end] - y Arr[end-u]) ≤ ε for u in 1:nε))  
    x, y = best  
else  
    break  
end  
return best.x
```

Algorithm 8.7. The adaptive simulated annealing algorithm, where  $f$  is the multivariate objective function,  $x$  is the starting point,  $v$  is starting step vector,  $t$  is the starting temperature, and  $\epsilon$  is the termination criterion parameter. The optional parameters are the number of cycles before running the step size adjustment  $n s$ , the number of cycles before reducing the temperature  $n t$ , the number of successive temperature reductions to test for termination  $n \epsilon$ , the temperature reduction coefficient  $\gamma$ , and the direction-wise varying criterion  $c$ .

Below is a flowchart for the adaptive simulated annealing algorithm as presented in the original paper.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/abe052be272272a64bd945f049f174493800d66aeb362739e291a1b49cf3550e.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a33319684084ca4b7620fe7de989e78d1a7f95a7c67e725d896d687c2a5d01f7.jpg)  
Figure 8.5. Simulated annealing with an exponentially decaying temperature, where the histograms indicate the probability of simulated annealing being at a particular position at that iteration.

```julia
using Distributions   
function cross_entropy_method(f,P,k_max,m=100,m_elite=10) for k in 1:k_max samples  $=$  rand(P,m) order  $=$  sortperm([f(samplese[;,i])for i in 1:m]) P  $=$  fit(typeof(P),samples[:,order[1:m_elite]]) end return P   
end
```

Algorithm 8.8. The cross-entropy method, which takes an objective function  $f$  to be minimized, a proposal distribution  $P$ , an iteration count  $k_{\text{max}}$ , a sample size  $m$ , and the number of samples to use when refitting the distribution  $m_{\text{elite}}$ . It returns the updated distribution over where the global minimum is likely to exist.

We can use Distributions.jl to represent, sample from, and fit proposal distributions. The parameter vector  $\theta$  is replaced by a distribution P. Calling rand(P,m) will produce an  $n\times m$  matrix corresponding to m samples of n-dimensional samples from P, and calling fit will fit a new distribution of the given input type.

```julia
import Random: seed!  
import LinearAlgebra: norm  
import Distributions: MvNormal  
seed!(0) # set random seed for reproducible results  
f = x→norm(x)  
μ = [0.5, 1.5]  
Σ = [1.00.2; 0.22.0]  
P = MvNormal(μ, Σ)  
k_max = 10  
P = cross_entropy_method(f, P, k_max)  
@show P.μ  
P.μ = [-1.7329565477016673e-6, -7.042440586544702e-7]
```

Example 8.3. An example of using the cross-entropy method.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/de328a71f063c9ebb6e5fb0338613e3d838d2a3d127a4096b3dda4d546ec7396.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/41d96eed10088d64c20a2eca92b3c862741746943a99dfb008c40d22037103cf.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a5e362efb5c3202474e2ae37ae238ed7e4e98f32a6678c1ee9382b10a4b71415.jpg)  
Figure 8.6. The cross-entropy method with  $m = 40$  applied to the Branin function (appendix B.3) using a multivariate Gaussian proposal distribution. The 10 elite samples in each iteration are in red.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e4f568c79b60014ac7cab34155c997a3cda558390b3acc07ddbc0f25a51c4442.jpg)

The distribution family should be flexible enough to capture the relevant features of the objective function. Here we show the limitations of using a normal distribution on a multimodal objective function, which assigns greater density in between the two minima. A mixture model is able to center itself over each minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/21535fa17f3eb1481a69178fde59e526ef312dcb3f172a8e77aa58e226594309.jpg)

Example 8.4. The normal distribution is unable to capture multiple local minima, in contrast to mixture models which can maintain several.

# 8.6 Natural Evolution Strategies

Like the cross-entropy method, natural evolution strategies<sup>19</sup> optimize a proposal distribution parameterized by  $\theta$ . We have to specify the proposal distribution family and the number of samples. The aim is to minimize the expectation  $\mathbb{E}_{\mathbf{x} \sim p(\cdot | \theta)}[f(\mathbf{x})]$ . Instead of fitting elite samples, evolution strategies apply gradient descent. The gradient is estimated from the samples:<sup>20</sup>

$$
\begin{array}{l} \nabla_{\boldsymbol {\theta}} \mathbb {E}_{\mathbf {x} \sim p (\cdot | \boldsymbol {\theta})} [ f (\mathbf {x}) ] = \int \nabla_{\boldsymbol {\theta}} p (\mathbf {x} \mid \boldsymbol {\theta}) f (\mathbf {x}) d \mathbf {x} (8.11) \\ = \int \frac{p (\mathbf {x} \mid \boldsymbol {\theta})}{p (\mathbf {x} \mid \boldsymbol {\theta})} \nabla_{\boldsymbol {\theta}} p (\mathbf {x} \mid \boldsymbol {\theta}) f (\mathbf {x}) d \mathbf {x} (8.12) \\ = \int p (\mathbf {x} \mid \theta) \nabla_{\theta} \log p (\mathbf {x} \mid \theta) f (\mathbf {x}) d \mathbf {x} (8.13) \\ = \mathbb {E}_{\mathbf {x} \sim p (\cdot | \boldsymbol {\theta})} [ f (\mathbf {x}) \nabla_{\boldsymbol {\theta}} \log p (\mathbf {x} \mid \boldsymbol {\theta}) ] (8.14) \\ \approx \frac{1}{m} \sum_{i = 1}^{m} f \left(\mathbf {x}^{(i)}\right) \nabla_{\theta} \log p \left(\mathbf {x}^{(i)} \mid \theta\right) (8.15) \\ \end{array}
$$

Although we do not need the gradient of the objective function, we do need the gradient of the log likelihood,  $\log p(\mathbf{x} \mid \boldsymbol{\theta})$ . Example 8.5 shows how to compute

19 I. Rechenberg, *Evolutionssstrategie Optimierung technischer Systemen nach Prinzipien der biologischen Evolution*. Frommann-Holzboog, 1973.

20 This gradient estimation has been successfully applied to proposal distributions represented by deep neural networks. T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, "Evolution Strategies as a Scalable Alternative to Reinforcement Learning," 2017. arXiv: 1703.03864.

the gradient of the log likelihood for the multivariate normal distribution. The estimated gradient can be used along with any of the descent methods discussed in previous chapters to improve  $\theta$ . Algorithm 8.9 uses gradient descent with a fixed step size. Figure 8.7 shows a few iterations of the algorithm.

```julia
using Distributions  
function natural évolution_strategies(f,  $\theta$ , k_max; m=100,  $\alpha = 0.01$ )  
for k in 1:k_max  
    samples = rand( $\theta$ , m)  
     $\theta$  -=  $\alpha * \sum (f(x) * \nabla \log p(x, \theta)$  for x in samples)/m  
end  
return  $\theta$   
end
```

# 8.7 Covariance Matrix Adaptation

Another popular method is covariance matrix adaptation,[21] which is also referred to as CMA-ES for covariance matrix adaptation evolutionary strategy. It has similarities with natural evolution strategies from section 8.6, but the two should not be confused. This method maintains a covariance matrix and is robust and sample efficient. Like the cross-entropy method and natural evolution strategies, a distribution is improved over time based on samples. Covariance matrix adaptation uses multivariate Gaussian distributions.[22]

Covariance matrix adaptation maintains a mean vector  $\mu$ , a covariance matrix  $\Sigma$ , and an additional step-size scalar  $\sigma$ . The covariance matrix only increases or decreases in a single direction with every iteration, whereas the step-size scalar is adapted to control the overall spread of the distribution. At every iteration,  $m$  designs are sampled from the multivariate Gaussian:[23]

$$
\mathbf {x} \sim \mathcal {N} (\boldsymbol {\mu}, \sigma^{2} \boldsymbol {\Sigma}) \tag {8.16}
$$

The designs are then sorted according to their objective function values such that  $f(\mathbf{x}^{(1)}) \leq f(\mathbf{x}^{(2)}) \leq \dots \leq f(\mathbf{x}^{(m)})$ . A new mean vector  $\boldsymbol{\mu}^{(k+1)}$  is formed using a weighted average of the first  $m_{\mathrm{elite}}$  sampled designs:

$$
\boldsymbol {\mu}^{(k + 1)} \leftarrow \sum_{i = 1}^{m_{\text{elite}}} w_{i} \mathbf {x}^{(i)} \tag {8.17}
$$

Algorithm 8.9. The natural evolution strategies method, which takes an objective function  $f$  to be minimized, an initial distribution parameter vector  $\theta$ , an iteration count  $k_{-} \max$ , a sample size  $m$ , and a step factor  $\alpha$ . An optimized parameter vector is returned. The method rand(θ) should sample from the distribution parameterized by  $\theta$ , and  $\nabla \log p(x, \theta)$  should return the log likelihood gradient.

21 It is common to use the phrase evolution strategies to refer specifically to covariance matrix adaptation.  
22 N. Hansen, "The CMA Evolution Strategy: A Tutorial," 2016. arXiv: 1604.00772.  
23 For optimization in  $\mathbb{R}^n$ , it is recommended to use at least  $m = 4 + \lfloor 3\ln n\rfloor$  samples per iteration, and  $m_{\mathrm{elite}} = \lfloor m / 2\rfloor$  elite samples.

The multivariate normal distribution  $\mathcal{N}(\mu, \Sigma)$  with mean  $\mu$  and covariance  $\Sigma$  is a popular distribution family due to having analytic solutions. The likelihood in  $d$  dimensions has the form

$$
p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = (2 \pi) ^{- \frac{d}{2}} | \boldsymbol {\Sigma} | ^{- \frac{1}{2}} \exp \left(- \frac{1}{2} (\mathbf {x} - \boldsymbol {\mu}) ^{\top} \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu})\right)
$$

where  $|\Sigma|$  is the determinant of  $\Sigma$ . The log likelihood is

$$
\log p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = - \frac{d}{2} \log (2 \pi) - \frac{1}{2} \log | \boldsymbol {\Sigma} | - \frac{1}{2} (\mathbf {x} - \boldsymbol {\mu}) ^{\top} \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu})
$$

The parameters can be updated using their log likelihood gradients:

$$
\nabla_{(\boldsymbol {\mu})} \log p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu})
$$

$$
\nabla_{(\boldsymbol {\Sigma})} \log p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = \frac{1}{2} \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu}) (\mathbf {x} - \boldsymbol {\mu}) ^{\top} \boldsymbol {\Sigma}^{- 1} - \frac{1}{2} \boldsymbol {\Sigma}^{- 1}
$$

The term  $\nabla_{(\Sigma)}$  contains the partial derivative of each entry of  $\Sigma$  with respect to the log likelihood.

Directly updating  $\pmb{\Sigma}$  may not result in a positive definite matrix, as is required for covariance matrices. One solution is to represent  $\pmb{\Sigma}$  as a product  $\mathbf{A}^{\top}\mathbf{A}$ , which guarantees that  $\pmb{\Sigma}$  remains positive semidefinite, and then update  $\mathbf{A}$  instead. Replacing  $\pmb{\Sigma}$  by  $\mathbf{A}^{\top}\mathbf{A}$  and taking the gradient with respect to  $\mathbf{A}$  yields:

$$
\nabla_{(\mathbf {A})} \log p (\mathbf {x} \mid \boldsymbol {\mu}, \mathbf {A}) = \mathbf {A} \Big [ \nabla_{(\boldsymbol {\Sigma})} \log p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) + \nabla_{(\boldsymbol {\Sigma})} \log p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) ^{\top} \Big ]
$$

Example 8.5. A derivation of the log likelihood gradient equations for the multivariate Gaussian distribution. For the original derivation and several more sophisticated solutions for handling the positive definite covariance matrix, see D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber, "Natural Evolution Strategies," Journal of Machine Learning Research, no. 15, pp. 949-980, 2014.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3c0d04f4a1ae3f87b36df3ebc0c1905b7a091af33edde44545b0ac532065b9ec.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b40f8632ef55dce5ed39f34f78de192fd1b13c689110437f73f27c962c88e496.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8f2b087bbded0d7c2e6f1a1afe34569d212d897754abff4391ed6603cca143ba.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9f4bf3c7e0af940a2115c7c41d03067b14bc4eef5865b612574358fd9e674069.jpg)  
Figure 8.7. Natural evolution strategies using multivariate Gaussian distributions applied to Wheeler's Ridge, appendix B.7.  
$x_{1}$

where the first  $m_{\text{elite}}$  weights sum to 1, and all the weights approximately sum to 0 and are ordered largest to smallest: $^{24}$

$$
\sum_{i = 1}^{m_{\text{elite}}} w_{i} = 1 \quad \sum_{i = 1}^{m} w_{i} \approx 0 \quad w_{1} \geq w_{2} \geq \dots \geq w_{m} \tag {8.18}
$$

We can approximate the mean update in the cross-entropy method by setting the first  $m_{\mathrm{elite}}$  weights to  $1 / m_{\mathrm{elite}}$ , and setting the remaining weights to zero. Covariance matrix adaptation instead distributes decreasing weight to all  $m$  designs, including some negative weights. The recommended weighting is obtained by normalizing

$$
w_{i}^{\prime} = \ln \frac{m + 1}{2} - \ln i \text{for} i \text{in} 1: m \tag {8.19}
$$

to obtain  $\mathbf{w}$ . The positive and negative weights are normalized separately. Figure 8.8 compares the mean updates for covariance matrix adaptation and the cross-entropy method.

The step size is updated using a cumulative variable  $\mathbf{p}_{\sigma}$  that tracks steps over time:

$$
\mathbf {p}_{\sigma}^{(1)} = \mathbf {0} \tag {8.20}
$$

$$
\mathbf {p}_{\sigma}^{(k + 1)} \leftarrow (1 - c_{\sigma}) \mathbf {p}_{\sigma} + \sqrt{c_{\sigma} (2 - c_{\sigma}) \mu_{\mathrm{eff}}} \left(\boldsymbol {\Sigma}^{(k)}\right) ^{- 1 / 2} \boldsymbol {\delta}_{w} \tag {8.21}
$$

where  $c_{\sigma} < 1$  controls the rate of decay and the right hand term determines whether the step size should be increased or decreased based on the observed samples with respect to the present scale of the distribution. The variance effective selection mass  $\mu_{\mathrm{eff}}$  has the form

$$
\mu_{\text{eff}} = \frac{1}{\sum_{i = 1}^{m_{\text{elite}}} w_{i}^{2}} \tag {8.22}
$$

and  $\delta_w$  is computed from the sampled deviations:

$$
\boldsymbol {\delta}_{w} = \sum_{i = 1}^{m_{\text{elite}}} w_{i} \boldsymbol {\delta}^{(i)} \quad \text{for} \quad \boldsymbol {\delta}^{(i)} = \frac{\mathbf {x}^{(i)} - \boldsymbol {\mu}^{(k)}}{\sigma^{(k)}} \tag {8.23}
$$

The new step size is obtained according to

$$
\sigma^{(k + 1)} \leftarrow \sigma^{(k)} \exp \left(\frac{c_{\sigma}}{d_{\sigma}} \left(\frac{\| \mathbf {p}_{\sigma} \|}{\mathbb {E} \| \mathcal {N} (\mathbf {0} , \mathbf {I}) \|} - 1\right)\right) \tag {8.24}
$$

24 In the recommended weighting, the first  $m_{\mathrm{elite}}$  samples are positive, and the remaining samples are nonpositive.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cd42847dd0e41d309ee752f0905be953fa030abd488e564e075fe92d4760316e.jpg)  
Figure 8.8. Shown is an initial proposal distribution (white contours), six samples (white dots), and the new updated means for both covariance matrix adaptation (blue dot) and the cross-entropy method (red dot) using three elite samples. Covariance matrix adaptation tends to update the mean more aggressively than the cross-entropy method (red dot), as it assigns higher weight to better sampled designs, and negative weight to worse sampled designs.

where

$$
\mathbb {E} \| \mathcal {N} (\mathbf {0}, \mathbf {I}) \| = \sqrt{2} \frac{\Gamma \left(\frac{n + 1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)} \approx \sqrt{n} \left(1 - \frac{1}{4 n} + \frac{1}{21 n^{2}}\right) \tag {8.25}
$$

is the expected length of a vector drawn from a Gaussian distribution. Comparing the length of  $\mathbf{p}_{\sigma}$  to its expected length under random selection provides the mechanism by which  $\sigma$  is increased or decreased. The constants  $c_{\sigma}$  and  $d_{\sigma}$  have recommended values:

$$
c_{\sigma} = \left(\mu_{\text{eff}} + 2\right) / \left(n + \mu_{\text{eff}} + 5\right) \tag {8.26}
$$

$$
d_{\sigma} = 1 + 2 \max \left(0, \sqrt{\left(\mu_{\text{eff}} - 1\right) / (n + 1)} - 1\right) + c_{\sigma} \tag {8.27}
$$

The covariance matrix is also updated using a cumulative vector:

$$
\mathbf {p}_{\Sigma}^{(1)} = \mathbf {0} \tag {8.28}
$$

$$
\mathbf {p}_{\Sigma}^{(k + 1)} \leftarrow (1 - c_{\Sigma}) \mathbf {p}_{\Sigma}^{(k)} + h_{\sigma} \sqrt{c_{\Sigma} (2 - c_{\Sigma}) \mu_{\mathrm{eff}}} \boldsymbol {\delta}_{w} \tag {8.29}
$$

where

$$
h_{\sigma} = \left\{ \begin{array}{l l} 1 & \text{if } \frac{\| \mathbf {p}_{\sigma} \|}{\sqrt{1 - (1 - c_{\sigma}) ^{2 (k + 1)}}} <   \left(1. 4 + \frac{2}{n + 1}\right) \mathbb {E} \| \mathcal {N} (\mathbf {0}, \mathbf {I}) \| \\ 0 & \text{otherwise} \end{array} \right. \tag {8.30}
$$

The  $h_{\sigma}$  stalls the update of  $\mathbf{p}_{\Sigma}$  if  $\| \mathbf{p}_{\Sigma}\|$  is too large, thereby preventing excessive increases in  $\boldsymbol{\Sigma}$  when the step size is too small.

The update requires the adjusted weights  $\mathbf{w}^{\prime}$ :

$$
w_{i}^{\prime} = \left\{ \begin{array}{c l} w_{i} & \text{if } w_{i} \geq 0 \\ \frac{n w_{i}}{\left\| \Sigma^{- 1 / 2} \boldsymbol {\delta}^{(i)} \right\| ^{2}} & \text{otherwise} \end{array} \right. \tag {8.31}
$$

The covariance update is then

$$
\boldsymbol {\Sigma}^{(k + 1)} \leftarrow \left(1 + \underbrace{c_{1} c_{\Sigma} (1 - h_{\sigma}) (2 - c_{\Sigma}) - c_{1} - c_{\mu}}_{\text{typicallyzero}}\right) \boldsymbol {\Sigma}^{(k)} + \underbrace{c_{1} \mathbf {p}_{\Sigma} \mathbf {p}_{\Sigma}^{\top}}_{\text{rank -oneupdate}} + \underbrace{c_{\mu} \sum_{i = 1}^{m_{\text{elite}}} w_{i}^{\prime} \boldsymbol {\delta}^{(i)} (\boldsymbol {\delta}^{(i)}) ^{\top}}_{\text{rank -m_{elite} u p d a t e}} \tag {8.32}
$$

The constants  $c_{\Sigma}, c_1$  and  $c_{\mu}$  have recommended values

$$
c_{\Sigma} = \frac{4 + \mu_{\text{eff}} / n}{n + 4 + 2 \mu_{\text{eff}} / n} \tag {8.33}
$$

$$
c_{1} = \frac{2}{(n + 1.3) ^{2} + \mu_{\mathrm{eff}}} \tag {8.34}
$$

$$
c_{\mu} = \min \left(1 - c_{1}, 2 \frac{\mu_{\text{eff}} - 2 + 1 / \mu_{\text{eff}}}{(n + 2) ^{2} + \mu_{\text{eff}}}\right) \tag {8.35}
$$

The covariance update consists of three components: the previous covariance matrix  $\pmb{\Sigma}^{(k)}$ , a rank-one update, and a rank- $m_{\mathrm{elite}}$  update. The rank-one update gets its name from the fact that  $\mathbf{p}_{\Sigma}\mathbf{p}_{\Sigma}^{\top}$  has rank one; it has only one eigenvector along  $\mathbf{p}_{\Sigma}$ . Rank-one updates using the cumulation vector allow for correlations between consecutive steps to be exploited, permitting the covariance matrix to elongate itself more quickly along a favorable axis.

The rank- $m_{\mathrm{elite}}$  update gets its name from the fact that  $\sum_{i=1}^{m_{\mathrm{elite}}} w_i' \delta^{(i)} \left( \delta^{(i)} \right)^\top$  has rank  $\min(m_{\mathrm{elite}}, n)$ . One important difference between the empirical covariance matrix update used by the cross-entropy method and the rank- $m_{\mathrm{elite}}$  update is that the former estimates the covariance about the new mean  $\mu^{(k+1)}$ , whereas the latter estimates the covariance about the original mean  $\mu^{(k)}$ . The  $\delta^{(i)}$  values thus help estimate the variances of the sampled steps rather than the variance within the sampled designs.

Covariance matrix adaptation is depicted in figure 8.9.

# 8.8 Summary

- Stochastic methods employ random numbers during the optimization process.  
- Mesh adaptive direct search is a pattern search method that uses random patterns.  
- The zero-order stochastic step is a random step used for optimizing very large objective functions that can be computed in-place using two objective evaluations and a pseudo-random number generator.  
- Simulated annealing uses a temperature that controls random exploration and which is reduced over time to converge on a local minimum.

```txt
function covariance_matrix_adaptation(f, x, k_max;  $\sigma = 1.0$  m  $= 4 +$  floor(Int, 3\*log(length(x))), m_elite  $=$  div(m,2))  $\mu ,n = \mathrm{copy}(x)$  length(x) ws  $= \log ((m + 1) / 2)$  .- log.(1:m) ws[1:m_elite]  $\text{一} =$  sum(ws[1:m_elite]).^2)  $\mathtt{c}\sigma = (\mu_{\text{eff}} + 2) / (n + \mu_{\text{eff}} + 5)$ $\mathtt{d}\sigma = 1 + 2\max (0$  sqrt((u_eff-1)/(n+1))-1)+co c∑  $= (4 + \mu_{\text{eff}} / n) / (n + 4 + 2\mu_{\text{eff}} / n)$  c1  $= 2 / ((n + 1.3)^{\wedge 2} + \mu_{\text{eff}})$  cμ  $= \min (1 - c1$  ，2\*(u_eff-2+1/μ_eff)/((n+2)^2+μ_eff)) ws[m_elite+1:end].  $\ast = -(1 + c1 / c\mu) / \mathrm{sum}(\mathrm{ws}[m\_elite + 1:end])$  E  $= n^{\wedge}0.5*(1 - 1 / (4n) + 1 / (21n^{\wedge}2))$  pσ,p∑,  $\Sigma =$  zeros(n), zeros(n), Matrix(1.0I(n)) for k in 1 : k_max P  $=$  MyNormal(μ,  $\sigma^{\wedge}2*\Sigma$  xs  $= [\mathrm{rand}(P)$  for i in 1 : m] ys  $= [f(x)$  for x in xs] is  $=$  sortperm(ys) # best to worst #selection and mean update  $\delta s = [(x - \mu) / \sigma$  for x in xs]  $\delta w = \mathrm{sum}(\mathrm{ws}[i]*\delta s[\mathrm{is}[i]]$  for i in 1 : m_elite)  $\mu += \sigma *\delta w$  # step-size control C  $= \Sigma^{\Lambda}$  -0.5 pσ  $= (1 - c\sigma)*p\sigma +\mathrm{sqrt}(c\sigma *(2 - c\sigma)*\mu_{\text{eff}})*C*\delta w$ $\sigma * = \exp (\mathrm{co} / \mathrm{d}\sigma *(\mathrm{norm}(\mathrm{p}\sigma) / E - 1))$  # covariance adaptation hσ  $= \mathrm{Int}(\mathrm{norm}(\mathrm{p}\sigma) / \mathrm{sqrt}(1-(1-\mathrm{c}\sigma)^{\wedge}(2k))< (1.4+2/(n+1))*E)$  p∑  $= (1 - c\Sigma)*p\Sigma +h o s^{*}q r t(c\Sigma *(2 - c\Sigma)*\mu_{\text{eff}})*\delta w$  wθ  $= [w s[i]\geq 0?$  ws[i]: n*ws[i]/norm(C*δs[i]])^2 for i in 1:m]  $\Sigma = (1 - c1 - c\mu)*\Sigma +$  c1\*(pΣ*pΣ' + (1-hσ)* cΣ*(2-cΣ)* Σ)+ cμ*sum(wθ[i]*δs[i][i]*δs[i][i]' for i in 1 : m)  $\Sigma = triu(\Sigma)+triu(\Sigma,1)"#enforce symmetry$  end return μ end
```

Algorithm 8.10. Covariance matrix adaptation, which takes an objective function  $f$  to be minimized, an initial design point  $x$ , and an iteration count  $k_{\text{max}}$ . One can optionally specify the step-size scalar  $\sigma$ , the sample size  $m$ , and the number of elite samples  $m_{\text{elite}}$ .

The best candidate design point is returned, which is the mean of the final sample distribution.

The covariance matrix undergoes an additional operation to ensure that it remains symmetric; otherwise small numerical inconsistencies can cause the matrix no longer to be positive definite.

This implementation uses a simplified normalization strategy for the negative weights. The original can be found in Equations 50-53 of N. Hansen, "The CMA Evolution Strategy: A Tutorial," 2016. arXiv: 1604.00772.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/41deb856f99ec793190a04e999259d45ffe1364ab91f51124c930d5b96747e52.jpg)  
Figure 8.9. Covariance matrix adaptation using multivariate Gaussian distributions applied to the flower function, appendix B.4.

- The cross-entropy method and evolution strategies maintain proposal distributions from which they sample in order to inform updates.  
- Natural evolution strategies uses gradient descent with respect to the log likelihood to update its proposal distribution.  
- Covariance matrix adaptation is a robust and sample-efficient optimizer that maintains a multivariate Gaussian proposal distribution with a full covariance matrix.

# 8.9 Exercises

Exercise 8.1. We have shown that mixture proposal distributions can better capture multiple minima. Why might their use in the cross-entropy method be limited?

Solution: The cross-entropy method must fit distribution parameters with every iteration. Unfortunately, no known analytic solutions for fitting multivariate mixture distributions exist. Instead, one commonly uses the iterative expectation maximization algorithm to converge on an answer.

Exercise 8.2. In the cross-entropy method, what is a potential effect of using an elite sample size that is very close to the total sample size?

Solution: If the number of elite samples is close to the total number of samples, then the resulting distribution will closely match the population. There will not be a significant bias toward the best locations for a minimizer, and so convergence will be slow.

Exercise 8.3. The log-likelihood of a value sampled from a Gaussian distribution with mean  $\mu$  and variance  $\nu$  is:

$$
\ell (x \mid \mu , \nu) = - \frac{1}{2} \ln 2 \pi - \frac{1}{2} \ln \nu - \frac{(x - \mu) ^{2}}{2 \nu}
$$

Show why evolution strategies using Gaussian distributions may encounter difficulties while applying a descent update on the variance when the mean is on the optimum,  $\mu = x^{*}$ .

Solution: The derivative of the log-likelihood with respect to  $\nu$  is:

$$
\begin{array}{l} \frac{\partial}{\partial \nu} \ell (x \mid \mu , \nu) = \frac{\partial}{\partial \nu} \left(- \frac{1}{2} \ln 2 \pi - \frac{1}{2} \ln \nu - \frac{(x - \mu) ^{2}}{2 \nu}\right) \\ = - \frac{1}{2 \nu} + \frac{(x - \mu) ^{2}}{2 \nu^{2}} \\ \end{array}
$$

The second term will be zero if the mean is already optimal. Thus, the derivative is  $-1 / 2\nu$  and decreasing  $\nu$  will increase the likelihood of drawing elite samples. Unfortunately,  $\nu$  is optimized by approaching arbitrarily close to zero. The asymptote near zero in the gradient update will lead to large step sizes, which cannot be taken as  $\nu$  must remain positive.

Exercise 8.4. Derive the maximum likelihood estimate for the cross-entropy method using multivariate normal distributions:

$$
\boldsymbol {\mu}^{(k + 1)} = \frac{1}{m} \sum_{i = 1}^{m} \mathbf {x}^{(i)}
$$

$$
\boldsymbol {\Sigma}^{(k + 1)} = \frac{1}{m} \sum_{i = 1}^{m} (\mathbf {x}^{(i)} - \boldsymbol {\mu}^{(k + 1)}) (\mathbf {x}^{(i)} - \boldsymbol {\mu}^{(k + 1)}) ^{\top}
$$

where the maximum likelihood estimates are the parameter values that maximize the likelihood of sampling the individuals  $\{\mathbf{x}^{(1)},\dots ,\mathbf{x}^{(m)}\}$ .

Solution: The probability density of a design  $\mathbf{x}$  under a multivariate normal distribution with mean  $\mu$  and covariance  $\boldsymbol{\Sigma}$  is

$$
p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = \frac{1}{(2 \pi | \boldsymbol {\Sigma} |) ^{1 / 2}} \exp \left(- \frac{1}{2} (\mathbf {x} - \boldsymbol {\mu}) ^{\top} \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu})\right)
$$

We can simplify the problem by maximizing the log-likelihood instead.25 The log-likelihood is:

$$
\begin{array}{l} \ln p (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = - \frac{1}{2} \ln (2 \pi | \boldsymbol {\Sigma} |) - \frac{1}{2} (\mathbf {x} - \boldsymbol {\mu}) ^{\top} \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu}) \\ = - \frac{1}{2} \ln (2 \pi | \boldsymbol {\Sigma} |) - \frac{1}{2} \left(\mathbf {x}^{\top} \boldsymbol {\Sigma}^{- 1} \mathbf {x} - 2 \mathbf {x}^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu} + \boldsymbol {\mu}^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu}\right) \\ \end{array}
$$

We begin by maximizing the log-likelihood of the  $m$  individuals with respect to the mean:

$$
\begin{array}{l} \ell \left(\boldsymbol {\mu} \mid \mathbf {x}^{(1)}, \dots , \mathbf {x}^{(m)}\right) = \sum_{i = 1}^{m} \ln p \left(\mathbf {x}^{(i)} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}\right) \\ = \sum_{i = 1}^{m} - \frac{1}{2} \ln (2 \pi | \boldsymbol {\Sigma} |) - \frac{1}{2} \left(\left(\mathbf {x}^{(i)}\right) ^{\top} \boldsymbol {\Sigma}^{- 1} \mathbf {x}^{(i)} - 2 \left(\mathbf {x}^{(i)}\right) ^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu} + \boldsymbol {\mu}^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu}\right) \\ \end{array}
$$

25 The log function is concave for positive inputs, so maximizing  $\log f(x)$  also maximizes a strictly positive  $f(x)$ .

We compute the gradient using the facts that  $\nabla_{\mathbf{z}}\mathbf{z}^{\top}\mathbf{A}\mathbf{z} = \left(\mathbf{A} + \mathbf{A}^{\top}\right)\mathbf{z},$  that  $\nabla_{\mathbf{z}}\mathbf{a}^{\top}\mathbf{z} = \mathbf{a},$  and that  $\boldsymbol{\Sigma}$  is symmetric and positive definite, and thus  $\boldsymbol{\Sigma}^{-1}$  is symmetric:

$$
\begin{array}{l} \nabla_{\boldsymbol {\mu}} \ell (\boldsymbol {\mu} \mid \mathbf {x}^{(1)}, \dots , \mathbf {x}^{(m)}) = \sum_{i = 1}^{m} - \frac{1}{2} \left(\nabla_{\boldsymbol {\mu}} \left(- 2 (\mathbf {x}^{(i)}) ^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu}\right) + \nabla_{\boldsymbol {\mu}} (\boldsymbol {\mu}^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu})\right) \\ = \sum_{i = 1}^{m} \left(\nabla_{\boldsymbol {\mu}} \left(\left(\mathbf {x}^{(i)}\right) ^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu}\right) - \frac{1}{2} \nabla_{\boldsymbol {\mu}} \left(\boldsymbol {\mu}^{\top} \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu}\right)\right) \\ = \sum_{i = 1}^{m} \boldsymbol {\Sigma}^{- 1} \mathbf {x}^{(i)} - \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu} \\ \end{array}
$$

We set the gradient to zero:

$$
\mathbf {0} = \sum_{i = 1}^{m} \boldsymbol {\Sigma}^{- 1} \mathbf {x}^{(i)} - \boldsymbol {\Sigma}^{- 1} \boldsymbol {\mu}
$$

$$
\sum_{i = 1}^{m} \boldsymbol {\mu} = \sum_{i = 1}^{m} \mathbf {x}^{(i)}
$$

$$
m \boldsymbol {\mu} = \sum_{i = 1}^{m} \mathbf {x}^{(i)}
$$

$$
\boldsymbol {\mu} = \frac{1}{m} \sum_{i = 1}^{m} \mathbf {x}^{(i)}
$$

Next we maximize with respect to the inverse covariance,  $\Lambda = \Sigma^{-1}$ , using the fact that  $|\mathbf{A}^{-1}| = 1 / |\mathbf{A}|$  with  $\mathbf{b}^{(i)} = \mathbf{x}^{(i)} - \boldsymbol{\mu}$ :

$$
\begin{array}{l} \ell \left(\mathbf {\Lambda} \mid \boldsymbol {\mu}, \mathbf {x}^{(1)}, \dots , \mathbf {x}^{(m)}\right) = \sum_{i = 1}^{m} - \frac{1}{2} \ln \left(2 \pi | \mathbf {\Lambda} | ^{- 1}\right) - \frac{1}{2} \left(\left(\mathbf {x}^{(i)} - \boldsymbol {\mu}\right) ^{\top} \mathbf {\Lambda} \left(\mathbf {x}^{(i)} - \boldsymbol {\mu}\right)\right) \\ = \sum_{i = 1}^{m} \frac{1}{2} \ln (| \boldsymbol {\Lambda} |) - \frac{1}{2} (\mathbf {b}^{(i)}) ^{\top} \boldsymbol {\Lambda} \mathbf {b}^{(i)} \\ \end{array}
$$

We compute the gradient using the facts that  $\nabla_{\mathbf{A}}|\mathbf{A}| = |\mathbf{A}|\mathbf{A}^{-\top}$  and  $\nabla_{\mathbf{A}}\mathbf{z}^{\top}\mathbf{A}\mathbf{z} = \mathbf{z}\mathbf{z}^{\top}$ :

$$
\begin{array}{l} \nabla_{\boldsymbol {\Lambda}} \ell (\boldsymbol {\Lambda} | \boldsymbol {\mu}, \boldsymbol {x}^{(1)}, \dots , \boldsymbol {x}^{(m)}) = \sum_{i = 1}^{m} \nabla_{\boldsymbol {\Lambda}} \left(\frac{1}{2} \ln (| \boldsymbol {\Lambda} |) - \frac{1}{2} (\boldsymbol {b}^{(i)}) ^{\top} \boldsymbol {\Lambda} \boldsymbol {b}^{(i)}\right) \\ = \sum_{i = 1}^{m} \frac{1}{2 | \boldsymbol {\Lambda} |} \nabla_{\boldsymbol {\Lambda}} | \boldsymbol {\Lambda} | - \frac{1}{2} \mathbf {b}^{(i)} (\mathbf {b}^{(i)}) ^{\top} \\ = \sum_{i = 1}^{m} \frac{1}{2 | \boldsymbol {\Lambda} |} | \boldsymbol {\Lambda} | \boldsymbol {\Lambda}^{- \top} - \frac{1}{2} \mathbf {b}^{(i)} (\mathbf {b}^{(i)}) ^{\top} \\ = \frac{1}{2} \sum_{i = 1}^{m} \boldsymbol {\Lambda}^{- \top} - \mathbf {b}^{(i)} \left(\mathbf {b}^{(i)}\right) ^{\top} \\ = \frac{1}{2} \sum_{i = 1}^{m} \boldsymbol {\Sigma} - \mathbf {b}^{(i)} \left(\mathbf {b}^{(i)}\right) ^{\top} \\ \end{array}
$$

and set the gradient to zero:

$$
\begin{array}{l} \mathbf {0} = \frac{1}{2} \sum_{i = 1}^{m} \boldsymbol {\Sigma} - \mathbf {b}^{(i)} \left(\mathbf {b}^{(i)}\right) ^{\top} \\ \sum_{i = 1}^{m} \boldsymbol {\Sigma} = \sum_{i = 1}^{m} \mathbf {b}^{(i)} \left(\mathbf {b}^{(i)}\right) ^{\top} \\ \boldsymbol {\Sigma} = \frac{1}{m} \sum_{i = 1}^{m} \left(\mathbf {x}^{(i)} - \boldsymbol {\mu}\right) \left(\mathbf {x}^{(i)} - \boldsymbol {\mu}\right) ^{\top} \\ \end{array}
$$

# 9 Population Methods

Previous chapters have focused on methods where a single design point is moved incrementally toward a minimum. This chapter presents a variety of population methods that involve optimization using a collection of design points, called individuals. Having a large number of individuals distributed throughout the design space can help the algorithm avoid becoming stuck in a local minimum. Information at different points in the design space can be shared between individuals to globally optimize the objective function. Many population methods are stochastic in nature, and it is generally easy to parallelize the computation. Several methods that we discuss in this chapter are loosely inspired by evolutionary processes and other biological phenomena.

# 9.1 Population Iteration

In contrast with local descent methods (chapter 4), which iteratively improve a single design, population methods iteratively improve a population of  $m$  designs  $\mathbf{x}^{(1)},\ldots ,\mathbf{x}^{(m)}$ . The population at a particular iteration is often referred to as a generation. The algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations. Population methods typically follow an iterative structure similar to local descent methods as shown in algorithm 9.1. The various methods discussed in this chapter differ in how they use the previous generation to generate the next generation.

Population methods begin with an initial population, just as descent methods require an initial design point. The initial population should be spread over the design space to increase the chances that the samples are close to the best regions. Some population methods require additional information to be associated with individuals, such as velocity in the case of particle swarm optimization.[2]

1 The superscript  $(i)$  denotes the ith individual in the population rather than the iteration number as in previous chapters.

2 Particle swarm optimization is discussed in section 9.4.

```julia
abstract type PopulationMethod end  
function population_method(M::PopulationMethod, f, designs, k_max)  
population = init!(M, f, designs)  
for k in 1:k_max  
    population = step!(M, f, population)  
end  
return population  
end
```

We can initialize the population by sampling from a variety of distributions. The choice of distribution can have a significant impact on the performance of the algorithm. The uniform distribution over a hyperrectangle is a common choice if the bounds on particular design variables are known. The multivariate normal distribution can be used to concentrate samples in areas of interest while also providing samples that broadly span the space. Alternatively, we can use the Cauchy distribution, which is a distribution that decays less rapidly than the Gaussian as we move away from the center. Figure 9.1 compares populations generated using different methods. More advanced sampling methods are discussed in chapter 16.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b34d68f2791aee7f0b52b697072e053cf767bc1176c17d2ee7914c3169623708.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e35bc792964a3a6121bc9f75d20b57d37797d1216a0ebb760d427c07cef2e89a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/039dbffad7de4a5dd297f1f7d388f06a60958f455d318a0110777934f4cf8f58.jpg)  
Figure 9.1. Initial populations of size 1,000 sampled using a uniform hyperrectangle bounded by  $\pm 2$ , a zero-mean normal distribution with diagonal covariance  $\Sigma = \mathbf{I}$ , and Cauchy distributions centered at the origin with scale  $\sigma = 1$ . The Distributions.jl package can be used to sample from a variety of distributions. M. Besançon, T. Papamarkou, D. Anthoff, A. Arslan, S. Byrne, D. Lin, et al., "Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem," Journal of Statistical Software, vol. 98, no. 16, pp. 1-30, 2021.

# 9.2 Genetic Algorithms

Genetic algorithms (algorithm 9.2) borrow inspiration from biological evolution,3 where fitter individuals are more likely to pass on their genes to the next generation.4 In this context, a gene corresponds to a design variable. An individual's

Algorithm 9.1. An iterated population method for minimizing  $f$  starting with a population given by designs. This implementation operates on an abstract PopulationMethod object that supports both an init! method for initialization and a step! method for executing a single population step. It executes k_max iterations. The best performing individual can then be extracted from the population.

3 In the biological context, DNA is evolved. DNA consists of a sequence of four nucleobases: adenine, thymine, cytosine, and guanine, which are often abbreviated A, T, C, and G. Genetic algorithms evolve individuals represented by assignments of values to design variables.

4 D. E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, 1989.

fitness for reproduction is inversely related to the value of the objective function at that point. The design point associated with an individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation after undergoing the genetic operations of crossover and mutation. There are some variations of genetic algorithms that incorporate elitism or elitist selection, where some of the best individuals are passed on to the next generation without undergoing these genetic operations.

```julia
struct GeneticAlgorithm <- PopulationMethod
    S # SelectionMethod
    C # CrossoverMethod
    U # MutationMethod
end
init!(M::GeneticAlgorithm, f, designs) = designs
function step!(M::GeneticAlgorithm, f, population)
    S, C, U = M.S, M.C, M.U
    parents = select(S, f.(population))
    children = [crossover(C, population[p[1]], population[p[2]])
        for p in parents]
    return [mutate(U, c) for c in children]
```

Algorithm 9.2. The genetic algorithm is defined by a selection method S, a crossover method C, and a mutation method U.

# 9.2.1 Selection

Selection is the process of choosing chromosomes to use as parents for the next generation. For a population with  $m$  chromosomes, a selection method will produce a list of  $m$  parental pairs for the  $m$  children of the next generation. The selected pairs may contain duplicates.

There are several approaches for biasing the selection toward the fittest (algorithm 9.3). In truncation selection (figure 9.2), we sample parents from among the best  $k$  chromosomes in the population. In tournament selection (figure 9.3), each parent is the fittest out of  $k$  randomly chosen chromosomes of the population. In roulette wheel selection (figure 9.4), also known as fitness proportionate selection, each parent is chosen with a probability proportional to its performance relative to the population. Since we are interested in minimizing an objective function  $f$ , the fitness of the  $i$ th individual  $\mathbf{x}^{(i)}$  is inversely related to  $y^{(i)} = f(\mathbf{x}^{(i)})$ . There are differ-

5 Alternatively, we can use groups if we want to combine more than two parents to form a child.

ent ways to transform a collection  $y^{(1)}, \ldots, y^{(m)}$  into fitnesses. A simple approach is to assign the fitness of individual  $i$  according to  $\max \{y^{(1)}, \ldots, y^{(m)}\} - y^{(i)}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/74796fcb53ada6e6281c55d150882f3dc2565c2096c6587f906d10f1e81ce76f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a4695f455d8490615a2a9aeb812ac7bba684d3e86953b379ca91346c048a5410.jpg)  
Figure 9.2. Truncation selection with a population size  $m = 7$  and sample size  $k = 3$ . The height of a bar indicates its objective function value whereas its color indicates what individual it corresponds to.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a8eca102c472bfb100445e8e53613f51c140b324958c4efae586b1b2e79d499a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a05f0a25ddf4f43d176454c2a2dc395fe6d832215de59ecebdd6f3ac995bb49d.jpg)  
Figure 9.3. Tournament selection with a population size  $m = 7$  and a sample size  $k = 3$ , which is run separately for each parent. The height of a bar indicates its objective function value whereas its color indicates what individual it corresponds to.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/045084535a4ecdb6ac7e9d58fa7fc0c51fba7ac50e010f466c2b668b0ad65a18.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1fb6ecdecb97a25bcb3d3646c80b4fdd378b91e7368894ec0c608f59ab38625a.jpg)  
Figure 9.4. Roulette wheel selection with a population size  $m = 7$  which is run separately for each parent. The approach used causes the individual with the worst objective function value to have a zero likelihood of being selected. The height of a bar indicates its objective function value (left), or its likelihood (right), whereas its color indicates what individual it corresponds to.

# 9.2.2 Crossover

Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes (algorithm 9.4).

```julia
abstract type SelectionMethod end   
# Pick pairs randomly from top k parents   
struct TruncationSelection  $<  :$  SelectionMethod k # top k to keep   
end   
function select(t::TruncationSelection, y) p  $=$  sortperm(y) return [p[rand(1:t.k, 2)] for i in y]   
end   
# Pick parents by choosing best among random subsets   
struct TournamentSelection  $<  :$  SelectionMethod k # top k to keep   
end   
function select(t::TournamentSelection, y) getparent()  $=$  begin p  $=$  randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent() ] for i in y]   
end   
# Sample parents proportionately to fitness   
struct RouletteWheelSelection  $<  :$  SelectionMethod end   
function select(::RouletteWheelSelection, y) y  $=$  maximum(y).-y cat  $=$  Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y]   
end
```

Algorithm 9.3. Several selection methods for genetic algorithms. Calling select with a SelectionMethod and a list of objective function values  $y$  will produce a list of parental pairs.

- In single-point crossover (figure 9.5), the first portion of parent A's chromosome forms the first portion of the child chromosome, and the latter portion of parent B's chromosome forms the latter part of the child chromosome. The crossover point where the transition occurs is determined uniformly at random.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/861d956c79198be45411a1cc48503c5ef7d910f3b0e2d29a653794f7910533cb.jpg)  
Figure 9.5. Single-point crossover.

- In two-point crossover (figure 9.6), we use two random crossover points.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f8c801f5505f54c0dfcf449b2f21cd2feb316c872b46dfa44bff1cccbc61c082.jpg)  
Figure 9.6. Two-point crossover.

- In uniform crossover (figure 9.7), each gene is independently selected with probability  $p$  from the first parent and probability  $1 - p$  from the second parent. Typically,  $p = 0.5$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0daac534e4de2b8059ac81e803680f85a9e8ef3446fdc3afa0d653cea222d4ae.jpg)  
Figure 9.7. Uniform crossover.

- In interpolation crossover values are linearly interpolated between the values of the parents  $\mathbf{x}_a$  and  $\mathbf{x}_b$ :

$$
\mathbf {x} \leftarrow (1 - \lambda) \mathbf {x}_{a} + \lambda \mathbf {x}_{b} \tag {9.1}
$$

where  $\lambda$  is a scalar parameter typically set to 0.5.

```julia
abstract type CrossoverMethod end  
struct SinglePointCrossover <: CrossoverMethod end  
function crossover(::SinglePointCrossover, a, b)  
    i = rand(eachindex(a))  
    return [a[1:i]; b[i+1:end]]  
end  
struct TwoPointCrossover <: CrossoverMethod end  
function crossover(::TwoPointCrossover, a, b)  
    n = length(a)  
    i, j = rand(1:n, 2)  
    if i > j  
        (i,j) = (j,i)  
    end  
    return [a[1:i]; b[i+1:j]; a[j+1:n]]  
end  
struct UniformCrossover <: CrossoverMethod  
p # crossover probability  
end  
function crossover(U::UniformCrossover, a, b)  
    return [rand() > U.p ? u : v for (u,v) in zip(a,b)]  
end  
struct InterpolationCrossover <: CrossoverMethod  
λ # interpolant  
end  
crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b
```

Algorithm 9.4. Several crossover methods for genetic algorithms. Calling crossover with a CrossoverMethod and two parents a and b will produce a child chromosome that contains a mixture of the parents' genetic codes.

# 9.2.3 Mutation

If new chromosomes were produced only through crossover, many traits that were not present in the initial random population could never occur, and the most-fit genes could saturate the population. Mutation allows new traits to spontaneously appear, allowing the genetic algorithm to explore more of the state space. Child chromosomes undergo mutation after crossover.

Each gene in the chromosome typically has a small probability  $\lambda$  of being changed. For a chromosome with  $m$  genes, this mutation rate is typically set to  $1 / m$ , yielding an average of one mutation per child chromosome. Mutation can be implemented in various ways, but a common way is to add noise selected from a distribution such as the zero-mean Gaussian distribution. Algorithm 9.5 provides implementations.

```julia
abstract type MutationMethod end   
struct DistributionMutation  $\ll$  MutationMethod  $\lambda$  # mutation rate D # mutation distribution   
end   
function mutate(M::DistributionMutation, child) return [rand()  $<$  M.  $\lambda ?\lor +$  rand(M.D) : v for v in child]   
end   
GaussianMutation(σ)  $=$  DistributionMutation(1.0, Normal(0,σ))
```

Figure 9.8 illustrates several generations of a genetic algorithm. Example 9.1 shows how to combine selection, crossover, and mutation strategies discussed in this section.

Algorithm 9.5. A mutation method for genetic algorithms that iterates through each gene in a chromosome and, with probability  $\lambda$ , adds a random value drawn from the distribution D. Gaussian mutation is a special case of this where all genes are perturbed by a zero-mean Gaussian distribution with standard deviation  $\sigma$ . Other mutation methods might include bit flips if the chromosome is binary-valued.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/de4d80447f7a970212d1cb79cb5ab57314adc9b7014d8567ef61c52ef77d1f22.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/adca792958d110d8a780a7b7d00c63af6e4c562bdbead16f5c420eca04de94c2.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4310174db9dbd010057d1172707a1d5a52e42d99f3a5613a7cefb1476075e205.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b3c1751009ff0fdb74884f914cc373bb1bf033c012f582266313565a4d3b052b.jpg)  
$x_{1}$  
Figure 9.8. A genetic algorithm with truncation selection, single point crossover, and Gaussian mutation with  $\sigma = 0.1$  on the Michalewicz function (appendix B.5).

We demonstrate using genetic algorithms to optimize  $f(\mathbf{x}) = \| \mathbf{x} \|$ .  
```julia
import Random: seed!   
import LinearAlgebra: norm   
seed!(0) # set random seed for reproducible results   
f  $=$  x  $\rightarrow$  norm(x)   
m  $= 100$  # population size   
k_max  $= 10$  # number of iterations   
function rand_populaton.uniform(m,a,b) d  $=$  length(a) return [a+rand(d).*b-a) for i in 1:m]   
end   
population  $=$  rand_populaton.uniform(m，[-3,3]，[3,3])   
M  $=$  GeneticAlgorithm( TruncationSelection(10)，#select top 10 SinglePointCrossover(), GaussianMutation(0.5)) # perturb with 0.5 standard deviation   
population  $=$  population_method(M,f,population,k_max)   
x  $=$  argmin(f,population) @show x   
 $\mathbf{x} = [-0.035070453820023516,0.03762926957036672]$
```

Example 9.1. Demonstration of using a genetic algorithm for optimizing a simple function.

# 9.3 Differential Evolution

Differential evolution (algorithm 9.6) attempts to improve each individual in the population using crossover with a candidate individual formed from the recombination of other individuals in the population.<sup>6</sup> It is parameterized by a differential weight  $w$  and crossover probability  $p$ .

For each individual  $\mathbf{x}$ :

1. Choose three random distinct individuals  $\mathbf{a}$ ,  $\mathbf{b}$ , and  $\mathbf{c}$ .  
2. Construct an interim design  $\mathbf{z} = \mathbf{a} + w\cdot (\mathbf{b} - \mathbf{c})$  as shown in figure 9.9.73. Construct the candidate individual  $\mathbf{x}'$  through uniform crossover with  $\mathbf{x}$  and  $\mathbf{z}$  with probability  $p$ , where

$$
x_{i}^{\prime} = \left\{ \begin{array}{l l} z_{i} & \text{with probability } p \\ x_{i} & \text{otherwise} \end{array} \right. \tag {9.2}
$$

4. Insert the better design between  $\mathbf{x}$  and  $\mathbf{x}'$  into the next generation.

The interim design is a base individual  $\mathbf{a}$  with an added perurbation  $w \cdot (\mathbf{b} - \mathbf{c})$ . The perturbation is necessary for exploration, and constructing it from the difference of two individuals allows the perturbations to naturally decrease as the population becomes more concentrated. Applying the perturbation to a base individual  $\mathbf{a}$  rather than  $\mathbf{x}$  itself allows information about good designs to be shared across individuals in the population. The crossover probability parameter controls how quickly the population will concentrate. Larger crossover probabilities will lead to faster concentration. The algorithm is demonstrated in figure 9.10.

# 9.4 Particle Swarm Optimization

Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.

$^\mathrm{6}S.$  Das and P.N. Suganthan, "Differential Evolution: A Survey of the State-of-the-Art," IEEE Transactions on Evolutionary Computation, vol. 15, no. 1, pp. 4-31, 2011.

7 Typically,  $w$  is between 0 and 2.

Studies on test function suites found that differential evolution performed best with either  $p \approx 1$  or with  $p \approx 0$  and one guaranteed mutation. This bifurcation was later attributed to whether the objective function was decomposable, with low mutation rates working better when the test function can be written as the sum of functions over disjoint subsets of the design. For such functions, steps along a single coordinate are more likely to lead to improvements, and those improvements can reliably be shared through crossover with other designs. K.V. Price, R.M. Storn, and J.A. Lampinen, Differential Evolution: A Practical Approach to Global Optimization. Springer, 2006.

Figure 9.9. Differential evolution takes three individuals a, b, and c and combines them to form the candidate individual z.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/18cb3e475a5673b2532afff5a3cec03e88acfdc53acf47395edb0050459dbffd.jpg)  
9J. Kennedy, R.C. Eberhart, and Y. Shi, Swarm Intelligence. Morgan Kaufmann, 2001.

```julia
mutable struct DifferentialEvolution <- PopulationMethod
p # crossover probability
w # differential weight
end
init!(M::DifferentialEvolution, f, designs) = designs
function step!(M::DifferentialEvolution, f, population)
p, w = M.p, M.w
n, m = length(population[1]), length(population)
for x in population
a, b, c = sample(population, 3, replace=false)
z = a + w*(b-c)
x' = crossover(UniformCrossover(p), x, z)
if f(x') < f(x)
x := x'
end
end
return population
end
```

Algorithm 9.6. Differential evolution, which updates populations through recombination with three other population members. The algorithm requires a crossover probability  $\mathfrak{p}$  and a differential weight  $w$ . This implementation makes two simplifications to the formulation typically presented in the literature. First, our implementation does not enforce that  $\mathbf{a}, \mathbf{b},$  and  $\mathbf{c}$  be distinct from  $x$ . In practice, they will be for large populations, but even if they are not, it is not particularly detrimental. Second, the original formulation ensures that uniform crossover has at least one mutation. When the mutation rate is very low, it is necessary to ensure that at least one mutation occurs in order for the candidate individual to differ.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/efe135cabb8125881dc879f13a11cf1f11fa187ad596e4617a26a6c8a82f028e.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b2d58512149009f018af5fe7171b09782d588752951556ca144b56a0bdc0f679.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f164d2ea540553d28871fe97a817a70923313f37b449fc8d307dec02cf8edc1f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6cc2384a9e54b4baa5974e2b1f43b53c5833f42d436596495dad3bb21f85c9c7.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7322ec56317e9a9176ed1b964bec27369cd8c42572c7366244d93fe7a80e6216.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/594cd62a695e5903067b0bfd6ea6f04d289b8b5ba82880ffd93ead30023c24d6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1fd1e1d954dc2062432227daa03b1f773b57e8e95fd5324d6f7a9967ed0b521a.jpg)  
Figure 9.10. Differential evolution with  $p = 0.5$  and  $w = 0.4$  on Ackley's function (appendix B.1).

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b24e197a34ca598b2b28b986f5b9b7e9e662950254aedddabc6004ebc2ddab5a.jpg)

At each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term, with separate random numbers being generated for each acceleration. The update equations are:

$$
\mathbf {x}^{(i)} \leftarrow \mathbf {x}^{(i)} + \mathbf {v}^{(i)} \tag {9.3}
$$

$$
\mathbf {v}^{(i)} \leftarrow w \mathbf {v}^{(i)} + c_{1} r_{1} \left(\mathbf {x}_{\text{best}}^{(i)} - \mathbf {x}^{(i)}\right) + c_{2} r_{2} \left(\mathbf {x}_{\text{best}} - \mathbf {x}^{(i)}\right) \tag {9.4}
$$

where  $\mathbf{x}_{\mathrm{best}}$  is the best location found so far over all particles;  $\mathbf{x}_{\mathrm{best}}^{(i)}$  is the best location found by the  $i$ th particle;  $w, c_{1}$ , and  $c_{2}$  are parameters; and  $r_{1}$  and  $r_{2}$  are random numbers drawn from the uniform distribution  $\mathcal{U}(0,1)$ .<sup>10</sup> Algorithm 9.7 provides an implementation. Figure 9.11 shows several iterations of the algorithm.

10 A common strategy is to allow the inertia  $w$  to decay over time.

# 9.5 Firefly Algorithm

The firefly algorithm (algorithm 9.8) was inspired by the manner in which fireflies flash their lights to attract mates of the same species.[11] In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies. At each iteration, all fireflies are moved toward all more attractive fireflies. A firefly  $\mathbf{a}$  is moved toward a firefly  $\mathbf{b}$  with greater attraction according to

$$
\mathbf {a} \leftarrow \mathbf {a} + \beta I (\| \mathbf {b} - \mathbf {a} \|) (\mathbf {b} - \mathbf {a}) + \alpha \epsilon \tag {9.5}
$$

where  $I$  is the attraction intensity and  $\beta$  is a scaling parameter. A random walk component is included as well, where  $\epsilon$  is drawn from a zero-mean, unit covariance multivariate Gaussian, and  $\alpha$  scales the step size. The resulting update is a random walk biased toward brighter fireflies. $^{12}$

The intensity  $I$  decreases as the distance  $r$  between the two fireflies increases and is defined to be 1 when  $r = 0$ . One approach is to model the intensity with a Gaussian brightness drop-off:

$$
I (r) = e^{- \gamma r^{2}} \tag {9.6}
$$

where  $\gamma > 0$  is a parameter that controls the rate of decay.

A firefly's attraction is proportional to its performance. Attraction affects only whether one fly is attracted to another fly, whereas intensity affects how much the less attractive fly moves toward the more attractive fly. Figure 9.12 shows a few iterations of the algorithm.

11 X.-S. Yang, Nature-Inspired Metaheuristic Algorithms, 2nd ed. Luminer Press, 2010. Interestingly, some females imitate the patterns of other species to attract the males of the other species, which they then eat.

12 Yang recommends  $\beta = 1$  and  $\alpha \in [0,1]$ . If  $\beta = 0$ , the behavior is a random walk.

```julia
mutable struct Particle
x # position
v # velocity
x_best # best design thus far
end
mutable struct ParticleSwarm <: PopulationMethod
w # inertia
c1 # first momentum coefficient
c2 # second momentum coefficient
V # initial particle velocity distribution
best # best overall design thus far, and its value
end
function init!(M::ParticleSwarm, f, designs)
population = [Particle(x, rand(M.V), copy(x)) for x in designs]
best = (x = copy(population[1].x), y = Inf)
for P in population
    y = f(P.x)
    if y < best.y; best = (x = P.x, y = y); end
    end
    M.best = best
    return population
end
function step!(M::ParticleSwarm, f, population)
w, c1, c2, best = M.w, M.c1, M.c2, M.best
n = length(best.x)
for P in population
    r1, r2 = rand(n), rand(n)
    P.x += P.v
    P.v = w * P.v + c1 * r1 * (P.x_best - P.x) +
        c2 * r2 * (best.x - P.x)
    y = f(P.x)
    if y < best.y; best = (x = copy(P.x), y = y); end
    if y < f(P.x_best); P.x_best = P.x; end
    end
    M.best = best
    return population
end
```

Algorithm 9.7. Particle swarm optimization, which incorporates momentum into population updates. It requires an inertia w, momentum coefficients c1 and c2, and an initial velocity distribution V.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dabd134885d28e33d999dc4bf6c1fe8880eec50034acc5390a4f6b072b25d6a6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/606e1155818a809e89bd4e1fc853984ef99f344f589c115c55b2957c03af3a9c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4bc8e533ea4c0b7bd6c35829dd7dc1d482dcbc5d834f4246b78a4f7d19c8ac62.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4bfc21f5de5467284c7d3ee8bbab1baa491d88ea96da70ca3f84383aff304f92.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d9fe353e984335678b995dcf0b5fc4084abbced84e3bcb1e5a72ae100fcebdf7.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3048034c9031d42f0e74d8ff83bafaa7105d1a2611a741f4c4e90b64fa18c1fc.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3ce3783f11a8b7ae7cf97f3d5748b6f3cd981d1b19991ed4dafe3b7a8b9663af.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/64bcecae4c99fde6f5f7aa693194f7c384db02fc97e770e895f02206663b370d.jpg)  
Figure 9.11. The particle swarm method with  $w = 0.1$ ,  $c_{1} = 0.25$  and  $c_{2} = 2$  on Wheeler's Ridge (appendix B.7).

```julia
struct Firefly <- PopulationMethod
     $\alpha$  # walk step size
     $\beta$  # source intensity
    brightness # intensity function
end
init!(M::Firefly, f, designs) = designs
function step!(M::Firefly, f, population)
     $\alpha$ ,  $\beta$ , brightness = M. $\alpha$ , M. $\beta$ , M.brightness
    m = length(population[1])
    N = MvNormal(I(m))
    for a in population, b in population
        if f(b) < f(a)
            r = norm(b-a)
            a.+=  $\beta$ *brightness(r)*(b-a) +  $\alpha$ *rand(N)
        end
    return population
end
```

Algorithm 9.8. Firefly search, where individuals are attracted to one another according to their luminance. The method requires a random walk step size  $\alpha$ , a source intensity  $\beta$ , and an intensity function brightness such as  $r \rightarrow \exp(-r^2)$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c3fa89b150f1df4a12ec045bd92363833f281d15f6581b720202472c0103f556.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/51847dc4a40a8b2b3d4d5dbf70736a053b372a78cea570e40de7aa824d365d7f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/58569740624b78ba117fca59922ced8ea505fd9d3b24b1f3cd9b885af6790506.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/394694866c3e8d63eca1541f5fbb7e0b5b06435439fa385e84263a2c4f2d2bd4.jpg)  
Figure 9.12. Firefly search with  $\alpha = 0.5$ ,  $\beta = 1$ , and  $\gamma = 0.1$  applied to the Branin function (appendix B.3).

# 9.6 Cuckoo Search

Cuckoo search (algorithm 9.9) is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism.[13] Cuckoos lay their eggs in the nests of other birds, often birds of other species. When this occurs, the host bird may detect the invasive egg and then destroy it or establish a new nest somewhere else. However, there is also a chance that the egg is accepted and raised by the host bird.[14]

In cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution. A new design point can replace a nest if it has a better objective function value, which is analogous to cuckoo eggs replacing the eggs of birds of other species.

The core rules are:

1. A cuckoo will lay an egg in a randomly chosen nest.  
2. The best nests with the best eggs will survive to the next generation.  
3. Cuckoo eggs have a chance of being discovered by the host bird, in which case the eggs are destroyed.

Cuckoo search relies on random flights to establish new nest locations. These flights start from an existing nest and then move randomly to a new location. While we might be tempted to use a uniform or Gaussian distribution for the walk, these restrict the search to a relatively concentrated region. Instead, cuckoo search uses a Cauchy distribution, which has a heavier tail. In addition, the Cauchy distribution has been shown to be more representative of the movements of other animals in the wild.[15] Figure 9.13 shows a few iterations of cuckoo search.

13 X.-S. Yang and S. Deb, "Cuckoo Search via Lévy Flights," in World Congress on Nature & Biologically Inspired Computing (NaBIC), 2009.  
14 Interestingly, an instinct of newly hatched cuckoos is to knock other eggs or hatchlings (those belonging to the host bird) out of the nest.

For example, a certain species of fruit fly explores its surroundings using Cauchy-like steps separated by  $90^{\circ}$  turns. A.M. Reynolds and M.A. Frye, "Free-Flight Odor Tracking in Drosophila is Consistent with an Optimal Intermittent Scale-Free Search," PLoS ONE, vol. 2, no. 4, e354, 2007.

```julia
mutable struct CuckooSearch :: PopulationMethod
p_s # search fraction
p_a # nest abandonment fraction
C # flight distribution
end
function init!(M::CuckooSearch, f, designs)
return [(x=x, y=f(x)) for x in designs]
end
function step!(M::CuckooSearch, f, population)
p_s, p_a, C = M.p_s, M.p_a, M.C
m, n = length(population), length(population[1].x)
m_search = round(Int, m*p_s)
m_abandon = round(Int, m*p_a)
for i in 1:m_search
j, k = rand(1:m), rand(1:m)
x = population[j].x + rand(C,n)
y = f(x)
if y < population[k].y
population[k] = (x=x, y=y)
end
end
p = sortperm(population, by=nest→nest.y, rev=true)
for i in 1:m_abandon
j = rand(1:m-m_abandon) + m_abandon
x' = population[p[j]].x + rand(C,n)
population[p[i]] = (x=x', y=f(x'))
end
return population
end
```

Algorithm 9.9. Cuckoo search, which takes an objective function  $f$ , an initial set of nests population, a number of iterations  $k_{\text{max}}$ , fraction of nests to abandon  $p_a$ , and flight distribution  $c$ . The flight distribution is typically a centered Cauchy distribution.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/74d6eb1d6cbaa953ff6426a2e0bd51807c25858b437bd3469cfde26e64380495.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ab749add8edc5cd918b1121a023a3ba1cbce1015e12652ade28b539c824bd6df.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7be2ffb48bd6b189a8c630d4051bd56ab7a65078925886a474c0fd0e63135bd9.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/20981cf43559dfee29828ffd062af6983beff6eb7dc07af66bc63bea383382ca.jpg)  
Figure 9.13. Cuckoo search on the Branin function (appendix B.3).

Other nature-inspired algorithms include the artificial bee colony, the gray wolf optimizer, the bat algorithm, glowworm swarm optimization, intelligent water drops, and harmony search.[16] There has been some criticism of the proliferation of methods that make analogies to nature without fundamentally contributing novel methods and understanding.[17]

# 9.7 Hybrid Methods

Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods. Several hybrid methods $^{18}$  have been developed to extend population methods with descent-based features to improve their performance in local search. There are two general approaches to combining population methods with local search techniques: $^{19}$

- In Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual's optimized counterpart and its objective function value.  
- In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual's objective function value. Individuals are not replaced but are merely associated with optimized objective function values, which are not the same as their actual objective function value. Baldwinian learning can help prevent premature convergence.

The difference between these approaches is illustrated in example 9.2.

# 9.8 Summary

- Population methods use a collection of individuals in the design space to guide progression toward an optimum.  
- Genetic algorithms leverage selection, crossover, and mutations to produce better subsequent generations.

$^{16}$  D. Simon, Evolutionary Optimization Algorithms. Wiley, 2013.

This viewpoint is expressed by K. Sorensen, "Metaheuristics—the Metaphor Exposed," International Transactions in Operational Research, vol. 22, no. 1, pp. 3-18, 2015.

In the literature, these kinds of techniques are also referred to as memetic algorithms or genetic local search.  
19 K.W.C. Ku and M.-W. Mak, "Exploring the Effects of Lamarckian and Baldwinian Learning in Evolving Recurrent Neural Networks," in IEEE Congress on Evolutionary Computation (CEC), 1997.

Consider optimizing  $f(x) = -e^{-x^2} - 2e^{-(x - 3)^2}$  using a population of individuals initialized near  $x = 0$ .

Example 9.2. A comparison of the Lamarckian and Baldwinian hybrid methods.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a115be4c42b4298a0555b42573c55db7ecf36411366c9ccf3947bd253f976789.jpg)  
Lamarckian

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e44d968bba21f88051b4952f4ed975c1bd46085fe4bda73ef9bde8c9bbc57152.jpg)  
Baldwinian

A Lamarckian local search update applied to this population would move the individuals toward the local minimum, reducing the chance that future individuals escape and find the global optimum near  $x = 3$ . A Baldwinian approach will compute the same update but leaves the original designs unchanged. The selection step will value each design according to its value from a local search.

- Differential evolution, particle swarm optimization, the firefly algorithm, and cuckoo search include rules and mechanisms for attracting design points to the best individuals in the population while maintaining suitable state space exploration.  
- Population methods can be extended with local search approaches to improve convergence.

# 9.9 Exercises

Exercise 9.1. What is the motivation behind the selection operation in genetic algorithms?

Solution: To bias survival to the fittest by biasing the selection toward the individuals with better objective function values.

Exercise 9.2. Why does mutation play such a fundamental role in genetic algorithms? How would we choose the mutation rate if we suspect there is a better optimal solution?

Solution: Mutation drives exploration using randomness. It is therefore essential in order to avoid local minima. If we suspect there is a better solution, we would need to increase the mutation rate and let the algorithm have time to discover it.

Exercise 9.3. If we observe that particle swarm optimization results in fast convergence to a nonglobal minimum, how might we change the parameters of the algorithm?

Solution: Increase the population size or the coefficient that biases the search toward individual minima.

Exercise 9.4. Differential evolution combines uniform crossover with a recombination of three other individuals in the population. Compare the following three variants of differential evolution on Ackley's function (appendix B.1):

Uniform crossover with one chosen individual  $(p = 0.5, w = 0.0)$ .  
- Perturbations applied to the design  $(p = 0.5, w = 0.5, \mathbf{a} = \mathbf{x})$ .  
- Differential evolution with  $(p = 0.5, w = 0.5)$ .  
- A high crossover probability  $(p = 0.9, w = 0.5)$ .  
- Guaranteed crossover ( $p = 1.0, w = 0.5$ ).

How do the various components of differential evolution contribute to its performance?

Solution: Below we show the distributions over the best value found so far over 1,000 trials each with populations of size 20 for the five algorithm variations.

Removing the perturbation vector  $(\mathbf{w} = 0)$  does the worst and tends not to converge to an optimum. The algorithm can only explore designs with components that already exist in the population; it cannot produce new design components.

Perturbations applied directly to the design  $(\mathbf{a} = \mathbf{x})$  is the next worst. It forces all updates to be local perturbations, which makes it difficult for the algorithm to escape local minima.

The method that does the best is the one with guaranteed crossover ( $p = 1.0$ ), which always replaces all components of  $\mathbf{x}$  with the interim design. Having a large but non-guaranteed crossover probability ( $p = 0.9$ ) does nearly as well.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7e5611a18ecf38880ff159cfc738819a5b883bce273add9f8d5167b6cc87da56.jpg)

This exercise demonstrates that differential evolution's construction of an interim design through  $\mathbf{a} + w \cdot (\mathbf{b} - \mathbf{c})$  relies on  $w \cdot (\mathbf{b} - \mathbf{c})$  to supply a perturbation that can be used to exlore the space, and relies on  $\mathbf{a}$  to allow the method to abandon designs in poor local minima by jumping close to  $\mathbf{a}$ . This particular experiment found less need for uniform crossover, but our design space only had two dimensions.

# 10 Constraints

Previous chapters have focused on unconstrained problems where the domain of each design variable is the space of real numbers. Many problems are constrained, which forces design points to satisfy certain conditions. This chapter presents a variety of approaches for transforming problems with constraints into problems without constraints, thereby permitting the use of the optimization algorithms we have already discussed. Analytical methods are also discussed, including the necessary conditions for optimality with constraints.

# 10.1 Constrained Optimization

Recall the core optimization problem from equation (1.1):

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \tag {10.1}
$$

$$
\text{subject} \quad \mathbf {x} \in \mathcal {X}
$$

In the previous chapters, the feasible set  $\mathcal{X}$  was assumed to be  $\mathbb{R}^n$ . In the constrained problems in this chapter, the feasible set is some subset of  $\mathbb{R}^n$ .

Some constraints are simply upper or lower bounds on the design variables, as we have seen in bracketed line search, in which  $x$  must lie between  $a$  and  $b$ . A bracketing constraint  $x \in [a, b]$  can be replaced by two inequality constraints:  $a \leq x$  and  $x \leq b$  as shown in figure 10.1. In multivariate problems, bracketing the input variables forces them to lie within a hyperrectangle as shown in figure 10.2.

Constraints arise naturally when formulating real problems. A hedge fund manager cannot sell more stock than they have, an airplane cannot have wings with zero thickness, and the number of hours you spend per week on your homework cannot exceed 168. We include constraints in such problems to prevent the optimization algorithm from suggesting an infeasible solution.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cbbcd160d441e8566a05c473c5ef9f1bc2e564b44a5949cb6c3f187941ec3a1f.jpg)

$$
\underset{x} {\text{minimize}} \quad f (x)
$$

$$
\text{subject} \quad x \in [ a, b ]
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e1037bd4d260ec47bd106e6b951a7c37d3e90880031762c54732190b06eef206.jpg)  
Figure 10.1. A simple optimization problem constrained by upper and lower bounds.  
Figure 10.2. Bracketing constraints force the solution to lie within a hyperrectangle.

Applying constraints to a problem can affect the solution, but this need not be the case as shown in figure 10.3.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6ca6a3baff5403108d668a6aee5120867289d244868f1e8c054c01818699ec35.jpg)  
Unconstrained

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7ced0aa4871289dc819021e5e752a15cfd1d81833723da160f37f5e26214a636.jpg)  
Constrained, Same Solution

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e6d8fcaca40497450e6fa8425ec946348ce69e887fc1e6440519f8523d21b97b.jpg)  
Constrained, New Solution  
Figure 10.3. Constraints can change the solution to a problem, but do not have to.

# 10.2 Constraint Types

This chapter focuses on representing  $\mathcal{X}$  using two types of constraints:

1. equality constraints,  $h(\mathbf{x}) = 0$  
2. inequality constraints,  $g(\mathbf{x}) \leq 0$

We have  $g$  representing a less-than inequality constraint. Greater-than inequality constraints can be translated into less-than inequality constraints by introducing a negative sign.

Any optimization problem can be rewritten using these constraints:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathbf {x})
$$

$$
\text{subject} \quad h_{i} (\mathbf {x}) = 0 \quad \text{for} i \text{in} 1: \ell \tag {10.2}
$$

$$
g_{j} (\mathbf {x}) \leq 0 \quad \mathrm{for} j \mathrm{in} 1: m
$$

Of course, we can convert an arbitrary set-membership constraint  $x \in \mathcal{X}$  into an equality constraint:

$$
h (\mathbf {x}) = (\mathbf {x} \notin \mathcal {X}) \tag {10.3}
$$

where Boolean expressions in parentheses evaluate to 0 if false and 1 if true.

We often use equality and inequality functions  $(h(\mathbf{x}) = 0, g(\mathbf{x}) \leq 0)$  to define constraints rather than set membership because the functions can provide information about how far a given point is from being feasible. This information helps drive solution methods toward feasibility.

Equality constraints are sometimes decomposed into two inequality constraints:

$$
h (\mathbf {x}) = 0 \quad \Longleftrightarrow \quad \left\{ \begin{array}{l} h (\mathbf {x}) \leq 0 \\ h (\mathbf {x}) \geq 0 \end{array} \right. \tag {10.4}
$$

However, sometimes we want to handle equality constraints separately, as we will discuss later in this chapter.

There are many other categories of constraints. For example, we may have constraints that require that some of the design variables be integers, as will be introduced in chapter 22. We may also have constraints that require that the design fall within a cone, as shown in figure 10.4. In some applications, our design space corresponds to the set of positive semidefinite matrices, which is a special type of cone that is the subject of semidefinite programming. Such problems can be solved using variations of the same techniques presented in this chapter, such as interior point methods.

# 10.3 Transformations to Remove Constraints

In some cases, it may be possible to transform a problem so that constraints can be removed. For example, an interval constraint  $a \leq x \leq b$  can be removed by passing  $x$  through a transform (figure 10.5):

$$
x = t_{a, b} (\hat {x}) = \frac{b + a}{2} + \frac{b - a}{2} \left(\frac{2 \hat {x}}{1 + \hat {x}^{2}}\right) \tag {10.5}
$$

Example 10.1 demonstrates this process.

Some equality constraints can be used to solve for  $x_{n}$  given  $x_{1},\ldots ,x_{n - 1}$ . In other words, if we know the first  $n - 1$  components of  $\mathbf{x}$ , we can use the constraint equation to obtain  $x_{n}$ . In such cases, the optimization problem can be reformulated over  $x_{1},\ldots ,x_{n - 1}$  instead, removing the constraint and removing one design variable. Example 10.2 demonstrates this process.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/af3fed8d3d5ca827591d7cd26bc1bcc05d9bf9406860d0e5bc8fe9ffda3510bc.jpg)  
Figure 10.4. A set  $\mathcal{C}$  is a cone if  $\theta \mathbf{x} \in \mathcal{C}$  for every  $\mathbf{x} \in \mathcal{C}$  and  $\theta \geq 0$ . Convex cones are of particular interest, which are cones where the line segment between any two points in the cone is entirely in the cone. A cone constraint is written  $\mathbf{x} \preceq_{\mathcal{C}} \mathbf{y}$ . Such a constraint is equivalent to  $\mathbf{y} - \mathbf{x} \in \mathcal{C}$ . These constraints are also called generalized inequality constraints because they generalize elementwise inequality constraints of the form  $\mathbf{x} \leq \mathbf{y}$ , which are equivalent to  $\mathbf{y} - \mathbf{x} \in \mathbb{R}_+^n$  where  $\mathbb{R}_+^n$  is the set of vectors with nonnegative components. S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bcc9701f623722922206018537851ccfee51a448515dcf532f17bf10e2663cb8.jpg)  
This and other transforms are discussed by S. K. Park, "A Transformation Method for Constrained-Function Minimization," National Aeronautics and Space Administration, Technical Note TN D-7983, 1975.  
Figure 10.5. This transform ensures that  $x$  is between  $a$  and  $b$ .

Consider the optimization problem

$$
\underset{x} {\text{minimize}} \quad x \sin (x)
$$

$$
\text{subject} \quad 2 \leq x \leq 6
$$

We can transform the problem to remove the constraints:

$$
\underset{\hat {x}} {\text{minimize}} \quad t_{2, 6} (\hat {x}) \sin (t_{2, 6} (\hat {x}))
$$

$$
\underset{\hat {x}} {\text{minimize}} \left(4 + 2 \left(\frac{2 \hat {x}}{1 + \hat {x}^{2}}\right)\right) \sin \left(4 + 2 \left(\frac{2 \hat {x}}{1 + \hat {x}^{2}}\right)\right)
$$

We can use the optimization method of our choice to solve the unconstrained problem. In doing so, we find two minima:  $\hat{x} \approx 0.242$  and  $\hat{x} \approx 4.139$ , both of which have a function value of approximately  $-4.814$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/aedc2ccda8b32529a3de52ee86db7537107779a177da384b3fd0225d3689f01b.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bdd838381f4b8cfc345e8ca02a27737db4fdc79334c58285a1a5de5c456e6437.jpg)

The solution for the original problem is obtained by passing  $\hat{x}$  through the transform. Both values of  $\hat{x}$  produce  $x = t_{2,6}(\hat{x}) \approx 4.914$ .

Example 10.1. Removing bounds constraints using a transform on the input variable.

Consider the constraint:

$$
h (\mathbf {x}) = c_{1} x_{1} + c_{2} x_{2} + \dots + c_{n} x_{n} = 0
$$

We can solve for  $x_{n}$  using the first  $n - 1$  variables:

$$
x_{n} = \frac{1}{c_{n}} (- c_{1} x_{1} - c_{2} x_{2} - \dots - c_{n - 1} x_{n - 1})
$$

We can transform

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathbf {x})
$$

$$
\text{subject} \quad h (\mathbf {x}) = 0
$$

into

$$
\underset{x_{1}, \dots , x_{n - 1}} {\text{minimize}} f \left(\left[ x_{1}, \dots , x_{n - 1}, \frac{1}{c_{n}} \left(- c_{1} x_{1} - c_{2} x_{2} - \dots - c_{n - 1} x_{n - 1}\right) \right]\right)
$$

Example 10.2. Removing an equality constraint and design variable through a transformation.

# 10.4 Removing Affine Equality Constraints

Many problems have affine equality constraints in the form  $\mathbf{A}\mathbf{x} = \mathbf{b}$  with  $\mathbf{A} \in \mathbb{R}^{m \times n}$ . If  $\mathbf{A}$  has rank  $m = n$ , then  $\mathbf{A}\mathbf{x} = \mathbf{b}$  has a unique solution. When  $\mathbf{A}$  has rank  $m < n$ , we can reformulate the problem with fewer design variables and eliminate the affine constraint. To do this, we begin by computing an LQ decomposition of  $\mathbf{A}$ :

$$
\mathbf {A} = \left[ \begin{array}{l l} \mathbf {L} & \mathbf {0} \end{array} \right] \mathbf {Q} \tag {10.6}
$$

where  $\mathbf{L} \in \mathbb{R}^{m \times m}$  is lower-triangular and  $\mathbf{Q} \in \mathbb{R}^{n \times n}$  is orthogonal. Our constraint becomes:

$$
\left[ \begin{array}{l l} \mathbf {L} & \mathbf {0} \end{array} \right] \mathbf {Q x} = \mathbf {b} \tag {10.7}
$$

We then apply a change of variables:

$$
\mathbf {Q} \mathbf {x} = \mathbf {y} = \left[ \begin{array}{c} \mathbf {y}_{1: m} \\ \mathbf {y}_{(m + 1: n)} \end{array} \right] \tag {10.8}
$$

and find that our constraint is satisfied<sup>3</sup> when  $\mathbf{y}_{1:m}^{*} = \mathbf{L}^{-1}\mathbf{b}$ . The remaining variables in  $\mathbf{y}_{(m+1:n)}$  can take on any values.

See appendix C.7.3 for a review.

3 We can efficiently solve for  $\mathbf{y}_{1:m}^{*}$  using backward substitution.

We then define a new problem with  $n - m$  design variables by replacing all instances of  $x$  in our original problem with:4

$$
\mathbf {x} = \mathbf {Q}^{\top} \mathbf {y} = \mathbf {Q}^{\top} \left[ \begin{array}{l} \mathbf {y}_{1: m}^{*} \\ \mathbf {y}_{(m + 1: n)} \end{array} \right] \tag {10.9}
$$

and remove the affine constraint. Once solved for  $\mathbf{y}_{(m + 1:n)}^*$ , we can recover  $\mathbf{x}^*$  with equation (10.8).

# 10.5 Lagrange Multipliers

The method of Lagrange multipliers is used to optimize a function subject to equality constraints. Consider an optimization problem with a single equality constraint:

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \tag {10.10}
$$

$$
\text{subject} \quad h (\mathbf {x}) = 0
$$

where  $f$  and  $h$  have continuous partial derivatives. Example 10.3 discusses such a problem.

If a point  $\mathbf{x}^*$  minimizes  $f$  along the contour  $h(\mathbf{x}) = 0$ , then the directional derivative of  $f$  at  $\mathbf{x}^*$  along  $h(\mathbf{x}) = 0$  must be zero. That is, small shifts of  $\mathbf{x}^*$  along  $h(\mathbf{x}) = 0$  cannot result in an improvement. The method of Lagrange multipliers is used to compute where a contour line of  $f$  is aligned with the contour line of  $h(\mathbf{x}) = 0$ .

Since the gradient of a function at a point is perpendicular to the contour line of that function through that point, we know the gradient of  $h$  will be perpendicular to the contour line  $h(\mathbf{x}) = 0$ . Hence, we need to find where the gradient of  $f$  and the gradient of  $h$  are aligned.

Two vectors are aligned if one is a scalar multiple of the other, so we seek the best  $\mathbf{x}$  such that the constraint

$$
h (\mathbf {x}) = 0 \tag {10.11}
$$

is satisfied and the gradients are aligned with

$$
\nabla f (\mathbf {x}) = \lambda \nabla h (\mathbf {x}) \tag {10.12}
$$

for some Lagrange multiplier  $\lambda$ . We need the scalar  $\lambda$  because the magnitudes of the gradients may not be the same, and they may have opposite signs. When  $\nabla f$  is zero, the Lagrange multiplier  $\lambda$  equals zero, irrespective of  $\nabla h$ .

4 The inverse of an orthogonal matrix is equal to its transpose.

5 Named for Italian mathematician, physicist, and astronomer Joseph-Louis Lagrange (1736-1813).

Consider the minimization problem:

$$
\underset{x} {\text{minimize}} \quad - \exp \left(- \left(x_{1} x_{2} - \frac{3}{2}\right) ^{2} - \left(x_{2} - \frac{3}{2}\right) ^{2}\right)
$$

$$
\text{subject} x_{1} - x_{2}^{2} = 0
$$

We substitute the constraint  $x_{1} = x_{2}^{2}$  into the objective function to obtain an unconstrained objective:

$$
f_{\mathrm{unc}} = - \exp \left(- \left(x_{2}^{3} - \frac{3}{2}\right) ^{2} - \left(x_{2} - \frac{3}{2}\right) ^{2}\right)
$$

whose derivative is:

$$
\frac{\partial}{\partial x_{2}} f_{\mathrm{unc}} = 6 \exp \left(- \left(x_{2}^{3} - \frac{3}{2}\right) ^{2} - \left(x_{2} - \frac{3}{2}\right) ^{2}\right) \left(x_{2}^{5} - \frac{3}{2} x_{2}^{2} + \frac{1}{3} x_{2} - \frac{1}{2}\right)
$$

Setting the derivative to zero and solving for  $x_{2}$  yields  $x_{2} \approx 1.165$ . The solution to the original optimization problem is thus  $\mathbf{x}^{*} \approx [1.358, 1.165]$ . The optimum lies where the contour line of  $f$  is aligned with  $h$ .

The contour lines of  $f$  are lines of constant  $f$ . If a contour line of  $f$  is tangent to that of  $h$ , then the directional derivative of  $f$  at that point, along the direction of the contour  $h(\mathbf{x}) = 0$ , must be zero.

Example 10.3. A motivating example of the method of Lagrange multipliers.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e059d832450f809903222d509ba6152b5e29cc9a7338805b8c0e026dcd7293d7.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0ab3b208ac6cfb932054431e130ca9dafa6e841d44dd0506fffa321deed2fcba.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5a158c8829cb7219cf5e21c42afa43ab9422b4bbfe47600860f53935794f7997.jpg)

When solving constrained optimization problems, we often use what is called the Lagrangian. It is a function of the design variables and the multiplier:

$$
\mathcal {L} (\mathbf {x}, \lambda) = f (\mathbf {x}) - \lambda h (\mathbf {x}) \tag {10.13}
$$

Solving  $\nabla \mathcal{L}(\mathbf{x},\lambda) = \mathbf{0}$  solves equations (10.11) and (10.12). Setting  $\nabla_{\mathbf{x}}\mathcal{L} = \mathbf{0}$  gives us the condition  $\nabla f = \lambda \nabla h$ , and  $\nabla_{\lambda}\mathcal{L} = 0$  gives us  $h(\mathbf{x}) = 0$ . Any solution is considered a critical point. Critical points can be local minima, global minima, or saddle points. Example 10.4 demonstrates this approach.

We can use the method of Lagrange multipliers to solve the problem in example 10.3. We form the Lagrangian

$$
\mathcal {L} \left(x_{1}, x_{2}, \lambda\right) = - \exp \left(- \left(x_{1} x_{2} - \frac{3}{2}\right) ^{2} - \left(x_{2} - \frac{3}{2}\right) ^{2}\right) - \lambda \left(x_{1} - x_{2}^{2}\right)
$$

and compute the gradient

$$
\begin{array}{l} \frac{\partial \mathcal {L}}{\partial x_{1}} = 2 x_{2} f (\mathbf {x}) \left(\frac{3}{2} - x_{1} x_{2}\right) - \lambda \\ \frac{\partial \mathcal {L}}{\partial x_{2}} = 2 \lambda x_{2} + f (\mathbf {x}) \left(- 2 x_{1} \left(x_{1} x_{2} - \frac{3}{2}\right) - 2 \left(x_{2} - \frac{3}{2}\right)\right) \\ \frac{\partial \mathcal {L}}{\partial \lambda} = x_{2}^{2} - x_{1} \\ \end{array}
$$

Setting these derivatives to zero and solving yields  $x_{1} \approx 1.358$ ,  $x_{2} \approx 1.165$  and  $\lambda \approx 0.170$ .

The method of Lagrange multipliers can be extended to multiple equality constraints. Here, rather than a single directional derivative being zero, the directional derivative of  $f$  along every constraint contour  $h_i(\mathbf{x}) = 0$  must be zero.

If we have a point  $\mathbf{x}$  that satisfies a single constraint  $h(\mathbf{x}) = 0$ , we are only allowed to improve  $f$  by traversing in a direction perpendicular to  $\nabla h(\mathbf{x})$ . For multiple constraints, we can only move a point  $\mathbf{x}$  that satisfies all constraints in a direction perpendicular to all gradients  $\nabla h_i(\mathbf{x})$ . For  $\mathbf{x}$  to be a critical point,  $f$  should not be locally improvable, which means that  $\nabla f(\mathbf{x})$  must not be perpendicular to all gradients  $\nabla h_i(\mathbf{x})$ . In other words,  $\nabla f(\mathbf{x})$  must be a linear combination of the

The method of Lagrange multipliers gives us a first-order necessary condition to test for optimality. We will extend this method to include inequalities.

Example 10.4. Using the method of Lagrange multipliers to solve the problem in example 10.3.

gradients  $\nabla h_{i}(\mathbf{x})$  ..

$$
\nabla f (\mathbf {x}) = \sum_{i} \lambda_{i} \nabla h_{i} (\mathbf {x}) \tag {10.14}
$$

We can define a Lagrangian with  $\ell$  Lagrange multipliers for problems with  $\ell$  equality constraints as follows:

$$
\mathcal {L} (\mathbf {x}, \boldsymbol {\lambda}) = f (\mathbf {x}) - \sum_{i = 1}^{\ell} \lambda_{i} h_{i} (\mathbf {x}) = f (\mathbf {x}) - \boldsymbol {\lambda}^{\top} \mathbf {h} (\mathbf {x}) \tag {10.15}
$$

Critical points that satisfy  $\nabla \mathcal{L} = \mathbf{0}$  will satisfy both equations (10.14) and  $h_i(\mathbf{x}) = 0$  for all  $\ell$  constraints.

# 10.6 Inequality Constraints

Consider a problem with a single inequality constraint:

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \tag {10.16}
$$

$$
\text{subject} \quad g (\mathbf {x}) \leq 0
$$

A solution may be either on the constraint boundary (figure 10.6) or in the interior of the feasible region (figure 10.7). We discuss these two cases separately.

If the solution lies at the constraint boundary where  $g(\mathbf{x}) \leq 0$ , then we know that the Lagrange condition holds

$$
\nabla f + \mu \nabla g = \mathbf {0} \tag {10.17}
$$

For inequality constraints, we use  $\mu$  for the Lagrange multiplier. In contrast with equality constraints, we know that  $\nabla g$  points in the opposite direction of  $\nabla f$ ; otherwise, we can move further into the feasible region and also decrease the objective. Hence,  $\mu \geq 0$  if the solution lies on the constraint boundary. When this occurs, the constraint is considered active.

If the solution to the problem does not lie at the constraint boundary, then the constraint is inactive. As with unconstrained optimization, the  $\nabla f$  will be zero at a solution. In this case, equation (10.17) will hold by setting  $\mu$  to zero.

We could optimize a problem with an inequality constraint by introducing an infinite step penalty for infeasible points:7

$$
f_{\infty \text{-step}} (\mathbf {x}) = f (\mathbf {x}) + \infty (g (\mathbf {x}) > 0) \tag {10.18}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c1168ae5d3f4a8c9319c6f6c1d5775ff142b46316232a49461c88f86ad79401a.jpg)  
Figure 10.6. An active inequality constraint. The corresponding contour line is shown in red.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/54a215f0bbcaec88e0b94e7292f460f57c50319614d03629049b03d284e43f89.jpg)  
Figure 10.7. An inactive inequality constraint.

The function  $\infty (\cdot)$  outputs  $\infty$  if the input is true and zero otherwise.

Unfortunately,  $f_{\infty \text{-step}}$  is inconvenient to optimize. It is discontinuous and non-differentiable. Search routines obtain no directional information to steer themselves toward feasibility. We can instead use a linear penalty  $\mu g(\mathbf{x})$ , which forms a lower bound on  $\infty (g(\mathbf{x}) > 0)$  and penalizes the objective toward feasibility when  $\mu > 0$ . This linear penalty is shown in figure 10.8.

We can use this linear penalty to construct what is called the generalized Lagrangian for inequality constraints

$$
\mathcal {L} (\mathbf {x}, \mu) = f (\mathbf {x}) + \mu g (\mathbf {x}) \tag {10.19}
$$

We can recover  $f_{\infty - \mathrm{step}}$  by maximizing with respect to  $\mu$

$$
f_{\infty - \text{step}} (\mathbf {x}) = \underset{\mu \geq 0} {\text{maximize}} \mathcal {L} (\mathbf {x}, \mu) \tag {10.20}
$$

For any infeasible  $\mathbf{x}$  we get infinity and for any feasible  $\mathbf{x}$  we get  $f(\mathbf{x})$ .

The new optimization problem is thus

$$
\underset{\mathbf {x}} {\text{minimize}} \underset{\mu \geq 0} {\text{maximize}} \mathcal {L} (\mathbf {x}, \mu) \tag {10.21}
$$

This reformulation is known as the primal problem. Optimizing the primal problem requires finding critical points  $\mathbf{x}^*$  such that:

1.  $g(\mathbf{x}^{*})\leq 0$

The point is feasible.

2.  $\mu \geq 0$

The penalty must point in the right direction. This requirement is sometimes called dual feasibility. In this context,  $\mu$  is referred to as a dual variable in the optimization. Further discussion of the notion of duality is contained in the next chapter.

3.  $\mu g(\mathbf{x}^{*}) = 0$

A feasible point on the boundary will have  $g(\mathbf{x}) = 0$ , whereas a feasible point with  $g(\mathbf{x}) < 0$  will have  $\mu = 0$  to recover  $f(\mathbf{x}^*)$  from the Lagrangian.

4.  $\nabla f(\mathbf{x}^{*}) + \mu \nabla g(\mathbf{x}^{*}) = \mathbf{0}$

When the constraint is active, we require that the contour lines of  $f$  and  $g$  be aligned, which is equivalent to saying that their gradients be aligned. When the constraint is inactive, our optimum will have  $\nabla f(\mathbf{x}^*) = \mathbf{0}$  and  $\mu = 0$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6cc379cd6ea22d1fc6bce46995946d84106f0516955b2fb1aa2c1571487c586e.jpg)  
Figure 10.8. The linear function  $\mu g(\mathbf{x})$  is a lower bound to the infinite step penalty when  $\mu \geq 0$ .

$$
\begin{array}{l} - \infty (g (\mathbf {x}) > 0) \\ - \mu g (\mathbf {x}) \\ \end{array}
$$

These four requirements can be generalized to optimization problems with any number of equality and inequality constraints:8

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathbf {x})
$$

$$
\text{subject} \quad \mathbf {g} (\mathbf {x}) \leq \mathbf {0} \tag {10.22}
$$

$$
\mathbf {h} (\mathbf {x}) = \mathbf {0}
$$

where each component of  $\mathbf{g}$  is an inequality constraint and each component of  $\mathbf{h}$  is an equality constraint. The four conditions are called the KKT conditions. $^9$

1. Feasibility: The constraints are all satisfied:

$$
\mathbf {g} \left(\mathbf {x}^{*}\right) \leq \mathbf {0} \tag {10.23}
$$

$$
\mathbf {h} \left(\mathbf {x}^{*}\right) = \mathbf {0} \tag {10.24}
$$

2. Dual feasibility: Penalties are toward feasibility:

$$
\mu \geq 0 \tag {10.25}
$$

3. Complementary slackness: For each constraint, either  $g_{i}(\mathbf{x}^{*})$  is zero or the associated Lagrange multiplier  $\mu_{i}$  is zero: $^{10}$

$$
\boldsymbol {\mu} \odot \mathbf {g} = \mathbf {0} \tag {10.26}
$$

A constraint or Lagrange multiplier is considered tight if it is equal to zero and slack if it is not.

4. Stationarity: The objective function contour is tangent to each active constraint: $^{11}$

$$
\nabla f \left(\mathbf {x}^{*}\right) + \sum_{i} \mu_{i} \nabla g_{i} \left(\mathbf {x}^{*}\right) + \sum_{j} \lambda_{j} \nabla h_{j} \left(\mathbf {x}^{*}\right) = \mathbf {0} \tag {10.27}
$$

These four conditions are first-order necessary conditions for optimality for problems with smooth constraints. Just as with the conditions for unconstrained optimization, we must verify that critical points are actually minima.

8 If  $\mathbf{u}$  and  $\mathbf{v}$  are vectors of the same length, then we say  $\mathbf{u} \leq \mathbf{v}$  when  $u_{i} \leq v_{i}$  for all  $i$ . We define  $\geq, <$ , and  $>$  similarly for vectors.

9 Named after American mathematician Harold W. Kuhn (1925-2014) and Canadian mathematician Albert W. Tucker (1905-1995) who published the conditions in 1951. It was later discovered that American mathematician William Karush (1917-1997) studied these conditions in an unpublished master's thesis in 1939. A historical prospective is provided by T. H. Kjeldsen, "A Contextualized Historical Analysis of the Kuhn-Tucker Theorem in Nonlinear Programming: The Impact of World War II," Historia Mathematica, vol. 27, no. 4, pp. 331-361, 2000.

10 The operation  $\mathbf{a} \odot \mathbf{b}$  is the element-wise product between vectors  $\mathbf{a}$  and  $\mathbf{b}$ .

11 Since the sign of  $\lambda_{j}$  is not restricted, we can reverse the sign for the equality constraints from the method of Lagrange multipliers.

# 10.7 Slack Variables

A slack variable is a design variable introduced to an optimization problem that converts an inequality constraint into an equality constraint. We can make the following transformation by introducing a slack variable  $s$ :

$$
\begin{array}{l l} \underset{\mathbf {x}} {\text{minimize}} & f (\mathbf {x}) \\ \text{subjectto} & g (\mathbf {x}) \leq 0 \end{array} \quad \Rightarrow \quad \begin{array}{l l} \underset{\mathbf {x}, s} {\text{minimize}} & f (\mathbf {x}) \\ \text{subjectto} & g (\mathbf {x}) + s = 0 \\ & s \geq 0 \end{array} \tag {10.28}
$$

The slack variable is constrained to be non-negative. When solved, the value of  $s^*$  is the amount by which the original inequality constraint function  $g(\mathbf{x})$  can increase and remain feasible. The value of  $s^*$  thus represents the available slack in the constraint. If  $s^* = 0$ , then the original inequality constraint is active, and if  $s^* > 0$ , then the original inequality constraint is inactive. Introducing a slack variable increases the number of design variables, but the added problem complexity can sometimes lead to structure exploitable by optimization algorithms.[12]

12 We will discuss such algorithms in chapters 12 and 14.

# 10.8 Penalty Methods

We can use penalty methods to convert constrained optimization problems into unconstrained optimization problems by adding penalty terms to the objective function, allowing us to use the methods developed in previous chapters.

Consider a general optimization problem:

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x})
$$

$$
\text{subject} \quad \mathbf {g} (\mathbf {x}) \leq \mathbf {0} \tag {10.29}
$$

$$
\mathbf {h} (\mathbf {x}) = \mathbf {0}
$$

A simple penalty method counts the violated constraints:

$$
p_{\text{count}} (\mathbf {x}) = \sum_{i} \left(g_{i} (\mathbf {x}) > 0\right) + \sum_{j} \left(h_{j} (\mathbf {x}) \neq 0\right) \tag {10.30}
$$

which results in the unconstrained problem that penalizes infeasibility

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) + \rho \cdot p_{\text{count}} (\mathbf {x}) \tag {10.31}
$$

where  $\rho > 0$  adjusts the penalty magnitude. Figure 10.9 shows an example.

Penalty methods start with an initial point  $\mathbf{x}$  and a small value for  $\rho$ . The unconstrained optimization problem equation (10.31) is then solved. The resulting design point is then used as the starting point for another optimization with an increased penalty. We continue with this procedure until the resulting point is feasible, or a maximum number of iterations has been reached. Algorithm 10.1 provides an implementation.

```julia
function penalty_method(f, p, x, k_max;  $\rho = 1.0$ ,  $\nu = 2.0$ )  
for k in 1: k_max  
    x = minimize(x  $\rightarrow$  f(x) +  $\rho * p(x)$ , x)  
     $\rho *= \nu$   
    if p(x) == 0  
        return x  
    end  
end  
return x  
end
```

This penalty will preserve the problem solution for large values of  $\rho$ , but it introduces a sharp discontinuity. Points not inside the feasible set lack gradient information to guide the search toward feasibility.

We can use quadratic penalties to produce a smooth objective function (figure 10.10):

$$
p_{\text{quadratic}} (\mathbf {x}) = \sum_{i} \max \left(g_{i} (\mathbf {x}), 0\right) ^{2} + \sum_{j} h_{j} (\mathbf {x}) ^{2} \tag {10.32}
$$

Quadratic penalties close to the constraint boundary are very small and may require  $\rho$  to approach infinity before the solution ceases to violate the constraints. It is common to add a small constant to  $g_{i}(\mathbf{x})$  in the penalty function above to push the search toward the interior of the feasible region.

It is also possible to mix a count and a quadratic penalty function (figure 10.11):

$$
p_{\text{mixed}} (\mathbf {x}) = \rho_{1} p_{\text{count}} (\mathbf {x}) + \rho_{2} p_{\text{quadratic}} (\mathbf {x}) \tag {10.33}
$$

Such a penalty mixture both provides a clear boundary between the feasible region and the infeasible region and can provide gradient information to the solver.

Figure 10.12 shows the progress of the penalty function as  $\rho$  is increased. Quadratic penalty functions cannot ensure feasibility as discussed in example 10.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3d3797dab29312313ae9ce104529f9b687902afcb94b613428bc33cd5d4339b3.jpg)  
Figure 10.9. The original and countenized objective functions for minimizing  $f$  subject to  $x \in [a, b]$ .

$$
\begin{array}{l} - f (x) \\ - f (x) + \rho p_{\text{count}} (x) \\ \end{array}
$$

Algorithm 10.1. The penalty method for objective function  $f$ , penalty function  $p$ , initial point  $x$ , number of iterations  $k_{\text{max}}$ , initial penalty  $\rho > 0$ , and penalty multiplier  $\gamma > 1$ . The method minimize should be replaced with a suitable unconstrained minimization method.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/69a296df3f2430acc25fff9ef578e216ad2183f0ac144cb17bf4444a12dfb459.jpg)

$$
\begin{array}{l} - - f (x) \\ - f (x) + \rho p_{\text{quadratic}} (x) \\ \end{array}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/097c0addd8ab4c944ae81fc4aeeaacdf3acb09af45479f1d37cf36ed61ce14d0.jpg)  
Figure 10.10. Using a quadratic penalty function for minimizing  $f$  subject to  $x \in [a, b]$ .  
Figure 10.11. Using both a mixed penalty function for minimizing  $f$  subject to  $x \in [a, b]$ .

$$
\begin{array}{l} - - f (x) \\ - f (x) + p_{\text{mixed}} (x) \\ \end{array}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/85ef293a7bb84f00adf1efcbca261b769082ad985c8f46cc8bc706d143c8a9ab.jpg)  
Figure 10.12. The penalty method applied to the flower function, appendix B.4, and the circular constraint  $x_1^2 + x_2^2 \geq 2$ .

Consider the problem

$$
\underset{x} {\text{minimize}} \quad x
$$

$$
\text{subject} \quad x \geq 5
$$

using a quadratic penalty function.

The unconstrained objective function is

$$
f (x) = x + \rho \max (5 - x, 0) ^{2}
$$

The minimum of the unconstrained objective function is

$$
x^{*} = 5 - \frac{1}{2 \rho}
$$

While the minimum of the constrained optimization problem is clearly  $x = 5$ , the minimum of the penalized optimization problem merely approaches  $x = 5$ , requiring an infinite penalty to achieve feasibility.

Example 10.5. An example showing how quadratic penalties cannot ensure feasibility.

# 10.9 Method of Multipliers

The method of multipliers, also known as the augmented Lagrange method, combines the quadratic penalty method with the linear penalties associated with Lagrange multipliers. $^{13}$  Unlike the penalty method, where  $\rho$  must sometimes approach infinity before a feasible solution is found, the method of multipliers will work with smaller values of  $\rho$ . It uses both a quadratic and a linear penalty for each constraint.

For an optimization problem with equality constraints  $\mathbf{h}(\mathbf{x}) = \mathbf{0}$ , the penalty function is<sup>14</sup>

$$
p_{\text{Lagrange}} (\mathbf {x}) = \frac{1}{2} \rho \sum_{i} \left(h_{i} (\mathbf {x})\right) ^{2} + \sum_{i} \lambda_{i} h_{i} (\mathbf {x}) \tag {10.34}
$$

In addition to increasing  $\rho$  with each iteration, the linear penalty vector is updated according to<sup>15</sup>

$$
\boldsymbol {\lambda}^{(k + 1)} = \boldsymbol {\lambda}^{(k)} + \rho \mathbf {h} (\mathbf {x}^{(k + 1)}) \tag {10.35}
$$

Algorithm 10.2 provides an implementation.

```julia
function method_ofmultipliers(f,h,x,k_max;  $\rho = 1.0$ $\gamma = 2.0)$ $\lambda =$  zeros(length(h(x)))   
for k in 1:k_max  $\mathsf{p}(\mathbf{x}) = \mathsf{\rho} / 2*\mathsf{sum}(\mathsf{h}(\mathsf{x}).\hat{\mathsf{A}} 2) + \lambda \cdot \mathsf{h}(\mathsf{x})$ $\texttt{x} =$  minimize  $(x\to f(x) + p(x),x)$ $\lambda += \rho *\mathsf{h}(\mathsf{x})$ $\rho * = \gamma$    
end   
return x   
end
```

# 10.10 Interior Point Methods

Interior point methods (algorithm 10.3), sometimes referred to as barrier methods, are optimization methods that ensure that the search points always remain feasible. $^{16}$  Interior point methods that are stopped early due to time or processing constraints can produce nearly optimal, though feasible, design points. They use a barrier function that approaches infinity as one approaches a constraint boundary. This barrier function  $p_{\text{barrier}}(\mathbf{x})$  must satisfy several properties:

This method dates back to at least the 1960s. M. R. Hestenes, "Multiplier and Gradient Methods," Journal of Optimization Theory and Applications, vol. 4, no. 5, pp. 303-320, 1969.  
14 This method can be generalized to inequality constraints. D.P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods. Athena Scientific, 1996.  
Exercise 10.14 provides a justification for this update.

Algorithm 10.2. The method of multipliers for objective function  $f$ , equality constraint function  $h$ , initial point  $x$ , number of iterations  $k_{-}$ max, initial penalty scalar  $\rho > 0$ , and penalty multiplier  $\gamma > 1$ . The function minimize can be any minimization method.

16 A. S. Nemirovski and M. J. Todd, "Interior-Point Methods for Optimization," Acta Numerica, vol. 17, pp. 191-234, 2008.

1.  $p_{\mathrm{barrier}}(\mathbf{x})$  is continuous  
2.  $p_{\mathrm{barrier}}(\mathbf{x})$  is nonnegative  $(p_{\mathrm{barrier}}(\mathbf{x})\geq 0)$  in the feasible region  
3.  $p_{\mathrm{barrier}}(\mathbf{x})$  approaches infinity as  $\mathbf{x}$  approaches any constraint boundary

Some examples of barrier functions include the inverse barrier:

$$
p_{\text{barrier}} (\mathbf {x}) = - \sum_{i} \frac{1}{g_{i} (\mathbf {x})} \tag {10.36}
$$

and the log barrier:

$$
p_{\text{barrier}} (\mathbf {x}) = - \sum_{i} \left\{ \begin{array}{l l} \log (- g_{i} (\mathbf {x})) & \text{if } g_{i} (\mathbf {x}) \geq - 1 \\ 0 & \text{otherwise} \end{array} \right. \tag {10.37}
$$

A problem with inequality constraints can be transformed into an unconstrained optimization problem

$$
\underset{\mathbf {x}} {\operatorname{minimize}} f (\mathbf {x}) + \frac{1}{\rho} p_{\text{barrier}} (\mathbf {x}) \tag {10.38}
$$

When  $\rho$  is increased, the penalty for approaching the boundary decreases (figure 10.13).

Special care must be taken such that line searches do not leave the feasible region. Line searches  $f(\mathbf{x} + \alpha \mathbf{d})$  are constrained to the interval  $\alpha = [0, \alpha_u]$ , where  $\alpha_u$  is the step to the nearest boundary. In practice,  $\alpha_u$  is chosen such that  $\mathbf{x} + \alpha \mathbf{d}$  is just inside the boundary to avoid the boundary singularity.

Like the penalty method, the interior point method begins with a low value for  $\rho$  and slowly increases it until convergence. The interior point method is typically terminated when the difference between subsequent points is less than a certain threshold. Figure 10.14 shows the effect of incrementally increasing  $\rho$ .

The interior point method requires a strictly feasible point from which to start the search. One method for finding such an interior point is to run the interior point method itself on a related problem with an additional variable  $s$ :

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \quad \Rightarrow \quad \underset{\mathbf {x}, s} {\text{minimize}} s \tag {10.39}
$$

$$
\text{subject} \quad \mathbf {g} (\mathbf {x}) \leq \mathbf {0} \quad \text{subject} \quad \mathbf {g} (\mathbf {x}) \leq s \mathbf {1}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d10e2785e0ff16289db60884b2479dd5735e7798bc4ae6f2549b744de32df755.jpg)  
Figure 10.13. The interior point method with an inverse barrier for minimizing  $f$  subject to  $x \in [a, b]$ .

$$
\begin{array}{l} - - f (x) \\ - f (x) + p_{\text{barrier}} (x) \\ - f (x) + \frac{1}{2} p_{\text{barrier}} (x) \\ - f (x) + \frac{1}{10} p_{\text{barrier}} (x) \\ \end{array}
$$

```matlab
function interior_point_method(f, p, x;  $\rho = 1.0$ ,  $\gamma = 2.0$ ,  $\epsilon = 0.001$ )  
 $\delta = \text{Inf}$   
while  $\delta > \epsilon$ $\mathbf{x}' = \text{minimize}(\mathbf{x} \to \mathbf{f}(\mathbf{x}) + \mathbf{p}(\mathbf{x}) / \rho, \mathbf{x})$ $\delta = \text{norm}(\mathbf{x}' - \mathbf{x})$ $\mathbf{x} = \mathbf{x}'$ $\rho *= \gamma$   
end  
return x  
end
```

Algorithm 10.3. The interior point method for objective function  $f$ , barrier function  $p$ , initial point  $x$ , initial penalty  $\rho > \theta$ , penalty multiplier  $\nu > 1$ , and stopping tolerance  $\epsilon > \theta$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2296d3cc173722fe8fb6c2b9eca3527bcc7133b6f7cc6371d27face69a4581c9.jpg)  
Figure 10.14. The interior point method with the inverse barrier applied to the flower function (appendix B.4) with the constraint  $x_{1}^{2} + x_{2}^{2} \geq 2$ .

This  $s$  serves as an upper bound on all inequality constraints. A feasible pair  $(\mathbf{x}, s)$  with a negative value for  $s$  must have a design  $\mathbf{x}$  that lies in the interior of the feasible set. The barrier method can be used to iterate to such a pair starting from any initial  $\mathbf{x}^{(0)}$  and a sufficiently large value for  $s^{(0)}$  such that  $s^{(0)} > \max_{i} g_{i}(\mathbf{x}^{(0)})$ .

# 10.11 Summary

- Constraints are requirements on the design points that a solution must satisfy.  
- Some constraints can be transformed or substituted into the problem to result in an unconstrained optimization problem.  
- Incorporation of Lagrange multipliers leads to the generalized Lagrangian and the necessary conditions for optimality under constraints.  
- Penalty methods penalize infeasible solutions and often provide gradient information to the optimizer to guide infeasible points toward feasibility.  
- Interior point methods use barrier functions to avoid leaving the feasible set.

# 10.12 Exercises

Exercise 10.1. Solve

$$
\underset{x} {\text{minimize}} \quad x
$$

$$
\text{subject} \quad x \geq 0
$$

using the quadratic penalty method with  $\rho > 0$ . Solve the problem in closed form.

Solution: First reformulate the problem as  $f(x) = x + \rho \max (-x,0)^2$  for which the derivative is

$$
f^{\prime} (x) = \left\{ \begin{array}{l l} 1 + 2 \rho x & \text{if } x <   0 \\ 1 & \text{otherwise} \end{array} \right.
$$

This unconstrained objective function can be solved by setting  $f'(x) = 0$ , which yields the solution  $x^{*} = -\frac{1}{2\rho}$ . Thus, as  $\rho \to \infty$  we have that  $x^{*} \to 0$ .

Exercise 10.2. Solve the problem above using the count penalty method with  $\rho > 1$  and compare it to the quadratic penalty method.

Solution: The problem is reformulated to  $f(x) = x + \rho (x < 0)$ . The unconstrained objective function is unbounded from below when  $\rho$  is finite and  $x$  approaches negative infinity. The correct solution is not found, whereas the quadratic penalty method is able to approach the correct solution.

Exercise 10.3. Suppose that we are solving a constrained problem with the penalty method. We notice that the iterates remain infeasible and you decide to stop the algorithm. What can we do to improve our search?

Solution: You might try to increase the penalty parameter  $\rho$ . It is possible that  $\rho$  is too small and the penalty term is ineffective. In such cases, the iterates may be reaching an infeasible region where the function decreases faster than the penalty terms, causing the method to converge on an infeasible solution.

Exercise 10.4. Consider a simple univariate minimization problem where you minimize a function  $f(x)$  subject to  $x \geq 0$ . Assume that we know that the constraint is active, that is,  $x^{*} = 0$  where  $x^{*}$  is the minimizer. Also, assume that we know that  $f^{\prime}(x^{*}) > 0$ . Show that solving the same problem with the penalty method

$$
f (x) + (\min (x, 0)) ^{2}
$$

yields an infeasible solution with respect to the original problem.

Solution: Let  $x_{p}^{*}$  be the solution to the unconstrained problem. Notice that  $x_{p}^{*}$  cannot be positive. Otherwise the penalty would be  $\min(x_{p}^{*}, 0)^{2} = 0$ , which would imply that  $x_{p}^{*}$  is a solution to the original problem. Now, suppose  $x_{p}^{*} = 0$ . The first-order optimality conditions for unconstrained problems state that  $f'(x_{p}^{*}) = 0$ , again a contradiction. Thus, if a minimizer exists, it must be infeasible.

Below we show the infeasible minimizer when applying this penalty method for  $f(x) = x$ :

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/86e593779b93b2dbe16bc813a106981de57cb49cd86b8ffe64f4c60220917b95.jpg)

Exercise 10.5. What is the advantage of the method of multipliers compared to the quadratic penalty method?

Solution: It does not require a large penalty  $\rho$  to produce a feasible solution.

Exercise 10.6. When would you use the barrier method in place of the penalty method?

Solution: We would use the barrier method when iterates should remain feasible.

Exercise 10.7. Give an example of a smooth optimization problem, such that, for any quadratic penalty parameter  $\rho > 0$ , there exists a starting point  $x^{(1)}$  for which the steepest descent method diverges in an infeasible region.

Solution: Consider the following:

$$
\underset{x} {\text{minimize}} \quad x^{3}
$$

$$
\text{subject} \quad x \geq 0
$$

which is minimized for  $x^{*} = 0$ . Using the quadratic penalty method, we can recast it as

$$
\underset{x} {\operatorname{minimize}} x^{3} + \rho (\min (x, 0)) ^{2}
$$

For any finite  $\rho$ , the function remains unbounded from below as  $x$  becomes infinitely negative. Furthermore, as  $x$  becomes infinitely negative the function becomes infinitely steep. In other words, if we start the steepest descent method too far to the left, we have  $x^3 + \rho x^2 \approx x^3$ , and the penalty would be ineffective, and the steepest descent method will diverge.

Exercise 10.8. If we are using a quadratic penalty function with the steepest descent method, and it fails to find a feasible point even for large values of penalty parameter  $\rho$ , what can we do to improve the search?

Solution: We can set the objective to 0, and then optimize the resulting problem with a quadratic penalty function using steepest descent. Once we find a feasible point, we can then optimize the original problem.

Exercise 10.9. Solve the constrained optimization problem

$$
\underset{x} {\text{minimize}} \quad \sin \left(\frac{4}{x}\right)
$$

$$
\text{subject} \quad x \in [ 1, 10 ]
$$

using both the transform  $x = t_{a,b}(\hat{x})$  introduced in equation (10.5) and a sigmoid transform for constraint bounds  $x \in [a, b]$  defined by:

$$
x = s_{a, b} (\hat {x}) = a + \frac{(b - a)}{1 + e^{- \hat {x}}}
$$

Why is the  $t_{a,b}$  transform better suited for optimizing our problem than the  $s_{a,b}$  transform?

Solution: The problem is minimized at  $x^{*} = 1$ , which is at the constraint boundary. Solving with the  $t_{a,b}$  transform yields the unconstrained objective function:

$$
f_{t} (\hat {x}) = \sin \left(\frac{4}{5.5 + 4.5 \frac{2 \hat {x}}{1 + \hat {x}^{2}}}\right)
$$

which has a single global minimum at  $\hat{x} = -1$ , correctly corresponding to  $x^{*}$ .

The  $s_{a,b}$  transform has an unconstrained objective function:

$$
f_{s} (\hat {x}) = \sin \left(\frac{4}{1 + \frac{9}{1 + e^{- \hat {x}}}}\right)
$$

Unfortunately, the lower-bound on  $x$  is reached only as  $\hat{x}$  approaches minus infinity. The unconstrained optimization problem obtained using the sigmoid transform does not have a solution, and the method fails to properly identify the solution of the original problem.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/202746845656fa8cffc76a14d2ac32c80bc6aa037d3d3cac0a1cad985b2386ba.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d7a67efbb5a222409d62123f534adf4ce7ef2176d5aa36a8faefc5e6c6df6b7a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/18a38e85dc7e7ce30f25b89558a84f167144165b026270d319cc59e2a6bfa5a7.jpg)

Exercise 10.10. Give an example of a quadratic objective function involving two design variables where the addition of a linear constraint results in a different optimum.

Solution: Minimize  $x_1^2 + x_2^2$  subject to  $x_1 \geq 1$ .

Exercise 10.11. Suppose we want to minimize  $x_1^3 + x_2^2 + x_3$  subject to the constraint that  $x_1 + 2x_2 + 3x_3 = 6$ . How might we transform this into an unconstrained problem with the same minimizer?

Solution: We can rearrange the constraint in terms of  $x_{1}$ :

$$
x_{1} = 6 - 2 x_{2} - 3 x_{3}
$$

and substitute the relation into the objective:

$$
\underset{x_{2}, x_{3}} {\text{minimize}} x_{2}^{2} + x_{3} - (2 x_{2} + 3 x_{3} - 6) ^{3}
$$

Exercise 10.12. Suppose we want to minimize  $-x_{1} - 2x_{2}$  subject to the constraints  $ax_{1} + x_{2} \leq 5$  and  $x_{1}, x_{2} \geq 0$ . If  $a$  is a bounded constant, what range of values of  $a$  will result in an infinite number of optimal solutions?

Solution: To have infinitely many solutions, we need the boundary of the feasible region defined by  $ax_{1} + x_{2} = 5$  to be aligned the contour of the objective function  $-x_{1} - 2x_{2}$ . Mathematically, we need  $[a,1] = \lambda [-1, - 2]$ . This can happen only when  $a = 0.5$  and  $\lambda = -0.5$ .

Exercise 10.13. Consider using a penalty method to optimize

$$
\underset{x} {\text{minimize}} \quad 1 - x^{2}
$$

$$
\text{subject} \quad | x | \leq 2
$$

Optimization with the penalty method typically involves running several optimizations with increasing penalty weights. Impatient engineers may wish to optimize once using a very large penalty weight. Explain what issues are encountered for both the count penalty method and the quadratic penalty method.

Solution: The transformed objective function is  $f(x) = 1 - x^{2} + \rho p(x)$ , where  $p$  is either a count penalty or a quadratic penalty:

$$
p_{\text{count}} (x) = (| x | > 2) \quad p_{\text{quadratic}} (x) = \max (| x | - 2, 0) ^{2}
$$

The count penalty method does not provide any gradient information to the optimization process. An optimization algorithm initialized outside of the feasible set will be drawn away from the feasible region because  $1 - x^2$  is minimized by moving infinitely far to the left or right from the origin. The large magnitude of the count penalty is not the primary issue; small penalties can lead to similar problems.

The quadratic penalty method does provide gradient information to the optimization process, guiding searches toward the feasible region. For very large penalties, the quadratic penalty method will produce large gradient values in the infeasible region. In this problem, the partial derivative is:

$$
\frac{\partial f}{\partial x} = - 2 x + \rho \left\{ \begin{array}{l l} 2 (x - 2) & \text{if } x > 2 \\ 2 (x + 2) & \text{if } x <   - 2 \\ 0 & \text{otherwise} \end{array} \right.
$$

For very large values of  $\rho$ , the partial derivative in the infeasible region is also large, which can cause problems for optimization methods. If  $\rho$  is not large, then infeasible points may not be sufficiently penalized, resulting in infeasible solutions.

Exercise 10.14. Write down the primal problem from equation (10.21) with a quadratic penalty function included. Derive an update equation for  $\lambda$  to solve the inner maximization problem for an updated iterate  $\mathbf{x}^{(k + 1)}$ . Use gradient ascent for this update with a step factor of  $\rho$ .

Solution: The primal problem with a quadratic penalty function is

$$
\underset{\mathbf {x}} {\operatorname{minimize}} \underset{\lambda} {\operatorname{maximize}} f (\mathbf {x}) + \frac{1}{2} \rho \mathbf {h} (\mathbf {x}) ^{\top} \mathbf {h} (\mathbf {x}) + \boldsymbol {\lambda}^{\top} \mathbf {h} (\mathbf {x})
$$

The update equation for  $\lambda$  with step factor  $\rho$  is

$$
\boldsymbol {\lambda}^{(k + 1)} = \boldsymbol {\lambda}^{(k)} + \rho \mathbf {h} (\mathbf {x}^{(k + 1)})
$$

Exercise 10.15. Suppose we have an optimization problem with a single squared equality constraint:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathbf {x})
$$

$$
\text{subject} \quad c^{2} (\mathbf {x}) = 0
$$

where  $f$  and  $c$  have continuous partial derivatives. Why can we not determine the Lagrange multiplier  $\lambda$  using the method of Lagrange multipliers in this case?

Solution: The method of Lagrange multipliers enforces the condition:

$$
\nabla f (\mathbf {x}) = \lambda \nabla h (\mathbf {x}) = 2 \lambda c (\mathbf {x}) \nabla c (\mathbf {x})
$$

We cannot determine  $\lambda$  from this equation because the gradient of the constraint is zero for all feasible points  $\mathbf{x}$ . However, we get necessary conditions for  $\mathbf{x}^*$ ; namely,  $\nabla f(\mathbf{x}^*) = 0$  and  $c(\mathbf{x}^*) = 0$ .

Exercise 10.16. How can the method of Lagrange multipliers be adapted to handle the optimization problem in the previous question?

Solution: Instead of using the constraint  $c^2(\mathbf{x}) = 0$ , we can use the equivalent constraint  $c(\mathbf{x}) = 0$ .

Exercise 10.17. Suppose we wish to solve an optimization problem using an interior point method. Could we find an initial feasible point by optimizing the quadratic penalty function?

$$
\underset{\mathbf {x}} {\text{minimize}} p_{\text{quadratic}} (\mathbf {x})
$$

Why might this be a good or a bad idea?

Solution: The quadratic penalty assigns zero penalty to the feasible set and a quadratic penalty outside of it. Hence, solutions to this quadratic penalty optimization problem are all feasible points in the original problem. In practice, optimization algorithms will tend to result in a design on or very close to the boundary of the feasible set when initialized with a point outside of the feasible set, but interior point methods require strictly feasible initial points.

# 11 Duality

The previous chapter presented constrained minimization problems and derived the first-order necessary conditions for optimality, which included requirements on the dual variables that were introduced. This chapter elaborates on the concept of duality, which allows us to transform a constrained minimization problem into an analogous maximization problem.<sup>1</sup> The dual version of a problem provides a lower bound on the original problem. In some cases, duality provides a means of verification that a solution has been found. This chapter discusses a variety of algorithms that operate on the dual problem.

# 11.1 Dual Problem

In the previous chapter, we defined a general constrained optimization problem as an optimization problem with any number of equality and inequality constraints:

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x})
$$

$$
\text{subject} \quad \mathbf {g} (\mathbf {x}) \leq \mathbf {0} \tag {11.1}
$$

$$
\mathbf {h} (\mathbf {x}) = \mathbf {0}
$$

The generalized Lagrangian corresponding to this general constrained minimization problem is

$$
\mathcal {L} (\mathbf {x}, \boldsymbol {\mu}, \lambda) = f (\mathbf {x}) + \sum_{i} \mu_{i} g_{i} (\mathbf {x}) + \sum_{j} \lambda_{j} h_{j} (\mathbf {x}) \tag {11.2}
$$

where  $\mu$  and  $\lambda$  are known as Lagrange multipliers or dual variables.

$^{1}$ Duality is covered in greater depth by S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.

The primal form of the problem is the original optimization problem formulated using the generalized Lagrangian:

$$
\underset{\mathbf {x}} {\text{minimize}} \underset{\mu \geq 0, \lambda} {\text{maximize}} \mathcal {L} (\mathbf {x}, \mu , \lambda) \tag {11.3}
$$

The interior maximization will drive the value to infinity if any constraints are violated, which ensures that the outer minimization will seek a feasible design. The primal problem is identical to the original problem and is just as difficult to optimize.

The dual form of the optimization problem reverses the order of the minimization and maximization in equation (11.3):

$$
\underset{\mu \geq 0, \lambda} {\text{maximize}} \underset{\mathbf {x}} {\text{minimize}} \mathcal {L} (\mathbf {x}, \mu , \lambda) \tag {11.4}
$$

The max-min inequality states that for any function  $f(\mathbf{a}, \mathbf{b})$ :

$$
\underset{\mathbf {a}} {\text{maximizeminimize}} f (\mathbf {a}, \mathbf {b}) \leq \underset{\mathbf {b}} {\text{minimizemaximize}} f (\mathbf {a}, \mathbf {b}) \tag {11.5}
$$

The solution to the dual problem is thus a lower bound to the solution of the primal problem. That is,  $d^{*} \leq p^{*}$ , where  $d^{*}$  is the dual value and  $p^{*}$  is the primal value.

The inner minimization in the dual problem is often folded into a dual function,

$$
\mathcal {D} (\boldsymbol {\mu}, \lambda) = \underset{\mathbf {x}} {\text{minimize}} \mathcal {L} (\mathbf {x}, \boldsymbol {\mu}, \lambda) \tag {11.6}
$$

for notational convenience. The dual function is concave. $^2$  Gradient ascent can be used with this concave function to converge to the global maximum. Optimizing the dual problem is efficient whenever minimizing the Lagrangian with respect to  $\mathbf{x}$  is efficient.

We know that maximize  $\mu \geq 0, \lambda \mathcal{D}(\mu, \lambda) \leq p^*$ . It follows that the dual function is always a lower bound on the primal problem (see example 11.1). For any  $\mu \geq 0$  and any  $\lambda$ , we have  $\mathcal{D}(\mu, \lambda) \leq p^*$ . This property is known as weak duality.

The difference  $p^* - d^*$  between the dual and primal values is called the duality gap. In some cases, the dual problem is guaranteed to have the same optimal value as the original problem, making the duality gap zero. This property is known as strong duality. Strong duality is guaranteed when the objective and constraints are convex and the constraints satisfy Slater's condition.<sup>3</sup> In such cases, duality can provide an alternative approach for optimizing our problem. Example 11.2 demonstrates this approach.

2 For a fixed  $\mathbf{x}$ ,  $\mathcal{L}(\mathbf{x},\boldsymbol{\mu},\lambda)$  is affine in  $\boldsymbol{\mu}$  and  $\lambda$ . Because the minimum of a set of affine functions is concave, the dual function is concave. For a detailed overview, see S. Nash and A. Sofer, Linear and Nonlinear Programming. McGraw-Hill, 1996.

3 Slater's condition and other conditions that guarantee zero duality gap are discussed by S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.

Consider the optimization problem:

$$
\underset{x} {\text{minimize}} \quad \sin (x)
$$

$$
\text{subject} \quad x^{2} \leq 1
$$

The generalized Lagrangian is  $\mathcal{L}(x,\mu) = \sin (x) + \mu (x^2 -1)$ , making the primal problem:

$$
\underset{x} {\text{minimize}} \underset{\mu \geq 0} {\text{maximize}} \sin (x) + \mu (x^{2} - 1)
$$

and the dual problem:

$$
\underset{\mu \geq 0} {\text{maximize}} \underset{x} {\text{minimize}} \sin (x) + \mu (x^{2} - 1)
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ce02330269e7de1e1a9dad5e57e323fcaefc2246ee996edb9591c34478acdb5c.jpg)

The objective function is plotted in black, with the feasible region is indicated in blue. The minimum is at  $x^{*} = -1$  with  $p^{*} \approx -0.841$ . The purple lines are the Lagrangian  $\mathcal{L}(x, \mu)$  for different values of  $\mu$ , each of which has a minimum lower than  $p^{*}$ . By inspection, we can see that strong duality also holds.

Example 11.1. The dual function is a lower bound of the primal problem.

Consider the problem:

$$
\underset{x} {\text{minimize}} \quad x_{1} + x_{2} + x_{1} x_{2}
$$

$$
\text{subject} \quad x_{1}^{2} + x_{2}^{2} = 1
$$

The Lagrangian is  $\mathcal{L}(x_1,x_2,\lambda) = x_1 + x_2 + x_1x_2 + \lambda (x_1^2 +x_2^2 -1)$ . We apply the method of Lagrange multipliers to obtain the necessary optimality conditions:

$$
\frac{\partial \mathcal {L}}{\partial x_{1}} = 1 + x_{2} + 2 \lambda x_{1} = 0
$$

$$
\frac{\partial \mathcal {L}}{\partial x_{2}} = 1 + x_{1} + 2 \lambda x_{2} = 0
$$

$$
\frac{\partial \mathcal {L}}{\partial \lambda} = x_{1}^{2} + x_{2}^{2} - 1 = 0
$$

Solving yields four potential solutions, and thus four critical points:

<table><tr><td>x1</td><td>x2</td><td>λ</td><td>x1+x2+x1x2</td></tr><tr><td>-1</td><td>0</td><td>1/2</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td>1/2</td><td>-1</td></tr><tr><td>√2+1/√2+2</td><td>√2+1/√2+2</td><td>1/2(-1-√2)</td><td>1/2+√2≈1.914</td></tr><tr><td>√2-1/√2-2</td><td>√2-1/√2-2</td><td>1/2(-1+√2)</td><td>1/2-√2≈-0.914</td></tr></table>

We find that the two optimal solutions are  $[-1,0]$  and  $[0, - 1]$

The dual function has the form

$$
\mathcal {D} (\lambda) = \underset{x_{1}, x_{2}} {\text{minimize}} x_{1} + x_{2} + x_{1} x_{2} + \lambda \left(x_{1}^{2} + x_{2}^{2} - 1\right)
$$

The dual function is unbounded below when  $\lambda$  is less than  $1/2$  (consider  $x_1 \to \infty$  and  $x_2 \to -\infty$ ). For  $\lambda > 1/2$ , setting the gradient to  $\mathbf{0}$  and solving yields  $x_2 = -1 - 2\lambda x_1$  and  $x_1 = (2\lambda - 1)/(1 - 4\lambda^2)$ . When  $\lambda = 1/2$ ,  $x_1 = -1 - x_2$  and  $\mathcal{D}(1/2) = -1$ . Substituting these into the dual function yields:

$$
\mathcal {D} (\lambda) = \left\{ \begin{array}{l l} - \lambda - \frac{1}{2 \lambda + 1} & \lambda \geq \frac{1}{2} \\ - \infty & \text{otherwise} \end{array} \right.
$$

The dual problem maximize  $\lambda$ $\mathcal{D}(\lambda)$  is maximized at  $\lambda = 1 / 2$

Example 11.2. An example of Lagrangian duality applied to a problem with an equality constraint. The top figure shows the objective function contour and the constraint with the four critical points marked by scatter points. We have used the first-order necessary conditions, which while not generally sufficient for optimality, are necessary for optimality. The bottom figure shows the dual function.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/396cd5b31ff0f3f564a386fc86867eb30ec8ccc394f9d9f751a9067bbfbf619f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/677447feb5de45fe4423a21c176e3816333e66b1113f5fa42165ab75f0371ac4.jpg)

# 11.2 Primal-Dual Methods

Primal-dual methods build upon the interior point methods introduced in the previous chapter by simultaneously updating the values of the dual variables.4 Such an approach can both speed convergence to a solution and also allows us to measure the duality gap. If the duality gap is ever sufficiently small, we know we can terminate.

As discussed in section 10.10, interior point methods can be used to solve the inequality-constrained optimization problem

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \tag {11.7}
$$

$$
\text{subject} \quad \mathbf {g} (\mathbf {x}) \leq \mathbf {0}
$$

by approximating infinite discontinuities at the constraint boundaries with smooth barriers:

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) + \frac{1}{\rho} p_{\text{barrier}} (\mathbf {x}) \tag {11.8}
$$

These problems are solved repeatedly with increasing values of  $\rho$ , yielding a sequence of designs that converge to a local minimum for the original problem. For the primal-dual method discussed here, we use a logarithmic barrier:

$$
\underset{\mathbf {x}} {\operatorname{minimize}} f (\mathbf {x}) - \frac{1}{\rho} \sum_{i} \log (- g_{i} (\mathbf {x})) \tag {11.9}
$$

Suppose we have a primal solution  $\mathbf{x}^*$  to this barrier problem for a particular value of  $\rho$ . The primal solution will satisfy

$$
\begin{array}{l} \mathbf {0} = \nabla \left[ f \left(\mathbf {x}^{*}\right) - \frac{1}{\rho} \sum_{i} \log \left(- g_{i} \left(\mathbf {x}^{*}\right)\right) \right] (11.10) \\ = \nabla f (\mathbf {x}^{*}) + \frac{1}{\rho} \sum_{i} \frac{1}{- g_{i} (\mathbf {x}^{*})} \nabla g_{i} (\mathbf {x}^{*}) (11.11) \\ = \nabla f \left(\mathbf {x}^{*}\right) + \sum_{i} \frac{- 1}{\rho g_{i} \left(\mathbf {x}^{*}\right)} \nabla g_{i} \left(\mathbf {x}^{*}\right) (11.12) \\ \end{array}
$$

If we set  $\mu_i^* = -1 / (\rho g_i(\mathbf{x}^*))$ , then we produce a Lagrangian for the inequality-constrained optimization problem in equation (11.7):

$$
\mathbf {0} = \nabla_{\mathbf {x}} \mathcal {L} \left(\mathbf {x}^{*}, \boldsymbol {\mu}^{*}\right) = \nabla f \left(\mathbf {x}^{*}\right) + \sum_{i} \mu_{i}^{*} \nabla g_{i} \left(\mathbf {x}^{*}\right) \tag {11.13}
$$

4 Primal-dual methods are covered in depth by S.J. Wright, Primal-Dual Interior-Point Methods. SIAM, 1997.

The dual variable satisfies  $\mu^{*} \geq 0$  because  $\mathbf{x}^{*}$  must be feasible. We are thus able to associate a dual value with every primal value obtained when iterating with the original interior point method.

Written another way, for a design  $\mathbf{x}^*$  to be optimal in equation (11.9), there must exist a  $\mu^{*}$  such that

$$
- \mu_{i}^{*} g_{i} \left(\mathbf {x}^{*}\right) = \frac{1}{\rho} \text{for} i \text{in} 1: m \tag {11.14}
$$

This new equation is analogous to complementary slackness, and approaches complementary slackness as  $\rho$  approaches infinity.

The design  $\mathbf{x}^*$  is not a solution to equation (11.7) in general. If there are  $m$  inequality constraints, then the duality gap for  $\mathbf{x}^*$  and  $\boldsymbol{\mu}^{*}$  is:

$$
\begin{array}{l} p^{*} - \mathcal {D} (\boldsymbol {\mu}^{*}) = f (\mathbf {x}^{*}) - \left(f (\mathbf {x}^{*}) + \sum_{i = 1}^{m} \mu_{i}^{*} g_{i} (\mathbf {x}^{*})\right) (11.15) \\ = - \sum_{i = 1}^{m} \frac{- 1}{\rho g_{i} \left(\mathbf {x}^{*}\right)} g_{i} \left(\mathbf {x}^{*}\right) (11.16) \\ = \frac{m}{\rho} (11.17) \\ \end{array}
$$

As  $\rho$  increases, the duality gap approaches 0. As a result,  $\mathbf{x}^*$  will approach the solution of our inequality-constrained optimization problem given in equation (11.7).

Our primal-dual method seeks to satisfy both stationarity and our alternative form of complementary slackness. We rearrange the equations such that they equal zero:

$$
\begin{array}{l} \nabla f (\mathbf {x}) + \sum_{i} \mu_{i} \nabla g_{i} (\mathbf {x}) = \mathbf {0} \tag {11.18} \\ - \mu_{i} g_{i} (\mathbf {x}) - \frac{1}{\rho} = 0 \quad \text{for} i \text{in} 1: m \\ \end{array}
$$

The residual is as follows:

$$
\mathbf {r} (\mathbf {x}, \mu , \rho) = \left[ \begin{array}{c} \nabla f (\mathbf {x}) + \sum_{i} \mu_{i} \nabla g_{i} (\mathbf {x}) \\ - \mu_{1} g_{1} (\mathbf {x}) - \frac{1}{\rho} \\ \vdots \\ - \mu_{m} g_{m} (\mathbf {x}) - \frac{1}{\rho} \end{array} \right] \tag {11.19}
$$

5 Complementary slackness is derived in exercise 11.1. The derivation for this condition is similar.

The primal-dual method will minimize  $\| \mathbf{r}\| _2$ , varying both  $\mathbf{x}$  and  $\mu$  over time, and increasing  $\rho$  as it proceeds. Its update is a line search in a primal-dual descent direction  $\mathbf{d}_{\mathbf{x}}$  and  $\mathbf{d}_{\mu}$ . We obtain a descent direction by applying Newton's method from equation (6.6) to this objective. This calculation is implemented in algorithm 11.1 and is demonstrated in example 11.3.

```julia
function primal_dual_descent_direction( f,  $\nabla f$  , Hf, gs,  $\nabla$  gs, Hgs,  $\rho$  , x,  $\mu$  )   
 $\mathsf{n},\mathsf{m} = \mathsf{length}(\mathsf{x})$  , length(  $\mu$  1   
r, H = zeros(n+m), zeros(n+m, n+m)   
r[1:n]  $=$ $\nabla f(x)+$  sum(  $\mu [i]*\nabla g s[i](x)$  for i in 1:m)   
H[1:n, 1:n]  $=$  Hf(x) + sum(  $\mu [i]*Hgs[i](x)$  for i in 1:m)   
for i in 1:m   
 $\begin{array}{rl} & {\mathrm{r}[n + i] = -\mu [i]*gs[i](x) - 1 / \rho}\\ & {\mathrm{H}[1:n,n + i] = \nabla g s[i](x)}\\ & {\mathrm{H}[n + i,1:n] = -\mu [i]*\nabla g s[i](x)}\\ & {\mathrm{H}[n + i,n + i] = -gs[i](x)} \end{array}$    
end   
d = pinv(H) \* -r   
return (d_x=d[1:n], d_ $\mu = d[n+1:]$  end)   
end
```

As with previous interior point methods, this primal-dual method also requires that its iterates remain in the interior of the feasible set:  $\mathbf{g}(\mathbf{x}) < 0$  and  $\mu > 0$ . Assuming the initial values are in the feasible set, all we have to do to ensure feasibility is enforce feasibility during the line search along the descent direction. This line search is a specialized form of backtracking line search (algorithm 4.3). If  $\mathbf{d}$  is a valid descent direction, then there must exist a sufficiently small step size that satisfies both the sufficient decrease condition and keeps the iterates in the feasible set.

Any negative components in  $\mathbf{d}_{\mu}$  could result in iterates violating  $\mu > 0$  if the steps are too large. Therefore, we bound an initial step size  $\alpha$

$$
\alpha = \min \left(\alpha , - (1 - \epsilon) \frac{\mu_{i}}{d_{\mu_{i}}}\right) \text{for} i \text{in} 1: m \text{where} d_{\mu_{i}} <   0 \tag {11.20}
$$

where  $\epsilon$  is a small positive value that ensures  $\mu_i > 0$  instead of  $\mu_i \geq 0$ . We then enforce  $\mathbf{g}(\mathbf{x}) < 0$  by further reducing  $\alpha$  by a reduction factor  $p$  until it is satisfied. Standard backtracking line search can be used to satisfy the sufficient decrease condition. Algorithm 11.2 provides an implementation, and figure 11.1 demonstrates how it works.

Algorithm 11.1. An algorithm for computing the descent direction for a primal-dual method. The algorithm takes an objective function  $f$ , its gradient  $\nabla f$ , its Hessian  $Hf$ ; a vector of inequality constraint functions  $gs$ , their gradients  $\nabla gs$  and their Hessians  $Hgs$ ; an interior point scalar  $\rho$ , a design  $x$ , and dual variables  $\mu$ .

6 The approach presented here is sufficient to ensure feasibility for convex constraints and objectives, but can still fail in the general case. This can happen, for example, if  $\mathbf{g}$  oscillates across zero multiple times in the search direction.

Suppose we wish to optimize

$$
\underset{\mathbf {x}} {\text{minimize}} \quad x^{2}
$$

$$
\text{subject} \quad (x - 3) ^{2} \leq 1
$$

Iterations of our primal-dual method seek to satisfy:

$$
\mathbf {r} = \left[ \begin{array}{c} \nabla f (x) + \mu \nabla g (x) \\ - \mu g (\mathbf {x}) - \frac{1}{\rho} \end{array} \right] = \left[ \begin{array}{c} 2 x + 2 \mu (x - 3) = 0 \\ - \mu (x - 3) ^{2} - 1 - 1 / \rho \end{array} \right] = \mathbf {0}
$$

The associated Hessian is:

$$
\mathbf {H} = \left[ \begin{array}{c c} \nabla^{2} f (x) + \mu \nabla^{2} g (x) & \nabla g (x) \\ - \mu \nabla g (x) & - g (x) \end{array} \right] = \left[ \begin{array}{c c} 2 + 2 \mu & 2 (x - 3) \\ - 2 \mu (x - 3) & (x - 3) ^{2} - 1 \end{array} \right]
$$

The descent direction is  $\mathbf{d} = -\mathbf{H}^{-1}\mathbf{r}$  where  $d_{1}$  is the descent direction for  $x$  and  $d_{2}$  is the descent direction for  $\mu$ .

Example 11.3. Calculating the primal-dual descent direction for a simple optimization problem.

# 11.3 Dual Ascent

Recall that the dual problem from equation (11.4) is always a concave optimization problem in  $\mu$  and  $\lambda$ :

$$
\underset{\mu \geq 0, \lambda} {\text{maximize}} \underset{\mathbf {x}} {\text{minimize}} \mathcal {L} (\mathbf {x}, \mu , \lambda) \tag {11.21}
$$

When strong duality holds, an optimum  $(\mathbf{x}^{*},\boldsymbol{\mu}^{*},\boldsymbol{\lambda}^{*})$  will satisfy both:

$$
\mathcal {L} \left(\mathbf {x}^{*}, \boldsymbol {\mu}^{*}, \lambda^{*}\right) = \underset{\mathbf {x}} {\operatorname{minimize}} \mathcal {L} \left(\mathbf {x}, \boldsymbol {\mu}^{*}, \lambda^{*}\right) \tag {11.22}
$$

and

$$
\mathcal {L} \left(\mathbf {x}^{*}, \boldsymbol {\mu}^{*}, \lambda^{*}\right) = \underset{\mu \geq 0, \lambda} {\text{maximize}} \mathcal {L} \left(\mathbf {x}^{*}, \boldsymbol {\mu}, \lambda\right) \tag {11.23}
$$

This means the optimum can be approached in two ways: by minimizing the Lagrangian with respect to the design with appropriate fixed dual values or by maximizing the Lagrangian with respect to dual values with an appropriate design.

```julia
function primal_dual_interior_point( f,  $\nabla f$  , Hf, gs,  $\nabla g s$  , Hgs,  $\rho$  , x,  $\mu$  .   
 $\alpha_{\mathrm{max}} = 1.0$ $\beta = 1e - 4$ $\gamma = 2$ $\epsilon = 0.01$  ,  $p = 0.9$    
res_min=1e-4,gap_min=1e-4)   
m = length(gs)   
r = (x,  $\mu$  ,  $\rho$  )  $\rightarrow$  [  $\nabla f(x)$  + sum(μ[i]  $\ast$ $\nabla g s[i]$  (x) for i in 1:m) [-μ[i]  $\ast$  gs[i](x) - 1/ρ for i in 1:m]   
]   
res = norm(r(x,  $\mu$  ,  $\rho$  ))   
gap = sum(-μ[i]  $\ast$  gs[i](x) for i in 1:m)   
while res > res_min || gap > gap_min   
 $\rho * = \gamma$    
d_x, d_ $\mu$  = primal_dual_descent_direction( f,  $\nabla f$  , Hf, gs,  $\nabla g s$  , Hgs,  $\rho$  , x,  $\mu$  ) # bound the step size to enforce  $\mu >0$ $\alpha = \alpha_{\mathrm{max}}$    
for i in 1:m if d_ $\mu$  [i] < 0  $\alpha = \min (\alpha , - (1 - \epsilon)*\mu [i]$  / d_ $\mu$  [i]) end   
end   
# multiply by a reduction factor until we have  $g(x)\leq 0$    
while any(g(x + α*d_x) > 0 for g in gs)   
 $\alpha * = p$    
end   
# continue to reduce until we have sufficient decrease   
while norm(r(x+α*d_x, μ+α*d_μ, ρ)) > (1-β*α)*res   
 $\alpha * = p$    
end   
x += d_x*α   
 $\mu * = d_{\mu}*\alpha$    
res = norm(r(x, μ, ρ))   
gap = sum(-μ[i]  $\ast$  gs[i](x) for i in 1:m)   
end   
return (x,  $\mu$  )
```

Algorithm 11.2. An implementation of a primal-dual method, which takes an objective function  $f$ , its gradient  $\nabla f$ , and its Hessian  $Hf$ ; a vector of inequality constraint functions  $gs$ , their gradients  $\nabla gs$  and their Hessians  $Hgs$ ; an initial interior point scalar  $\rho > 0$ , a design  $x$ , and dual variables  $\mu$ .

The algorithm proceeds until the norm of the residual is sufficiently decreased and the duality gap is sufficiently small. Not all problems have optimizers with zero duality gap, in which case a much larger duality gap threshold can be used.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8e3d61b7a3c8d8f3e8cc9cbb0d2e4d47788b6df386d0656dcfb94c7231d009f2.jpg)  
Figure 11.1. The primal dual interior point method applied to the Rosenbrock banana function with a circular constraint:

$$
g (x) = (x_{1} + 1) ^{2} + (x_{2} - 1) ^{2} - 2
$$

This run was initialized with  $\mathbf{x} = [-1.75,0.75]$  and  $\mu = [0.01]$  using  $\rho = 1.5$ .

In dual ascent, we alternate between solving for the primal values given the current values of the dual variables and applying an iteration of gradient ascent to the Lagrangian to update our dual variables. The aim is to move toward a  $(\lambda^{*},\mu^{*})$  that maximizes the Lagrangian for  $\mathbf{x}^*$ . For an iterate  $(\boldsymbol{\mu}^{(k)}\geq \mathbf{0},\boldsymbol{\lambda}^{(k)})$ , our update is:7

$$
\mathbf {x}^{(k)} = \underset{\mathbf {x}} {\arg \min } \mathcal {L} (\mathbf {x}, \boldsymbol {\mu}^{(k)}, \boldsymbol {\lambda}^{(k)}) \tag {11.24}
$$

$$
\boldsymbol {\mu}^{(k + 1)} = \boldsymbol {\mu}^{(k)} + \alpha^{(k)} \nabla_{\boldsymbol {\mu}} \mathcal {L} (\mathbf {x}^{(k)}, \boldsymbol {\mu}^{(k)}, \boldsymbol {\lambda}^{(k)}) = \max \left(\boldsymbol {\mu}^{(k)} + \alpha^{(k)} \mathbf {g} (\mathbf {x}^{(k)}), \mathbf {0}\right) \tag {11.25}
$$

$$
\boldsymbol {\lambda}^{(k + 1)} = \boldsymbol {\lambda}^{(k)} + \alpha^{(k)} \nabla_{\boldsymbol {\lambda}} \mathcal {L} \left(\boldsymbol {x}^{(k)}, \boldsymbol {\mu}^{(k)}, \boldsymbol {\lambda}^{(k)}\right) = \boldsymbol {\lambda}^{(k)} + \alpha^{(k)} \mathbf {h} \left(\boldsymbol {x}^{(k)}\right) \tag {11.26}
$$

Dual ascent thus ascends on the dual problem. Algorithm 11.3 provides an implementation.

Dual ascent applies to problems with both equality and inequality constraints, does not require backtracking line search, and can include infeasible primal iterates. However, dual ascent may fail on problems even when they have zero duality gap. For example, problems with linear objectives can have unbounded  $\mathbf{x}$  updates.

Dual ascent is related to the method of multipliers (section 10.9). Consider augmenting an equality constrained optimization problem with a quadratic penalty term:8

The max here is elementwise to ensure  $\mu \geq 0$

This augmented problem has the same solution, since  $\mathbf{h} = \mathbf{0}$  when solved.

```txt
function dual_ascent(f, g, h, x, μ, λ; α=1.0, γ=0.9, Δ_min=1e-4)  
L(x, μ, λ) = f(x) + μ·g(x) + λ·h(x)  
Δμ, Δλ = Inf, Inf  
while Δμ > Δ_min || Δλ > Δ_min  
x = minimize(x → L(x, μ, λ), x)  
μ' = max.(μ + α*g(x), 0)  
λ' = λ + α*h(x)  
Δμ, Δλ = norm(μ - μ', 2), norm(λ - λ', 2)  
μ, λ = μ', λ'  
α *= γ  
end  
return (x, μ, λ)  
end
```

$$
\underset{\mathbf {x}} {\operatorname{minimize}} \quad f (\mathbf {x}) + \frac{1}{2} \alpha \| \mathbf {h} (\mathbf {x}) \| ^{2} \tag {11.27}
$$

$$
\text{subject} \quad \mathbf {h} (\mathbf {x}) = \mathbf {0}
$$

Dual ascent on equation (11.27) reproduces the method of multipliers from section 10.9:

$$
\mathbf {x}^{(k)} = \underset{\mathbf {x}} {\arg \min } f (\mathbf {x}) + \boldsymbol {\lambda}^{\top} \mathbf {h} (\mathbf {x}) + \frac{1}{2} \alpha \| \mathbf {h} (\mathbf {x}) \| ^{2} \tag {11.28}
$$

$$
\boldsymbol {\lambda}^{(k + 1)} = \boldsymbol {\lambda}^{(k)} + \alpha \mathbf {h} \left(\boldsymbol {x}^{(k)}\right) \tag {11.29}
$$

# 11.4 Alternating Direction Method of Multipliers

The alternating direction method of multipliers (ADMM) is a method that can often scale to very large or distributed problems. Like conjugate gradient descent, ADMM can exhibit fast convergence with modest accuracy and produce practical results in a few steps. As with other local descent methods, ADMM is not guaranteed to converge to a global optimum on nonconvex problems. It is particularly useful when the problem can be split across multiple processes or machines. ADMM operates on problems of the form:

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} f_{1} (\mathbf {x}_{1}) + f_{2} (\mathbf {x}_{2}) \tag {11.30}
$$

$$
\text{subject} \quad \mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} = \mathbf {b}
$$

Algorithm 11.3. The dual descent method for solving a constrained optimization problem by ascending on the dual problem. The algorithm takes an objective function  $f$ , a vector-valued inequality constraint function  $g$ , a vector-valued equality constraint function  $h$ , and initial variables  $x, \mu$ , and  $\lambda$ . The algorithm can additionally be parameterized by an initial step size  $\alpha$ , a step size reduction factor  $\gamma$ , and a termination threshold  $\Delta_{\min}$  on the change in the dual variables.

9S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers," Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1-122, 2011. Z. Lin, H. Li, and C. Fang, Alternating Direction Method of Multipliers for Machine Learning. Springer, 2022. E.K. Ryu and W. Yin, Large-Scale Convex Optimization: Algorithms and Analyses via Monotone Operators. Cambridge University Press, 2023.

Notice that there are two design variables  $\mathbf{x}_1$  and  $\mathbf{x}_2$ , which do not necessarily have the same dimension. The objective is separable in  $\mathbf{x}_1$  and  $\mathbf{x}_2$ , and the constraint enforces an affine relationship between  $\mathbf{x}_1$  and  $\mathbf{x}_2$ . Although this problem formulation may appear quite narrow with the additive decomposition and the linearity of the constraints, many problems—including general constrained optimization problems—can be reformulated in this form as discussed in the next section.

We apply the method of multipliers to equation (11.30), introducing a quadratic penalty term controlled by a scalar  $\rho > 0$ :

$$
f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \frac{1}{2} \rho \| \mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b} \| _{2}^{2} \tag {11.31}
$$

This penalty term helps smooth the optimization problem with respect to the equality constraint, and it does not change the optimal value because all feasible points have 0 quadratic penalty.

We then construct the augmented Lagrangian:

$$
\mathcal {L}_{\rho} \left(\mathbf {x}_{1}, \mathbf {x}_{2}, \boldsymbol {\lambda}\right) = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \boldsymbol {\lambda}^{\top} \left(\mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b}\right) + \frac{1}{2} \rho \| \mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b} \| _{2}^{2} \tag {11.32}
$$

The method of multipliers would iterate according to:10

$$
\left(\mathbf {x}_{1}^{(k + 1)}, \mathbf {x}_{2}^{(k + 1)}\right) = \underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\arg \min } \mathcal {L}_{\rho} \left(\mathbf {x}_{1}, \mathbf {x}_{2}, \boldsymbol {\lambda}^{(k)}\right) \tag {11.33}
$$

$$
\lambda^{(k + 1)} = \lambda^{(k)} + \rho \left(\mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b}\right) \tag {11.34}
$$

The alternating direction method of multipliers instead alternates between minimizing  $\mathbf{x}_1$  and  $\mathbf{x}_2$  in every iteration:

$$
\mathbf {x}_{1}^{(k + 1)} = \underset{\mathbf {x}_{1}} {\arg \min } \mathcal {L}_{\rho} \left(\mathbf {x}_{1}, \mathbf {x}_{2}^{(k)}, \boldsymbol {\lambda}^{(k)}\right) \tag {11.35}
$$

$$
\mathbf {x}_{2}^{(k + 1)} = \underset{\mathbf {x}_{2}} {\arg \min } \mathcal {L}_{\rho} \left(\mathbf {x}_{1}^{(k + 1)}, \mathbf {x}_{2}, \boldsymbol {\lambda}^{(k)}\right) \tag {11.36}
$$

$$
\boldsymbol {\lambda}^{(k + 1)} = \boldsymbol {\lambda}^{(k)} + \rho \left(\mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k + 1)} - \mathbf {b}\right) \tag {11.37}
$$

This alternation of direction gives ADMM its name.

Algorithm 11.4 provides a basic implementation of ADMM. This implementation uses stopping criteria based on two residuals, the primal residual  $\mathbf{r}$  that measures how much the equality constraint is violated:

$$
\mathbf {r}^{(k + 1)} = \mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k + 1)} - \mathbf {b} \tag {11.38}
$$

10 Note the similarity to dual ascent.

and the dual residual  $\mathbf{s}$ :

$$
\mathbf {s}^{(k + 1)} = \rho \mathbf {A}_{1}^{\top} \mathbf {A}_{2} \left(\mathbf {x}_{2}^{(k + 1)} - \mathbf {x}_{2}^{(k)}\right) \tag {11.39}
$$

The dual residual measures how close the design is to satisfying stationarity<sup>11</sup>, in particular that

$$
\nabla_{\mathbf {x}_{1}} \mathcal {L} = \nabla_{\mathbf {x}_{1}} f_{1} (\mathbf {x}_{1}) + \mathbf {A}_{1}^{\top} \boldsymbol {\lambda} = \mathbf {0} \tag {11.40}
$$

Exercise 11.4 derives this relation.

The other stationarity requirement,

$$
\nabla_{\mathbf {x}_{2}} \mathcal {L} = \nabla_{\mathbf {x}_{2}} f_{2} (\mathbf {x}_{2}) + \mathbf {A}_{2}^{\top} \boldsymbol {\lambda} = \mathbf {0} \tag {11.41}
$$

is automatically enforced with every iteration.

```matlab
function admm(  
f1, f2, A1, A2, b, x1, x2;  
 $\rho = 1$ $\gamma = 2$ $\epsilon r = 1e - 3$ $\epsilon s = 1e - 3$ $\lambda =$  zeros(size(A2, 1))  
r = fill(Inf, size(A2, 1))  
s = fill(Inf, size(A1, 2))  
h(x1,x2) = A1*x1 + A2*x2 - b  
L(x1,x2, $\lambda ,\rho$ ) = f1(x1)+f2(x2)+ $\lambda \cdot h(x1,x2) + \rho /2*h(x1,x2)*h(x1,x2)$   
while norm(r) > er || norm(s) > es  
x1' = minimize(x1 → L(x1, x2, $\lambda ,\rho$ ), x1)  
x2' = minimize(x2 → L(x1',x2, $\lambda ,\rho$ ), x2)  
r = h(x1',x2')  
 $\lambda += \rho *\mathbf{r}$ $\rho *= \gamma$   
s =  $\rho *A1^{\prime} * A2^{\prime} (x2^{\prime} - x2)$   
x1, x2 = x1', x2'  
end  
return x1  
end
```

It can be beneficial to keep the primal and dual residuals close to one another as ADMM progresses. $^{12}$  Increasing the penalty parameter will tend to encourage reduction of the primal residual. A simple scheme can be used to automatically regulate the penalty parameter:

$$
\rho^{(k + 1)} = \left\{ \begin{array}{l l} \gamma^{+} \rho^{(k)} & \text{if } \| \mathbf {r}^{(k)} \| _{2} > \mu \| \mathbf {s}^{(k)} \| _{2} \\ \gamma^{-} \rho^{(k)} & \text{if } \| \mathbf {s}^{(k)} \| _{2} > \mu \| \mathbf {r}^{(k)} \| _{2} \\ \rho^{(k)} & \text{otherwise} \end{array} \right. \tag {11.42}
$$

Algorithm 11.4. The alternating direction method of multipliers (ADMM) for the objective functions f1 and f2; the equality constraint given by A1, A2, and b; initial design x1 and x2; initial penalty scalar  $\rho >\theta$  and penalty multiplier  $\gamma >1$  ; and convergence thresholds er and es. This implementation uses warm starting, where the current value for x1 and x2 are used to initialize the minimization procedures. The current value is typically close to the minimizer, especially in later iterations, and using it tends to lead to faster convergence than using default initial values.

12 The suboptimality of the current point,  $f_{1}(\mathbf{x}_{1}^{(k)}) + f_{2}(\mathbf{x}_{2}^{(k)}) - p^{*}$  can be shown to be bounded by  $\| \boldsymbol{\lambda}^{(k)}\|_2\|\mathbf{r}^{(k)}\|_2 + \| \mathbf{s}^{(k)}\|_2$  for a  $d \geq \| \mathbf{x}^{(k)} - \mathbf{x}^{*}\|_2$ . Both residuals must approach zero for convergence to occur. If one residual reduces too quickly, it can slow down the rate at which the other can catch up.

where  $\gamma^{+} > 1, 1 > \gamma^{-} > 0$ , and  $\mu > 1$ . This approach scales  $\rho$  when the primal residual is significantly larger than the dual residual, and shrinks it when the dual residual is significantly smaller than the primal residual. These residuals are shown in figure 11.2, where ADMM is optimizing the flower function with a circle constraint.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f09a12125f3e9da1a8d5a681a635f3e28e381fe26c74f54bcc0a81040f821fb2.jpg)

The ADMM updates can often be simplified by converting the Lagrangian to scaled form. We replace  $\mathbf{A}_1\mathbf{x}_1 + \mathbf{A}_2\mathbf{x}_2 - \mathbf{b}$  with the residual  $\mathbf{r}$  and write:

$$
\begin{array}{l} \mathcal {L}_{\rho} \left(\mathbf {x}_{1}, \mathbf {x}_{2}, \boldsymbol {\lambda}\right) = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \boldsymbol {\lambda}^{\top} \mathbf {r} + \frac{1}{2} \rho \| \mathbf {r} \| _{2}^{2} (11.43) \\ = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \boldsymbol {\lambda}^{\top} \mathbf {r} + \frac{1}{2} \rho \| \mathbf {r} \| _{2}^{2} + \frac{1}{2 \rho} \| \boldsymbol {\lambda} \| _{2}^{2} - \frac{1}{2 \rho} \| \boldsymbol {\lambda} \| _{2}^{2} (11.44) \\ = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \lambda^{\top} \mathbf {r} + \frac{1}{2} \rho \mathbf {r}^{\top} \mathbf {r} + \frac{1}{2 \rho} \lambda^{\top} \lambda - \frac{1}{2 \rho} \lambda^{\top} \lambda (11.45) \\ = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \frac{1}{2} \rho \left(\mathbf {r} + \frac{1}{\rho} \boldsymbol {\lambda}\right) ^{\top} \left(\mathbf {r} + \frac{1}{\rho} \boldsymbol {\lambda}\right) - \frac{1}{2 \rho} \boldsymbol {\lambda}^{\top} \boldsymbol {\lambda} (11.46) \\ \end{array}
$$

${}^{13}$  Suggested values include  ${\gamma }_{ + } =$  2,  ${\gamma }_{ - } = {0.5}$  ,and  $\mu  = {10}$  . B.S. He,H.Yang,and S.L.Wang,"Alternating Direction Method with Self-Adaptive Penalty Parameters for Monotone Variational Inequalities," Journal of Optimization Theory and Applications, vol. 106, no. 2, pp. 337-156, 2000.

Figure 11.2. ADMM applied to the flower function (appendix B.4) with the constraint  $\| \mathbf{x}_1\|^2 = 2$ . The transformation used to rewrite the problem into ADMM form will be covered in the next section. The designs  $\mathbf{x}_1$  and  $\mathbf{x}_2$  are initialized to the same value, but diverge because each optimizes different objectives. These quickly reconverge as the method progresses and  $\rho$  increases. The norms of the primal and dual residuals approach zero as the algorithm progresses.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f6dfb2326bb0452f04106c28e04617391ca4505191290862afccfd25f0b367c7.jpg)

We can introduce a norm and the scaled dual variable defined to be  $\mathbf{u} = (1 / \rho)\lambda$ :

$$
\begin{array}{l} \mathcal {L}_{\rho} \left(\mathbf {x}_{1}, \mathbf {x}_{2}, \boldsymbol {\lambda}\right) = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \frac{1}{2} \rho \left\| \mathbf {r} + \frac{1}{\rho} \boldsymbol {\lambda} \right\| _{2}^{2} - \frac{1}{2 \rho} \| \boldsymbol {\lambda} \| _{2}^{2} (11.47) \\ = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \frac{1}{2} \rho \| \mathbf {r} + \mathbf {u} \| _{2}^{2} - \frac{1}{2} \rho \| \mathbf {u} \| _{2}^{2} (11.48) \\ = f_{1} \left(\mathbf {x}_{1}\right) + f_{2} \left(\mathbf {x}_{2}\right) + \frac{1}{2} \rho \| \mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b} + \mathbf {u} \| _{2}^{2} - \frac{1}{2} \rho \| \mathbf {u} \| _{2}^{2} (11.49) \\ \end{array}
$$

Reformulating the ADMM updates with this scaled form yields:

$$
\mathbf {x}_{1}^{(k + 1)} = \underset{\mathbf {x}_{1}} {\arg \min } f_{1} (\mathbf {x}_{1}) + \frac{1}{2} \rho \left\| \mathbf {A}_{1} \mathbf {x}_{1} - \mathbf {v}^{(k)} \right\| _{2}^{2} \tag {11.50}
$$

$$
\mathbf {x}_{2}^{(k + 1)} = \underset{\mathbf {x}_{2}} {\arg \min } f_{2} (\mathbf {x}_{2}) + \frac{1}{2} \rho \left\| \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {w}^{(k)} \right\| _{2}^{2} \tag {11.51}
$$

$$
\mathbf {u}^{(k + 1)} = \mathbf {u}^{(k)} + \mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k + 1)} - \mathbf {b} = \mathbf {u}^{(0)} + \sum_{j = 1}^{k + 1} \mathbf {r}^{(j)} \tag {11.52}
$$

where  $\mathbf{v}^{(k)} = -\mathbf{A}_2\mathbf{x}_2^{(k)} + \mathbf{b} - \mathbf{u}^{(k)}$  and  $\mathbf{w}^{(k)} = -\mathbf{A}_1\mathbf{x}_1^{(k + 1)} + \mathbf{b} - \mathbf{u}^{(k)}$ . The scaled dual variable is identical to the running sum of the residuals.

# 11.5 ADMM Applications

Many problems that have been studied over the years can be expressed in ADMM form. This section presents a few examples.

# 11.5.1 General Constrained Optimization

Any constrained optimization problem can be reformulated in ADMM form. An optimization problem expressed as:

$$
\underset{\mathbf {x}} {\text{minimize}} f (\mathbf {x}) \tag {11.53}
$$

$$
\text{subject} \quad \mathbf {x} \in \mathcal {X}
$$

has the ADMM form:

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} f_{1} (\mathbf {x}_{2}) + f_{2} (\mathbf {x}_{2}) \tag {11.54}
$$

$$
\text{subject} \quad \mathbf {x}_{1} - \mathbf {x}_{2} = \mathbf {0}
$$

with  $f_{1} = f$  and

$$
f_{2} \left(\mathbf {x}_{2}\right) = \left\{ \begin{array}{l l} 0 & \text{if } \mathbf {x}_{2} \in \mathcal {X} \\ \infty & \text{otherwise} \end{array} \right. \tag {11.55}
$$

We introduced  $\mathbf{x}_2$  as a duplicate of the design vector  $\mathbf{x}_1$ . We continue to minimize the original objective function, but we also introduce a secondary objective  $f_2(\mathbf{x}_2)$  to reflect whether or not our constraint is satisfied. The equality constraint uses  $\mathbf{A}_1 = \mathbf{I}$ ,  $\mathbf{A}_2 = -\mathbf{I}$ , and  $\mathbf{b} = \mathbf{0}$ , resulting in a constraint that enforces equality between  $\mathbf{x}_1$  and  $\mathbf{x}_2$  when the problem is solved. This approach allows  $\mathbf{x}_1$  to balance minimizing  $f$  and reaching consensus with  $\mathbf{x}_2$ , and  $\mathbf{x}_2$  to balance feasibility and reaching consensus with  $\mathbf{x}_1$ . The penalty scalar  $\rho$  increases over time to draw the two designs together for eventual convergence.

Example 11.4 rewrites a constrained optimization problem in ADMM form and shows its  $\mathbf{x}_1$ - and  $\mathbf{x}_2$ -updates. The progression of ADMM on this problem is shown in figure 11.3.

The ADMM updates for problems in the form of equation (11.54) are:

$$
\mathbf {x}_{1}^{(k + 1)} = \underset{\mathbf {x}_{1}} {\arg \min } f_{1} \left(\mathbf {x}_{1}\right) + \boldsymbol {\lambda}^{\top} \mathbf {x}_{1} + \frac{1}{2} \rho \left\| \mathbf {x}_{1} - \mathbf {x}_{2}^{(k)} \right\| _{2}^{2} \tag {11.56}
$$

$$
\mathbf {x}_{2}^{(k + 1)} = \underset{\mathbf {x}_{2}} {\arg \min } f_{2} (\mathbf {x}_{2}) - \boldsymbol {\lambda}^{\top} \mathbf {x}_{2} + \frac{1}{2} \rho \left\| \mathbf {x}_{1}^{(k + 1)} - \mathbf {x}_{2} \right\| _{2}^{2} \tag {11.57}
$$

$$
\boldsymbol {\lambda}^{(k + 1)} = \boldsymbol {\lambda}^{(k)} + \rho \left(\mathbf {x}_{1}^{(k + 1)} - \mathbf {x}_{2}^{(k + 1)}\right) \tag {11.58}
$$

Both the  $\mathbf{x}_1$ - and  $\mathbf{x}_2$ -updates have an additional quadratic term that biases the solution toward the other design. This form of quadratic minimization is called proximal minimization, with its name reflecting how the minimizer lies in the proximity of the other design. The larger that  $\rho$  is, the larger the bias. Proximal minimization has close ties to gradient descent as shown in example 11.5.

Proximal minimization is also particularly useful for the  $\mathbf{x}_2$ -update. As we recall, in equation (11.54),  $f_2(\mathbf{x}_2)$  is an indicator function for a set membership constraint:

$$
f_{2} \left(\mathbf {x}_{2}\right) = \left\{ \begin{array}{l l} 0 & \text{if } \mathbf {x}_{2} \in \mathcal {X} \\ \infty & \text{otherwise} \end{array} \right. \tag {11.59}
$$

If we ignore the  $\lambda^{\top}\mathbf{x}_2$  term, proximal minimization will find the feasible point closest to  $\mathbf{x}_1^{(k + 1)}$  as shown in example 11.6. In other words,  $\mathbf{x}_2^{(k + 1)}$  is the projection of  $\mathbf{x}_1^{(k + 1)}$  onto  $\mathcal{X}$ .

14 N. Parikh and S. Boyd, "Proximal Algorithms," Foundations and Trends in Optimization, vol. 1, no. 3, pp. 127-239, 2014.

Consider the optimization problem

$$
\underset{x} {\text{minimize}} \quad \exp (x)
$$

$$
\text{subject} \quad (x - 2) ^{2} \leq 1
$$

This problem can be written in ADMM form as:

$$
\underset{x_{1}, x_{2}} {\text{minimize}} \quad \exp (x_{1}) + f_{2} (x_{2})
$$

$$
\text{subject} \quad x_{1} - x_{2} = 0
$$

where

$$
f_{2} (x_{2}) = \left\{ \begin{array}{l l} 0 & \text{if } (x_{2} - 2) ^{2} \leq 1 \\ \infty & \text{otherwise} \end{array} \right.
$$

Its  $x_{1}$ -update is:

$$
x_{1}^{(k + 1)} = \underset{x_{1}} {\arg \min } \exp (x_{1}) + \lambda \cdot \left(x_{1} - x_{2}^{(k)}\right) + \frac{1}{2} \rho \left(x_{1} - x_{2}^{(k)}\right) ^{2}
$$

and its  $x_{2}$  -update is:

$$
x_{2}^{(k + 1)} = \underset{x_{2}} {\arg \min } f_{2} (x_{2}) + \lambda \cdot \left(x_{1}^{(k + 1)} - x_{2}\right) + \frac{1}{2} \rho \left(x_{1}^{(k + 1)} - x_{2}\right) ^{2}
$$

which is equivalent to:

$$
x_{2}^{(k + 1)} = \underset{x_{2}} {\arg \min } \quad \lambda \cdot \left(x_{1}^{(k + 1)} - x_{2}\right) + \frac{1}{2} \rho \left(x_{1}^{(k + 1)} - x_{2}\right) ^{2}
$$

$$
\text{subject} \quad (x_{2} - 2) ^{2} \leq 1
$$

Example 11.4. A simple constrained convex optimization problem rewritten in ADMM form.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/42d326a7e649018967608188493ac517df2689886f889fd3fa061551a7994e02.jpg)  
$x_{1}$  updates

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7ddede73eb3044c26f7e8fb1ed5b14b2147c0eaa71cbef87a196f5cd9ecb58fb.jpg)  
$x_{2}$  updates

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c8e8a8b0233c9348af2e81f1539ad8e72ed0aecd574f2bb341cd36bb375348b6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2db22dab7f1ba4b0a411d69eabf08752f1c0336cb4f9c4ab8faa67370812222c.jpg)  
Figure 11.3. ADMM applied to the optimization problem in example 11.4. Iterations proceed top to bottom, with  $\mathbf{x}_1$ -updates depicted on the left and  $\mathbf{x}_2$ -updates depicted on the right. Each plot shows  $\exp(x)$  in black along with the optimization function being optimized by the respective update. The gray dot shows the value at the start of the iteration, and the colored dot shows the updated value.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dff630979acf5d7f8de14f6a3537bb34cebbcda7aeece4ad3cb1e5bbd45282a6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/575441940c43f594f119c1f189738e84c7673524c74ad7668afd643ffbca888b.jpg)  
This progression started with  $\rho = 1$  and used  $\gamma = 1.5$ . In this simple problem,  $x_{2}$  snaps to 1, the closest feasible point, and then remains there throughout subsequent iterations. Notice how the  $\mathbf{x}_1$ -update is slowly contorted toward enforcing  $x = 1$  as  $\rho$  increases, thereby converging to the optimal solution.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/796a9b29eb30d220e7583d7b0119968ca22544c644cff8a7b7862d15ee8d3d42.jpg)  
$f(x)$  （20  $x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2e0181ca7708a76e8c3bf73e12b1d308103414c93ec74a26ed7c56c1510fa2f4.jpg)  
$f(x) + \lambda (x_{1} - x_{2}) + \frac{1}{2}\rho (x_{1} - x_{2})^{2}$  
$f(x)$  2  
$\lambda (x_1^{\prime} - x_2) + \frac{1}{2}\rho (x_1^{\prime} - x_2)^2$

Proximal minimization of a function  $f$  with respect to a point  $\mathbf{p}$  produces a minimizer  $\mathbf{x}$  in the proximity of  $\mathbf{p}$ :

$$
\underset{\mathbf {x}} {\arg \min } f (\mathbf {x}) + \frac{1}{2} \rho \| \mathbf {x} - \mathbf {p} \| _{2}^{2}
$$

If  $f$  is differentiable, then we can approximate  $f$  with a first-order Taylor expansion about  $\mathbf{p}$ , where  $f(\mathbf{x}) \approx f(\mathbf{p}) + \nabla f(\mathbf{p})^\top (\mathbf{x} - \mathbf{p})$ . Proximal minimization applied to this approximation yields

$$
\arg \min_{\mathbf {x}} \left(f (\mathbf {p}) + \nabla f (\mathbf {p}) ^{\top} (\mathbf {x} - \mathbf {p}) + \frac{1}{2} \rho \| \mathbf {x} - \mathbf {p} \| _{2}^{2}\right) = \mathbf {p} - \frac{1}{\rho} \nabla f (\mathbf {p})
$$

The update is a standard gradient step from  $\mathbf{p}$  with step factor  $1 / \rho$ .

Example 11.5. Proximal minimization applied to a first-order Taylor approximation yields a gradient step update.

# 11.5.2 Alternating Projections

ADMM can be used to re-derive the alternating projections method. $^{15}$  We have two convex sets  $\mathcal{X}_1$  and  $\mathcal{X}_2$ , and we want to find a design  $\mathbf{x}$  that lies in both of them.

This problem can be written in ADMM form as:

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} f_{1} (\mathbf {x}_{1}) + f_{2} (\mathbf {x}_{2}) \tag {11.60}
$$

subject to  $\mathbf{x}_1 - \mathbf{x}_2 = \mathbf{0}$

15 R. L. Dykstra, "An Algorithm for Restricted Least Squares Regression," Journal of the American Statistical Association, vol. 78, no. 384, pp. 837-842, 1983.

where

$$
f_{i} \left(\mathbf {x}_{i}\right) = \left\{ \begin{array}{l l} 0 & \text{if } \mathbf {x}_{i} \in \mathcal {X}_{i} \\ \infty & \text{otherwise} \end{array} \right. \tag {11.61}
$$

Consider an objective function that is zero when  $\mathbf{x}_1$  satisfies a convex inequality constraint and positive infinity otherwise. The  $\mathbf{x}_1$ -update is

$$
\begin{array}{l} \mathbf{x}_{1}^{\prime} = \operatorname *{arg  min}_{\mathbf{x}_{1}}\infty (\mathbf{C}\mathbf{x}_{1} > \mathbf{d}) + \frac{1}{2}\rho \| \mathbf{x}_{1} - \mathbf{v}\|_{2}^{2} \\ = \underset{\mathbf {x}_{1} \mid \mathbf {C x}_{1} \leq \mathbf {d}} {\arg \min } \frac{1}{2} \rho \| \mathbf {x}_{1} - \mathbf {v} \| _{2}^{2} \\ \end{array}
$$

The output of proximal minimization for such a set is shown below for different values of  $\mathbf{v}$ . The feasible set  $\mathcal{X} = \{\mathbf{x}_1 \mid \mathbf{C}\mathbf{x}_1 \leq \mathbf{d}\}$  is drawn in blue. We can see how proximal minimization produces the design  $\mathbf{x}_1$  that is in  $\mathcal{X}$  but closest to  $\mathbf{v}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/411289ded11f0f88aa944c9cb684e5d243549a7c0e6815acdd6fbfd423a0c759.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fdf1a13f00d405f6f6307cf073857af512c6ff58696f16d674be4edce0a3a6e8.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/22c4ce02e453ec023fc965e51f71fcfc5dbbb1e499641173f952ce4ae5e984ad.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/30f292c419067b59720cc8e59a048fe265c0ba8900b9fb12ad7058f405f69be3.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/80c0daeb447892d0a89a50913dac940b6c8c9992c69b0c88b6c10509c9b13a00.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4076ba79326e2463625e0fd12372f7fcea3fa918b61fd51723d3128501915a7e.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/08b1968a8c87ff55d7500cfa469a135a7657c208dc18aa536f4af23af4fdf30e.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/14e1ce48f3ecd39434547a7846dc0258c1e5fabc55886943cc1c4c54381542b5.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e43245e2a95bab6c65151e0a4e4d5f1efc617c20fba7193550000225589f60c4.jpg)

Example 11.6. Proximal minimization can be used to enforce constraints.

The ADMM update reduces to:16

$$
\mathbf {x}_{1}^{(k + 1)} = \underset{\mathbf {x}_{1} \in \mathcal {X}_{1}} {\arg \min } \| \mathbf {x}_{1} - \mathbf {x}_{2}^{(k)} + \mathbf {u}^{(k)} \| _{2}^{2} \tag {11.62}
$$

$$
\mathbf {x}_{2}^{(k + 1)} = \underset{\mathbf {x}_{2} \in \mathcal {X}_{2}} {\arg \min } \| \mathbf {x}_{1}^{(k + 1)} - \mathbf {x}_{2} + \mathbf {u}^{(k)} \| _{2}^{2} \tag {11.63}
$$

$$
\mathbf {u}^{(k + 1)} = \mathbf {u}^{(k)} + \mathbf {x}_{1}^{(k + 1)} - \mathbf {x}_{2}^{(k + 1)} \tag {11.64}
$$

For this problem, the primal residual  $\mathbf{r}^{(k)} = \mathbf{x}_1^{(k)} - \mathbf{x}_2^{(k)}$  is the difference between a point in  $\mathcal{X}_1$  and a point in  $\mathcal{X}_2$ . Terminating the ADMM iterations when  $\| \mathbf{r}^{(k)}\|_2$  falls below a threshold  $\epsilon$  guarantees that  $\mathcal{X}_1$  and  $\mathcal{X}_2$  are no more than  $\epsilon$  apart. This algorithm is demonstrated in figure 11.4.

# 11.5.3 Least Absolute Deviation

Unconstrained least squares problems can be formulated as quadratic programs, as covered in section 13.2. Least squares problems assume that the distance metric being minimized is the  $L_{2}$  norm. Using an  $L_{1}$  norm instead results in a least absolute deviation problem, which is often preferred over least squares to provide robustness against large outliers:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \| \mathbf {A} \mathbf {x} - \mathbf {b} \| _{1} \tag {11.65}
$$

Such a problem can be written in ADMM form using  $f_{1}(\mathbf{x}_{1}) = 0$  and letting  $\mathbf{x}_2 = \mathbf{A}\mathbf{x}_1 - \mathbf{b}$ :

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} \| \mathbf {x}_{2} \| _{1} \tag {11.66}
$$

$$
\text{subject} \quad \mathbf {A} \mathbf {x}_{1} - \mathbf {x}_{2} = \mathbf {b}
$$

This decomposition neatly decouples the  $L_{1}$  minimization problem from the affine constraint. If  $\mathbf{A}^{\top}\mathbf{A}$  is invertible, the ADMM update is:17

$$
\mathbf {x}_{1}^{\prime} = \left(\mathbf {A}^{\top} \mathbf {A}\right) ^{- 1} \mathbf {A}^{\top} (\mathbf {b} + \mathbf {x}_{2} - \mathbf {u}) \tag {11.67}
$$

$$
\mathbf {x}_{2}^{\prime} = S_{\frac{1}{\rho}} \left(\mathbf {A} \mathbf {x}_{1}^{\prime} - \mathbf {b} + \mathbf {u}\right) \tag {11.68}
$$

$$
\mathbf {u}^{\prime} = \mathbf {u} + \mathbf {A x}_{1}^{\prime} - \mathbf {x}_{2}^{\prime} - \mathbf {b} \tag {11.69}
$$

16 The update is obtained using equations (11.50) to (11.52). For example, the  $\mathbf{x}_1$  update can be derived as follows:

$$
\begin{array}{l} \mathbf {x}_{1}^{(k + 1)} = \\ = \underset{\mathbf {x}_{1}} {\arg \min } f_{1} (\mathbf {x}_{1}) + \frac{1}{2} \rho \left\| \mathbf {A}_{1} \mathbf {x}_{1} - \mathbf {v}^{(k)} \right\| _{2}^{2} \\ = \underset{\mathbf {x}_{1} \in \mathcal {X}_{1}} {\arg \min } \left\| \mathbf {x}_{1} - \left(\mathbf {x}_{2}^{(k)} - \mathbf {u}^{(k)}\right) \right\| _{2}^{2} \\ = \underset{\mathbf {x}_{1} \in \mathcal {X}_{1}} {\arg \min } \left\| \mathbf {x}_{1} - \mathbf {x}_{2}^{(k)} + \mathbf {u}^{(k)} \right\| _{2}^{2} \\ \end{array}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e56fd56de15ac59d58054a1d795ae97f1ed18a6b0cc6e322f23ecef0ff74216b.jpg)  
Figure 11.4. The first few iterations of the alternating projections method. The projections alternate between the two sets, drawing closer to the set's intersection.

17 The update is obtained by using the scaled form updates, equations (11.50) to (11.52). For example, the  $\mathbf{x}_1$  update is:

$$
\begin{array}{l} \mathbf {x}_{1}^{(k + 1)} = \\ = \arg \min_{\mathbf {x}_{1}} f_{1} (\mathbf {x}_{1}) + \frac{1}{2} \rho \left\| \mathbf {A} \mathbf {x}_{1} - \mathbf {v}^{(k)} \right\| _{2}^{2} \\ = \underset{\mathbf {x}_{1}} {\arg \min } \left\| \mathbf {A x}_{1} - \mathbf {v}^{(k)} \right\| _{2}^{2} \\ = \left(\mathbf {A}^{\top} \mathbf {A}\right) ^{- 1} \mathbf {A}_{1}^{\top} \mathbf {v}^{(k)} \\ = \left(\mathbf {A}^{\top} \mathbf {A}\right) ^{- 1} \mathbf {A}_{1}^{\top} \left(- (- \mathbf {I}) \mathbf {x}_{2}^{(k)} + \mathbf {b} - \mathbf {u}^{(k)}\right) \\ = \left(\mathbf {A}^{\top} \mathbf {A}\right) ^{- 1} \mathbf {A}_{1}^{\top} \left(\mathbf {b} + \mathbf {x}_{2}^{(k)} - \mathbf {u}^{(k)}\right) \\ \end{array}
$$

where the soft thresholding operator  $S_{\kappa}$ , shown in figure 11.5, is

$$
S_{\kappa} (\mathbf {x}) _{i} = \left\{ \begin{array}{l l} x_{i} + \kappa & \text{if } x_{i} <   - \kappa \\ 0 & \text{if } | x_{i} | \leq \kappa = \max (x_{i} - \kappa , 0) - \max (- x_{i} - \kappa , 0) \\ x_{i} - \kappa & \text{if } x_{i} > \kappa \end{array} \right. \tag {11.70}
$$

This problem can be solved very efficiently because we can compute the Cholesky factorization (appendix C.7.1) of  $\mathbf{A}^{\top}\mathbf{A}$  once and re-use it to perform the  $\mathbf{x}_1$ -update in every iteration. An implementation is provided in algorithm 11.5.

```matlab
function least Absolutely_deviation(A, b, x1, x2;  $\rho = 1$ ,  $\gamma = 2$ ,  $\epsilon r = 1e - 3$ )  
u = zeros(length(x2))  
r = fill(Inf, length(x2))  
F = factorize(A^{\prime} * A)  
S( $\kappa, x$ ) = max.(x -.k,0) - max.(-x -.k,0)  
while norm(r) >  $\epsilon r$   
    x1' = F \ A^{\prime} * (b + x2 - u)  
    x2' = S(1/ $\rho$ , A*x1' - b + u)  
    u' = A*x1' - b + u - x2'  
     $\rho *= \gamma$   
    r = (A*x1' - b) - x2'  
    x1, x2, u = x1', x2', u'  
end  
return x1  
end
```

This method can be extended to other distance metrics, such as the Huber function:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \operatorname{huber} (\mathbf {A x} - \mathbf {b}) \tag {11.71}
$$

where

$$
\operatorname{huber} (\mathbf {x}) = \sum_{i} \operatorname{huber} \left(x_{i}\right) \tag {11.72}
$$

and

$$
\operatorname{huber} (x) = \left\{ \begin{array}{l l} \frac{1}{2} x^{2} & \text{if } | x | \leq 1 \\ | x | - \frac{1}{2} & \text{otherwise} \end{array} \right. \tag {11.73}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/795393ec1208e0f1c5dcee087f333338411301c481907fd85e54e06f3fc5b365.jpg)  
Figure 11.5. The soft thresholding operator for  $\kappa = 2$  plotted in one dimension.

Algorithm 11.5. ADMM applied to solve a least absolute deviation problem parameterized by A and b; initial design  $x_1$  and  $x_2$ ; initial penalty scalar  $\rho > 0$  and penalty multiplier  $\gamma > 1$ ; and convergence threshold  $\epsilon r$ . The Julia method factorize will find an appropriate factorization for the given matrix, and due to type dispatching, solving  $\mathbf{Fx} = \mathbf{b}$  through  $F \setminus b$  will use that factorization.

18 Named for the Swiss statistician Peter Jost Huber (1934-) who introduced it in 1964. P.J. Huber, "Robust Estimation of a Location Parameter," Annals of Mathematical Statistics, vol. 35, no. 1, pp. 73-101, 1964. The Huber function is often parameterized such that the transition from quadratic to linear can be varied.

The Huber function is convex and often used for loss functions in machine learning. Being linear for large deviations and quadratic for small deviations makes it resilient to outliers while remaining continuously differentiable. The Huber function is shown in figure 11.6.

The ADMM form of a minimum Huber deviation problem has the same structure:

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} \quad \operatorname{huber} \left(\mathbf {x}_{2}\right) \tag {11.74}
$$

$$
\text{subject} \quad \mathbf {A x}_{1} - \mathbf {x}_{2} = \mathbf {b}
$$

and has the same updates, except the  $\mathbf{x}_2$ -update is now:

$$
\mathbf {x}_{2}^{\prime} = \frac{\rho}{1 + \rho} \left(\mathbf {A x}_{1}^{\prime} - \mathbf {b} + \mathbf {u}\right) + \frac{1}{1 + \rho} S_{1 + 1 / \rho} \left(\mathbf {A x}_{1}^{\prime} - \mathbf {b} + \mathbf {u}\right) \tag {11.75}
$$

# 11.5.4 Basis Pursuit

Equality-constrained  $L_{1}$  minimization problems, known as basis pursuit problems, have the form:

$$
\underset{\mathbf {x}} {\text{minimize}} \| \mathbf {x} \| _{1} \tag {11.76}
$$

$$
\text{subject} \quad \mathbf {A x} = \mathbf {b}
$$

Such problems are solved to find sparse solutions to  $\mathbf{A}\mathbf{x} = \mathbf{b}$  when the system of linear equations is underdetermined. Applying ADMM to this problem gives rise to the split Bregman method. $^{19}$  A basis pursuit problem can be written in ADMM form as:

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} f_{1} (\mathbf {x}_{1}) + \| \mathbf {x}_{2} \| _{1} \tag {11.77}
$$

$$
\text{subject} \quad \mathbf {x} - \mathbf {x}_{2} = \mathbf {0}
$$

where

$$
f_{1} \left(\mathbf {x}_{1}\right) = \left\{ \begin{array}{l l} 0 & \text{if } \mathbf {A x}_{1} = \mathbf {b} \\ \infty & \text{otherwise} \end{array} \right. \tag {11.78}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e443d3635c6623948111ea2097aeca8a3330bb2cc11abc2c816e3da3ccbea94f.jpg)  
Figure 11.6. The Huber function plotted in one dimension.

19 Named for the Soviet and Israeli mathematician Lev M. Bregman (1941-2023). L. Bregman, "The Relaxation Method of Finding the Common Point of Convex Sets and Its Application to the Solution of Problems in Convex Programming," USSR Computational Mathematics and Mathematical Physics, vol. 7, no. 3, pp. 200-217, 1967. T. Goldstein and S. Osher, "The Split Bregman Method for  $L_{1}$ -Regularized Problems," SIAM Journal on Imaging Sciences, vol. 2, no. 2, pp. 323-343, 2009.

with the ADMM update:

$$
\begin{array}{l} \mathbf {x}_{1}^{\prime} = \underset{\mathbf {x}_{1} | \mathbf {A x}_{1} = \mathbf {b}} {\operatorname{minimize}} \| (\mathbf {x}_{2} - \mathbf {u}) - \mathbf {x}_{1} \| _{2} (11.79) \\ = \left(\mathbf {I} - \mathbf {A}^{\top} \left(\mathbf {A} \mathbf {A}^{\top}\right) ^{- 1} \mathbf {A}\right) \left(\mathbf {x}_{2} - \mathbf {u}\right) + \mathbf {A}^{\top} \left(\mathbf {A} \mathbf {A}^{\top}\right) ^{- 1} \mathbf {b} (11.80) \\ \end{array}
$$

$$
\mathbf {x}_{2}^{\prime} = S_{\frac{1}{\rho}} \left(\mathbf {x}_{1}^{\prime} + \mathbf {u}\right) \tag {11.81}
$$

$$
\mathbf {u}^{\prime} = \mathbf {u} + \mathbf {x}_{1}^{\prime} - \mathbf {x}_{2}^{\prime} \tag {11.82}
$$

The  $\mathbf{x}_1$ -update finds the  $\mathbf{x}_1'$  closest to  $\mathbf{x}_2 - \mathbf{u}$  that satisfies the equality constraint. In this case we can precompute most of the terms in the  $\mathbf{x}_1$ -update once and re-use them across iterations. We can again factorize  $\mathbf{A}^\top \mathbf{A}$  once and use the factorization to compute the  $\mathbf{x}_1$ -update terms. An algorithm for solving basis pursuit problems with ADMM is given in algorithm 11.6.

```matlab
function basis_pursuit(A, b, x1, x2;  $\rho = 1$ ,  $\gamma = 2$ ,  $\epsilon r = 1e - 3$ )  
u = zeros(length(x2))  
r = fill(Inf, length(x2))  
F = factorize(A*A')  
C = I - A*(F \ \ A)  
d = A*(F \ \ b)  
S = (K, x) → max.(x \ \ -.\ K, 0) - max.(-x \ \ -.\ K, 0)  
while norm(r) >  $\epsilon r$   
x1' = C*(x2 - u) + d  
x2' = S(1/ $\rho$ , x1' + u)  
u' = u + x1' - x2'  
 $\rho * = \gamma$   
r = x1' - x2'  
x1, x2, u = x1', x2', u'  
end  
return x1  
end
```

Algorithm 11.6. ADMM applied to solve a basis pursuit problem parameterized by A and b; initial designs x1 and x2; initial penalty scalar  $\rho >0$  and penalty multiplier  $\gamma >1$ ; and convergence threshold  $\epsilon r$ .

# 11.5.5 Lasso

Linear regression is underdetermined when there are more features than data points. One common method for addressing both underdetermined problems and overfitting $^{20}$  the model to the data is to use regularization, where a penalty

20 Overfitting is discussed in more detail in section 17.5.

term is added to the objective function to prevent the solution from becoming too complex. If the penalty term is an  $L_{1}$  norm, it is known as the lasso.[21] The  $L_{1}$  term biases  $\mathbf{x}^{\star}$  toward sparse solutions, where entries in  $\mathbf{x}$  are encouraged to be zero, effectively downselecting the feature set. Lasso methods solve problems of the form:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \frac{1}{2} \| \mathbf {A x} - \mathbf {b} \| _{2}^{2} + \lambda \| \mathbf {x} \| _{1} \tag {11.83}
$$

where  $\lambda > 0$  scales the  $L_{1}$  regularization term. The lasso method increases  $\lambda$  until a solution with the desired sparsity is found.

An  $L_{1}$ -regularized linear regression problem can be written in ADMM form as:

$$
\underset{\mathbf {x}_{1}, \mathbf {x}_{2}} {\text{minimize}} \quad \frac{1}{2} \| \mathbf {A} \mathbf {x}_{1} - \mathbf {b} \| _{2}^{2} + \lambda \| \mathbf {x}_{2} \| _{1} \tag {11.84}
$$

$$
\text{subject} \quad \mathbf {x}_{1} - \mathbf {x}_{2} = \mathbf {0}
$$

with the ADMM update:[22]

$$
\mathbf {x}_{1}^{\prime} = \left(\mathbf {A}^{\top} \mathbf {A} + \rho \mathbf {I}\right) ^{- 1} \left(\mathbf {A}^{\top} \mathbf {b} + \rho \left(\mathbf {x}_{2} - \mathbf {u}\right)\right) \tag {11.85}
$$

$$
\mathbf {x}_{2}^{\prime} = S_{\frac{\lambda}{\rho}} \left(\mathbf {x}_{1}^{\prime} + \mathbf {u}\right) \tag {11.86}
$$

$$
\mathbf {u}^{\prime} = \mathbf {u} + \mathbf {x}_{1}^{\prime} - \mathbf {x}_{2}^{\prime} \tag {11.87}
$$

Algorithm 11.7 solves  $L_{1}$ -regularized linear regression problems with ADMM.

```txt
function l1.regularized(linear_regression( A,b,  $\lambda$  ,x1,x2;  $\rho = 1$ $\nu = 2$ $\epsilon r = 1e - 3)$ $\mathsf{u} =$  zeros(length(x2))   
r  $=$  fill(Inf,length(x2))   
 $S = (\kappa ,x)\rightarrow \max .(x\cdot -\kappa ,0)$  - max.(-x.-K,0)   
while norm(r) > er   
 $\begin{array}{rl} & {\mathrm{x1^{\prime} = (A^{\prime}A + \rho^{*}\mathrm{I})\setminus (A^{\prime}\mathrm{b} + \rho^{*}(x2 - u))}}\\ & {\mathrm{x2^{\prime} = S(\lambda / \rho,x1^{\prime} + u)}}\\ & {\mathrm{u^{\prime} = u + x1^{\prime} - x2^{\prime}}}\\ & {\mathrm{\rho * =V}}\\ & {\mathrm{r =x1^{\prime} - x2^{\prime}}}\\ & {\mathrm{x1,x2,u =x1^{\prime},x2^{\prime},u^{\prime}}}\\ & {\mathrm{end}}\\ & {\mathrm{returnx1}}\\ & {\mathrm{end}} \end{array}$
```

21 The name comes from "least absolute shrinkage and selection operator." R. Tibshirani, "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 58, no. 1, pp. 267-288, 1996.

22 The matrix  $\mathbf{A}^{\top}\mathbf{A} + \rho \mathbf{I}$  for  $\rho >0$  is always invertible.

Algorithm 11.7. ADMM applied to solve an  $L_{1}$ -regularized linear regression problem, central to lasso, parameterized by A, b, and  $\lambda$ ; initial design  $\times 1$  and  $\times 2$ ; initial penalty scalar  $\rho > 0$  and penalty multiplier  $\gamma > 1$ ; and convergence threshold  $\epsilon r$ .

# 11.6 Distributed Methods

ADMM is amendable to distributed computation across multiple processors to improve efficiency. Modern problems in fields as diverse as artificial intelligence, computational biology, and logistics often operate on immense, typically high-dimensional, datasets. Many of these problems only become tractable when they can be processed in a distributed manner. The ADMM form can naturally be parallelized, making solving large problems tractable.

# 11.6.1 Consensus

One form of consensus problem consists of a sum of  $k$  convex objectives:23

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f_{1} (\mathbf {x}) + \dots + f_{k} (\mathbf {x}) \tag {11.88}
$$

This problem can be written in ADMM form by producing  $k$  separate copies of  $\mathbf{x}_1$  and a single  $\mathbf{x}_2$  vector:

$$
\underset{\mathbf {x}_{1}^{(1: k)}, \mathbf {x}_{2}} {\text{minimize}} \quad \sum_{i = 1}^{k} f_{i} \left(\mathbf {x}_{1}^{(i)}\right) \tag {11.89}
$$

$$
\text{subject} \quad \mathbf {x}_{1}^{(i)} = \mathbf {x}_{2} \qquad \text{for} i \text{in} 1: k
$$

The copies of  $\mathbf{x}_1$  all represent the same design variable, which all must necessarily match each other when solved. At the beginning, when  $\rho$  is small, each  $\mathbf{x}_1^{(i)}$  will roughly independently minimize its own objective component. Consensus between the  $\mathbf{x}_1$ 's will ultimately be reached as  $\rho$  is increased, and all terms are penalized toward  $\mathbf{x}_2$ .

The ADMM update for a basic consensus problem is:[24]

$$
\begin{array}{l} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} = \underset{\mathbf {x}_{1}^{(i)}} {\arg \min } f_{i} \left(\mathbf {x}_{1}^{(i)}\right) + \left(\boldsymbol {\lambda}^{(i)}\right) ^{\top} \left(\mathbf {x}_{1}^{(i)} - \mathbf {x}_{2}\right) + \frac{1}{2} \rho \left\| \mathbf {x}_{1}^{(i)} - \mathbf {x}_{2} \right\| _{2}^{2} (11.90) \\ \mathbf {x}_{2}^{\prime} = \frac{1}{k} \sum_{i = 1}^{k} \left(\left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} + \frac{1}{\rho} \boldsymbol {\lambda}^{(i)}\right) \quad \text{consensusstep} (11.91) \\ \end{array}
$$

$$
\left(\boldsymbol {\lambda}^{(i)}\right) ^{\prime} = \boldsymbol {\lambda}^{(i)} + \rho \left(\left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} - \mathbf {x}_{2}^{\prime}\right) \tag {11.92}
$$

The primal and dual residuals are:

$$
\mathbf {r} = \left[ \begin{array}{c} \mathbf {x}_{1}^{(1)} - \frac{1}{k} \sum_{i = 1}^{k} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} \\ \vdots \\ \mathbf {x}_{1}^{(k)} - \frac{1}{k} \sum_{i = 1}^{k} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} \end{array} \right] \quad \mathbf {s} = - \rho \left[ \begin{array}{c} \left(\frac{1}{k} \sum_{i = 1}^{k} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime}\right) - \left(\frac{1}{k} \sum_{i = 1}^{k} \left(\mathbf {x}_{1}^{(i)}\right)\right) \\ \vdots \\ \left(\frac{1}{k} \sum_{i = 1}^{k} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime}\right) - \left(\frac{1}{k} \sum_{i = 1}^{k} \left(\mathbf {x}_{1}^{(i)}\right)\right) \end{array} \right] \tag {11.93}
$$

The  $\mathbf{x}_1^{(i)}$  updates can be parallelized across processes on a single machine, but can also be parallelized across multiple machines. Synchronization across processes or machines only requires that the inputs  $\mathbf{x}_1^{(i)}, \boldsymbol{\lambda}^{(i)}, \mathbf{x}_2,$  and  $\rho$  be passed back and forth. The implementations of each  $f_i$ , which may be large or contain sensitive information, need not be shared across update processes or the central consensus algorithm. As such, consensus approaches may not purely be motivated by speed.

An implementation is given in algorithm 11.8. The algorithm is demonstrated on a simple problem in figure 11.7.

# 11.6.2 General Consensus Optimization

We now consider the case where the local vectors  $\mathbf{x}_1^{(i)}$  need not contain full copies of  $\mathbf{x}_2$ . In many cases, individual  $\mathbf{x}_1^{(i)}$ -updates need only consider a subset of the full set of design variables.

In general consensus optimization, the objective continues to be separable with  $f(\mathbf{x}) = f_{1}(\mathbf{x}^{(1)}) + \dots + f_{k}(\mathbf{x}^{(k)})$ , but the local vectors may have different lengths. Each component of each local vector  $\mathbf{x}_{1}^{(i)}$  corresponds to a component of the global variable  $\mathbf{x}_{2}$ . More than one  $\mathbf{x}_{1}^{(i)}$  component may correspond to the same component of  $\mathbf{x}_{2}$ . When consensus is achieved, each component in  $\mathbf{x}_{1}^{(i)}$  will equal its corresponding component in  $\mathbf{x}_{2}$ . We let  $\mathbf{x}_{2}^{(i)}$  denote the vector of  $\mathbf{x}_{2}$  components associated with the local design  $\mathbf{x}_{1}^{(i)}$  such that  $\mathbf{x}_{1}^{(i)} = \mathbf{x}_{2}^{(i)}$  at convergence.[25]

We can formulate this problem as ADMM:

$$
\underset{\mathbf {x}_{1}^{(1: k)}, \mathbf {x}_{2}} {\text{minimize}} \quad \sum_{i = 1}^{k} f_{i} \left(\mathbf {x}_{1}^{(i)}\right) \tag {11.94}
$$

subject to  $\mathbf{x}_1^{(i)} = \mathbf{x}_2^{(i)}$  for  $i$  in  $1:k$

25 This is merely convenient notation, and does not introduce additional variables.

```julia
using BaseThreads   
function consensus(fs,x1s，x2;  $\rho = 1$  ，y=2，εr=1e-3，es=1e-3) k=length(fs)  $\lambda s =$  [zeros(size(x2)) for i in 1:k] x1s'=[zeros(size(x2))for i in 1:k] norm_r=Inf norm_s=Inf x1_bar  $=$  mean(x1s)   
while norm_r>er||norm_s>es @threads for i in 1:k f,  $\lambda =$  fs[i],  $\lambda s$  [i] obj=x1→f(x1)+  $\lambda \bullet (x1 - x2) + \rho /2*(x1 - x2)\cdot (x1 - x2)$  x1s'[i]=minimize(obj,x1s[i]) end x1_bar'  $\equiv$  mean(x1s') x2'  $\equiv$  x1_bar'+mean(λs)./p   
@threads for i in 1:k  $\lambda s[\mathrm{i}]\quad +=\rho *(x1s^{\prime}[\mathrm{i}] - x2^{\prime})$  end  $\rho *\gamma$    
norm_r=sqrt(sum(norm(x1s'[i]-x1_bar',2)^2 for i in 1:k)) norm_s  $\equiv$ $\rho *\text{sqrt}(k)*\text{norm}(x1\_ bar^{\prime} - x1\_ bar,2)$  x1s[：]，x2，x1_bar  $\equiv$  x1s'，x2'，x1_bar'   
end   
return x2   
end
```

Algorithm 11.8. Consensus ADMM for solving a convex optimization problem with additive costs fs. The algorithm additionally receives initial designs  $\times 1s$  and  $\times 2$ ; initial penalty scalar  $\rho > \theta$  and penalty multiplier  $\gamma > 1$ ; and convergence thresholds  $\epsilon r$  and  $\epsilon s$ .

This method executes its updates in parallel across available threads using the @threads macro. Julia can be started with multiple threads, for example with julia -t 4, or with the environment variable JULIA_NUMThreads.

For an overview of how to implement ADMM in distributed computing environments, see chapter 10 of S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers," Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1-122, 2011.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/704d15d948b57345368a0f0d319fb0a3b51eab64278708b4badcfbd5cc82ebf2.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/baf70a6641067b8e4863f19622e21711e68e87036c81a1166dbc31e54c900f64.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1201328a77e6b271a0194bce8c47536dcad94862c9552cb04a9be0e6071bf2b9.jpg)  
Figure 11.7. ADMM being used to solve a consensus problem in which we find a point that minimizes the sum of the  $L_{2}$  distances to a set of points (black). The algorithm proceeds left to right, and top to bottom. The analytical solution will naturally be the mean of the points.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1e83ddb187848554f21481c5170811a7adddc1352315253fb51f3eb216c50295.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cf09f76d74dc27e4ef651113b40074932a9e89a2255c36748a45c7423fa0af8c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f59a49613528c8de1b58864fc867cf92e7db91d2015968b06e221cd32be60665.jpg)  
The designs (blue) and  $\mathbf{x}_2$  (red) are initialized to the same value. There is a separate design for each target point, with each design's objective minimizing the distance to its target point. In the first iteration, each design snaps to its target. In subsequent iterations, as  $\rho$  increases, the designs converge to  $\mathbf{x}_2$ , which resides at the mean.

The ADMM update is:

$$
\begin{array}{l} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} = \underset{\mathbf {x}_{1}^{(i)}} {\arg \min } f_{i} \left(\mathbf {x}_{1}^{(i)}\right) + \left(\boldsymbol {\lambda}^{(i)}\right) ^{\top} \left(\mathbf {x}_{1}^{(i)} - \mathbf {x}_{2}^{(i)}\right) + \frac{1}{2} \rho \left\| \mathbf {x}_{1}^{(i)} - \mathbf {x}_{2}^{(i)} \right\| _{2}^{2} (11.95) \\ \left. \left(\mathbf {x}_{2}\right) _{g}^{\prime} = \frac{1}{n_{g}} \sum_{i = 1}^{k} \left(\sum_{j} \left(\mathbf {x}_{1}^{(i)}\right) _{j}^{\prime} \text{wherecomponent} j \text{correspondsto} \left(\mathbf {x}_{2}\right) _{g}^{\prime}\right) \right. (11.96) \\ \text{for} g \text{in} 1: | \mathbf {x}_{2} | (11.97) \\ \end{array}
$$

$$
\left(\boldsymbol {\lambda}^{(i)}\right) ^{\prime} = \boldsymbol {\lambda}^{(i)} + \rho \left(\left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} - \left(\mathbf {x}_{2}^{(i)}\right) ^{\prime}\right) \tag {11.98}
$$

where  $n_g$  is the number of components across all  $\mathbf{x}_1$  designs that correspond to the  $g$ th component of  $\mathbf{x}_2$ . The  $\mathbf{x}_1$  and  $\lambda$  updates can again be executed in parallel.

The  $x_2$ -update continues to average over the local vectors, but because each local vector only contains certain components, it will only average the entries that correspond to each global component. Here,  $k_g$  is the number of local vector entries for the  $g$ th global component. Example 11.7 shows how a problem can be formulated as a global consensus problem.

Suppose we are trying to place 3 bakeries in order to best serve 6 establishments:

$$
\text{twocafesat} \mathbf {e}^{(1)} = [ 0. 0, 0. 0 ] \text{and} \mathbf {e}^{(2)} = [ 0. 5, 0. 0 ]
$$

$$
\text{twobrunchspotsat} \mathbf {e}^{(3)} = [ 3. 0, 2. 0 ] \text{and} \mathbf {e}^{(4)} = [ 0. 5, 3. 0 ]
$$

$$
\text{two} \mathbf {e}^{(5)} = [ 3. 5, - 0. 5 ] \text{and} \mathbf {e}^{(6)} = [ 2. 5, 0. 7 ]
$$

One bakery, at  $\mathbf{b}^{(1)}$ , produces croissants, which are needed at cafes and brunch spots. Another bakery, at  $\mathbf{b}^{(2)}$ , produces baguettes, which are needed at brunch spots and restaurants. The third bakery, at  $\mathbf{b}^{(3)}$ , produces dinner rolls, which are only needed at restaurants. We want to minimize the distance from each establishment to its farthest compatible bakery:

$$
\underset{\mathbf {b}^{(1: 3)}} {\text{minimize}} \quad \left\| \mathbf {e}^{(1)} - \mathbf {b}^{(1)} \right\| + \| \mathbf {e}^{(2)} - \mathbf {b}^{(1)} \| +
$$

$$
\max (\| \mathbf {e}^{(3)} - \mathbf {b}^{(1)} \|, \| \mathbf {e}^{(3)} - \mathbf {b}^{(2)} \|) +
$$

$$
\max \left(\| \mathbf {e}^{(4)} - \mathbf {b}^{(1)} \|, \| \mathbf {e}^{(4)} - \mathbf {b}^{(2)} \|\right) +
$$

$$
\max \left(\| \mathbf {e}^{(5)} - \mathbf {b}^{(2)} \|, \| \mathbf {e}^{(5)} - \mathbf {b}^{(3)} \|\right) +
$$

$$
\max \left(\| \mathbf {e}^{(6)} - \mathbf {b}^{(2)} \|, \| \mathbf {e}^{(6)} - \mathbf {b}^{(3)} \|\right)
$$

We can formulate this problem as a general consensus optimization problem with  $\mathbf{x}_2 = [b_1^{(1)}, b_2^{(1)}, b_1^{(2)}, b_2^{(2)}, b_1^{(3)}, b_2^{(3)}]$ :

$$
\begin{array}{l} \underset{\mathbf {x}_{1}^{(1: 6)}, \mathbf {x}_{2}} {\text{minimize}} \| \mathbf {e}^{(1)} - \mathbf {x}_{1}^{(1)} \| + \| \mathbf {e}^{(2)} - \mathbf {x}_{1}^{(2)} \| + \\ \max (\| \mathbf {e}^{(3)} - (\mathbf {x}_{1}^{(3)}) _{1: 2} \|, \| \mathbf {e}^{(3)} - (\mathbf {x}_{1}^{(3)}) _{3: 4} \|) + \\ \max (\| \mathbf {e}^{(4)} - (\mathbf {x}_{1}^{(4)}) _{1: 2} \|, \| \mathbf {e}^{(4)} - (\mathbf {x}_{1}^{(4)}) _{3: 4} \|) + \\ \max (\| \mathbf {e}^{(5)} - (\mathbf {x}_{1}^{(5)}) _{1: 2} \|, \| \mathbf {e}^{(5)} - (\mathbf {x}_{1}^{(5)}) _{1: 2} \|) + \\ \max (\| \mathbf {e}^{(6)} - (\mathbf {x}_{1}^{(6)}) _{1: 2} \|, \| \mathbf {e}^{(6)} - (\mathbf {x}_{1}^{(6)}) _{1: 2} \|) \\ \end{array}
$$

$$
\text{subject} \quad \mathbf {x}_{1}^{(1)} = (\mathbf {x}_{2}) _{1: 2} \quad \mathbf {x}_{1}^{(2)} = (\mathbf {x}_{2}) _{1: 2} \quad \mathbf {x}_{1}^{(3)} = (\mathbf {x}_{2}) _{1: 4}
$$

$$
\mathbf {x}_{1}^{(4)} = (\mathbf {x}_{2}) _{1: 4} \quad \mathbf {x}_{1}^{(5)} = (\mathbf {x}_{2}) _{3: 6} \quad \mathbf {x}_{1}^{(6)} = (\mathbf {x}_{2}) _{3: 6}
$$

Example 11.7. A bakery placement problem formulated as a general consensus problem. A solution is plotted below.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fae22ecdd1f9115207f64a8d773654d0b17bad69eb76103524e07737cfd796d3.jpg)  
cafe  
- brunch  
$\triangle$  restaurant  
$\bigcirc$  croissant  
baguette  
○ dinner roll

# 11.7 Summary

- A constrained optimization problem has a dual problem formulation that can be easier to solve and whose solution is a lower bound of the solution to the original problem.  
- The primal-dual method for inequality constrained problems directly seeks to satisfy the KKT conditions by optimizing the primal and dual variables simultaneously.  
- Dual ascent is a technique for alternating between optimizing the primal and dual variables.  
- The Alternating Direction Method of Multipliers is an extension of the method of multipliers and dual ascent for solving convex problems in a special canonical form.  
- ADMM can be applied to any constrained optimization problem.  
- ADMM proceeds by optimizing two objectives independently, but eventually reaches agreement by penalizing the separate design vectors toward one another over time.  
- Multiple applications for ADMM were presented, giving rise to efficient algorithms simply by applying the ADMM machinery.  
- Consensus algorithms can allow for parallelizing ADMM and running it in a decentralized manner.

# 11.8 Exercises

Exercise 11.1. Show how complementary slackness, equation (10.26), must hold for an optimal  $(\mathbf{x}^{*},\boldsymbol{\mu}^{*},\boldsymbol{\lambda}^{*})$  in a problem with a duality gap of zero.

Solution: A problem with a duality gap of zero will have equal primal and dual values:

$$
p^{*} = f (\mathbf {x}^{*}) = \mathcal {D} (\boldsymbol {\mu}^{*}, \boldsymbol {\lambda}^{*}) = d^{*}
$$

We know that the dual function is:

$$
\mathcal {D} (\boldsymbol {\mu}^{*}, \boldsymbol {\lambda}^{*}) = \underset{\mathbf {x}} {\operatorname{minimize}} \left(f (\mathbf {x}) + \sum_{i} \mu_{i}^{*} g_{i} (\mathbf {x}) + \sum_{j} \lambda_{j}^{*} h_{j} (\mathbf {x})\right)
$$

The dual function is a lower bound on the Lagrangian:

$$
\underset{\mathbf {x}} {\operatorname{minimize}} \left(f (\mathbf {x}) + \sum_{i} \mu_{i}^{*} g_{i} (\mathbf {x}) + \sum_{j} \lambda_{j}^{*} h_{j} (\mathbf {x})\right) \leq f \left(\mathbf {x}^{*}\right) + \sum_{i} \mu_{i}^{*} g_{i} \left(\mathbf {x}^{*}\right) + \sum_{j} \lambda_{j}^{*} h_{j} \left(\mathbf {x}^{*}\right)
$$

Since we have zero duality gap, we have:

$$
f \left(\mathbf {x}^{*}\right) + \sum_{i} \mu_{i}^{*} g_{i} \left(\mathbf {x}^{*}\right) + \sum_{j} \lambda_{j}^{*} h_{j} \left(\mathbf {x}^{*}\right) = f \left(\mathbf {x}^{*}\right)
$$

and since each  $h_j(\mathbf{x}^*) = 0$  and each  $\mu_i^* g_i(\mathbf{x}^*) \leq 0$  at a solution, we conclude that complementary slackness must hold:

$$
\sum_{i} \mu_{i}^{*} g_{i} (\mathbf {x}^{*}) = 0 \text{for all} i
$$

Exercise 11.2. Is the dual problem for a dual problem the same as the primal problem?

Solution: Suppose we have a primal problem with value  $p^*$  and its dual with value  $d^*$ , and then we construct the dual of the dual problem with value  $dd^*$ . From the max-min inequality, we know that  $d^* \leq p^*$  and that  $dd^* \leq d^*$ , so  $dd^* \leq p^*$ . As such, if there is a duality gap, the dual of a dual will not be equal to the primal problem. Hence, the dual of a dual is not necessarily the same as the primal problem.

Exercise 11.3. Consider the  $x_{1}$ -update for an ADMM problem with a quadratic objective:

$$
f_{1} (\mathbf {x}_{1}) = \frac{1}{2} \mathbf {x}_{1}^{\top} \mathbf {Q} \mathbf {x}_{1} + \mathbf {q}^{\top} \mathbf {x}_{1} + q
$$

with positive semidefinite  $\mathbf{Q}$ . What is the update's analytic solution?

Solution: The update can be written:

$$
\underset{\mathbf {x}_{1}} {\arg \min } \frac{1}{2} \mathbf {x}_{1}^{\top} \mathbf {Q} \mathbf {x}_{1} + \mathbf {q}^{\top} \mathbf {x}_{1} + q + \lambda^{\top} \mathbf {x}_{1} + \frac{1}{2} \rho \| \mathbf {A} \mathbf {x}_{1} + \mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b} \|
$$

We substitute  $\mathbf{v} = \mathbf{q} + \lambda$  and  $\mathbf{w} = \mathbf{A}_2\mathbf{x}_2 - \mathbf{b}$ . The  $\mathbf{x}_1$ -update becomes:

$$
\underset{\mathbf {x}_{1}} {\arg \min } \frac{1}{2} \mathbf {x}_{1}^{\top} \mathbf {Q} \mathbf {x}_{1} + \mathbf {v}^{\top} \mathbf {x}_{1} + q + \frac{1}{2} \rho \| \mathbf {A}_{1} \mathbf {x}_{1} + \mathbf {w} \|
$$

$$
\arg \min_{\mathbf {x}_{1}} \frac{1}{2} \mathbf {x}_{1}^{\top} \mathbf {Q} \mathbf {x}_{1} + \mathbf {v}^{\top} \mathbf {x}_{1} + \frac{1}{2} \rho \mathbf {x}_{1}^{\top} \mathbf {A}_{1}^{\top} \mathbf {A}_{1} \mathbf {x}_{1} - \rho \mathbf {w}^{\top} \mathbf {A}_{1} \mathbf {x}_{1} + \frac{1}{2} \rho \mathbf {w}^{\top} \mathbf {w}
$$

$$
\underset{\mathbf {x}_{1}} {\arg \min } \frac{1}{2} \mathbf {x}_{1}^{\top} \left(\mathbf {Q} + \rho \mathbf {A}_{1}^{\top} \mathbf {A}_{1}\right) \mathbf {x}_{1} + \left(\mathbf {v} - \rho \mathbf {A}_{1}^{\top} \mathbf {w}\right) ^{\top} \mathbf {x}_{1}
$$

The updated design  $\mathbf{x}_1^{\prime}$  can thus be found by solving:

$$
\left(\mathbf {Q} + \rho \mathbf {A}_{1}^{\top} \mathbf {A}_{1}\right) \mathbf {x}_{1}^{\prime} = \left(\rho \mathbf {A}_{1}^{\top} \left(\mathbf {A}_{2} \mathbf {x}_{2} - \mathbf {b}\right) - \mathbf {q} - \lambda\right)
$$

Running ADMM on this problem thus requires repeatedly solving similar systems of linear equations  $\mathbf{Q}_{\rho}\mathbf{x}_1' = \mathbf{q}_{\rho}$ . Such a linear system is typically solved by computing the Cholesky decomposition  $\mathbf{L}\mathbf{L}^{\top} = \mathbf{Q}_{\rho}$  and then efficiently solving  $\mathbf{L}\mathbf{L}^{\top}\mathbf{x}_1' = \mathbf{q}_{\rho}$ . We can save computation by limiting how often  $\rho$  is updated, thereby allowing the factorization for  $\mathbf{Q}_{\rho}$  to be reused.[26]

Exercise 11.4. Show that the dual residual, equation (11.39), measures how close the design is to satisfying stationarity in  $\mathbf{x}_1$ .

Solution: The  $\mathbf{x}_1$  update minimizes  $\mathcal{L}_{\rho}\left(\mathbf{x}_1, \mathbf{x}_2^{(k)}, \lambda^{(k)}\right)$ . As such, the resulting  $\mathbf{x}_1^{(k+1)}$  satisfies

$$
\begin{array}{l} \mathbf {0} = \nabla_{\mathbf {x}_{1}} \mathcal {L}_{\rho} \left(\mathbf {x}_{1}^{(k + 1)}, \mathbf {x}_{2}^{(k)}, \boldsymbol {\lambda}^{(k)}\right) \\ = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \boldsymbol {\lambda}^{(k)} + \rho \mathbf {A}_{1}^{\top} \left(\mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k)} - \mathbf {b}\right) \\ = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \left(\boldsymbol {\lambda}^{(k)} + \rho \mathbf {r}^{(k + 1)} - \rho \mathbf {r}^{(k + 1)} + \rho \left(\mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k)} - \mathbf {b}\right)\right) \\ = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \left(\boldsymbol {\lambda}^{(k)} + \rho \mathbf {r}^{(k + 1)} - \rho \left(\mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k + 1)} - \mathbf {b}\right) + \rho \left(\mathbf {A}_{1} \mathbf {x}_{1}^{(k + 1)} + \mathbf {A}_{2} \mathbf {x}_{2}^{(k)} - \mathbf {b}\right)\right) \\ = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \left(\boldsymbol {\lambda}^{(k)} + \rho \mathbf {r}^{(k + 1)} + \rho \mathbf {A}_{2} \left(\mathbf {x}_{2}^{(k)} - \mathbf {x}_{2}^{(k + 1)}\right)\right) \\ = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \left(\boldsymbol {\lambda}^{(k + 1)} + \rho \mathbf {A}_{2} \left(\mathbf {x}_{2}^{(k)} - \mathbf {x}_{2}^{(k + 1)}\right)\right) \\ = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \boldsymbol {\lambda}^{(k + 1)} + \rho \mathbf {A}_{1}^{\top} \mathbf {A}_{2} \left(\mathbf {x}_{2}^{(k)} - \mathbf {x}_{2}^{(k + 1)}\right) \\ \end{array}
$$

Solving and rearranging produces:

$$
\rho \mathbf {A}_{1}^{\top} \mathbf {A}_{2} \left(\mathbf {x}_{2}^{(k + 1)} - \mathbf {x}_{2}^{(k)}\right) = \nabla_{\mathbf {x}_{1}} f_{1} \left(\mathbf {x}_{1}^{(k + 1)}\right) + \mathbf {A}_{1}^{\top} \boldsymbol {\lambda}^{(k + 1)}
$$

The left hand side of the above equation is the dual residual, and the right hand side is the gradient of the unmodified Lagrangian, which must be zero when stationarity is satisfied. This means the dual residual measures how close the design is to satisfying stationarity in  $\mathbf{x}_1$  for the original problem equation (11.30).

Exercise 11.5. Prior to Dykstra's method for alternating projections (section 11.5.2), John von Neumann used a simpler algorithm for finding a point in the intersection of two affine sets that does not use the dual variable  $\mathbf{u}$ :<sup>27</sup>

$$
\mathbf{x}_{1}^{(k + 1)} = \operatorname *{arg  min}_{\mathbf{x}_{1}\in \mathcal{X}_{1}}\| \mathbf{x}_{1} - \mathbf{x}_{2}^{(k)}\|
$$

$$
\mathbf{x}_{2}^{(k + 1)} = \operatorname *{arg  min}_{\mathbf{x}_{2}\in \mathcal{X}_{2}}\| \mathbf{x}_{1}^{(k + 1)} - \mathbf{x}_{2}\|
$$

where  $\mathcal{X}_1 = \{\mathbf{x}|\mathbf{C}_1\mathbf{x}\leq \mathbf{d}_1\}$  and  $\mathcal{X}_2 = \{\mathbf{x}|\mathbf{C}_2\mathbf{x}\leq \mathbf{d}_2\}$ .

26 For more examples of how structure can be exploited to improve efficiency, see section 4.2 of S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers," Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1-122, 2011.

27 J. Von Neumann, "On Rings of Operators. Reduction Theory," Annals of Mathematics, pp. 401-485, 1949.

Run both Dykstra's and von Neumann's alternating projections algorithms on the sets formed by

$$
\mathbf {C}_{1} = \left[ \begin{array}{c c} 3. 0 & 1. 0 \\ 0. 5 & - 1. 0 \end{array} \right]
$$

$$
\mathbf {d}_{1} = \left[ \begin{array}{c} - 3. 5 \\ 3. 5 \end{array} \right]
$$

$$
\mathbf {C}_{2} = \left[ \begin{array}{c c} - 1. 5 & 1. 0 \\ - 0. 9 & - 1. 0 \end{array} \right]
$$

$$
\mathbf {d}_{2} = \left[ \begin{array}{c} - 2. 75 \\ 3. 95 \end{array} \right]
$$

from  $\mathbf{x}_1^{(0)} = \mathbf{x}_2^{(0)} = \mathbf{0}$ . Does the dual variable help accelerate convergence?

Solution: Each algorithm's progression is shown below:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c569ed8ecf99b5e1672ce0ed1c0dd172268a74a3f0c8e6581050a43bf5397a47.jpg)  
Von Neumann's Alternating Projections

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/056c7e80e0377b5dd29452cbbf6ab506eb63e4c86201afb8f8941ff12fa6ea83.jpg)  
Dykstra's Alternating Projections

Von Neumann's alternating projections method is slower to converge in narrow canyons, where the algorithm has to slowly zig-zag its way toward a feasible point. The ADMM approach, which is equivalent to Dykstra's alternating projections method, has the same first iteration, but the dual vector  $\mathbf{u}$  acts as a momentum term that propels subsequent iterations.

Exercise 11.6. Show how the ADMM update for a consensus problem given in equations (11.90) to (11.90) can be simplified, removing the  $\mathbf{x}_2$ -update entirely. As a hint, replace  $(1 / k)\sum_{i = 1}^{k}\mathbf{x}_{1}^{(i)}$  with  $\bar{\mathbf{x}}_1$ , and show that the  $\mathbf{x}_2$ -update can be written as  $\mathbf{z}_2^{\prime} = \bar{\mathbf{x}}_1^{\prime}$ .

Solution: The  $\mathbf{x}_2$ -update for a consensus problem:

$$
\mathbf {x}_{2}^{\prime} = \frac{1}{k} \sum_{i = 1}^{k} \left(\left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} + \frac{1}{\rho} \boldsymbol {\lambda}^{(i)}\right)
$$

can be simplified to:

$$
\mathbf {x}_{2}^{\prime} = \bar {\mathbf {x}}_{1}^{\prime} + \frac{1}{\rho} \bar {\boldsymbol {\lambda}}
$$

The average over the dual variables,  $\bar{\lambda}$ , is updated according to:

$$
\begin{array}{l} \bar {\lambda}^{\prime} = \bar {\lambda} + \rho \left(\bar {\mathbf {x}}_{1}^{\prime} - \mathbf {x}_{2}^{\prime}\right) \\ = \bar {\lambda} + \rho \left(\bar {x}_{1}^{\prime} - \left(\bar {x}_{1}^{\prime} + \frac{1}{\rho} \bar {\lambda}\right)\right) \\ = 0 \\ \end{array}
$$

As such, we can drop the second term from the  $\mathbf{x}_2$ -update and use  $\mathbf{x}_2' = \bar{\mathbf{x}}_1'$ , resulting in the ADMM update:

$$
\begin{array}{l} \left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} = \underset{\mathbf {x}_{1}} {\arg \min } \left(f_{i} \left(\mathbf {x}_{1}^{(i)}\right) + \left(\boldsymbol {\lambda}^{(i)}\right) ^{\top} \left(\mathbf {x}_{1}^{(i)} - \bar {\mathbf {x}}_{1}\right) + \frac{1}{2} \rho \left\| \mathbf {x}_{1}^{(i)} - \bar {\mathbf {x}}_{1} \right\| _{2}^{2}\right) \\ \left(\boldsymbol {\lambda}^{(i)}\right) ^{\prime} = \boldsymbol {\lambda}^{(i)} + \rho \left(\left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} - \bar {\mathbf {x}}_{1}^{\prime}\right) \\ \end{array}
$$

Exercise 11.7. Suppose we have run a shortest-path algorithm on a constrained triangle mesh, and now have a path comprised of tile traversals, using the center of each triangle edge. We now wish to locally optimize this path to find the shortest path that continues to pass through the same triangles as before.[28] How can this problem be framed using general consensus optimization?[29]

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1fb36cb7a0cb7ccdeed271790438c32f1334c2f16a8cd579dad3fb427e2ba6b0.jpg)

Solution: To formulate this problem to be solved using general consensus, we first need to formulate it as an optimization problem. We wish to minimize the total path length. Our path traverses through a series of predefined triangles. We are free, however, to change where in these triangles we traverse.

Let  $\mathbf{p}^{(0)}$  be the starting vertex,  $\mathbf{p}^{(n + 1)}$  be the destination vertex, and  $\mathbf{p}^{(i)}$  for  $i$  in  $1:n$  be the  $n$  vertices at the triangle edges that our path must traverse. We can slide each  $\mathbf{p}^{(i)}$  between its leftmost and rightmost value:

$$
\mathbf {p}^{(i)} = \alpha_{i} \mathbf {p}_{\ell}^{(i)} + (1 - \alpha_{i}) \mathbf {p}_{r}^{(i)}
$$

where each interpolant  $\alpha_{i}$  lies in the unit bounds,  $\alpha_{i} \in [0,1]$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/64c037256ee5fe6e277f7b145cba89d4b227972b497a17b90abd8620721c6433.jpg)

28 This local form of path optimization is sometimes referred to as an elastic band optimization. We can imagine the path being turned into a tightened elastic band that stretches and presses against its bounds, forming the shortest path.

29 The overhead associated with parallelizing this problem is likely not worth it beyond its merits as an academic exercise, particularly because this problem can be formulated as a quadratic program and thus can be solved very efficiently. However, variations on this problem, such as using connecting segments that respect vehicle dynamic limits and motion constraints like maximum turning rates can quickly increase problem complexity, which can make general consensus optimization worthwhile.

The total path length is

$$
\sum_{i = 0}^{n} \left\| \mathbf {p}^{(i + 1)} - \mathbf {p}^{(i)} \right\| _{2}
$$

Putting this together, our optimization problem is

$$
\underset{\boldsymbol {\alpha}_{1: n}} {\text{minimize}} \quad \sum_{i = 0}^{n} \left\| \mathbf {p}^{(i + 1)} - \mathbf {p}^{(i)} \right\| _{2}
$$

$$
\text{subject} \quad \mathbf {p}^{(i)} = \alpha_{i} \mathbf {p}_{\ell}^{(i)} + (1 - \alpha_{i}) \mathbf {p}_{r}^{(i)}
$$

$$
0 \leq \alpha \leq 1
$$

This problem can be framed using general consensus. The objective function is already broken apart into components, with each component requiring two interpolants:

$$
\begin{array}{l} f_{i} \left(\alpha_{i}, \alpha_{i + 1}\right) = \left\| \mathbf {p}^{(i + 1)} - \mathbf {p}^{(i)} \right\| _{2} \\ = \left\| \left(\alpha_{i + 1} \mathbf {p}_{\ell}^{(i + 1)} + (1 - \alpha_{i + 1}) \mathbf {p}_{r}^{(i + 1)}\right) - \left(\alpha_{i} \mathbf {p}_{\ell}^{(i)} + (1 - \alpha_{i}) \mathbf {p}_{r}^{(i)}\right) \right\| _{2} \\ \end{array}
$$

The interpolants must all lie within the unit bounds. This can be enforced as an additional objective:

$$
f_{n + 1} (\boldsymbol {\alpha}) = \left\{ \begin{array}{l l} \infty & \text{if } \mathbf {0} \leq \boldsymbol {\alpha} \leq 1 \\ 0 & \text{otherwise} \end{array} \right.
$$

However, it is often simpler to enforce the unit bounds in the  $\mathbf{x}_1^{(i)}$  -updates:

$$
\left(\mathbf {x}_{1}^{(i)}\right) ^{\prime} = \underset{\mathbf {0} \leq \mathbf {x}_{1} \leq 1} {\arg \min } f_{i} (\mathbf {x}_{1}^{(i)}) + \left(\boldsymbol {\lambda}^{(i)}\right) ^{\top} \left(\mathbf {x}_{1}^{(i)} - \mathbf {x}_{2}^{(i)}\right) + \frac{1}{2} \rho \left\| \mathbf {x}_{1}^{(i)} - \mathbf {x}_{2}^{(i)} \right\| _{2}^{2}
$$

In general consensus, we would maintain a global copy of the interpolants,  $\mathbf{x}_2 = \alpha$ . Each component would then optimize a local vector  $\mathbf{x}_1^{(i)}$  that only contains entries corresponding to relevant components.

Exercise 11.8. Consider a simplified model of economic trade in the United States with three commodities: artichokes, oranges, and potatoes. The states California, Florida, and Idaho each produce some quantity of these commodities every year:

$$
\mathbf {p}_{\mathrm{C}} = [ 3. 0, 1. 5, 1. 5 ]
$$

$$
\mathbf {p}_{F} = [ 0. 0, 3. 0, 0. 5 ]
$$

$$
\mathbf {p}_{I} = [ 0. 0, 0. 0, 5. 0 ]
$$

Each state can give commodities to or receive commodities from a central exchange. Let the amount of commodities received by a state be represented by  $\mathbf{x}$ . For example,  $\mathbf{x}_{\mathrm{C}} = [-0.6, 0.2, 0.3]$  would mean that California provides 0.6 units of artichokes to the exchange, but then receives 0.2 units of oranges and 0.3 units of potatoes. The exchange must enforce equilibrium, with the net inflow of every commodity matching its outflow:

$$
\sum_{s \in (C, F, I)} \mathbf {x}_{s} = \mathbf {0}
$$

Each state desires all commodities according to a sigmoid utility function

$$
u (c) = \frac{2}{1 + e^{- c}} - 1
$$

where  $c = p + x$  is the amount of a particular commodity that the state ends up with. A state's total utility depends both on the amount of a particular commodity and the total amount of commodities:

$$
U (\mathbf {c}) = u \left(c_{\text{artichokes}}\right) + u \left(c_{\text{oranges}}\right) + u \left(c_{\text{potatoes}}\right) + u \left(\| \mathbf {c} \|\right)
$$

Express this problem in ADMM form, despite the fact that the utility function is nonconvex. What is its ADMM update, in scaled form?

Solution: Our optimization problem is

$$
\underset{\mathbf {x}} {\text{maximize}} \quad \sum_{s \in \mathcal {S}} U \left(\mathbf {p}_{s} + \mathbf {x}_{s}\right)
$$

$$
\text{subject} \quad \sum_{s \in \mathcal {S}} x_{s} = 0
$$

where  $\mathcal{S} = \{C,F,I\}$  is the set of states.

The objective function is nonconvex. Nevertheless, the corresponding ADMM form is

$$
\underset{\mathbf {x}, \mathbf {z}} {\text{minimize}} \quad \left(\sum_{s \in \mathcal {S}} - U (\mathbf {p}_{s} + \mathbf {x}_{s})\right) + \left(\sum_{s \in \mathcal {S}} \mathbf {z}_{s} = \mathbf {0}\right) \cdot \infty
$$

$$
\text{subject} \quad \mathbf {x} - \mathbf {z} = \mathbf {0}
$$

where we have used  $\mathbf{x}$  and  $\mathbf{z}$  instead of  $\mathbf{x}_1$  and  $\mathbf{x}_2$  for notational convenience.

The ADMM update for this problem in scaled form is thus

$$
\mathbf {x}_{s}^{(k + 1)} = \underset{\mathbf {x}} {\arg \min } - U (\mathbf {p}_{s} + \mathbf {x}) + \frac{1}{2} \rho \sum_{s \in \mathcal {S}} \left\| \mathbf {x}_{s} - \mathbf {z}_{s}^{(k)} + \mathbf {u}_{s}^{(k)} \right\| _{2}^{2}
$$

$$
\mathbf {z}^{(k + 1)} = \underset{\mathbf {z} | \sum_{s \in \mathcal {S}} \mathbf {z}_{s} = \mathbf {0}} {\arg \min } \frac{1}{2} \rho \sum_{s \in \mathcal {S}} \left\| \mathbf {x}_{s}^{(k + 1)} - \mathbf {z}_{s} + \mathbf {u}_{s}^{(k)} \right\| _{2}^{2}
$$

$$
\mathbf {u}_{s}^{(k + 1)} = \mathbf {u}_{s}^{k} + \mathbf {x}_{s}^{(k + 1)} - \mathbf {z}_{s}^{(k + 1)}
$$

Exercise 11.9. We can simplify the  $\mathbf{z}$ -update for the exchange problem in the previous exercise by noticing that solving the  $\mathbf{z}$ -update produces:

$$
\mathbf {z}_{s}^{(k + 1)} = \mathbf {x}_{s}^{(k + 1)} + \mathbf {u}_{s}^{(k)} + \frac{1}{| \mathcal {S} |} \sum_{t \in \mathcal {S}} \left(\mathbf {x}_{t}^{(k + 1)} + \mathbf {u}_{t}^{(k)}\right) = \mathbf {x}_{s}^{(k + 1)} + \mathbf {u}_{s}^{(k)} + \bar {\mathbf {x}}^{(k + 1)} + \bar {\mathbf {u}}^{(k)}
$$

Make this substitution and derive a simplified ADMM update that neither requires a  $z$ -update nor separate  $\mathbf{u}$  vectors for each state. Can any aspects of this problem be solved in parallel?

Solution: Substituting the updated  $\mathbf{z}$  vectors into the  $u$ -update produces:

$$
\begin{array}{l} \mathbf {u}_{\mathrm{s}}^{(k + 1)} = \mathbf {u}_{\mathrm{s}}^{k} + \mathbf {x}_{\mathrm{s}}^{(k + 1)} - \mathbf {z}_{\mathrm{s}}^{(k + 1)} \\ = \mathbf {u}_{s}^{k} + \mathbf {x}_{s}^{(k + 1)} - \left(\mathbf {x}_{s}^{(k + 1)} + \mathbf {u}_{s}^{(k)} + \bar {\mathbf {x}}^{(k + 1)} + \bar {\mathbf {u}}^{(k)}\right) \\ = \bar {\mathbf {u}}^{(k)} + \bar {\mathbf {x}}^{(k + 1)} \\ \end{array}
$$

from which we can see that the  $\mathbf{u}_s$  values are all equal. We can thus use a single  $\mathbf{u}$ .

The final procedure for this exchange problem is:

$$
\begin{array}{l} \mathbf {x}_{s}^{(k + 1)} = \underset{\mathbf {x}} {\arg \min } - U (\mathbf {p}_{s} + \mathbf {x}) + \frac{1}{2} \rho \left\| \mathbf {x}_{s} - \mathbf {x}_{s}^{(k)} + \bar {\mathbf {x}}^{(k)} + \mathbf {u}^{(k)} \right\| _{2}^{2} \\ \mathbf {u}^{(k + 1)} = \mathbf {u}^{k} + \bar {\mathbf {x}}_{\mathrm{s}}^{(k + 1)} \\ \end{array}
$$

for which the  $x$ -updates can all be carried out in parallel.

Exercise 11.10. Solve the exchange problem from the previous two exercises.

Solution: The objective function is not convex, but descent methods can still be used to solve the  $x$ -updates within the ADMM procedure.

Solving the problem yields:

$$
\begin{array}{l} \mathbf {x}_{C} = \left[ \begin{array}{l l} - 0. 44 8, - 0. 03 5, & 0. 08 0 \end{array} \right] \\ \mathbf {x}_{F} = \left[ \begin{array}{c c} 0. 22 9, - 0. 28 2, & 0. 37 3 \end{array} \right] \\ \mathbf {x}_{I} = \left[ \begin{array}{c c} 0. 21 9, & 0. 31 7, - 0. 45 3 \end{array} \right] \\ \end{array}
$$

The goods received by each state as the algorithm progresses is presented below. We can see how each state primarily trades away the good that it has in excess in order to receive goods that it lacks from other states. The initial quantities do not sum to zero, but as the algorithm progresses and  $\rho$  increases, that constraint is increasingly enforced.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cddee36a39e819b537392d06e1da7ff48c07646831304355faedf9803725b9be.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bbdb1d104029e3d32d56da25822410d6f0f4b5f031b61df8a08f24f046b5f7f1.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/24850c7509be06daa0a4dbf1324a510d94bdb922b4a77441712d10f9cba14772.jpg)

Exercise 11.11. Consider a problem of the form<sup>30</sup>

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathbf {x})
$$

$$
\text{subject} \quad \mathbf {g} (\mathbf {x}) \leq \mathbf {1}
$$

$$
\mathbf {h} (\mathbf {x}) = \mathbf {1}
$$

$$
x > 0
$$

where each equality constraint function has the form

$$
h_{i} (\mathbf {x}) = c_{i} x_{1}^{a_{i 1}} x_{2}^{a_{i 2}} \dots x_{m}^{a_{i m}} = c_{i} \prod_{j} x_{j}^{a_{i j}}
$$

for  $c_{i} > 0$  and  $a_{ij} \in \mathbb{R}$ , called a monomial. The objective function and the inequality constraint functions are sums of any number of monomials.

30 A problem in this form is called a geometric program. Modern interior-point algorithms (see algorithm 10.3) can solve a geometric program with 1,000 variables and 10,000 constraints in under a minute. S. Boyd, S.-J. Kim, L. Vandenberghe, and A. Hassibi, "A Tutorial on Geometric Programming," Optimization and Engineering, vol. 8, pp. 67-127, 2007.

Problems of this form can be converted into convex problems by taking the log of the objective function and all constraint functions and applying the change of variables  $z_{i} = \log x_{i}$ . Show that these changes produce a problem with a log-sum-exp objective, log-sum-exp inequality constraints, and affine equality constraints.

Solution: The log of a monomial is:

$$
\log \left(c_{i} \prod_{j} x_{j}^{a_{i j}}\right) = \log c_{i} + \sum_{j} a_{i j} \log x_{j}
$$

Applying the change of variables  $z_{j} = \log x_{j}$  results in an affine constraint:

$$
\log \left(c_{i} \prod_{j} x_{j}^{a_{i j}}\right) = 1 \quad \Rightarrow \quad \sum_{j} a_{i j} z_{j} = - \log c_{i}
$$

The log of a sum of monomials is:

$$
\log \left(\sum_{i} c_{i} \prod_{j} x_{j}^{a_{i j}}\right)
$$

If we apply the same change of variables, rewritten as  $\exp z_{j} = x_{j}$ , we get a function in log-sum-exp form:

$$
\log \left(\sum_{i} c_{i} \prod_{j} \left(\exp z_{j}\right) ^{a_{i j}}\right) = \log \left(\sum_{i} \exp \left(\log c_{i} + \sum_{j} a_{i j} z_{j}\right)\right)
$$

The resulting problem thus has such a log-sum-exp objective, log-sum-exp inequality constraints, and affine equality constraints.

# 12 Linear Programming

Linear programming involves solving optimization problems with linear objective functions and linear constraints. Many problems are naturally described by linear programs, including problems from fields as diverse as transportation, communication networks, manufacturing, economics, and operations research. Many problems that are not naturally linear can often be approximated by linear programs. Several methods have been developed for exploiting the linear structure. Modern techniques and hardware can globally minimize problems with millions of variables and millions of constraints. $^{1}$

# 12.1 Problem Formulation

A linear programming problem, called a linear program, can be expressed in several forms. Each linear program consists of a linear objective function and a set of linear constraints:2

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad \mathbf {w}_{\mathrm{LE}}^{(i) \top} \mathbf {x} \leq b_{i} \quad \text{for all} i \tag {12.1}
$$

$$
\mathbf {w}_{\mathrm{EQ}}^{(j) \top} \mathbf {x} = b_{j} \quad \text{for all} j
$$

where  $i$  and  $j$  vary over finite sets of constraints. Such an optimization problem is given in example 12.1. Transforming real problems into this mathematical form is often nontrivial. This text focuses on the algorithms for obtaining solutions, but other texts discuss how to go about modeling real problems.<sup>3</sup> Several interesting conversions are given in example 12.2.

This chapter is a short introduction to linear programs and one variation of the simplex algorithm used to solve them. Several textbooks are dedicated entirely to linear programs, including R.J. Vanderbei, Linear Programming: Foundations and Extensions, 4th ed. Springer, 2014. There are a variety of packages for solving linear programs, such as Convex.jl and JuMP.jl, both of which include interfaces to open-source and commercial solvers.

As discussed in section 10.2, we can transform greater-than inequalities into less-than inequalities.

3 See, for example, H. P. Williams, Model Building in Mathematical Programming, 5th ed. Wiley, 2013.

The following problem has a linear objective and linear constraints, making it a linear program.

$$
\underset{x_{1}, x_{2}, x_{3}} {\text{minimize}} 2 x_{1} - 3 x_{2} + 7 x_{3}
$$

subject to  $2x_{1} + 3x_{2} - 8x_{3}\leq$  5

$$
4 x_{1} + x_{2} + 3 x_{3} \leq 9
$$

$$
x_{1} - 5 x_{2} - 3 x_{3} \geq - 4
$$

$$
\begin{array}{l l} x_{1} + & x_{2} + 2 x_{3} = \quad 1 \end{array}
$$

Example 12.1. An example linear program.

Many problems can be converted into linear programs that have the same solution. Two examples are  $L_{1}$  and  $L_{\infty}$  minimization problems:

$$
\operatorname{minimize} \| \mathbf {A x} - \mathbf {b} \| _{1} \quad \text{minimize} \| \mathbf {A x} - \mathbf {b} \| _{\infty}
$$

The first problem is equivalent to solving

$$
\underset{\mathbf {x}, \mathbf {s}} {\text{minimize}} \quad \mathbf {1}^{\top} \mathbf {s}
$$

$$
\text{subject} \quad \mathbf {A x} - \mathbf {b} \leq \quad \mathbf {s}
$$

$$
\mathbf {A} \mathbf {x} - \mathbf {b} \geq - \mathbf {s}
$$

with the additional variables  $\mathbf{s}$ .

The second problem is equivalent to solving

$$
\underset{\mathbf {x}, t} {\text{minimize}} \quad t
$$

$$
\text{subject} \quad \mathbf {A x} - \mathbf {b} \leq \quad t \mathbf {1}
$$

$$
\mathbf {A} \mathbf {x} - \mathbf {b} \geq - t \mathbf {1}
$$

with the additional variable  $t$ .

Example 12.2. Common norm minimization problems that can be converted into linear programs.

# 12.1.1 General Form

We can write linear programs more compactly using matrices and arrive at the general form:4

$$
\underset{x} {\text{minimize}} \quad \mathbf {c}^{\top} x
$$

$$
\text{subject} \quad \mathbf {A}_{\mathrm{LE}} \mathbf {x} \leq \mathbf {b}_{\mathrm{LE}} \tag {12.2}
$$

$$
\mathbf {A}_{\mathrm{EQ}} \mathbf {x} = \mathbf {b}_{\mathrm{EQ}}
$$

Here, each constraint is elementwise. For example, in writing

$$
\mathbf {a} \leq \mathbf {b},
$$

we mean  $a_{i}\leq b_{i}$  for all  $i$

# 12.1.2 Standard Form

The general linear program given in equation (12.2) can be converted into standard form where all constraints are less-than inequalities and the design variables are nonnegative

$$
\underset{x} {\text{minimize}} \quad \mathbf {c}^{\top} x
$$

$$
\text{subject} \quad \mathbf {A x} \leq \mathbf {b} \tag {12.3}
$$

$$
x \geq 0
$$

Equality constraints are split in two:

$$
\mathbf {A}_{\mathrm{EQ}} \mathbf {x} = \mathbf {b}_{\mathrm{EQ}} \rightarrow \left\{\begin{array}{c}\mathbf {A}_{\mathrm{EQ}} \mathbf {x} \leq \mathbf {b}_{\mathrm{EQ}}\\- \mathbf {A}_{\mathrm{EQ}} \mathbf {x} \leq - \mathbf {b}_{\mathrm{EQ}}\end{array}\right. \tag {12.4}
$$

We must ensure that all  $\mathbf{x}$  entries are nonnegative as well. Suppose we start with a linear program where  $\mathbf{x}$  is not constrained to be nonnegative:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x} \tag {12.5}
$$

$$
\text{subject} \quad \mathbf {A x} \leq \mathbf {b}
$$

We replace  $\mathbf{x}$  with  $\mathbf{x}^{+} - \mathbf{x}^{-}$  and constrain  $\mathbf{x}^{+} \geq \mathbf{0}$  and  $\mathbf{x}^{-} \geq \mathbf{0}$ :

$$
\begin{array}{l} \underset{\mathbf {x}^{+}, \mathbf {x}^{-}} {\text{minimize}} \quad \left[ \begin{array}{c c} \mathbf {c}^{\top} & - \mathbf {c}^{\top} \end{array} \right] \left[ \begin{array}{c} \mathbf {x}^{+} \\ \mathbf {x}^{-} \end{array} \right] \\ \text{subject} \quad \left[ \begin{array}{l l} \mathbf {A} & - \mathbf {A} \end{array} \right] \left[ \begin{array}{l} \mathbf {x}^{+} \\ \mathbf {x}^{-} \end{array} \right] \leq \mathbf {b} \tag {12.6} \\ \left[ \begin{array}{c} \mathbf {x}^{+} \\ \mathbf {x}^{-} \end{array} \right] \geq \mathbf {0} \\ \end{array}
$$

The linear objective function  $\mathbf{c}^{\top}\mathbf{x}$  forms a flat ramp. The function increases in the direction of  $\mathbf{c}$ , and, as a result, all contour lines are perpendicular to  $\mathbf{c}$  and parallel to one another as shown in figure 12.1.

A single inequality constraint  $\mathbf{w}^{\top}\mathbf{x} \leq b$  forms a half-space, or a region on one side of a hyperplane. The hyperplane is perpendicular to  $\mathbf{w}$  and is defined by  $\mathbf{w}^{\top}\mathbf{x} = b$  as shown in figure 12.2. The region  $\mathbf{w}^{\top}\mathbf{x} > b$  is on the  $+\mathbf{w}$  side of the hyperplane, whereas  $\mathbf{w}^{\top}\mathbf{x} < b$  is on the  $-\mathbf{w}$  side of the hyperplane.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/681e4a605e6bd219f00aca78ba511122a744ecab1cee4a60d87e3fd0f958d066.jpg)  
Figure 12.1. The contours of a linear objective function  $\mathbf{c}^{\top}\mathbf{x}$ , which increase in the direction of  $\mathbf{c}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0c3cf72f520bf1653704ed519a8a91b4f108b89a04c090047889108d61b1d5fa.jpg)  
Figure 12.2. A linear constraint.

Half-spaces are convex sets (see appendix C.3), and the intersection of convex sets is convex, as shown in figure 12.3. Thus, the feasible set of a linear program will always form a convex set. Convexity of the feasible set, along with convexity of the objective function, implies that if we find a local feasible minimum, it is also a global feasible minimum.

The feasible set is a convex region enclosed by flat faces. Depending on the region's configuration, the solution can lie at a vertex, on an edge, or on an entire face. If the problem is not properly constrained, the solution can be unbounded, and, if the system is over-constrained, there is no feasible solution. Several such cases are shown in figure 12.4.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/13f1b42456b04c5b992b62848f2ec7bc5043c52b9b3e6f6234cccab8dd8c032b.jpg)  
Figure 12.3. The intersection of linear constraints is a convex set.

# 12.1.3 Equality Form

Linear programs are often solved in equality form:

$$
\underset{x} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

subject to  $\mathbf{A}\mathbf{x} = \mathbf{b}$  (12.7)

$$
x \geq 0
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7e95487a2d35b3ade5305185548cbf0deeea073af3e827a0d89bb61832831ce9.jpg)  
One Solution

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/981b791bffcd5ec16220fbfa3f0762d4d9e29b999f7d37f7c08a7f3db2a4cba4.jpg)  
Unbounded Solution

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e37602544614b612e6384307a521c184308d448c61eaf432579ed1a5cbf62aac.jpg)  
Infinite Solutions

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/285fd48f27167c4979abeba81f6507c6a197d785bee0deac34c14d0c107583da.jpg)  
No Solution  
Figure 12.4. Several different linear problem forms with different solutions.

where  $\mathbf{x}$  and  $\mathbf{c}$  each have  $n$  components,  $\mathbf{A}$  is an  $m \times n$  matrix, and  $\mathbf{b}$  has  $m$  components. In other words, we have  $n$  nonnegative design variables and a system of  $m$  equations defining equality constraints.

The equality form has constraints in two parts. The first,  $\mathbf{Ax} = \mathbf{b}$ , forces the solution to lie in an affine subspace. Such a constraint is convenient because search techniques can constrain themselves to the constrained affine subspace to remain feasible. The second part of the constraints requires  $\mathbf{x} \geq \mathbf{0}$ , which forces the solution to lie in the nonnegative quadrant. The feasible set is thus the nonnegative portion of an affine subspace. Example 12.3 provides a visualization of a simple linear program.

Any linear program in standard form can be transformed to equality form. The constraints are converted as follows:

$$
\mathbf {A} \mathbf {x} \leq \mathbf {b} \quad \rightarrow \quad \mathbf {A} \mathbf {x} + \mathbf {s} = \mathbf {b}, \quad \mathbf {s} \geq \mathbf {0} \tag {12.8}
$$

by introducing slack variables  $\mathbf{s}$ . These variables take up the extra slack to enforce equality.

Starting with a linear program:

$$
\begin{array}{l} \underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x} \\ \text{subject} \quad \mathbf {A x} \leq \mathbf {b} \tag {12.9} \\ x \geq 0 \\ \end{array}
$$

Consider the standard-form linear program:

$$
\underset{x} {\text{minimize}} \quad x
$$

$$
\text{subject} \quad x \geq 1
$$

When we convert this to equality form, we get

$$
\underset{x, s} {\text{minimize}} x
$$

$$
\text{subject} \quad x - s = 1
$$

$$
x, s \geq 0
$$

The equality constraint requires that feasible points fall on the line  $x - s = 1$ . That line is a one-dimensional affine subspace of the two-dimensional Euclidean space.

Example 12.3. Feasible sets for the equality form are hyperplanes.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3a7b43db6e355da130de17e56afaa71b1cc517747d606a7ab749ec66e5d28dc5.jpg)

We introduce the slack variables:

$$
\underset{\mathbf {x}, \mathbf {s}} {\text{minimize}} \quad \left[ \begin{array}{c c} \mathbf {c}^{\top} & \mathbf {0}^{\top} \end{array} \right] \left[ \begin{array}{c} \mathbf {x} \\ \mathbf {s} \end{array} \right]
$$

$$
\text{subject} \quad \left[ \begin{array}{l l} \mathbf {A} & \mathbf {I} \end{array} \right] \left[ \begin{array}{l} \mathbf {x} \\ \mathbf {s} \end{array} \right] = \mathbf {b} \tag {12.10}
$$

$$
\left[ \begin{array}{c} \mathbf {x} \\ \mathbf {s} \end{array} \right] \geq \mathbf {0}
$$

Example 12.4 demonstrates converting from standard to equality form.

Consider the linear program

$$
\underset{\mathbf {x}} {\text{minimize}} \quad 5 x_{1} + 4 x_{2}
$$

$$
\text{subject} \quad 2 x_{1} + 3 x_{2} \leq 5
$$

$$
\begin{array}{l l} 4 x_{1} + & x_{2} \leq 11 \end{array}
$$

To convert to equality form, we first introduce two slack variables:

$$
\underset{x, s} {\text{minimize}} \quad 5 x_{1} + 4 x_{2}
$$

$$
\text{subject} \quad 2 x_{1} + 3 x_{2} + s_{1} = 5
$$

$$
4 x_{1} + x_{2} + s_{2} = 11
$$

$$
s_{1}, s_{2} \geq 0
$$

We then split  $x$ :

$$
\underset{\mathbf {x}^{+}, \mathbf {x}^{-}, \mathbf {s}} {\text{minimize}} \quad 5 (x_{1}^{+} - x_{1}^{-}) + 4 (x_{2}^{+} - x_{2}^{-})
$$

$$
\text{subject} \quad 2 \left(x_{1}^{+} - x_{1}^{-}\right) + 3 \left(x_{2}^{+} - x_{2}^{-}\right) + s_{1} = 5
$$

$$
4 \left(x_{1}^{+} - x_{1}^{-}\right) + \left(x_{2}^{+} - x_{2}^{-}\right) + s_{2} = 11
$$

$$
x_{1}^{+}, x_{1}^{-}, x_{2}^{+}, x_{2}^{-}, s_{1}, s_{2} \geq 0
$$

# 12.2 Simplex Algorithm

The simplex algorithm solves linear programs by moving from vertex to vertex of the feasible set. The method is guaranteed to arrive at an optimal solution so long as the linear program is feasible and bounded.

The simplex algorithm operates on equality-form linear programs ( $\mathbf{Ax} = \mathbf{b}$ ,  $\mathbf{x} \geq \mathbf{0}$ ). We assume that the rows of  $\mathbf{A}$  are linearly independent. We also assume that the problem has no more equality constraints than it has design variables ( $m \leq n$ ), which ensures that the problem is not over constrained. A preprocessing phase guarantees that  $\mathbf{A}$  satisfies these conditions.

Example 12.4. Converting a linear program to equality form. Below we show the original linear program. The linear program in equality form has 6 dimensions, making it more difficult to visualize.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/956ba1cb115f9889610138080930a5aaba72db352f3432d8be715c2ef010403f.jpg)

The simplex algorithm was originally developed in the 1940s by George Dantzig. A history of the development can be found here: G.B.Dantzig, "Origins of the Simplex Method," in A History of Scientific Computing, S.G. Nash, ed., ACM, 1990, pp. 141-151.

7 A matrix whose rows are linearly independent is said to have full row rank. Linear independence is achieved by removing redundant equality constraints.

# 12.2.1 Vertices

Linear programs in equality form have feasible sets in the form of convex polytopes, which are geometric objects with flat faces. These polytopes are formed by the intersection of the equality constraints with the nonnegative quadrant. Associated with a polytope are vertices, which are points in the feasible set that do not lie between any other points in the feasible set.

The feasible set consists of several different types of design points. Points on the interior are never optimal because they can be improved by moving along  $-\mathbf{c}$ . Points on faces can be optimal only if the face is perpendicular to  $\mathbf{c}$ . Points on faces not perpendicular to  $\mathbf{c}$  can be improved by sliding along the face in the direction of the projection of  $-\mathbf{c}$  onto the face. Similarly, points on edges can be optimal only if the edge is perpendicular to  $\mathbf{c}$ , and can otherwise be improved by sliding along the projection of  $-\mathbf{c}$  onto the edge. Finally, vertices can also be optimal.

The simplex algorithm produces an optimal vertex. If a linear program has a bounded solution, then it also contains at least one vertex. Furthermore, at least one solution must lie at a vertex. In the case where an entire edge or face is optimal, a vertex solution is just as good as any other.

Every vertex for a linear program in equality form can be uniquely defined by  $n - m$  components of  $\mathbf{x}$  that equal zero. These components are actively constrained by the nonnegative quadrant requirement.

The equality constraint  $\mathbf{A}\mathbf{x} = \mathbf{b}$  has a unique solution when  $\mathbf{A}$  is square. We have assumed that  $m \leq n$ , so choosing  $m$  design variables and setting the remaining variables to zero effectively removes  $n - m$  columns of  $\mathbf{A}$ , yielding an  $m \times m$  constraint matrix (see example 12.5).

This is analogous to identifying a vertex in standard form with  $n - m$  active constraints, including original equality constraints and a set of active inequality constraints necessary to uniquely identify a vertex. Zeroing any slack variables associated with standard form inequality constraints causes those original inequality constraints to be active.

For a problem with 5 design variables and 3 constraints, setting 2 variables to zero uniquely defines a point.

$$
\left[ \begin{array}{l l l l l} a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\ a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\ a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \end{array} \right] \left[ \begin{array}{l} x_{1} \\ 0 \\ x_{3} \\ x_{4} \\ 0 \end{array} \right] = \left[ \begin{array}{l l l} a_{11} & a_{13} & a_{14} \\ a_{21} & a_{23} & a_{24} \\ a_{31} & a_{33} & a_{34} \end{array} \right] \left[ \begin{array}{l} x_{1} \\ x_{3} \\ x_{4} \end{array} \right] = \left[ \begin{array}{l} b_{1} \\ b_{2} \\ b_{3} \end{array} \right]
$$

Example 12.5. Setting  $n - m$  components of  $\mathbf{x}$  to zero can uniquely define a point.

The indices into the components  $1:n$  of any vertex can be partitioned into two sets,  $\mathcal{B}$  and  $\mathcal{V}$ , such that:

- The design values associated with indices in  $\mathcal{V}$  are zero:

$$
i \in \mathcal {V} \quad \Longrightarrow \quad x_{i} = 0 \tag {12.11}
$$

- The design values associated with indices in  $\mathcal{B}$  may or may not be zero:

$$
i \in \mathcal {B} \quad \Longrightarrow x_{i} \geq 0 \tag {12.12}
$$

-  $\mathcal{B}$  has exactly  $m$  elements and  $\nu$  has exactly  $n - m$  elements.

We use  $\mathbf{x}_{\mathcal{B}}$  to refer to the vector consisting of the components of  $\mathbf{x}$  that are in  $\mathcal{B}$  and  $\mathbf{x}_{\mathcal{V}}$  to refer to the vector consisting of the components of  $\mathbf{x}$  that are in  $\mathcal{V}$ . Note that  $\mathbf{x}_{\mathcal{V}} = \mathbf{0}$ .

The vertex associated with a partition  $(\mathcal{B},\mathcal{V})$  can be obtained using the  $m\times m$  matrix  $\mathbf{A}_{\mathcal{B}}$  formed by the  $m$  columns of  $\mathbf{A}$  selected by  $\mathcal{B}$ :

$$
\mathbf {A} \mathbf {x} = \mathbf {A}_{\mathcal {B}} \mathbf {x}_{\mathcal {B}} = \mathbf {b} \quad \rightarrow \quad \mathbf {x}_{\mathcal {B}} = \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {b} \tag {12.13}
$$

Knowing  $\mathbf{x}_{\mathcal{B}}$  is sufficient to construct  $\mathbf{x}$ ; the remaining design variables are zero. Algorithm 12.1 implements this procedure, and example 12.6 demonstrates verifying that a given design point is a vertex.

```julia
struct LinearProgram
A # LP in equality form:
b # minimize x·c
c # subject to Ax = b
# x ≥ 0
end
function get Vertex(B, LP)
A, b, c = LP.A, LP.b, LP.c
b_inds = sort(B)
AB = A(:, b_inds]
xB = AB\b
x = zeros(length(c))
x[b_inds] = xB
return x
end
```

While every vertex has an associated partition  $(\mathcal{B},\mathcal{V})$ , not every partition corresponds to a vertex. A partition corresponds to a vertex only if  $\mathbf{A}_{\mathcal{B}}$  is nonsingular

9 Sometimes  $\mathcal{B}$  is referred to the set of basic indices, in the sense that the corresponding variables are part of the current solution basis. The set  $\nu$  contains the non-basic indices, whose corresponding variables are set to zero. A mnemonic that we can use here is that the indices in  $\mathcal{B}$  are "busy" in the sense that we will be using them to solve the linear system, and that the indices in  $\nu$  are vacant in the sense that they are set to zero.

10 If  $\mathcal{B}$  and  $\nu$  identify a vertex, then the columns of  $\mathbf{A}_B$  must be linearly independent because  $\mathbf{Ax} = \mathbf{b}$  must have exactly one solution. Hence,  $\mathbf{A}_B\mathbf{x}_B = \mathbf{b}$  must have exactly one solution. This linear independence guarantees that  $\mathbf{A}_B$  is invertible.

Algorithm 12.1. A method for extracting the vertex associated with a partition B and a linear program LP in equality form. We introduce the special type LinearProgram for linear programs in equality form.

and the design obtained by applying equation (12.13) is feasible. $^{11}$  Identifying partitions that correspond to vertices is nontrivial, and we show in section 12.2.4 that finding such a partition involves solving a linear program. The simplex algorithm operates in two phases—an initialization phase that identifies a vertex partition and an optimization phase that transitions between vertex partitions toward a partition corresponding to an optimal vertex. We will discuss both of these phases later in this section.

Consider the constraints:

$$
\left[ \begin{array}{r r r r} 1 & 1 & 1 & 1 \\ 0 & - 1 & 2 & 3 \\ 2 & 1 & 2 & - 1 \end{array} \right] \mathbf {x} = \left[ \begin{array}{r} 2 \\ - 1 \\ 3 \end{array} \right], \quad \mathbf {x} \geq \mathbf {0}
$$

Consider the design point  $\mathbf{x} = [1, 1, 0, 0]$ . We can verify that  $\mathbf{x}$  is feasible and that it has no more than three nonzero components. We can choose either  $\mathcal{B} = \{1, 2, 3\}$  or  $\mathcal{B} = \{1, 2, 4\}$ . Both

$$
\mathbf {A}_{\{1, 2, 3 \}} = \left[ \begin{array}{c c c} 1 & 1 & 1 \\ 0 & - 1 & 2 \\ 2 & 1 & 2 \end{array} \right]
$$

and

$$
\mathbf {A}_{\{1, 2, 4 \}} = \left[ \begin{array}{c c c} 1 & 1 & 1 \\ 0 & - 1 & 3 \\ 2 & 1 & - 1 \end{array} \right]
$$

are invertible. Thus,  $\mathbf{x}$  is a vertex of the feasible set polytope.

# 12.2.2 First-Order Necessary Conditions

The first-order necessary conditions for optimality are used to determine when a vertex is optimal and to inform how to transition to a more favorable vertex. We construct a Lagrangian for the equality form of the linear program:12

$$
\mathcal {L} (\mathbf {x}, \mu , \lambda) = \mathbf {c}^{\top} \mathbf {x} - \mu^{\top} \mathbf {x} - \lambda^{\top} (\mathbf {A x} - \mathbf {b}) \tag {12.14}
$$

with the following necessary conditions:

1 For example,  $\mathcal{B} = \{1,2\}$  for the constraints

$$
\left[ \begin{array}{c c c} 1 & 2 & 0 \\ 1 & 2 & 1 \end{array} \right] \left[ \begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array} \right] = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right]
$$

corresponds to

$$
\left[ \begin{array}{c c} 1 & 2 \\ 1 & 2 \end{array} \right] \left[ \begin{array}{c} x_{1} \\ x_{2} \end{array} \right] = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right]
$$

which does not produce an invertible  $\mathbf{A}_B$  and does not have a unique solution.

Example 12.6. Verifying that a design point is a vertex for constraints in equality form.

12 Note that in  $\mathbf{x} \geq 0$  the polarity of the inequality must be inverted by multiplying both sides by  $-1$ , yielding the negative sign in front of  $\mu$ . The Lagrangian can be defined with either positive or negative  $\lambda$ .

1. feasibility:  $\mathbf{Ax} = \mathbf{b},\mathbf{x}\geq 0$  
2. dual feasibility:  $\mu \geq 0$  
3. complementary slackness:  $\mu \odot x = 0$  
4. stationarity:  $\mathbf{A}^{\top}\lambda +\mu = \mathbf{c}$

The necessary conditions are also sufficient conditions for optimality for linear programs. Thus, if  $\mu$  and  $\lambda$  can be computed for a given vertex and all four necessary conditions are satisfied, then the vertex is optimal.

We can decompose the stationarity condition into  $\mathcal{B}$  and  $\nu$  components:

$$
\mathbf {A}^{\top} \lambda + \mu = \mathbf {c} \quad \rightarrow \quad \left\{\begin{array}{l}\mathbf {A}_{\mathcal {B}}^{\top} \lambda + \mu_{\mathcal {B}} = \mathbf {c}_{\mathcal {B}}\\\mathbf {A}_{\mathcal {V}}^{\top} \lambda + \mu_{\mathcal {V}} = \mathbf {c}_{\mathcal {V}}\end{array}\right. \tag {12.15}
$$

We can choose  $\mu_{\mathcal{B}} = 0$  to satisfy complementary slackness. The value of  $\lambda$  can be computed from  $\mathcal{B}$ :<sup>13</sup>

$$
\mathbf {A}_{\mathcal {B}}^{\top} \lambda + \underbrace{\mu_{\mathcal {B}}}_{= 0} = \mathbf {c}_{\mathcal {B}} \tag {12.16}
$$

$$
\lambda = \mathbf {A}_{\mathcal {B}}^{- \top} \mathbf {c}_{\mathcal {B}} \tag {12.17}
$$

We can use this to obtain

$$
\mathbf {A}_{\mathcal {V}}^{\top} \boldsymbol {\lambda} + \boldsymbol {\mu}_{\mathcal {V}} = \mathbf {c}_{\mathcal {V}} \tag {12.18}
$$

$$
\boldsymbol {\mu}_{\mathcal {V}} = \mathbf {c}_{\mathcal {V}} - \mathbf {A}_{\mathcal {V}}^{\top} \boldsymbol {\lambda} \tag {12.19}
$$

$$
\boldsymbol {\mu}_{\mathcal {V}} = \mathbf {c}_{\mathcal {V}} - \left(\mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\mathcal {V}}\right) ^{\top} \mathbf {c}_{\mathcal {B}} \tag {12.20}
$$

Knowing  $\mu_{\mathcal{V}}$  allows us to assess the optimality of the vertices. If  $\mu_{\mathcal{V}}$  contains negative components, then dual feasibility is not satisfied and the vertex is suboptimal.

# 12.2.3 Optimization Phase

The simplex algorithm maintains a partition  $(\mathcal{B},\mathcal{V})$ , which corresponds to a vertex of the feasible set polytope. The partition can be updated by swapping indices between  $\mathcal{B}$  and  $\mathcal{V}$ . Such a swap equates to moving from one vertex along an edge of the feasible set polytope to another vertex. If the initial partition corresponds to a vertex and the problem is bounded, the simplex algorithm is guaranteed to converge to an optimum.

13 We use  $\mathbf{A}^{-\top}$  to refer to the transpose of the inverse of  $\mathbf{A}$ :

$$
\mathbf {A}^{- \top} = \left(\mathbf {A}^{- 1}\right) ^{\top} = \left(\mathbf {A}^{\top}\right) ^{- 1}
$$

14 The vertex given by  $\mathcal{B} = \{1,2,3\}$  and  $\mathcal{V} = \{4\}$  in example 12.6 can be moved to the vertex given by  $\mathcal{B} = \{1,2,4\}$  and  $\mathcal{V} = \{3\}$  by swapping indices 3 and 4 between  $\mathcal{B}$  and  $\mathcal{V}$ .

A transition  $\mathbf{x} \rightarrow \mathbf{x}'$  between vertices must satisfy  $\mathbf{A}\mathbf{x}' = \mathbf{b}$ . Starting with a partition defined by  $\mathcal{B}$ , we choose an entering index  $q \in \mathcal{V}$  that is to enter  $\mathcal{B}$  using one of the heuristics described near the end of this section. The new vertex  $\mathbf{x}'$  must satisfy:

$$
\mathbf {A} \mathbf {x}^{\prime} = \mathbf {A}_{\mathcal {B}} \mathbf {x}_{\mathcal {B}}^{\prime} + \mathbf {A}_{\{q \}} x_{q}^{\prime} = \mathbf {A}_{\mathcal {B}} \mathbf {x}_{\mathcal {B}} = \mathbf {A} \mathbf {x} = \mathbf {b} \tag {12.21}
$$

Originally, the value at the entering index  $q \in \mathcal{V}$  is zero. To obtain  $x_{q}^{\prime}$ , we increase  $x_{q}$  until the component of  $\mathbf{x}^{\prime}$  corresponding to the leaving index  $p \in \mathcal{B}$  becomes zero, after which  $p$  can be moved to  $\mathcal{V}$ . We do not increase  $x_{q}$  any further because it could result in other components in  $\mathcal{B}$  becoming negative. This action is referred to as pivoting.

We can solve for the new design point

$$
\mathbf {x}_{\mathcal {B}}^{\prime} = \mathbf {x}_{\mathcal {B}} - \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{q \}} x_{q}^{\prime} \tag {12.22}
$$

A particular leaving index  $p \in \mathcal{B}$  becomes active when:

$$
\left(\mathbf {x}_{\mathcal {B}}^{\prime}\right) _{p} = 0 = \left(\mathbf {x}_{\mathcal {B}}\right) _{p} - \left(\mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{q \}}\right) _{p} x_{q}^{\prime} \tag {12.23}
$$

and is thus obtained by increasing  $x_{q} = 0$  to  $x_{q}^{\prime}$  with:

$$
x_{q}^{\prime} = \frac{\left(\mathbf {x}_{\mathcal {B}}\right) _{p}}{\left(\mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{q \}}\right) _{p}} \tag {12.24}
$$

The leaving index  $p$  is selected from  $\mathcal{B}$  so that it increases the value of variable with entering index  $q$  the least. Since this new value of the variable associated with index  $q$  is given by the ratio shown in equation (12.24), this selection criterion for the leaving index  $p$  is referred to as the minimum ratio test. The minimum ratio test computes equation (12.24) for each potential leaving index and selects the one with the minimum  $x_{q}^{\prime}$ . We then swap  $p$  and  $q$  between  $\mathcal{B}$  and  $\mathcal{V}$ . This edge transition is implemented in algorithm 12.2.

The effect that a transition has on the objective function can be computed using  $x_{q}^{\prime}$ . The objective function value at the new vertex is

$$
\mathbf {c}^{\top} \mathbf {x}^{\prime} = \mathbf {c}_{\mathcal {B}}^{\top} \mathbf {x}_{\mathcal {B}}^{\prime} + c_{q} x_{q}^{\prime} \tag {12.25}
$$

We apply equation (12.22) and expand:

$$
\begin{array}{l} \mathbf {c}^{\top} \mathbf {x}^{\prime} = \mathbf {c}_{\mathcal {B}}^{\top} \left(\mathbf {x}_{\mathcal {B}} - \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{q \}} x_{q}^{\prime}\right) + c_{q} x_{q}^{\prime} (12.26) \\ = \mathbf {c}_{\mathcal {B}}^{\top} \mathbf {x}_{\mathcal {B}} - \mathbf {c}_{\mathcal {B}}^{\top} \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{q \}} x_{q}^{\prime} + c_{q} x_{q}^{\prime} (12.27) \\ \end{array}
$$

```javascript
function edge_transition(LP,B,q) A,b  $=$  LP.A,LP.b n  $=$  size(A,2) b_inds  $=$  sort(B) n_inds  $=$  sort(setdiff(1:n,B)) AB  $=$  A[:,b_inds] d,xB  $=$  AB\A[:,n_inds[q]],AB\b best  $\equiv$  (p=0,xq' =Inf) for p in eachindex(d) ifd[p]  $\rightharpoondown$  0 v=xB[p]/d[p] ifv  $<  <$  best.xq' best  $=$  (p=p，xq'=v) end end end return best end
```

Algorithm 12.2. A method for computing the index  $p$  and the new coordinate value  $x_{q}^{\prime}$  obtained by increasing index q of the vertex defined by the partition B in the equality-form linear program LP.

By applying equation (12.18) to the entering index, we obtain

$$
\mathbf {A}_{\{q \}}^{\top} \boldsymbol {\lambda} + \mu_{q} = c_{q} \tag {12.28}
$$

We can replace  $\lambda$  using equation (12.17) and rearrange, yielding:

$$
\mathbf {A}_{\{q \}}^{\top} \mathbf {A}_{\mathcal {B}}^{- \top} \mathbf {c}_{\mathcal {B}} = \mathbf {c}_{\mathcal {B}}^{\top} \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{q \}} = c_{q} - \mu_{q} \tag {12.29}
$$

Substituting equation (12.29) into equation (12.27) yields:

$$
\begin{array}{l} \mathbf {c}^{\top} \mathbf {x}^{\prime} = \mathbf {c}_{\mathcal {B}}^{\top} \mathbf {x}_{\mathcal {B}} - \left(c_{q} - \mu_{q}\right) x_{q}^{\prime} + c_{q} x_{q}^{\prime} (12.30) \\ = \mathbf {c}^{\top} \mathbf {x} + \mu_{q} x_{q}^{\prime} (12.31) \\ \end{array}
$$

Choosing an entering index  $q$  thus decreases the objective function value by

$$
\mathbf {c}^{\top} \mathbf {x}^{\prime} - \mathbf {c}^{\top} \mathbf {x} = \mu_{q} x_{q}^{\prime} \tag {12.32}
$$

The objective function decreases only if  $\mu_q$  is negative. In order to progress toward optimality, we must choose an index  $q$  in  $\mathcal{V}$  such that  $\mu_q$  is negative. If all components of  $\mu_V$  are nonnegative, we have found a global optimum.

Since there can be multiple negative entries in  $\mu_{\mathcal{V}}$ , different heuristics can be used to select an entering index: $^{15}$

Greedy heuristic, which chooses a  $q$  that maximally reduces  $\mathbf{c}^{\top}\mathbf{x}$

Modern implementations use more sophisticated rules. For example, see J.J. Forrest and D. Goldfarb, "Steepest-Edge Simplex Algorithms for Linear Programming," Mathematical Programming, vol. 57, no. 1, pp. 341-374, 1992.

- Dantzig's rule, which chooses the  $q$  with the most negative entry in  $\mu$ . This rule is easy to calculate, but it does not guarantee the maximum reduction in  $\mathbf{c}^{\top}\mathbf{x}$ . It is also sensitive to scaling of the constraints.16  
- Bland's rule, which chooses the first  $q$  with a negative entry in  $\mu$ . When used on its own, Bland's rule tends to result in poor performance in practice. However, this rule can help us prevent cycles, which occur when we return to a vertex we have visited before without decreasing the objective function. This rule is usually used only after no improvements have been made for several iterations of a different rule to break out of a cycle and ensure convergence.

One iteration of the simplex method's optimization phase moves a vertex partition to a neighboring vertex based on a heuristic for the entering index. Algorithm 12.3 implements such an iteration with the greedy heuristic. Example 12.7 demonstrates using the simplex algorithm starting from a known vertex partition to solve a linear program.

# 12.2.4 Initialization Phase

The optimization phase of the simplex algorithm is implemented in algorithm 12.4. Unfortunately, algorithm 12.4 requires an initial partition that corresponds to a vertex. If we do not have such a partition, we must solve an auxiliary linear program to obtain this partition as part of an initialization phase.

The auxiliary linear program to be solved in the initialization phase includes extra variables  $\mathbf{z} \in \mathbb{R}^m$ , which we seek to zero out:

$$
\begin{array}{l} \underset{x, z} {\text{minimize}} \quad \left[ \begin{array}{c c} \mathbf {0}^{\top} & \mathbf {1}^{\top} \end{array} \right] \left[ \begin{array}{c} \mathbf {x} \\ \mathbf {z} \end{array} \right] \\ \text{subject} \quad \left[ \begin{array}{l l} \mathbf {A} & \mathbf {Z} \end{array} \right] \left[ \begin{array}{l} \mathbf {x} \\ \mathbf {z} \end{array} \right] = \mathbf {b} \tag {12.33} \\ \left[ \begin{array}{c} \mathbf {x} \\ \mathbf {z} \end{array} \right] \geq \mathbf {0} \\ \end{array}
$$

where  $\mathbf{Z}$  is a diagonal matrix whose diagonal entries are

$$
Z_{i i} = \left\{ \begin{array}{l l} + 1 & \text{if } b_{i} \geq 0 \\ - 1 & \text{otherwise .} \end{array} \right. \tag {12.34}
$$

For constraint  $\mathbf{A}^{\top}\mathbf{x} = \mathbf{b}\rightarrow$ $\alpha \mathbf{A}^{\top}\mathbf{x} = \alpha \mathbf{b},\alpha >0,$  we do not change the solution but the Lagrange multipliers are scaled,  $\lambda \to \alpha^{-1}\lambda$

Consider the equality-form linear program with

$$
\mathbf {A} = \left[ \begin{array}{c c c c} 1 & 1 & 1 & 0 \\ - 4 & 2 & 0 & 1 \end{array} \right], \quad \mathbf {b} = \left[ \begin{array}{c} 9 \\ 2 \end{array} \right], \quad \mathbf {c} = \left[ \begin{array}{c} 3 \\ - 1 \\ 0 \\ 0 \end{array} \right]
$$

and the initial vertex defined by  $\mathcal{B} = \{3,4\}$ . After verifying that  $\mathcal{B}$  defines a feasible vertex, we can begin one iteration of the simplex algorithm.

We extract  $\mathbf{x}_{\mathcal{B}}$

$$
\mathbf {x}_{\mathcal {B}} = \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {b} = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] ^{- 1} \left[ \begin{array}{c} 9 \\ 2 \end{array} \right] = \left[ \begin{array}{c} 9 \\ 2 \end{array} \right]
$$

We then compute  $\lambda$ :

$$
\boldsymbol {\lambda} = \mathbf {A}_{\mathcal {B}}^{- \top} \mathbf {c}_{\mathcal {B}} = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] ^{- \top} \left[ \begin{array}{c} 0 \\ 0 \end{array} \right] = \mathbf {0}
$$

and  $\mu_{\mathcal{V}}$

$$
\boldsymbol {\mu}_{\mathcal {V}} = \mathbf {c}_{\mathcal {V}} - \left(\mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\mathcal {V}}\right) ^{\top} \mathbf {c}_{\mathcal {B}} = \left[ \begin{array}{l} 3 \\ - 1 \end{array} \right] - \left(\left[ \begin{array}{l l} 1 & 0 \\ 0 & 1 \end{array} \right] ^{- 1} \left[ \begin{array}{l l} 1 & 1 \\ - 4 & 2 \end{array} \right]\right) ^{\top} \left[ \begin{array}{l} 0 \\ 0 \end{array} \right] = \left[ \begin{array}{l} 3 \\ - 1 \end{array} \right]
$$

Because  $\mu_{\mathcal{V}}$  contains negative elements, our current  $\mathcal{B}$  is suboptimal. We will pivot on the index of the only negative element,  $q = 2$ . An edge transition is run from  $\mathbf{x}_{\mathcal{B}}$  in the direction  $-\mathbf{A}_{\mathcal{B}}^{-1}\mathbf{A}_{\{q\}} = [1,2]$ .

Using equation (12.22), we increase  $x_{q}^{\prime}$  until a new constraint becomes active. In this case,  $x_{q}^{\prime} = 1$  causes  $x_{4}$  to become zero. We update  $\mathcal{B}$  to  $\{2,3\}$ .

In the second iteration, we find:

$$
\mathbf {x}_{\mathcal {B}} = \left[ \begin{array}{c} 1 \\ 8 \end{array} \right], \quad \boldsymbol {\lambda} = \left[ \begin{array}{c} 0 \\ - 1 / 2 \end{array} \right], \quad \boldsymbol {\mu}_{\mathcal {V}} = \left[ \begin{array}{c} 1 \\ 1 / 2 \end{array} \right].
$$

The vertex is optimal because  $\mu_{\mathcal{V}}$  has no negative entries. Our algorithm thus terminates with  $\mathcal{B} = \{2,3\}$ , for which the design point is  $\mathbf{x}^{*} = [0,1,8,0]$ .

Example 12.7. Solving a linear program with the simplex algorithm.

```julia
function step_lp!(B, LP)  
A, b, c = LP.A, LP.b, LP.c  
n = size(A, 2)  
b_indns = sort(B)  
n_indns = sort(setdiff(1:n, B))  
AB, AV = A(:, b_indns], A(:, n_indns]  
xB = AB\b  
cB = c[b_indns]  
λ = AB' \ cB  
cV = c[n_indns]  
μV = cV - AV'*λ  
best = (q=0, p=0, xq' = Inf, Δ=Inf)  
for q in eachindex(μV)  
    if μV[q] < 0  
        p, xq' = edge_transition(LP, B, q)  
        if μV[q]*xq' < best.Δ  
            best = (q=q, p=p, xq' = xq', Δ=μV[q]*xq')  
        end  
    end  
end  
if best.q == 0  
    return (B, true) # optimal point found  
end  
if isinf(best.xq') || best.xq' < -eps()  
    error("unbounded")  
end  
p, q = best.p, best.q  
j = findfirst(isequal(b_indns[p]), B)  
B[j] = n_indns[q] # swap indices  
return (B, false) # new vertex but not optimal
```

```julia
function minimize_lp!(B, LP)  
done = false  
while !done  
B, done = step_lp!(B, LP)  
end  
return B  
end
```

Algorithm 12.3. A single iteration of the simplex algorithm in which the set  $\mathcal{B}$  is moved from one vertex to a neighbor while maximally decreasing the objective function. Here, step_lp! takes a partition defined by B and a linear program LP.

Algorithm 12.4. Minimizing a linear program given an initial vertex partition defined by B and a linear program LP.

The values for  $\mathbf{z}$  represent the amount by which  $\mathbf{A}\mathbf{x} = \mathbf{b}$  is violated. By zeroing out  $\mathbf{z}$ , we find a feasible point. If, in solving the auxiliary problem, we do not find a vertex with a zeroed-out  $\mathbf{z}$ , then we can conclude that the problem is infeasible.

This auxiliary linear program is solved using algorithm 12.4 with an initial partition that selects only the  $\mathbf{z}$  components. The corresponding vertex has  $\mathbf{x} = \mathbf{0}$ , and each element in  $\mathbf{z}$  is set to the absolute value of the corresponding value in  $\mathbf{b}$ . Example 12.8 demonstrates using an auxiliary linear program to obtain a feasible vertex.

In other words,  $z_{j} = |b_{j}|$ .

Consider the equality-form linear program:

$$
\begin{array}{l} \underset{x_{1}, x_{2}, x_{3}} {\text{minimize}} \quad c_{1} x_{1} + c_{2} x_{2} + c_{3} x_{3} \\ \text{subject} \quad 2 x_{1} - 1 x_{2} + 2 x_{3} = 1 \\ 5 x_{1} + 1 x_{2} - 3 x_{3} = - 2 \\ x_{1}, x_{2}, x_{3} \geq 0 \\ \end{array}
$$

We can identify a feasible vertex by solving:

$$
\begin{array}{l} \underset{x_{1}, x_{2}, x_{3}, z_{1}, z_{2}} {\text{minimize}} \quad z_{1} + z_{2} \\ \text{subject} \quad 2 x_{1} - 1 x_{2} + 2 x_{3} + z_{1} = 1 \\ 5 x_{1} + 1 x_{2} - 3 x_{3} - z_{2} = - 2 \\ x_{1}, x_{2}, x_{3}, z_{1}, z_{2} \geq 0 \\ \end{array}
$$

with an initial vertex defined by  $\mathcal{B} = \{4,5\}$ .

The initial vertex has:

$$
\mathbf {x}_{\mathcal {B}}^{(1)} = \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {b}_{\mathcal {B}} = \left[ \begin{array}{c c} 1 & 0 \\ 0 & - 1 \end{array} \right] ^{- 1} \left[ \begin{array}{c} 1 \\ - 2 \end{array} \right] = \left[ \begin{array}{c} 1 \\ 2 \end{array} \right]
$$

and is thus  $\mathbf{x}^{(1)} = [0,0,0,1,2]$ . Solving the auxiliary problem yields  $\mathbf{x}^* \approx [0.045, 1.713, 1.312, 0, 0]$ . Thus [0.045, 1.713, 1.312] is a feasible vertex in the original problem.

The partition  $\mathcal{B}$  can only be used to solve the original linear program if its indices all correspond to entries in  $\mathbf{x}$ . It is possible that some indices in the compo

Example 12.8. Using an auxiliary linear program to obtain a feasible vertex.

nents of  $\mathbf{z}$  are included in the partition. This can happen when the initialization phase finds a feasible vertex where some components of  $\mathbf{x}$  are zero. We can replace any entries in  $\mathcal{B}$  that correspond to  $\mathbf{z}$  with unused indices that correspond to  $\mathbf{x}$ .

Algorithm 12.5 implements the complete simplex algorithm.

```julia
function find_partition(LP) A, b, c = LP.A, LP.b, LP.c m, n = size(A) Z = Diagonal([j ≥ 0 ? 1 : -1 for j in b]) A' = [A Z] b' = b c' = [zeros(n); ones(m)] LP_init = LinearProgram(A', b', c') B = collect(1:m).+n minimize_lp!(B, LP_init) # Scan through the vertex partition and replace # entries corresponding to z-values with x-values available_xs = setdiff(1:n, B) zs_to_replace = collect(findall(x → x > n, B)) B[zs_to_replace] = available_xs[1:length(zs_to_replace)] sort!(B) return B end function minimize_lp(LP) B = find_partition(LP) minimize_lp!(B, LP) return (x=get Vertex(B, LP), B) end
```

Algorithm 12.5. The simplex algorithm for solving linear programs in equality form when an initial partition is not known. We first construct and solve an auxiliary linear program in order to find a vertex partition, and then that vertex partition is used to solve the original linear program. This method returns both the optimal design and its partition.

# 12.3 Dual Certificates

Linear programs are linear and convex, and can be shown to have zero duality gap. The zero duality gap implies that the optimal value of the dual problem  $d^{*}$  is equal to the optimal value of the primal problem  $p^{*}$ . The inclusion of optimal dual variables for a candidate primal solution  $\mathbf{x}^{\star}$  can be used to verify its optimality. Verifying optimality using dual certificates (algorithm 12.6) is useful in many cases, such as debugging our linear program code.

The primal linear program can be converted to its dual form:

$$
\begin{array}{l} \max_{\mu \geq 0, \lambda} \min_{\mathbf {x}} \mathcal {L} (\mathbf {x}, \mu , \lambda) = \max_{\mu \geq 0, \lambda} \min_{\mathbf {x}} \mathbf {c}^{\top} \mathbf {x} - \mu^{\top} \mathbf {x} - \lambda^{\top} (\mathbf {A x} - \mathbf {b}) (12.35) \\ = \max_{\mu \geq 0, \lambda} \min_{x} (\mathbf {c} - \boldsymbol {\mu} - \mathbf {A}^{\top} \boldsymbol {\lambda}) ^{\top} \mathbf {x} + \boldsymbol {\lambda}^{\top} \mathbf {b} (12.36) \\ \end{array}
$$

From the first-order necessary conditions, we know  $\mathbf{c} - \boldsymbol{\mu} - \mathbf{A}^{\top}\boldsymbol{\lambda} = \mathbf{0}$ , which allows us to drop the first term in the objective above. In addition, we know  $\boldsymbol{\mu} = \mathbf{c} - \mathbf{A}^{\top}\boldsymbol{\lambda} \geq \mathbf{0}$ , which implies  $\mathbf{A}^{\top}\boldsymbol{\lambda} \leq \mathbf{c}$ . In summary, we have:

Primal Form (equality)

Dual Form

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\underset{\lambda} {\text{maximize}} \quad \mathbf {b}^{\top} \boldsymbol {\lambda}
$$

$$
\text{subject} \quad \mathbf {A x} = \mathbf {b}
$$

$$
\text{subject} \quad \mathbf {A}^{\top} \lambda \leq \mathbf {c}
$$

$$
x \geq 0
$$

If the primal problem has  $n$  variables and  $m$  equality constraints, then the dual problem has  $m$  variables and  $n$  constraints. Furthermore, the dual of the dual is the primal problem.

Optimality can be assessed by verifying three properties. If someone claims  $(\mathbf{x}^{*},\lambda^{*})$  is optimal, we can quickly verify the claim by checking whether all three of the following conditions are satisfied:

1.  $\mathbf{x}^*$  is feasible in the primal problem.  
2.  $\lambda^*$  is feasible in the dual problem.  
3.  $p^* = \mathbf{c}^\top \mathbf{x}^* = \mathbf{b}^\top \boldsymbol{\lambda}^* = d^*$ .

Dual certificates are used in example 12.9 to verify the solution to a linear program.

```julia
function dual_certicate(LP, x, λ, ε=1e-6)  
A, b, c = LP.A, LP.b, LP.c  
primal_feasible = all(x ≥ 0) && A*x ≈ b  
dual_feasible = all(A' * λ ≤ c)  
return primal_feasible && dual_feasible && isapprox(c·x, b·λ, atol=ε)  
end
```

18 An alternative to the simplex algorithm, the self-dual simplex algorithm, tends to be faster in practice. It does not require that the matrix  $\mathbf{A}_B$  satisfy  $\mathbf{x}_B = \mathbf{A}_B^{-1}\mathbf{b} \geq \mathbf{0}$ . The self-dual simplex algorithm is a modification of the simplex algorithm for the dual of the linear programming problem in standard form.

Algorithm 12.6. A method for checking whether a candidate solution given by design point  $x$  and dual point  $\lambda$  for the linear program LP in equality form is optimal. The parameter  $\epsilon$  controls the tolerance for the equality constraint.

Consider the standard-form linear program with

$$
\mathbf {A} = \left[ \begin{array}{c c c} 1 & 1 & - 1 \\ - 1 & 2 & 0 \\ 1 & 2 & 3 \end{array} \right], \quad \mathbf {b} = \left[ \begin{array}{c} 1 \\ - 2 \\ 5 \end{array} \right], \quad \mathbf {c} = \left[ \begin{array}{c} 1 \\ 1 \\ - 1 \end{array} \right]
$$

We would like to determine whether  $\mathbf{x}^{*} = [2,0,1]$  and  $\lambda^{*} = [1,0,0]$  are an optimal solution pair. We first verify that  $\mathbf{x}^{*}$  is feasible:

$$
\mathbf {A} \mathbf {x}^{*} = [ 1, - 2, 5 ] = \mathbf {b}, \quad \mathbf {x}^{*} \geq \mathbf {0}
$$

We then verify that  $\lambda^{*}$  is dual-feasible:

$$
\mathbf {A}^{\top} \boldsymbol {\lambda}^{*} = [ 1, 1, - 1 ] \leq \mathbf {c}
$$

Finally, we verify that  $p^*$  and  $d^{*}$  are the same:

$$
p^{*} = \mathbf {c}^{\top} \mathbf {x}^{*} = 1 = \mathbf {b}^{\top} \boldsymbol {\lambda}^{*} = d^{*}
$$

We conclude that  $(\mathbf{x}^{*},\lambda^{*})$  are optimal.

Example 12.9. Verifying a solution using dual certificates.

# 12.4 Summary

- Linear programs are problems consisting of a linear objective function and linear constraints.  
- The simplex algorithm can optimize linear programs globally in an efficient manner.  
- A vertex is a feasible point that satisfies a set of linear constraints.  
- The simplex algorithm iteratively moves from one vertex to an adjacent vertex, improving the objective function with each step.  
- The initialization phase of the simplex algorithm solves an auxiliary linear program to find an initial vertex.  
- Dual certificates allow us to verify that a candidate primal-dual solution pair is optimal.

# 12.5 Exercises

Exercise 12.1. Suppose you do not know any optimization algorithm for solving a linear program. You decide to evaluate all the vertices and determine, by inspection, which one minimizes the objective function. Give a loose upper bound on the number of possible minimizers you will examine. Furthermore, does this method properly handle all linear constrained optimization problems?

Solution: We have chosen to minimize a linear program by evaluating every vertex in the convex polytope formed by the constraints. Every vertex is thus a potential minimizer. Vertices are defined by intersections of active constraints. As every inequality constraint can either be active or inactive, and assuming there are  $n$  inequalities constraints, we do not need to examine more than  $2^n$  combinations of constraints.

This method does not correctly report unbounded linear constrained optimization problems as unbounded.

Exercise 12.2. If the program in example 12.1 is bounded below, argue that the simplex method must converge.

Solution: The simplex method is guaranteed either to improve with respect to the objective function with each step or to preserve the current value of the objective function. Any linear program will have a finite number of vertices. So long as a heuristic, such as Bland's rule, is employed such that cycling does not occur, the simplex method must converge on a solution.

Exercise 12.3. Suppose we want to minimize  $6x_{1} + 5x_{2}$  subject to the constraint  $3x_{1} - 2x_{2} \geq 5$ . How would we translate this problem into a linear program in equality form with the same minimizer?

Solution: The linear program was given to us in general form. To transform it to equality form, we first invert the inequality constraint:

$$
\underset{x_{1}, x_{2}} {\text{minimize}} \quad 6 x_{1} + 5 x_{2}
$$

$$
\text{subject} \quad - 3 x_{1} + 2 x_{2} \leq - 5
$$

We then split our design variables by sign to obtain standard form:

$$
\underset{x_{1}^{\pm}, x_{2}^{\pm}} {\text{minimize}} \quad 6 x_{1}^{+} + 5 x_{2}^{+} - 6 x_{1}^{-} - 5 x_{2}^{-}
$$

subject to

$$
x_{1}^{\pm}, x_{2}^{\pm} \geq 0
$$

Finally, we add a slack variable  $x_{3} \geq 0$  to obtain equality form:

$$
\underset{x_{1}^{\pm}, x_{2}^{\pm}, x_{3}} {\text{minimize}} \quad 6 x_{1}^{+} + 5 x_{2}^{+} - 6 x_{1}^{-} - 5 x_{2}^{-}
$$

subject to  $-3x_{1}^{+} + 2x_{2}^{+} + 3x_{1}^{-} - 2x_{2}^{-} + x_{3} = -5$

$$
x_{1}^{\pm}, x_{2}^{\pm}, x_{3} \geq 0
$$

Exercise 12.4. Consider the equality-form linear program with

$$
\mathbf {A} = \left[ \begin{array}{c c c c c c} - 1 & - 1 & 1 & 0 & 0 & 0 \\ 1 & - 2 & 0 & 1 & 0 & 0 \\ 2 & 1 & 0 & 0 & 1 & 0 \\ - 1 & 1 & 0 & 0 & 0 & 1 \end{array} \right], \quad \mathbf {b} = \left[ \begin{array}{c} - 1 \\ 1 \\ 7 \\ 1 \end{array} \right], \quad \mathbf {c} = \left[ \begin{array}{c} 1 \\ 3 \\ 0 \\ 0 \\ 0 \\ 0 \end{array} \right]
$$

and the initial vertex defined by  $\mathcal{B} = \{1,2,3,4\}$ . Verify that  $\mathcal{B}$  defines a feasible vertex, and then execute one iteration of the simplex algorithm.

Solution: We can verify that  $\mathcal{B} = \{1,2,3,4\}$  is a feasible vertex by verifying that  $\mathbf{A}_{\mathcal{B}}$  is invertible. We have:

$$
\mathbf {A}_{\mathcal {B}} = \left[ \begin{array}{c c c c} - 1 & - 1 & 1 & 0 \\ 1 & - 2 & 0 & 1 \\ 2 & 1 & 0 & 0 \\ - 1 & 1 & 0 & 0 \end{array} \right]
$$

which is invertible.

We begin an iteration of the simplex algorithm by extracting  $\mathbf{x}_{\mathcal{B}}$ :

$$
\mathbf {x}_{\mathcal {B}} = \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {b} = \left[ \begin{array}{c c c c} - 1 & - 1 & 1 & 0 \\ 1 & - 2 & 0 & 1 \\ 2 & 1 & 0 & 0 \\ - 1 & 1 & 0 & 0 \end{array} \right] ^{- 1} \left[ \begin{array}{c} - 1 \\ 1 \\ 7 \\ 1 \end{array} \right] = \left[ \begin{array}{c} 2 \\ 3 \\ 4 \\ 5 \end{array} \right]
$$

We then compute  $\mu_{\mathcal{V}}$

$$
\boldsymbol {\mu}_{\mathcal {V}} = \mathbf {c}_{\mathcal {V}} - \left(\mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\mathcal {V}}\right) ^{\top} \mathbf {c}_{\mathcal {B}} = \left[ \begin{array}{l} 0 \\ 0 \end{array} \right] - \left(\left[ \begin{array}{c c c c} - 1 & - 1 & 1 & 0 \\ 1 & - 2 & 0 & 1 \\ 2 & 1 & 0 & 0 \\ - 1 & 1 & 0 & 0 \end{array} \right] ^{- 1} \left[ \begin{array}{c c} 0 & 0 \\ 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{array} \right]\right) ^{\top} \left[ \begin{array}{l} 1 \\ 3 \\ 0 \\ 0 \end{array} \right] = \left[ \begin{array}{l} - 4 / 3 \\ - 5 / 3 \end{array} \right]
$$

We find that  $\mu_{\gamma}$  contains negative elements, so our current  $\mathcal{B}$  is suboptimal. We can select from the two negative elements, and will choose according to the minimum ratio test. The ratios are:

$$
\begin{array}{l} \mathbf {x}_{(q = 5)}^{\prime} = \left[ \frac{(\mathbf {x}_{\mathcal {B}}) _{p}}{(\mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\{5 \}}) _{p}} \mathrm{for} p \in \{1, 2, 3, 4 \} \right] = [ 6, 9, 6, 16 ] \\ \mathbf {x}_{(q = 6)}^{\prime} = \left[ \frac{(\mathbf {x}_{B}) _{p}}{(\mathbf {A}_{B}^{- 1} \mathbf {A}_{\{6 \}}) _{p}} \mathrm{for} p \in \{1, 2, 3, 4 \} \right] = [ - 6, 4. 5, 12, 3 ] \\ \end{array}
$$

The minimum (positive) ratio is thus 3, achieved with  $q = 6$  and  $p = 4$ . Hence, we update  $\mathcal{B}$  by removing 4 and introducing 6 to obtain  $\mathcal{B}' = \{1,2,3,6\}$ .

Exercise 12.5. Suppose your optimization algorithm has found a search direction  $\mathbf{d}$  and you want to conduct a line search. However, you know that there is a linear constraint  $\mathbf{w}^{\top}\mathbf{x} \geq 0$ . How would you modify the line search to take this constraint into account? You can assume that your current design point is feasible.

Solution: If the current iterate  $\mathbf{x}$  is feasible, then  $\mathbf{w}^{\top}\mathbf{x} = b\geq 0$ . We want the next point to maintain feasibility, and thus we require  $\mathbf{w}^{\top}(\mathbf{x} + \alpha \mathbf{d})\geq 0$ . If the obtained value for  $\alpha$  is positive, that  $\alpha$  is an upper bound on the step length. If the obtained value for  $\alpha$  is negative, it can be ignored.

Exercise 12.6. Suppose we have a linear program with

$$
\mathbf {A} = \left[ \begin{array}{c c c c} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 1 & 1 \end{array} \right] \qquad \mathbf {b} = \left[ \begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right]
$$

and we perform an edge transition to move index  $q = 4$  into  $\mathcal{B} = \{1,2,3\}$ . What would happen if we were to select an entering index that does not satisfy the minimum ratio test?

Solution: The minimum ratio test produces:

$$
x_{q}^{\prime} = 1 \text{for} p = 1
$$

$$
x_{q}^{\prime} = 2 \text{for} p = 2
$$

$$
x_{q}^{\prime} = 3 \text{for} p = 3
$$

According to the minimum ratio test, we would select  $p = 1$  as our entering index. If we choose  $p = 2$  instead, then our new design would have  $\mathcal{B} = \{1,3,4\}$ , which corresponds to the design:

$$
\mathbf {x}_{\mathcal {B}} = \left[ \begin{array}{c c c} 1 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 1 & 1 \end{array} \right] ^{- 1} \left[ \begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right] = \left[ \begin{array}{c} - 1 \\ 1 \\ 2 \end{array} \right]
$$

This design has a negative component, which is not feasible for a linear program in standard form.

Exercise 12.7. Reformulate the linear program

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad \mathbf {A x} \geq \mathbf {0}
$$

into an unconstrained optimization problem with a log barrier penalty.

Solution: We can rewrite the problem:

$$
\underset{\mathbf {x}} {\operatorname{minimize}} \mathbf {c}^{\top} \mathbf {x} - \mu \sum_{i} \ln \left(\mathbf {A}_{\{i \}}^{\top} \mathbf {x}\right)
$$

# 13 Quadratic Programming

Quadratic programming involves solving optimization problems with quadratic objective functions and linear constraints. Many problems are naturally described by quadratic programs, including problems in physics, controls, economics, and operations research. Many problems that are not naturally quadratic can often be approximated by quadratic programs. $^{1}$

# 13.1 Problem Formulation

Like a linear program, a quadratic programming problem, or quadratic program, can be expressed in multiple forms. One general form for a quadratic program is the linear program from equation (12.2) with an additional quadratic objective term:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \mathbf {q}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad \mathbf {C}_{\mathrm{LE}} \mathbf {x} \leq \mathbf {d}_{\mathrm{LE}} \tag {13.1}
$$

$$
\mathbf {C}_{\mathrm{EQ}} \mathbf {x} = \mathbf {d}_{\mathrm{EQ}}
$$

Here  $1/2\mathbf{x}^{\top}\mathbf{Q}\mathbf{x}$  produces quadratic components  $1/2Q_{ii}x_i^2$  and product components  $(1/2Q_{ij} + 1/2Q_{ji})x_ix_j$ .

Quadratic programs naturally arise when creating a second-order approximation of a function  $f$  about a reference point  $\mathbf{x}_{\mathrm{ref}}$ . In this case,  $\mathbf{Q} = \nabla^2 f(\mathbf{x}_{\mathrm{ref}})$  is set to the Hessian and  $\mathbf{q} = \nabla f(\mathbf{x}_{\mathrm{ref}})$  is set to the gradient. Because Hessian matrices are symmetric, many quadratic program specifications assume a symmetric  $\mathbf{Q}$ . Newton's method (equation (6.6)) solves unconstrained optimization problems through sequential second-order approximations. Quadratic programs can therefore be seen as Newton's method subject to linear constraints.

This chapter is a short introduction to quadratic programs and one method used to solve them. Several textbooks are dedicated entirely to quadratic programs, or the closely related least squares problems, including C. L. Lawson and R. J. Hanson, Solving Least Squares Problems. Prentice-Hall, 1974. Like linear programs, quadratic programs can also be solved using common mathematical packages like Convex.jl and JuMP.jl.

When  $\mathbf{Q}$  is positive definite, then  $\mathbf{x}^{\top}\mathbf{Q}\mathbf{x} > 0$  for all  $\mathbf{x} \neq \mathbf{0}$  and the objective is convex. It is then possible to factor  $\mathbf{Q}$  into  $\mathbf{U}^{\top}\mathbf{U}$  using a Cholesky decomposition, and the quadratic program can be written as a least-squares problem:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \| \mathbf {A} \mathbf {x} - \mathbf {b} \|
$$

$$
\text{subject} \quad \mathbf {C}_{\mathrm{LE}} \mathbf {x} \leq \mathbf {d}_{\mathrm{LE}} \tag {13.2}
$$

$$
\mathbf {C}_{\mathrm{EQ}} \mathbf {x} = \mathbf {d}_{\mathrm{EQ}}
$$

with  $\mathbf{A} = \mathbf{U}$  and  $\mathbf{b} = -\mathbf{U}^{-\top}\mathbf{q}$ . A solution to a least squares problem  $\mathbf{x}^*$  is a vector that minimizes the Euclidean length of  $\mathbf{Ax} - \mathbf{b}$  while satisfying the linear constraints, as shown in example 13.1.2

This chapter will focus on quadratic programs that can be represented as least squares problems. We will approach the solution of such a quadratic program through a sequence of problem transformations (figure 13.1). The result of the final transformation is a problem we can solve directly, and the results can be propagated back to determine the solution to the original program.

The feasible set for a quadratic program has the same form as the feasible set for a linear program, which is a convex set formed by the intersection of half-spaces defined by linear inequality constraints. Solutions to a quadratic program need not lie at a vertex of the feasible set, which is in contrast with solutions for linear programs. The quadratic nature of the objective means that a solution can lie within the feasible set or at a point along the feasible set's boundary. Several such cases are shown in figure 13.2.

# 13.2 Unconstrained Least Squares Problems

We first consider least squares problems without any constraints:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \| \mathbf {A} \mathbf {x} - \mathbf {b} \| \tag {13.3}
$$

The objective is minimized when  $\mathbf{A}\mathbf{x}$  is as close to  $\mathbf{b}$  as possible. If  $\mathbf{A}$  is invertible, then we have a unique solution that drives the objective to zero:

$$
\mathbf {x}^{*} = \mathbf {A}^{- 1} \mathbf {b} \tag {13.4}
$$

2 Minimizing the Euclidean length is equivalent to minimizing its square:  $\| \mathbf{A}\mathbf{x} - \mathbf{b}\| ^2$  , which is a quadratic objective function. Problem equivalence is demonstrated in exercise 13.5.  
3 The least squares problem derived from a quadratic program through the Cholesky decomposition will have a square matrix A. However, the least squares algorithms in this chapter support nonsquare matrices.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d1c26d4a1ae630b16b563b030da37115089fc8a210d5a95527abdb2d1aa1da23.jpg)  
Figure 13.1. The series of transformations applied to quadratic programs with positive-definite  $\mathbf{Q}$  matrices.

4 This problem is solved in Julia using the backslash operator by writing  $x = A \backslash b$  as discussed in appendix A.1.5. This section outlines how to solve this problem using the pseudoinverse.

The following problem has a quadratic objective and linear constraints, making it a quadratic program:

$$
\begin{array}{l l} \underset{x_{1}, x_{2}} {\text{minimize}} & \left\| \left[ \begin{array}{c c} 2 & 1 \\ - 4 & 3 \end{array} \right] \left[ \begin{array}{c} x_{1} \\ x_{2} \end{array} \right] - \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] \right\| \\ \text{subjectto} & \begin{array}{l l l} 2 x_{1} & - 1 x_{2} & \geq 2 \\ 1 x_{1} & - 3 x_{2} & \leq 4 \end{array} \end{array}
$$

Below we show a contour plot of the objective function. The linear constraint boundaries are shown with the feasible set shaded.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/549bc8f30eaeef15ca6cc91b1375976187e6e2f5798dc89f9ce94f5b460722f5.jpg)

Example 13.1. An example quadratic program. The objective function forms elliptical contours, the center of which may not lie in the feasible set. Notice that the minimum need not lie at a vertex. Here, the solution is at  $\mathbf{x}^{*} = [1.4,0.8]$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b272ca1c9d9bd832de3b65fb96057f2d3842dadcb981985a2a508fcafe420957.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8c57e60ba7597cd15d1c1f4c4e61d47eda1c29fec796e10d6bd411220fd52e54.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c150489e10bcc4d52b99a8d7690952f86a239792d0ab16d52b8b721ab7b7de7d.jpg)  
Figure 13.2. Quadratic program constraint forms with different solutions. In the last case,  $\mathbf{Q}$  is not positive definite.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f8baa2d0e76a037cdc38b4ad4c7e4769fc1b3adcfec8ba179321f4f53910326d.jpg)

If  $\mathbf{A}$  is not invertible, then solving the least squares problem is slightly more complicated, but we can still derive a solution analytically. In fact, similar to linear programs, multiple solutions can exist. An  $\mathbf{A} \in \mathbb{R}^{m \times n}$  that is rank  $k$  always has a complete orthogonal decomposition given by

$$
\mathbf {A} = \mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top} \tag {13.5}
$$

for orthogonal matrix  $\mathbf{U} \in \mathbb{R}^{m \times m}$ , orthogonal matrix  $\mathbf{V} \in \mathbb{R}^{n \times n}$ , and full-rank matrix  $\mathbf{T} \in \mathbb{R}^{k \times k}$ . We can substitute this decomposition into our objective function:

$$
\left\| \mathbf {A} \mathbf {x} - \mathbf {b} \right\| ^{2} = \left\| \mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top} \mathbf {x} - \mathbf {b} \right\| ^{2} \tag {13.6}
$$

An orthogonal matrix's inverse is equal to its transpose, and multiplication by an orthogonal matrix preserves Euclidean length:

$$
\begin{array}{l} \left\| \mathbf {A} \mathbf {x} - \mathbf {b} \right\| ^{2} = \left\| \mathbf {U}^{- 1} \mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top} \mathbf {x} - \mathbf {U}^{- 1} \mathbf {b} \right\| ^{2} (13.7) \\ = \left\| \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \left(\mathbf {V}^{\top} \mathbf {x}\right) - \left(\mathbf {U}^{\top} \mathbf {b}\right) \right\| ^{2} (13.8) \\ \end{array}
$$

If we split our matrices after the first  $k$  rows, we can further simplify to:

$$
\begin{array}{l} \left\| \mathbf {A} \mathbf {x} - \mathbf {b} \right\| ^{2} = \left\| \left[ \begin{array}{c} \mathbf {T V}_{(1: k, \cdot)}^{\top} \mathbf {x} - \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \\ - \mathbf {U}_{(k + 1: m, \cdot)}^{\top} \mathbf {b} \end{array} \right] \right\| ^{2} (13.9) \\ = \left\| \mathbf {T} \mathbf {V}_{(1: k, \cdot)}^{\top} \mathbf {x} - \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \right\| ^{2} + \left\| \mathbf {U}_{(k + 1: m, \cdot)}^{\top} \mathbf {b} \right\| ^{2} (13.10) \\ \end{array}
$$

Let us split  $\mathbf{V}^{\top}\mathbf{x}$  into a component  $\mathbf{v}_{\mathrm{active}} = \mathbf{V}_{(1:k,\cdot)}^{\top}\mathbf{x}$  that is active in the objective and a component  $\mathbf{v}_{\mathrm{free}} = \mathbf{V}_{(k + 1:n,\cdot)}^{\top}\mathbf{x}$  that is not. Varying the free component does not change the objective value, making it free to vary.

The objective only depends on the active component:

$$
\left\| \mathbf {A} \mathbf {x} - \mathbf {b} \right\| ^{2} = \left\| \mathbf {T} \mathbf {v}_{\text{active}} - \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \right\| ^{2} + \left\| \mathbf {U}_{(k + 1: m, \cdot)}^{\top} \mathbf {b} \right\| ^{2} \tag {13.11}
$$

and is minimized when

$$
\mathbf {v}_{\text{active}}^{*} = \mathbf {T}^{- 1} \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \tag {13.12}
$$

Alternative derivations are provided in Section 12.2 of the textbook by S. Boyd and L. Vandenberghe, Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. Cambridge University Press, 2018.  
One popular choice is the singular value decomposition.

While  $\mathbf{v}_{\mathrm{active}}^{*}$  is unique, the overall optimal design is nonunique because  $\mathbf{v}_{\mathrm{free}}$  can take on any value. We can ensure uniqueness by stipulating that  $\mathbf{v}_{\mathrm{free}} = \mathbf{0}$ . In this case,

$$
\begin{array}{l} \mathbf {x}^{*} = \mathbf {V} \left[ \begin{array}{c} \mathbf {v}_{\text{active}}^{*} \\ \mathbf {0} \end{array} \right] (13.13) \\ = \mathbf {V} \left[ \begin{array}{c} \mathbf {T}^{- 1} \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \\ \mathbf {0} \end{array} \right] (13.14) \\ = \mathbf {V} \left[ \begin{array}{l l} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \left[ \begin{array}{l l} \mathbf {U}_{(1: k, \cdot)}^{\top} & \mathbf {U}_{(k + 1: n, \cdot)}^{\top} \end{array} \right] \mathbf {b} (13.15) \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \mathbf {b} (13.16) \\ \end{array}
$$

We can show that the equation above is equivalent to

$$
\mathbf {x}^{*} = \mathbf {A}^{+} \mathbf {b} \tag {13.17}
$$

where  $\mathbf{A}^{+}$  is the pseudoinverse of  $\mathbf{A}$ . The pseudoinverse can be computed without a complete orthogonal decomposition:

$$
\mathbf {A}^{+} = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \tag {13.18}
$$

In particular,  $\mathbf{A}^{+}$  can be computed using matrix transposes, inverses, and products depending on the linear independence of the rows or columns:8

$$
\mathbf {A}^{+} = \left(\mathbf {A}^{\top} \mathbf {A}\right) ^{- 1} \mathbf {A}^{\top} \quad \text{whenthecolumnsofAarelinearlyindependent} \tag {13.19}
$$

$$
\mathbf {A}^{+} = \mathbf {A}^{\top} \left(\mathbf {A} \mathbf {A}^{\top}\right) ^{- 1} \quad \text{whentherowsofAarelinearlyindependent} \tag {13.20}
$$

$$
\mathbf {A}^{+} = \mathbf {A}^{- 1} \quad \text{when} \mathbf {A} \text{isinvertible} \tag {13.21}
$$

The function pinv in Julia computes the pseudoinverse of a matrix.

Exercise 13.7 shows that these formulations are equivalent to equation (13.18).

# 13.3 Least Squares with Linear Inequalities

The general least squares program given in equation (13.2) can be converted into a least squares with linear inequalities. Here, all constraints are greater-than inequalities:9

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \| \mathbf {A} \mathbf {x} - \mathbf {b} \| \tag {13.22}
$$

$$
\text{subject} \quad \mathbf {C x} \geq \mathbf {d}
$$

To arrive at this form, less-than inequalities can simply be negated. While equality constraints could be split into two, as was done with linear programs, it is more common to eliminate them as covered in section 10.4. Elimination avoids numerical instability that can occur when working with competing inequalities without an interior.

Equality constraints are eliminated using an LQ decomposition  $\mathbf{C}_{\mathrm{EQ}} = \mathbf{L}\mathbf{Q}$  and replacing  $\mathbf{x}$  in our problem with:

$$
\mathbf {x} = \mathbf {Q}^{\top} \left[ \begin{array}{l} \mathbf {y}_{1: m}^{*} \\ \mathbf {y}_{(m + 1: n)} \end{array} \right] \tag {13.23}
$$

where  $\mathbf{y}_{1:m}^{*} = \mathbf{L}^{-1}\mathbf{b}$ . We can substitute this to produce:

$$
\underset{\mathbf {y}_{(m + 1: n)}} {\text{minimize}} \quad \left\| \mathbf {A Q}^{\top} \left[ \begin{array}{l} \mathbf {y}_{1: m}^{*} \\ \mathbf {y}_{(m + 1: n)} \end{array} \right] - \mathbf {b} \right\| \tag {13.24}
$$

$$
\text{subjectto} \quad \mathbf {C Q}^{\top} \left[ \begin{array}{c} \mathbf {y}_{1: m}^{*} \\ \mathbf {y}_{(m + 1: n)} \end{array} \right] \geq \mathbf {d}
$$

which can be simplified to:

$$
\underset{\mathbf {y}_{(m + 1: n)}} {\text{minimize}} \left. \left\| \left(\mathbf {A Q}^{\top}\right) _{(m + 1: n, \cdot)} \mathbf {y}_{(m + 1: n)} - \left(\mathbf {b} - \left(\mathbf {A Q}^{\top}\right) _{(1: m, \cdot)} \mathbf {y}_{1: m}^{*}\right) \right\| \right. \tag {13.25}
$$

$$
\text{subject} \left(\mathbf {C Q}^{\top}\right) _{(n + 1: m, \cdot)} \mathbf {y}_{(m + 1: n)} \geq \left(\mathbf {d} - \left(\mathbf {C Q}^{\top}\right) _{(1: n, \cdot)} \mathbf {y}_{1: m}^{*}\right)
$$

Algorithm 13.1 implements a method for solving a quadratic program in general form by following this conversion. Example 13.2 demonstrates this algorithm.

9 Use of greater-than inequalities is both standard in the literature and prepares for an eventual  $\mathbf{x} \geq 0$  form in a later representation.

```julia
struct QuadraticProgram
# minimize ||Ax - b||
A # matrix of size n×m
b # vector of size n
# subject to C_LE x ≤ d_LE
C_LE # matrix of size (num ≤ constraints)×m
d_LE # vector of size (num ≤ constraints)
# subject to C_EQ x = d_EQ
C_EQ # matrix of size (num = constraints)×m
d_EQ # vector of size (num = constraints)
end
struct Least SquaresWith Inequalities
# minimize ||Ax - b||
A # matrix of size n×m
b # vector of size n
# subject to C x ≥ d
C # matrix of size c×m
d # vector of size c
end
function solve(qp::QuadraticProgram)
A, b, C_LE, d_LE = qp.A, qp.b, qp.C_LE, qp.d_LE
C_EQ, d_EQ = qp.C_EQ, qp.d_EQ
m, n = size(C_EQ)
L, Q = lq(C_EQ)
y1 = L[;,1:m] \ d_EQ
AQ = A*Q'
A1, A2 = AQ[;,1:m], AQ[;,m+1:end]
CQ = -C_LE*Q'
C1, C2 = CQ[;,1:m], CQ[;,m+1:end]
d = -d_LE
lsi = Least SquaresWith Inequalities( A2, b - A1*y1, C2, d - C1*y1)
y2, solved = solve(lsi)
return (Q * [y1; y2], solved)
end
```

Algorithm 13.1. A method for solving a quadratic program in general form. This is accomplished by converting it into a least squares problem with linear inequalities, solving that, and then backing out the original solution. This method assumes that  $\mathbf{C}_{\mathrm{EQ}}\in \mathbb{R}^{c\times n}$  has rank  $c < n$

Consider the following quadratic program in general form:

$$
\begin{array}{l} \underset{\mathbf {x}} {\text{minimize}} \quad \left\| \left[ \begin{array}{r r r} 1 & 2 & 3 \\ - 2 & 0 & 0 \end{array} \right] \mathbf {x} - \left[ \begin{array}{l} 1 \\ 2 \end{array} \right] \right\| \\ \text{subjectto} \qquad \left[ \begin{array}{c c c} 0 & 0 & 1 \\ 1 & 0 & 0 \end{array} \right] \mathbf {x} \leq \left[ \begin{array}{c} 1 \\ 1 \end{array} \right] \\ \left[ \begin{array}{c c c} 0 & 0 & 1 \\ 1 & 0 & 0 \end{array} \right] \mathbf {x} \geq \left[ \begin{array}{c} 0 \\ 0 \end{array} \right] \\ \left[ \begin{array}{c c c} 0 & - 1 & 1 \end{array} \right] \mathbf {x} = \left[ \begin{array}{c} 0 \end{array} \right] \\ \end{array}
$$

This quadratic program is constrained to lie within a square planar region  $0 \leq x_{1} \leq 1, 0 \leq x_{3} \leq 1$ , and  $x_{2} = x_{3}$ .

We can solve this quadratic program by first transforming it into a least squares program with inequalities according to algorithm 13.1. When we do this, we get  $\mathbf{y}_1 = [0]$  and we solve:

$$
\begin{array}{l} \underset{\mathbf {y}_{2}} {\text{minimize}} \quad \left\| \left[ \begin{array}{c c} 3. 20 7 & 1. 79 3 \\ - 1. 41 4 & 1. 41 4 \end{array} \right] \mathbf {y}_{2} - \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] \right\| \\ \text{subjectto} \quad \left[ \begin{array}{c c} 0. 50 0 & 0. 50 0 \\ 0. 70 7 & - 0. 70 7 \\ - 0. 50 0 & - 0. 50 0 \\ - 0. 70 7 & 0. 70 7 \end{array} \right] \mathbf {y}_{2} \geq \left[ \begin{array}{c} 0 \\ 0 \\ - 1 \\ - 1 \end{array} \right] \\ \end{array}
$$

Below we show the original problem (left) and the simplified problem (right). The feasible region in both problems is shown in blue.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/774b956e959557f79e2fbb29633824d923d32ca05d2f57ebcb6b03edfc18e2e1.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0824ec5f82cc740039bf2d185b88419a864a4d5148956218c9e4b6aabb728bf8.jpg)  
y2,1

Example 13.2. An example showing how a 3-dimensional quadratic program with an equality constraint can be transformed into a 2-dimensional least-squares problem with linear inequalities. The solution is  $\mathbf{x} = [0,0.2,0.2]$ .

# 13.4 Least Distance Programming

A least squares problem with linear inequalities can be solved by transforming it into a least distance program:

$$
\underset{\mathbf {x}} {\text{minimize}} \| \mathbf {x} \| \tag {13.26}
$$

$$
\text{subject} \quad \mathbf {G x} \geq \mathbf {h}
$$

where  $\mathbf{G} \in \mathbb{R}^{m \times n}$ ,  $\mathbf{x} \in \mathbb{R}^n$ , and  $\mathbf{h} \in \mathbb{R}^m$ . In other words, we have  $n$  design variables and we seek a design that is as close to the origin as possible while satisfying the inequalities  $\mathbf{Gx} \geq \mathbf{h}$ . Figure 13.3 shows a simple least distance program.

As we saw in equation (13.11), minimizing  $\| \mathbf{A}\mathbf{x} = \mathbf{b}\|$  in our least squares problem with linear inequalities is equivalent to minimizing

$$
\left\| \mathbf {T} \mathbf {v}_{\text{active}} - \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \right\| ^{2} \tag {13.27}
$$

where  $\mathbf{v}_{\mathrm{active}} = \mathbf{V}_{(1:k,\cdot)}^{\top}\mathbf{x}$  and  $\mathbf{U},\mathbf{T},$  and  $\mathbf{V}$  are given by a complete orthogonal decomposition (equation (C.38)).

Using this objective and substituting  $\mathbf{x} = \mathbf{V}_{(1:k,\cdot)}\mathbf{v}_{\mathrm{active}}$  into the inequality for our least squares problem with linear inequalities yields

$$
\underset{\mathbf {v}} {\text{minimize}} \quad \left\| \mathbf {T} \mathbf {v}_{\text{active}} - \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \right\| ^{2} \tag {13.28}
$$

$$
\text{subject} \quad \mathbf {C V}_{(1: k, r)} \mathbf {v}_{\text{avite}} \geq \mathbf {d}
$$

We then drop the squaring and apply another change of variables,

$$
\mathbf {z} = \mathbf {T} \mathbf {v}_{\text{active}} - \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b} \tag {13.29}
$$

and our problem becomes a least distance program:

$$
\underset{\mathbf {z}} {\text{minimize}} \| \mathbf {z} \| \tag {13.30}
$$

$$
\text{subject} \quad \mathbf {C V}_{(1: k, \cdot)} \mathbf {T}^{- 1} \mathbf {z} \geq \mathbf {d} - \mathbf {C V}_{(1: k, \cdot)} \mathbf {T}^{- 1} \mathbf {U}_{(1: k, \cdot)}^{\top} \mathbf {b}
$$

Algorithm 13.2 converts a least squares problem with inequalities into a least distance program, solves it, and backs out the solution. Figure 13.4 shows a least distance program obtained from a least squares problem with inequalities. Before we discuss how to solve least distance programs, we will first introduce and solve a final quadratic problem form.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/66cf017af2f5b4d9782e08772eb0fc085abfd368dafd6515957233c4a85b389b.jpg)  
Figure 13.3. An example least distance program with

$$
\mathbf {G} = \left[ \begin{array}{c c} 2. 0 & - 2. 0 \\ - 1. 0 & 5. 0 \end{array} \right]
$$

$$
\mathbf {h} = \left[ \begin{array}{c} 2. 0 \\ - 7. 0 \end{array} \right]
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7a3cc5e693cf4318a8edb12096bf6037cb5014d4febfff6f3cf877c92f318448.jpg)  
Figure 13.4. The least-squares problem with inequalities from example 13.2 can be converted to a least distance program, as shown here. It has:

$$
\mathbf {G} = \left[ \begin{array}{c c} - 0. 17 6 & 0. 13 8 \\ - 0. 09 9 & - 0. 49 \\ 0. 17 6 & - 0. 13 8 \\ 0. 09 9 & 0. 49 \end{array} \right]
$$

$$
\mathbf {h} = \left[ \begin{array}{c} - 0. 4 \\ 1. 0 \\ - 0. 6 \\ - 2. 0 \end{array} \right]
$$

```julia
struct LeastDistanceProgram
    # minimize ||x||
    # subject to G x ≥ h
    G # matrix of size g×m
    h # vector of size g
end
function solve(qp::LeastSquaresWithInequalities)
    A, b, C, d = qp.A, qp.b, qp.C, qp.d
    res = svd(A) # thin factorization
    k = somethingfindfirst(v → v ≈ 0, res.S),
        length(res.S)+1)-1
    U1 = res.U(:, 1:k] # m × k
    Tinv = Diagonal(res.S[1:k].^-1)
    V1 = res.V(:, 1:k] # n × k
    ldp = LeastDistanceProgram(
        C*V1*Tinv, d - C*V1*Tinv*U1'*b)
    z, solved = solve(ldp)
    return (V1*Tinv*(z + U1'*b), solved)
end
```

Algorithm 13.2. A method for solving a least squares program with linear inequalities. This is accomplished by converting it into a least distance program, solving that, and then backing out the original solution. This method uses the singular value decomposition (appendix C.7.4) for its complete orthogonal decomposition.

# 13.5 Nonnegative Least Squares

Quadratic programs are often solved in the form of a nonnegative least squares (NNLS) problem:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \| \mathbf {E} \mathbf {x} - \mathbf {f} \| \tag {13.31}
$$

$$
\text{subject} \quad \mathbf {x} \geq \mathbf {0}
$$

where  $\mathbf{E} \in \mathbb{R}^{m \times n}$ ,  $\mathbf{x} \in \mathbb{R}^n$ , and  $\mathbf{f} \in \mathbb{R}^n$ . In other words, we have  $n$  nonnegative design variables and we seek to minimize the Euclidean length of a system of  $m$  equations. Geometrically, we are trying to find the point in the positive quadrant closest to  $\mathbf{f}$  under the linear transform  $\mathbf{E}$ . Figure 13.5 show a simple nonnegative least squares problem.

Recall that the simplex algorithm (section 12.2) partitions its constraints into two sets  $\mathcal{B}$  and  $\mathcal{V}$ , and traverses from vertex to vertex by reassigning constraints between these sets. Nonnegative least squares problems can be solved in a similar manner.

The  $n$  design values are again partitioned into two sets  $\mathcal{B}$  and  $\mathcal{V}$ . The design values associated with indices in  $\mathcal{V}$  are zero, whereas the design values associated

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bd4de796d25c48c9393870fbad32ed5e5b5ebeb0e0ef14a7a36518a9f5b30a0a.jpg)  
Figure 13.5. An example nonnegative least squares quadratic program with

$$
\mathbf {E} = \left[ \begin{array}{c c} 2 & 1 \\ - 4 & 3 \end{array} \right] \quad \mathbf {f} = \left[ \begin{array}{c} 3 \\ - 10 \end{array} \right]
$$

with indices in  $\mathcal{B}$  may be positive:

$$
i \in \mathcal {B} \Rightarrow x_{i} \geq 0 \tag {13.32}
$$

$$
i \in \mathcal {V} \Rightarrow x_{i} = 0 \tag {13.33}
$$

Intuitively,  $\mathcal{V}$  refers to all active constraints, and  $\mathcal{B}$  refers to all inactive constraints. The indices in  $\mathcal{B}$  and  $\mathcal{V}$  are changed as the algorithm proceeds, which corresponds to enforcing or relaxing equality constraints.

Suppose we have a partitioning of constraints between  $\mathcal{B}$  and  $\nu$  that corresponds to a solution  $\mathbf{x}^*$  to our nonnegative least squares problem. Then,  $\mathbf{x}_{\nu} = \mathbf{0}$  and  $\mathbf{x}^*$  is a solution to minimize  $\mathbf{x} \| \mathbf{E}_{\mathcal{B}} \mathbf{x} - \mathbf{f} \|$ .

Furthermore, from the KKT conditions, we know that the gradient of our objective function at a solution is balanced by our active constraints:

$$
\nabla f \left(\mathbf {x}^{*}\right) + \sum_{i} \mu_{i} \nabla g_{i} \left(\mathbf {x}^{*}\right) = \mathbf {0} \tag {13-34}
$$

$$
\nabla_{\mathbf {x}} \frac{1}{2} \| \mathbf {E} \mathbf {x} - \mathbf {f} \| ^{2} + \mu (- 1) = \mathbf {0} \tag {13.35}
$$

$$
\boldsymbol {\mu} = \mathbf {E}^{\top} (\mathbf {E x} - \mathbf {f}) \tag {13.36}
$$

where  $\mu_i \geq 0$  for all  $i \in \mathcal{V}$  and  $\mu_i = 0$  for all  $i \in \mathcal{B}$  for which  $x_i > 0$ . Each  $\mu_i$  reflects the degree to which a particular constraint fights against the gradient.

We begin with all constraints active,  $\mathcal{B} = \emptyset$ , which corresponds to  $\mathbf{x} = \mathbf{0}$ , and compute  $\mu$  using equation (13.36). If the KKT conditions hold, then we have found a solution. Otherwise, we relax a constraint, selecting an index  $t \in \mathcal{V}$  that will produce a positive value for  $\mu_t$ . In our implementation, we select the index  $t$  with the most negative  $\mu_t$ , and it from  $\mathcal{V}$  to  $\mathcal{B}$ .

Next, we compute a new tentative solution,  $\mathbf{z} = \mathrm{minimize}_{\mathbf{x}}\| \mathbf{E}_{\mathcal{B}}\mathbf{x} - \mathbf{f}\|$ . We then modify our working solution  $\mathbf{x}$  by taking the largest possible step toward  $\mathbf{z}$  while keeping all entries nonnegative:

$$
\mathbf {x}_{\mathcal {B}} \leftarrow \mathbf {x}_{\mathcal {B}} + \alpha \left(\mathbf {z}_{\mathcal {B}} - \mathbf {x}_{\mathcal {B}}\right) \tag {13.37}
$$

for  $\alpha \in (0,1]$ . Here,  $\alpha$  is usually one, but relaxing the constraint may cause other constraints to be violated, in which case at least one entry in  $\mathbf{z}$  is negative. When that happens, each component  $j$  will need to satisfy  $x_{j} + \alpha (z_{j} - x_{j})\geq 0$ . Rearranging gives us  $\alpha \leq x_{j} / (x_{j} - z_{j})$ . We set  $\alpha$  to the smallest  $x_{j} / (x_{j} - z_{j})$  among all violated entries.

Any newly constrained entries in  $\mathbf{x}$  (where  $x_{i} = 0$ , or  $x_{i} < 0$  due to numerical imprecision) are set to zero and are moved back to  $\mathcal{V}$ . This process is continued until all constraints are satisfied.

Algorithm 13.3 iteratively relaxes constraints until a solution is found. Note that every NNLS problem has a solution, so this algorithm will always produce an answer. This method is demonstrated in example 13.3.

# 13.6 Solving Least Distance Programs

A least distance program, equation (13.26), is solved by forming a nonnegative least squares problem, equation (13.31):

$$
\underset{\mathbf {y}} {\text{minimize}} \quad \| \mathbf {E} \mathbf {y} - \mathbf {f} \| \tag {13.38}
$$

$$
\text{subject} \quad \mathbf {y} \geq \mathbf {0}
$$

where

$$
\mathbf {E} = \left[ \begin{array}{l} \mathbf {G}^{\top} \\ \mathbf {h}^{\top} \end{array} \right] \quad \mathbf {f} = \left[ \begin{array}{l} \mathbf {0} \\ 1 \end{array} \right] \tag {13.39}
$$

The corresponding NNLS problem thus has as many design variables as  $\mathbf{G}$  has rows.

The solution  $\mathbf{y}^*$  to the NNLS problem can be used to obtain the solution  $\mathbf{x}^*$  to our least distance problem. First, we compute the residual vector:

$$
\mathbf {r} = \mathbf {E} \mathbf {y}^{*} - \mathbf {f} \tag {13.40}
$$

Then, we construct an  $n$ -dimensional solution vector  $\mathbf{x}^*$  with components:

$$
x_{j}^{*} = - \frac{r_{j}}{r_{n + 1}} \tag {13.41}
$$

This procedure is implemented in algorithm 13.4. It is used to solve an example least distance program in example 13.4.

The solution  $\mathbf{x}^*$  can only be optimal in our original problem if the KKT conditions are satisfied. To show this, we will use the fact that the squared length of

```julia
struct NonnegativeLeastSquares
    # minimize ||E x - f||
    # subject to x ≥ 0
    E # matrix of size e×m
    f # vector of size e
end
function solve(qp::NonnegativeLeastSquares)
    E, f = qp.E, qp.f
    m, n = size(E)
    x, μ, P, Ep = zeros(n), -E'*f, falses(n), zeros(m, n)
    t prevail = 0
    for iter in 1:3n
        if all(P) || all(μ ≥ 0.0)
            return (x, true) # success!
        end
        t = findmin(i -> (!P[i] && i != t prevail) ? μ[i] : Inf, 1:n)[2]
        t prevail = t
        P[t] = true
        Ep[;,t] = E[;,t]
        α = 0.0
    while any(P) && α != 1.0
        z = pinv(Ep) * f
        if z[t] ≤ 0.0
            μ[t] = 0.0
            break
        end
        α = minimum(z[i] < 0.0 ? x[i] / (x[i] - z[i]) : 1.0
            for i in findall(P))
        x[P] += α * (z[P] - x[P])
        for i in 1:n
            if x[i] ≤ 0.0
                x[i] = 0.0
                if P[i]
                    P[i] = false
                    Ep[;,i] := 0.0
            end
        end
    end
    μ := E'*(E*x-f)
end
return (x, false) # too many iterations - malformed problem
```

Algorithm 13.3. A method for solving a nonnegative least squares problem qp. The solve method returns both the solution vector and whether a solution was found. Repeatedly solving the equation  $\mathbf{z} = \mathrm{minimize}_{\mathbf{x}}\| \mathbf{E}_B\mathbf{x} - \mathbf{f}\|$  with pinv(Ep)  $*$  f is computationally expensive. This algorithm can be made more efficient by maintaining a QR decomposition and applying single-column changes. For an overview and implementation, see C.L. Lawson and R.J. Hanson, Solving Least Squares Problems. Prentice-Hall, 1974.

Algorithm 13.3 solves nonnegative least square problems by starting with all constraints active ( $\mathbf{x} = \mathbf{0}$ ), and iteratively relaxing constraints until a solution is found. Below we show its progression on problems with

$$
\mathbf {E} = \left[ \begin{array}{c c} 2. 0 & 1. 0 \\ - 4. 0 & 3. 0 \end{array} \right]
$$

$\mathbf{f} = [-5.0, 0.0]$ , solved immediately:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3d0987bfadf54081f1431f0265386eddfbec5b437924b8c75fa1cbfd8acf81fc.jpg)

$\mathbf{f} = [-3.0, -12.0]$ , solved after 1 relaxation:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a04e3b9dafe3858854cc4b077f12b9678142f721da795558dd06ee6b136b1583.jpg)

$\mathbf{f} = [5.0, 5.0]$ , solved after 2 relaxations:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bb1215a82bb660b17d7286ae878acfa993a477ef75b50d1c2433e8344ed590c0.jpg)

Example 13.3. Algorithmic progressions on nonnegative least square problems.

```julia
function solve(qp::LeastDistanceProgram,  $\varepsilon = \sqrt{\text{eps}}$  (Float64))  
G, h = qp.G, qp.h  
m, n = size(G)  
x = zeros(n)  
E = [G'; h']  
f = [zeros(n); 1]  
y, solved = solve(NonnegativeLeastSquares(E, f))  
if !solved  
return (x, false) # failed to solve NNLS  
end  
r = E*y - f  
if r[n+1] ≥ - $\varepsilon$   
return (x, false) # zero residual - malformed problem  
end  
return (-r[1:n]./r[n+1], true) # success  
end
```

Algorithm 13.4. A method for solving a least distance program qp. The solve method returns both the solution vector and whether a solution was found.

our residual vector is  $-r_{n + 1}$ . This can be shown by expanding:

$$
\begin{array}{l} \left\| \mathbf {r} \right\| ^{2} = \mathbf {r}^{\top} \mathbf {r} (13.42) \\ = \mathbf {r}^{\top} \left(\mathbf {E y}^{*} - \mathbf {f}\right) (13.43) \\ = \mathbf {r}^{\top} \mathbf {E} \mathbf {y}^{*} - \mathbf {r}^{\top} \mathbf {f} (13.44) \\ = \left(\mathbf {E}^{\top} \mathbf {r}\right) ^{\top} \mathbf {y}^{*} - \mathbf {r}^{\top} \mathbf {f} (13.45) \\ \end{array}
$$

The left term involves a dot product with the gradient of the NNLS objective function at our solution:

$$
\nabla_{\mathbf {y}} \frac{1}{2} \| \mathbf {E} \mathbf {y} - \mathbf {f} \| ^{2} \Bigg | _{\mathbf {y}^{*}} = \mathbf {E}^{\top} \left(\mathbf {E} \mathbf {y}^{*} - \mathbf {f}\right) = \mathbf {E}^{\top} \mathbf {r} \tag {13.46}
$$

We can show that the left term  $\left(\mathbf{E}^{\top}\mathbf{r}\right)^{\top}\mathbf{y}^{*}$  is zero. By equation (13.36), we have  $\boldsymbol{\mu}^{\top}\mathbf{y}^{*}$ , and by complementary slackness we know  $\boldsymbol{\mu}^{\top}\mathbf{g}(\mathbf{y}^{*}) = 0$ . Since  $\mathbf{g}(\mathbf{y}^{*}) = -\mathbf{y}^{*}$ , the term is zero.

The right term,  $\mathbf{r}^\top \mathbf{f}$ , is equal to  $r_{n+1}$  because the first  $n$  components of  $\mathbf{f}$  in our NNLS problem are zero. Hence,  $\|\mathbf{r}\|^2 = -r_{n+1}$ .

Now we can show that  $\mathbf{x}^*$  is a feasible solution. If we solved our NNLS problem, then the gradient of its objective function  $\mathbf{E}^\top \mathbf{r}$  will be nonnegative.<sup>10</sup> As such:

This arises from equation (13.36) and the fact that  $\mu \geq 0$

$$
\mathbf {0} \leq \mathbf {E}^{\top} \mathbf {r} = \left[ \begin{array}{l l} \mathbf {G} & \mathbf {h} \end{array} \right] \left[ \begin{array}{l} \mathbf {x}^{*} \\ - 1 \end{array} \right] (- r_{n + 1}) = (\mathbf {G} \mathbf {x}^{*} - \mathbf {h}) \| \mathbf {r} \| ^{2} \tag {13.47}
$$

We can rearrange to obtain  $\mathbf{G}\mathbf{x}^{*}\geq \mathbf{h}$  , which proves feasibility.

Next we will show that the KKT condition of stationarity holds. The gradient of our objective function at our solution is balanced by our active constraints:

$$
\nabla f \left(\mathbf {x}^{*}\right) + \sum_{i} \mu_{i} \nabla g_{i} \left(\mathbf {x}^{*}\right) = \mathbf {0} \tag {13.48}
$$

$$
\left. \nabla_{\mathbf {x}} \frac{1}{2} \| \mathbf {x} \| ^{2} \right| _{\mathbf {x}^{*}} - \mu \nabla_{\mathbf {x}} \left(G^{\top} \mathbf {x}^{*} - \mathbf {h}\right) = \mathbf {0} \tag {13.49}
$$

$$
\mathbf {x}^{*} - \mathbf {G}^{\top} \boldsymbol {\mu} = \mathbf {0} \tag {13.50}
$$

$$
\mathbf {x}^{*} = \mathbf {G}^{\top} \boldsymbol {\mu} \tag {13.51}
$$

We can match this with our construction for  $\mathbf{x}^*$ :

$$
\begin{array}{l} \mathbf {x}^{*} = - \frac{1}{r_{n + 1}} \mathbf {r}_{1: n} (13.52) \\ = - \frac{1}{r_{n + 1}} \left(\mathbf {E} \mathbf {y}^{*} - \mathbf {f}\right) _{1: n} (13.53) \\ = - \frac{1}{r_{n + 1}} \mathbf {G}^{\top} \mathbf {y}^{*} (13.54) \\ = \mathbf {G}^{\top} \mathbf {y}^{*} \| \mathbf {r} \| ^{- 2} (13.55) \\ \end{array}
$$

We satisfy stationarity with  $\mu = \mathbf{y}^{*}\| \mathbf{r}\|^{-2}$ . Furthermore, we satisfy both dual feasibility and complementary slackness as  $\mathbf{y}_i^*$  is zero for active constraints and positive for inactive constraints. We thus have a primal-dual pair satisfying the KKT conditions for a convex problem, which is sufficient for optimality, implying  $\mathbf{x}^*$  is optimal for the least-distance problem.

# 13.7 Dual Certificates

Quadratic programs that satisfy Slater's condition will have zero duality gap. As with section 12.3, we can verify the optimality of particular primal and dual variables by checking for primal feasibility, dual feasibility, and zero duality gap.

Consider the least distance program from figure 13.3 with

$$
\mathbf {G} = \left[ \begin{array}{c c} 2. 0 & - 2. 0 \\ - 1. 0 & 5. 0 \end{array} \right] \quad \mathbf {h} = \left[ \begin{array}{c} 2. 0 \\ - 7. 0 \end{array} \right]
$$

To solve it, we first form a nonnegative least squares problem with

$$
\mathbf {E} = \left[ \begin{array}{c c} 2. 0 & - 1. 0 \\ - 2. 0 & 5. 0 \\ 2. 0 & - 7. 0 \end{array} \right] \qquad \mathbf {h} = \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right]
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0fb9290008a5ca605dd91c15303bb14e387676a6693e41360e4ebfd07bd66c89.jpg)

The solution to the NNLS problem is  $\mathbf{y}^{*} = [0.167, 0.000]$ .

We then compute the residual, which is  $\mathbf{r} = [0.333, -0.333, -0.667]$ , and after verifying that it has positive norm, compute the solution to our least distance program of  $\mathbf{x}^* = [0.500, -0.500]$ .

Example 13.4. Solving a simple least distance program using an intermediary nonnegative least squares problem.

Consider a general constrained quadratic program

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \mathbf {q}^{\top} \mathbf {x} + q
$$

$$
\text{subject} \quad \mathbf {A x} \leq \mathbf {b} \tag {13.56}
$$

$$
\mathbf {C} \mathbf {x} = \mathbf {d}
$$

with positive-definite  $\mathbf{Q}$ .

Its Lagrangian is also quadratic:

$$
\begin{array}{l} \mathcal {L} (\mathbf {x}, \boldsymbol {\mu}, \boldsymbol {\lambda}) = \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \mathbf {q}^{\top} \mathbf {x} + q + \boldsymbol {\mu}^{\top} (\mathbf {A} \mathbf {x} - \mathbf {b}) + \boldsymbol {\lambda}^{\top} (\mathbf {C} \mathbf {x} - \mathbf {d}) (13.57) \\ = \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \left(\mathbf {q} + \mathbf {A}^{\top} \boldsymbol {\mu} + \mathbf {C}^{\top} \boldsymbol {\lambda}\right) ^{\top} \mathbf {x} + \left(q - \boldsymbol {\mu}^{\top} \mathbf {b} - \boldsymbol {\lambda}^{\top} \mathbf {d}\right) (13.58) \\ = \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \mathbf {e}^{\top} \mathbf {x} + f (13.59) \\ \end{array}
$$

where we choose to define  $\mathbf{e} = \mathbf{q} + \mathbf{A}^{\top}\boldsymbol{\mu} + \mathbf{C}^{\top}\boldsymbol{\lambda}$  and  $f = q - \boldsymbol{\mu}^{\top}\mathbf{b} - \boldsymbol{\lambda}^{\top}\mathbf{d}$ .

The dual function is

$$
\begin{array}{l} \mathcal {D} (\mu \geq 0, \lambda) = \underset{\mathbf {x}} {\operatorname{minimize}} \mathcal {L} (\mathbf {x}, \mu , \lambda) (13.60) \\ = \underset{\mathbf {x}} {\operatorname{minimize}} \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \mathbf {e}^{\top} \mathbf {x} + f (13.61) \\ \end{array}
$$

The dual function is an unconstrained convex optimization problem, and can thus be solved by finding where the gradient is zero. We thus get an analytic solution,  $\mathbf{x}^{*} = -\mathbf{Q}^{-1}\mathbf{e}$ . If we make that substitution and simplify, we get:

$$
\mathcal {D} (\boldsymbol {\mu} \geq \mathbf {0}, \boldsymbol {\lambda}) = - \frac{1}{2} \mathbf {e}^{\top} \mathbf {Q}^{- 1} \mathbf {e} + f \tag {13.62}
$$

The dual problem is thus:

$$
\underset{\mu , \lambda} {\text{maximize}} - \frac{1}{2} \mathbf {e}^{\top} \mathbf {Q}^{- 1} \mathbf {e} + f \tag {13.63}
$$

$$
\text{subject} \quad \mu \geq 0
$$

This dual problem may be easier to solve than the primal problem.

# 13.8 Summary

- Quadratic programs are optimization problems with quadratic objectives and linear constraints.

- A convex quadratic program can be equivalently written as a least squares problem.  
- Unconstrained least squares problems can be solved exactly, using the pseudoinverse.  
- A general least squares problem can be transformed into a least squares problems with linear inequalities.  
- A least square problem with linear inequalities can be solved by transforming it into a least distance program, which in turn can be solved by transforming it into a nonnegative least squares problem.  
- By strong duality, the solution to the dual problem can be used to verify that a candidate primal solution is optimal.

# 13.9 Exercises

Exercise 13.1. Are nonnegative least squares problems always feasible?

Solution: Yes, nonnegative least squares problems are always feasible, as the feasible set  $\mathbf{x} \geq \mathbf{0}$  always allows the positive quadrant.

Exercise 13.2. Are least distance programs always feasible?

Solution: No, least distance programs are not always feasible, as the feasible set  $\mathbf{G}\mathbf{x} \geq \mathbf{h}$  may be empty. We can easily construct an empty feasible set, for example, by combining  $\mathbf{1}^{\top}\mathbf{x} \geq 1$  and  $-\mathbf{1}^{\top}\mathbf{x} \geq 1$ .

Exercise 13.3. Do least distance programs have unique solutions?

Solution: Yes, if a least distance program has a solution, then it is unique.

Suppose that we have two optimal solutions  $\mathbf{a}$  and  $\mathbf{b}$  with  $\| \mathbf{a} \| = \| \mathbf{b} \|$ . Then we could construct  $\mathbf{c} = \frac{1}{2} (\mathbf{a} + \mathbf{b})$ , which would both be feasible (because our feasible set is convex) and  $\mathbf{c}$  would have smaller norm. This is a contradiction, as then  $\mathbf{a}$  and  $\mathbf{b}$  would not be optimal. As such, solutions to LDPs are unique.

Exercise 13.4. Can all linear programs be represented as quadratic programs and least squares problems?

Solution: Any linear program can trivially be represented as a quadratic program with  $\mathbf{Q} = \mathbf{0}$ .

For a linear program to be translated into a least squares problem, we would need to be able to write a linear objective function  $\mathbf{c}^{\top}\mathbf{x}$  in a quadratic form. We can expand the squared-norm objective:

$$
\begin{array}{l} \mathbf {c}^{\top} \mathbf {x} = \frac{1}{2} \| \mathbf {A x} + \mathbf {b} \| ^{2} = \frac{1}{2} (\mathbf {A x} + \mathbf {b}) ^{\top} (\mathbf {A x} + \mathbf {b}) \\ = \frac{1}{2} (\mathbf {A x}) ^{\top} \mathbf {A x} + \frac{1}{2} \mathbf {b}^{\top} \mathbf {A x} + \frac{1}{2} (\mathbf {A x}) ^{\top} \mathbf {b} + \frac{1}{2} \mathbf {b}^{\top} \mathbf {b} \\ = \frac{1}{2} \mathbf {x}^{\top} \mathbf {A}^{\top} \mathbf {A x} + \mathbf {b}^{\top} \mathbf {A x} + \frac{1}{2} \mathbf {b}^{\top} \mathbf {b} \\ \end{array}
$$

While we could ignore the constant  $\frac{1}{2}\mathbf{b}^{\top}\mathbf{b}$ , we would need  $\mathbf{b}^{\top}\mathbf{A} = \mathbf{c}^{\top}$  and  $\frac{1}{2}\mathbf{A}^{\top}\mathbf{A} = \mathbf{0}$ . Because these constraints are not reconcileable, linear programs cannot generally be translated into least squares problems.

Exercise 13.5. Show that a least-squares problem (equation (13.2)) is equivalent to a general form problem (equation (13.1)) for a positive definite  $\mathbf{Q} = \mathbf{U}^{\top}\mathbf{U}$ .

Solution: The constraints in equations (13.1) and (13.2) are the same. To show equivalence, we can focus on the objectives:

$$
\begin{array}{l} \underset{\mathbf {x}} {\text{minimize}} \| \mathbf {A} \mathbf {x} - \mathbf {b} \| = \underset{\mathbf {x}} {\text{minimize}} \left\| \mathbf {U} \mathbf {x} + \mathbf {U}^{- \top} \mathbf {q} \right\| \\ = \underset{\mathbf {x}} {\operatorname{minimize}} \left\| \mathbf {U} \mathbf {x} + \mathbf {U}^{- \top} \mathbf {q} \right\| ^{2} \\ = \underset{\mathbf {x}} {\operatorname{minimize}} \left(\mathbf {U} \mathbf {x} + \mathbf {U}^{- \top} \mathbf {q}\right) ^{\top} \left(\mathbf {U} \mathbf {x} + \mathbf {U}^{- \top} \mathbf {q}\right) \\ = \underset{\mathbf {x}} {\operatorname{minimize}} \left(\mathbf {x}^{\top} \mathbf {U}^{\top} + \mathbf {q}^{\top} \mathbf {U}^{- 1}\right) \left(\mathbf {U} \mathbf {x} + \mathbf {U}^{- \top} \mathbf {q}\right) \\ = \underset{\mathbf {x}} {\text{minimize}} \mathbf {x}^{\top} \mathbf {U}^{\top} \mathbf {U x} + \mathbf {x}^{\top} \mathbf {U}^{\top} \mathbf {U}^{- \top} \mathbf {q} + \mathbf {q}^{\top} \mathbf {U}^{- 1} \mathbf {U x} + \mathbf {q}^{\top} \mathbf {U}^{- 1} \mathbf {U}^{- \top} \mathbf {q} \\ = \underset{x} {\text{minimize}} x^{\top} Q x + x^{\top} q + q^{\top} x + q^{\top} q \\ = \underset{\mathbf {x}} {\text{minimize}} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + 2 \mathbf {q}^{\top} \mathbf {x} \\ = \underset{\mathbf {x}} {\text{minimize}} \frac{1}{2} \mathbf {x}^{\top} \mathbf {Q} \mathbf {x} + \mathbf {q}^{\top} \mathbf {x} \\ \end{array}
$$

Exercise 13.6. What are some relative advantages and disadvantages between solving a general least squares problem using the techniques presented in this chapter as opposed to using an interior point method?

Solution: The method presented in this chapter applies a set of transformations until the general least squares problem is represented as a nonnegative least squares problem. The NNLS problem is solved in an iterative manner in which constraints are iteratively relaxed until a solution is found. There are a finite number of constraints, so one can derive timing bounds on how long it will take to solve a problem of a certain size. These guarantees are perhaps the method's greatest advantage.

11 We introduce a factor of  $1/2$ , which does not change the minimizer.

Interior point methods can also be used to solve quadratic programs. Interior point methods must start with a feasible point, and finding a feasible point for a general least squares problem may be non-trivial. However, once such a point is identified, the problem can be optimized directly without needing to apply successive transformations. Interior point methods will converge to an optimum rather than arrive exactly at an optimum, but can also be stopped early and still provide a feasible solution. They are also more general, and can support problems beyond general least squares problems.

Exercise 13.7. Use the complete orthogonal decomposition in equation (C.38) together with equations (13.19) to (13.21) to show that the psuedoinverse of a matrix is always

$$
\mathbf {A}^{+} = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}
$$

Solution: When  $\mathbf{A}$  is invertible, the pseudoinverse is given by equation (13.21):

$$
\mathbf {A}^{+} = \mathbf {A}^{- 1} = \left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{- 1} = \left(\mathbf {V}^{\top}\right) ^{- 1} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] ^{- 1} \mathbf {U}^{- 1} = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}
$$

where we use the fact that  $\mathbf{T}$  is diagonal,  $\mathbf{U}$  and  $\mathbf{V}$  are orthogonal, and an orthogonal matrix's inverse is equal to its transpose.

When the columns of  $\mathbf{A}$  are linearly independent, the pseudoinverse is given by equation (13.19):

$$
\begin{array}{l} \mathbf {A}^{+} = \left(\mathbf {A}^{\top} \mathbf {A}\right) ^{- 1} \mathbf {A}^{\top} \\ = \left(\left(\mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{\top} \left(\mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right)\right) ^{- 1} \left(\mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{\top} \\ = \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{- 1} \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \\ = \left(\mathbf {V} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{- 1} \left(\mathbf {V} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \\ = \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{- 1} \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \\ = \left(\mathbf {V}^{\top}\right) ^{- 1} \left[ \begin{array}{c c} \mathbf {T}^{2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] ^{- 1} \mathbf {V}^{- 1} \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top} \mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \\ \end{array}
$$

When the rows of  $\mathbf{A}$  are linearly independent, the pseudoinverse is given by equation (13.20):

$$
\begin{array}{l} \mathbf {A}^{+} = \mathbf {A}^{\top} \left(\mathbf {A} \mathbf {A}^{\top}\right) ^{- 1} \\ = \left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{\top} \left(\left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) \left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top}\right) ^{\top}\right) ^{- 1} \\ = \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top} \mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) ^{- 1} \\ = \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) ^{- 1} \\ = \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \left(\mathbf {U} \left[ \begin{array}{c c} \mathbf {T}^{2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) ^{- 1} \\ = \left(\mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top}\right) \left(\left(\mathbf {U}^{\top}\right) ^{- 1} \left[ \begin{array}{c c} \mathbf {T}^{2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] ^{- 1} (\mathbf {U}) ^{- 1}\right) \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \mathbf {U} \left[ \begin{array}{c c} \mathbf {T}^{- 2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \left[ \begin{array}{c c} \mathbf {T}^{- 2} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \\ = \mathbf {V} \left[ \begin{array}{c c} \mathbf {T}^{- 1} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {U}^{\top} \\ \end{array}
$$

# 14 Disciplined Convex Programming

Convex optimization problems are those with a convex objective function and constraints that define a convex feasible set. Many general techniques have been developed to efficiently compute a global optimum. Unfortunately, determining whether an arbitrary nonlinear program is convex is intractable. This chapter introduces the concept of a disciplined convex program (DCP),<sup>1</sup> which is a type of convex optimization problem that can be written in such a way that automated methods can both verify that the problem is convex and automatically transcribe the problem into a canonical form.<sup>2</sup> Although not all convex problems can be framed in terms of a disciplined convex program, a wide variety of convex problems fall within this class. The canonical form can then be efficiently solved using a variety of methods.

# 14.1 Canonical Form

A convex program is a general optimization problem in which the objective and all inequality constraints are convex, and the equality constraints are affine:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathbf {x})
$$

$$
\text{subject} \quad g_{j} (\mathbf {x}) \leq 0 \quad \text{for} j \text{in} 1: m \tag {14.1}
$$

$$
\mathbf {A} \mathbf {x} = \mathbf {b} \quad \text{for} \mathbf {A} \in \mathbb {R}^{\ell \times n}, \mathbf {b} \in \mathbb {R}^{\ell}
$$

The canonical form of a disciplined convex program is:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x} + d
$$

$$
\text{subject} \quad \mathbf {x} \in S \tag {14.2}
$$

$$
\mathbf {A} \mathbf {x} = \mathbf {b} \quad \text{for} \mathbf {A} \in \mathbb {R}^{\ell \times n}, \mathbf {b} \in \mathbb {R}^{\ell}
$$

1 M. Grant, S. Boyd, and Y. Ye, "Disciplined Convex Programming," Global Optimization: From Theory to Implementation, pp. 155-210, 2006.

2 There are several disciplined convex programming packages spanning many of the commonly used languages, including Convex.jl for Julia and cvxpy for Python. These packages support various backend solvers such as SCS, ECOS, and OSQP. S. Diamond and S. Boyd, "CVXPY: A PythonEmbedded Modeling Language for Convex Optimization," Journal of Machine Learning Research, vol. 17, pp. 1-5, 2016. A. Domahidi, E. Chu, and S. Boyd, "ECOS: An SOCP Solver for Embedded Systems," in European Control Conference (ECC), 2013. B. Stellato, G. Banjac, P. Goulart, A. Temporad, and S. Boyd, "OSQP: an Operator Splitting Solver for Quadratic Programs," Mathematical Programming Computation, vol. 12, no. 4, pp. 637-672, 2020.

where  $S \subset \mathbb{R}^n$  is a convex set. We will discuss  $S$  and the canonical form in greater detail after we have covered necessary background content. The process of disciplined convex programming converts problems into this canonical form. Example 14.1 shows an example of a disciplined convex program that can be transformed programmatically and optimized using a standard solver.

We can use Convex.jl to transform the following optimization problem into a canonical form that can then be solved using the SCS solver:

```txt
$\begin{array}{rl} & {\underset{x}{\mathrm{minimize}}\quad \| \mathbf{Ax} - \mathbf{b}\|_{1}}\\ & {\mathrm{subject~to}\quad \| \mathbf{Cx} - \mathbf{d}\|_{1.5}\leq 3} \end{array}$    
julia> using Convex, SCS;  
julia> A = [2 -1; 13]; b = [4, -5];  
julia> C = [01; 3 -4]; d = [-1, 0];  
julia> x = Variable(2);  
julia> problem = minimize(  
norm(A*x - b, 1),  
norm(C*x - d, 1.5) ≤ 3  
);  
julia> solve!(problem, SCS.Optimizer, silent=true);  
julia> problem.status  
OPTIMAL::TerminationStatusCode = 1  
julia> round.(evaluate(x), digits=3)  
2-element Vector{Float64}:  
-0.867  
-1.378
```

Example 14.1. Using disciplined convex programming to solve a simple optimization problem. The SCS solver was introduced by B. O'Donoghue, E. Chu, N. Parikh, and S. Boyd, "Conic Optimization via Operator Splitting and Homogeneous Self-Dual Embedding," Journal of Optimization Theory and Applications, vol. 169, no. 3, pp. 1042-1068, 2016.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cd094c47fedeba77cd6d0b3206c01fd8f1b89651663fe6c15760eed463cfaad5.jpg)

The optimized design lies on the boundary of the feasible region, which is outlined in white.

# 14.2 Verification

We wish to have an automated way to verify whether an input problem is convex. For example, a given minimization problem is a convex program if its objective function is convex, if all of its inequality constraints are convex, and if its equality constraints are affine. Automatically verifying convex program convexity is called verification.

To conduct this verification, we define a grammar.3 Any input problem that adheres to this grammar is considered to be equivalent to a disciplined convex program, and thus a convex program. Furthermore, we can then automatically convert that input problem into canonical form.

The input problem can be a minimization, a maximization, or a feasibility problem.4 All of these can be turned into minimization problems, either by negating the objective or by introducing a constant as the objective. Hence, verification can, without loss of generality, operate on minimization problems.

The constraints in a given minimization problem must be convex if they are inequality constraints, and affine if they are equality constraints. We can stipulate this more concretely by introducing a few top-level production rules for an input constraint  $\mathcal{C}$ :5

$$
\begin{array}{l} \mathcal {C} \mapsto \text{affine} = \text{affine} \\ \mathcal {C} \mapsto \text{convex} \leq \text{concave} \\ \mathcal {C} \mapsto \text{convex} <   \text{concave} \\ \begin{array}{l l} \text{14.3} \end{array} \\ \mathcal {C} \mapsto \operatorname{concave} \geq \operatorname{convex} \\ \mathcal {C} \mapsto \text{concave} > \text{convex} \\ \mathcal {C} \mapsto (\text{affine}, \text{affine}, \dots , \text{affine}) \in \text{convexset} \\ \end{array}
$$

These production rules ensure that all disciplined convex program constraints have one of the above forms. The final entry ensures that set membership constraints only involve affine expressions.

As we can see, determining whether an expression is affine, convex, or concave is central to disciplined convex programming. We will call this property the curvature of the expression. The many subsequent rules in the disciplined convex grammar will be used to determine curvature.

# 14.2.1 Atom Library

We need base symbols from which to construct our more complicated affine, convex, and concave expressions. Our most basic terminal symbols are numeric constants and problem variables, both of which are affine:

$$
a f f i n e \mapsto \text{const} \mid \text{var} \tag {14.4}
$$

3Grammars are covered in section 23.1.  
4 A constrained problem with a constant objective is called a feasibility problem and is solved by finding any feasible design.  
An inequality  $f(\mathbf{x}) \leq h(\mathbf{x})$  for convex  $f$  and concave  $h$  is the same as  $f(\mathbf{x}) - h(\mathbf{x}) \leq 0$ , where  $g(\mathbf{x}) = f(\mathbf{x}) - h(\mathbf{x})$  is necessarily convex. This form matches the inequality constraints for convex programs.

However, we want to compose these constants and variables with functions and sets. To this end, we define an atom library that encodes additional elements. We associate additional properties with each element in the atom library, most notably whether it is affine, convex, or concave. For functions, monotonicity and range are also included. We will use these properties to determine curvature when these expressions are combined.

Table 14.1 shows examples of atoms in an atom library. The last atom in that table defines  $x^p$  with a restricted domain. When we restrict the curvature of a function to a particular domain, we are effectively defining a new function where values outside of that domain are infinite. A function that can take on infinite values is called an extended real-valued function, or an extended-valued function for short. Such an extension is convenient because we do not have to include a domain qualifier. Figure 14.1 shows the extended-valued form of  $x^p$  for  $p = 3$ . We can use such restrictions to provide multiple definitions for a single function, capturing different curvature regions.

<table><tr><td>Function</td><td>Args</td><td>Curvature</td><td>Monotonicity</td><td>Range</td></tr><tr><td>exp(x)</td><td>var x</td><td>convex</td><td>increasing in x</td><td>≥0</td></tr><tr><td>log(x)</td><td>var x</td><td>concave for x &gt; 0</td><td>increasing in x</td><td>IR</td></tr><tr><td>xp</td><td>var x, const p</td><td>convex for x ≥ 0, p &gt; 1</td><td>increasing in x</td><td>≥0</td></tr></table>

Such basic atoms give us basic forms of composition. Other forms of composition involve functions with multiple inputs. For example, we can define a more complicated expression such as  $\max(\exp(x), 2 - x^3)$  by including  $\max(x, y)$  as an element and allowing composition. Table 14.2 provides other examples.

<table><tr><td>Function</td><td>Args</td><td>Curvature</td><td>Monotonicity</td><td>Range</td></tr><tr><td>x + y</td><td>var x, var y</td><td>affine</td><td>increasing in x and y</td><td>R</td></tr><tr><td>x - y</td><td>var x, var y</td><td>affine</td><td>increasing in x, decreasing in y</td><td>R</td></tr><tr><td>max(x, y)</td><td>var x, var y</td><td>convex</td><td>increasing in x and y</td><td>R</td></tr></table>

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/baed5af8eaa899cc800feb54c725fefaa81fdfe1ae59cb1e570aec6679524ac5.jpg)  
Figure 14.1. The function  $x^3$  is not convex, but if we restrict ourselves to  $x \geq 0$ , then the extended-valued form is convex over all  $\mathbb{R}$ . Monotonicity only holds for the constrained range  $(x \geq 0)$ .

Table 14.1. An example of atoms in an atom library.

Table 14.2. Examples of composition with multiple inputs.

The atom library also contains definitions for sets. Sets can be marked as convex or nonconvex. Table 14.3 provides examples.

<table><tr><td>Set</td><td>Args</td><td>Convex</td><td>Definition</td></tr><tr><td>real numbers</td><td>var x</td><td>yes</td><td>{x ∈ ℝn}</td></tr><tr><td>positive orthant</td><td>var x</td><td>yes</td><td>{x ∈ ℝn | x ≥ 0}</td></tr><tr><td>positive definite</td><td>var X</td><td>yes</td><td>{X ∈ ℝn×n | u^\top Xu ≥ 0 for all u ≠ 0}</td></tr></table>

The atom library is extensible. Domain experts are free to introduce new functions and sets, and in so doing, enhance the capabilities of the disciplined convex solver.

# 14.2.2 Product-Free Rules

Disciplined convex programming does not allow products between non-constant expressions. This is because, in general, knowing the affine / convex / concave nature of each of the two expressions involved in a product is not enough to know whether the overall expression is affine, convex, or concave.<sup>6</sup> All numeric expressions must be product-free expressions:<sup>7</sup>

$$
a + b_{1} x_{1} + b_{2} x_{2} + \dotsb b_{n} x_{n} + c_{1} f_{1} (\boldsymbol {\epsilon}_{1}) + c_{2} f_{2} (\boldsymbol {\epsilon}_{2}) + \dotsb + c_{q} f_{q} (\boldsymbol {\epsilon}_{q}) \quad (14. 5)
$$

where  $\mathbf{x}$  is the design vector;  $a$ ,  $\mathbf{b}$ , and  $\mathbf{c}$  are constants; the functions  $f_{1:q}$  are defined in the atom library; and each  $\epsilon_j$  represents function arguments. All arguments to atom-library functions must also be product-free expressions.

For sets, unions can create ambiguity over whether the resulting set is affine, convex, concave, or otherwise. A similar set of rules exists for set expressions that constrains valid set expressions to a union-free representation.

# 14.2.3 Sign Rules

We can now define rules that establish whether a product-free expression is convex, concave, affine, or none of the above. We know that the  $a + \mathbf{b}^\top \mathbf{x}$  portion of any product-free expression is affine, so the curvature will ultimately depend on the functions  $\mathbf{f}_{1:q}$  and the constants  $\mathbf{c}$ . These rules are called the sign rules because they stipulate the signs of the  $\mathbf{c}$  constants.

The first sign rules state that for  $c_{j}f_{j}(\cdot)$  to be a convex expression:

Table 14.3. Examples of set definitions.

For example,  $f(x_{1},x_{2}) = x_{1}x_{2}$  is affine in either  $x_{1}$  or  $x_{2}$ , but it is not jointly convex in  $x_{1}$  and  $x_{2}$ .  
Some examples include constants, for which  $\mathbf{b} = \mathbf{0}$  and  $q = 0$ ; affine expressions  $a + \mathbf{b}^{\top}\mathbf{x}$ , for which  $q = 0$ , and basic function calls, such as  $\log (x_2)$ , where  $a = 0$ ,  $\mathbf{b} = \mathbf{0}$ , and  $q = 1$ .

- If  $f_{j}(\cdot)$  is convex, then  $c_{j}$  must be nonnegative.  
- If  $f_{j}(\cdot)$  is concave, then  $c_{j}$  must be nonpositive.  
- If  $f_{j}(\cdot)$  is affine, then  $c_{j}$  can have any sign.

Similarly, for  $c_{j}f_{j}(\cdot)$  to be a concave expression,  $c_{j}$  must have the opposite sign than it would for a convex expression. An affine expression requires that each  $f_{j}(\cdot)$  and all of its arguments be affine. A constant expression must have  $\mathbf{b} = \mathbf{0}$  and an empty  $\mathbf{f}$ .

Consider the following convex optimization problem:

$$
\underset{x_{1}, x_{2}} {\text{minimize}} c_{1} \exp \left(x_{1} + 3 x_{2}\right)
$$

$$
\text{subject} \quad 3 + 2 x_{1} + c_{2} \max \left(x_{1}, x_{2}\right) <   c_{3} \log \left(x_{2}\right)
$$

We can verify that our objective function is convex. We have  $c_{1} \exp(x_{1} + 3x_{2})$ , which involves a call to exp in our atom library. We know from our atom library that exp is convex, and so our sign rule requires that  $c_{1} \geq 0$ . Finally, we verify that the argument is affine, which  $x_{1} + 3x_{2}$  is.

We next check our constraint, and we see that it must obey the form convex  $<$  concave. Consequently,  $3 + 2x_{1} + c_{2}\max (x_{1},x_{2})$  must be convex, which involves a call to max in our atom library. We look up max, and see that it is convex, and so our sign rule requires that  $c_{2}\geq 0$ .

Finally,  $c_{3} \log (x_{2})$  must be concave, which involves a call to log in our atom library. We look up log and see that it is concave, and so our sign rule requires that  $c_{3} \leq 0$ .

Example 14.2. An application of the sign rules to a simple convex optimization problem.

# 14.2.4 Composition Rules

We now turn to enforcing the curvature of composed expressions, or expressions with function arguments that are themselves dependent on functions from the atom library, such as  $f(g(x))$ .

Suppose  $f$  and  $g$  are extended-valued single-argument functions.

- If  $f$  is convex and nondecreasing over the range of  $g$ , and  $g$  is convex, then  $f(g(x))$  is convex.

8 We extend convex functions to  $+\infty$  and concave functions to  $-\infty$

- If  $f$  is convex and nonincreasing over the range of  $g$ , and  $g$  is concave, then  $f(g(x))$  is convex.  
- If  $f$  is concave and nondecreasing over the range of  $g$ , and  $g$  is concave, then  $f(g(x))$  is concave.  
- If  $f$  is concave and nonincreasing over the range of  $g$ , and  $g$  is convex, then  $f(g(x))$  is concave.

Consider the expression  $\exp (\sin (x))$  for  $x\in [-\pi ,0]$ . In this case, we have  $f(g(x))$  where  $f(x) = \exp (x)$  and  $g(x) = \sin (x)$ . We know that  $\sin (x)$  is convex over  $[- \pi ,0]$ , with a range of  $[-1,0]$ . We also know that  $\exp x$  is convex and nondecreasing for all  $x$ , so the composition rules state that  $\exp (\sin x)$  is convex for  $x\in [-\pi ,0]$ .

Next consider the expression  $\sqrt{\exp(x)}$ . In this case, we have  $f(g(x))$  where  $f(x) = \sqrt{x}$  and  $g(x) = \exp(x)$ . We know that  $\exp(x)$  is convex, with a range of  $[0, \infty]$ . We also know that  $\sqrt{x}$  is concave and nondecreasing for  $x \in [0, \infty]$ , so the composition rules are inconclusive. In fact,  $\sqrt{\exp(x)}$  is convex because its second derivative  $\exp(x/2)/4$  is a positive function. We cannot use the composition rules to determine the curvature of  $\sqrt{\exp(x)}$ . Such an expression is invalid in disciplined convex programming.

Applying these rules requires ensuring that the range of  $g$  is contained within the domain of  $f$  in which the curvature of  $f$  holds. Since we restrict ourselves to cases where  $g$  is a product-free expression, we can produce a conservative bound on its range by accumulating all of the ranges of each design variable and each constituent function: $^9$

Example 14.3. Applying the composition rules to example expressions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6bb806c324be5118e35ad262f326805cabdae0cabaac5185aa2948f3e1a28c49.jpg)

9 These ranges and domains of constituent functions are stored in the atom library metadata.

$$
\operatorname{range} (g) \subseteq a + b_{1} \operatorname{range} \left(x_{1}\right) + \dots + b_{n} \operatorname{range} \left(x_{n}\right) + c_{1} \operatorname{range} \left(f_{1}\right) + \dots + c_{q} \operatorname{range} \left(f_{q}\right) \tag {14.6}
$$

where  $\mathrm{range}(f)$  provides simple interval bounds for a function  $f$  and  $+$  denotes Minkowski addition.

This process holds for functions with multiple inputs,  $f(\epsilon_1, \ldots, \epsilon_m)$ . We use equation (14.6) to construct a bounded range  $\mathcal{R}_i$  for each argument  $\epsilon_i$ . The domain of  $f$  is therefore  $(\epsilon_1, \ldots, \epsilon_m) \in \mathcal{R} = \mathcal{R}_1 \times \dots \times \mathcal{R}_m$ . Then, exactly one of the following holds:

10 If  $x$  is bounded to the interval  $[a,b]$  and  $y$  is bounded to the interval  $[c,d]$ , then  $x + y$  is bounded to be within the interval  $[a + c,b + d]$ . 11 If  $A$  and  $B$  are sets of vectors, then the Minkowski sum of these two sets is given by  $A + B = \{\mathbf{a} + \mathbf{b}\mid \mathbf{a}\in A,\mathbf{b}\in B\}$ .

Consider an  $f(g(\mathbf{x}))$  expression with

$$
f (x) = 1 / x
$$

$$
g (\mathbf {x}) = 3 + 2 \log (1 + x_{1}) - \sin x_{2}
$$

for  $x_1 \in [0, \infty]$  and  $x_2 \in [-\pi, 0]$ . We know that  $f(x)$  is convex and nonincreasing for  $x > 0$ , and concave and nonincreasing for  $x < 0$ . Thus, the range of  $g(\mathbf{x})$  is crucial in determining the curvature of our expression.

We know that  $g(\mathbf{x})$  is concave, because  $2\log x$  for  $x \geq 1$  is concave and  $\sin x$  for  $x \in [-\pi, 0]$  is convex, and so  $-\sin x$  is concave. We apply equation (14.6) to our  $g(\mathbf{x})$  expression to bound its range:

$$
\begin{array}{l} \operatorname{range} (g) \subseteq 3 + 2 \cdot \operatorname{range} (\log (1 + x_{1})) - \sin x_{2} \\ = [ 3, 3 ] + 2 \cdot [ 0, \infty ] - [ - 1, 0 ] \\ = [ 3, 3 ] + [ 0, \infty ] + [ 0, 1 ] \\ = [ 3, \infty ] \\ \end{array}
$$

The bounded range is thus  $g(\mathbf{x}) \in [3, \infty]$ , a domain within which  $f(x)$  is convex and nonincreasing. This fact, and the fact that  $g(\mathbf{x})$  is concave, means  $f(g(\mathbf{x}))$  is convex.

Example 14.4. Applying the composition rules to a more complicated expression.

- If  $f(\epsilon_1, \epsilon_2, \ldots, \epsilon_m)$  is expected to be convex, then  $f$  must be affine or convex, and for each argument  $\epsilon_i$ :

-  $f$  is nondecreasing in  $\epsilon_{i}$  over  $\mathcal{R}$ , and  $\epsilon_{i}$  is convex, or  
-  $f$  is nonincreasing in  $\epsilon_{i}$  over  $\mathcal{R}$ , and  $\epsilon_{i}$  is concave, or  
-  $\epsilon_{i}$  is affine

- If  $f(\epsilon_1, \epsilon_2, \ldots, \epsilon_m)$  is expected to be concave, then  $f$  must be affine or concave, and for each argument  $\epsilon_i$ :

-  $f$  is nondecreasing in  $\epsilon_{i}$  over  $\mathcal{R}$ , and  $\epsilon_{i}$  is concave, or  
-  $f$  is nonincreasing in  $\epsilon_{i}$  over  $\mathcal{R}$ , and  $\epsilon_{i}$  is convex, or  
-  $\epsilon_{i}$  is affine

- If  $f(\epsilon_1, \epsilon_2, \ldots, \epsilon_m)$  is expected to be affine, then  $f$  must be affine, and all arguments must be affine.

Examples for single and multi-input expressions are given in example 14.4 and example 14.5.

The composition rules guarantee that efficient decompositions are possible. For example, if the first rule applies and  $f$  is convex and nondecreasing, then we can decompose the inequality  $f(g(x)) \leq y$  into two inequalities involving a new variable  $z$ :

$$
\begin{array}{l} f (g (x)) \leq y \quad \Longleftrightarrow \quad f (z) \leq y \tag {14.7} \\ g (x) \leq z \\ \end{array}
$$

These decompositions are later used to partition our problem.

# 14.2.5 Automatic Verification

Automated verification takes a given input optimization problem and verifies that it adheres to the disciplined convex programming requirements. This verification process is typically done in two stages. The first stage verifies that all expressions are product-free. The second stage then verifies the top-level, sign, and composition rules.

Consider max, a convex function with two arguments. Below we show  $\max (\epsilon (x_1),x_2)$  for several forms of  $\epsilon$ :

max(x1,x2)

max  $(\sin (x_{1}),x_{2})$

$\max (x_1^2 -3,x_2)\max (\log (x_1) - 1,x_2)$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bcfeaa4eebcb39525fcbd2b274f2957d03ed3e825e5fbedc4489efda7cde2aaf.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4d31c95d0585cfad5581ff72e05eb092f35b631fbc851e32f873483ffe6761f6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b84c9eebd50918bfaeb337c8df59c3a4003113eee5883cfe02872a2f4e317987.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/81795374a5aa253b5162e80a0ac26f3eed2ee741d76489ea29b803628a0e6211.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7c8654a52bc15438597d147292c62bd11bfbece9102d44aa06d91e873afe7fcc.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3e12ba6026090847b3f6404e5e2b68e343b2585a54b29a0381dc4eef261f589e.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/024cadd0a0ee4c1b39d1337a02d69ed393b6485dbde8a615256960790c975a4c.jpg)  
$x_{1}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dad793c0f44a80454b30c9419fe774a15a192e3bb06b48d231fdf73aff564ad9.jpg)  
$x_{1}$

We know that max is increasing in both arguments, and here the second argument is affine in all cases. In the case of  $\epsilon_1 = x_1$ , both arguments are affine and the domain of our expression is  $(x_1, x_2) \in \mathbb{R}^1 \times \mathbb{R}^1 = \mathbb{R}^2$ . For  $\epsilon_1 = \sin(x_1)$ , our argument is neither convex nor concave, so the overall expression is not convex. For  $\epsilon_1 = x_1^2 - 3$ , our argument is convex, so the overall expression is convex with domain  $(x_1, x_2) \in \mathbb{R}^1 \times \mathbb{R}^1 = \mathbb{R}^2$ . For  $\epsilon_1 = \log x_1 - 1$  for  $x_1 > 0$ , our argument is concave, so the overall expression is not convex.

Example 14.5. Applying the composition rules to an expression with multiple inputs. The top row of plots shows  $\max (\epsilon (x_1),x_2)$ . The bottom row shows  $\max (\epsilon (x_1),0)$  in black and  $\epsilon (x_{1})$  in blue.

Verifying that an expression is product-free is a recursive process. Given an input expression, we first check whether it can be written in the form of equation (14.5). Then, after verifying that all called functions are registered in the atom library, we recurse into each argument and verify that it is similarly a product-free expression. This process is shown in figure 14.2

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ee8649b7a03ddc046d2a5dd906c9ec85540985638db03cb22d6f50eea2626772.jpg)  
Figure 14.2. The recursive nature of product-free expressions allows them to be written as expression trees where each node is a product-free expression. The verification process produces such a tree. A demonstration of this process can be found at: https://dcp.stanford.edu. Note that  $\mathrm{pow}(x,p)$  is the power function  $x^p$ .

The second stage in automatic verification verifies that the top-level, sign, and composition rules hold. The top-level rules determine the expected curvature for the objective function and the expressions on either sides of constraints. The sign rules and composition rules can then be applied as one works down the problem's expression tree. Example 14.7 shows an instance of this process.

Packages like Convex.jl will perform this automatic verification. Example 14.7 presents the results from running on the problem from example 14.1.

# 14.3 Canonicalization

Canonicalization is the process of finding a disciplined convex program in a canonical form for an original problem specification. A collection of problem transformations can be used to produce a DCP in canonical form. These transformations additionally allow for dual information to be maintained. This information can help produce lower bounds, provide dual certificates, and assess convergence.

Consider the following optimization problem:

$$
\underset{x_{1}, x_{2}} {\text{minimize}} \operatorname{pow} \left(a_{1} x_{1} + a_{2} x_{2} - b_{1}, 2\right)
$$

$$
\text{subject} \quad c_{1} x_{1} + c_{2} x_{2} = d
$$

$$
\sqrt{\min \left(x_{1} , x_{2}\right)} \geq 1 - \log (x_{2})
$$

In the first stage of verification, we construct expression trees for the objective and each side of each constraint and verify that these expressions are product-free. In the second stage of verification, we construct an expression tree for the overall problem, and verify the additional DCP rules.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/87dfc50303a1eb387e71866d358a3a7893a51828f9b2570e1f11e97318ca3598.jpg)

Note that this process can be used to discover missing conditions for the problem to be a valid disciplined convex program. In this case, we require that  $x_{2} > 0$  in order for  $\log (x_{2})$  to be valid, and that  $\min (x_1,x_2)\geq 0$  in order for the square root to be valid. Hence, we need  $x_{1}\geq 0$  and  $x_{2} > 0$ .

Example 14.6. An illustration of automatic verification for disciplined convex programming.

We can use Convex.jl to show the results of automatic verification on the problem from example 14.1:

```txt
julia> A = [2 -1; 13]; b = [4, -5];  
julia> C = [01; 3 -4]; d = [-1, 0];  
julia> x = Variable(2);  
julia> problem = minimize(  
norm(A*x - b, 1),  
norm(C*x - d, 1.5) ≤ 3)  
)  
Problem statistics  
problem is DCP : true  
number of variables : 1 (2 scalar elements)  
number of constraints : 1 (1 scalar elements)  
number of coefficients : 13  
number of atoms : 8  
Solution summary  
termination status : OPTIMIZE_NOT Called  
primal status : NO SOLUTION  
dual status : NO SOLUTION  
Expression graph  
minimize  
∑ sum (convex; positive)  
∑ abs (convex; positive)  
∑ + (affine; real)  
∑ ...  
∑ ...  
subject to  
∑ ≤ constraint (convex)  
∑ + (convex; real)  
∑ rationalnorm (convex; positive)  
∑ ...  
[−3;]
```

Example 14.7. Using Convex.jl to perform automatic verification.

Canonicalization will produce a problem in partitioned canonical form:

$$
\underset{\mathbf {x}^{(1)}, \dots , \mathbf {x}^{(k)}} {\text{minimize}} d + \sum_{i = 1}^{k} (\mathbf {c}^{(i)}) ^{\top} \mathbf {x}^{(i)}
$$

$$
\text{subject} \quad \mathbf {x}^{(i)} \in S^{(i)} \quad \text{for} i \text{in} 1: k \tag {14.8}
$$

$$
\sum_{i = 1}^{k} \mathbf {A}^{(i)} \mathbf {x}^{(i)} = \mathbf {b}
$$

where each set  $S^{(i)}$  is a convex set defined in the atom library<sup>12</sup>. These sets contain any nonlinearities necessary for representing the optimization problem. Each set operates directly on a subset of  $\mathbf{x}$ , so any coupling happens through  $\mathbf{A}\mathbf{x} = \mathbf{b}$ .

12 This includes standard sets like  $\mathbb{R}^n$  and the positive orthant. As we will see later, this also include epigraphs or hypographs of functions in the atom library.

# 14.3.1 Linearization

Linearization makes all of our product-free expressions linear and makes all function arguments affine. This process introduces many new variables and constraints, but individual expressions and constraints are simplified.

Linearization operates on the problem's product-free expressions. These expressions can either be objective functions, in which case they are standalone, or the left or right-hand sides of a constraint. Given a product-free expression:

$$
a + \mathbf {b}^{\top} \mathbf {x} + \sum_{j = 1}^{q} c_{j} f_{j} (\boldsymbol {\epsilon}_{j}) \tag {14.9}
$$

the linearized form is:

$$
a + \mathbf {b}^{\top} \mathbf {x} + \mathbf {c}^{\top} \mathbf {v} \tag {14.10}
$$

with:

$$
f_{j} (\epsilon_{j}) \leq v_{j} \text{if } f_{j} \text{isconvex}
$$

$$
f_{j} (\epsilon_{j}) \geq v_{j} \text{if } f_{j} \text{isconcave} \tag {14.11}
$$

$$
f o r j \text{in} 1: q
$$

where we have introduced  $q$  atom variables  $\mathbf{v}$  and  $q$  new inequality constraints.

We must simplify any arguments that are not already simple variables. If  $f_{j}$  takes in  $\epsilon_{j} = \{\epsilon_{j,1},\epsilon_{j,2},\ldots \}$ , then each input  $\epsilon_{j_a}$  that is not already a simple variable is replaced by a new variable  $u_{j,a}$  and bound to it with an equality constraint  $u_{j,a} = \epsilon_{j_a}$ .<sup>13</sup>

When linearizing a product-free expression in an inequality constraint, we introduce a slack variable  $s \geq 0$  to transform the inequality into an equality:

$$
a + \mathbf {b}^{\top} \mathbf {x} + \mathbf {c}^{\top} \mathbf {v} \leq d \Rightarrow a + \mathbf {b}^{\top} \mathbf {x} + \mathbf {c}^{\top} \mathbf {v} + s = d \tag {14.12}
$$

$$
a + \mathbf {b}^{\top} \mathbf {x} + \mathbf {c}^{\top} \mathbf {v} \geq d \Rightarrow a + \mathbf {b}^{\top} \mathbf {x} + \mathbf {c}^{\top} \mathbf {v} - s = d
$$

This process is applied recursively until all expressions have been linearized. In practice, it can be simpler to work bottom up, starting with the innermost function arguments and working our way up to the top level inequalities and objective functions.

We can see how linearization produces a problem closer to the partitioned canonical form in equation (14.8). Any objective function has been linearized, with any nonlinearities moving into the constraints. Our variables are all either free, lie in the positive orthant (like  $s$ ), are tied with an equality constraint (like  $u$ ), or are tied with an inequality constraint (like  $v$ ).<sup>14</sup> Solving an optimization problem with the linear forms is equivalent to solving one with the original product-free expressions.

An example of product-free expression linearization is given in example 14.8.

Linearization produces a much larger problem, but individual expressions are vastly simplified. Though there are more variables and constraints, solving these problems is no more difficult than solving the original problem, because solvers can take their sparse structure into account. Furthermore, the linear forms allow the next operation, graph expansion, to be applied, which elegantly converts problems into more solvable forms.[15]

# 14.3.2 Graph Expansion

The linearized form may not yet be directly solvable by a convex solver. Most convex solvers operate on specific forms, such as linear programs, quadratic programs, or second order cone programs.[16] Atoms in our original problem may not be supported by these solvers. Fortunately, it is often possible to transform atoms into other forms with equivalent solutions that are amenable to being solved by convex solvers.[17]

For example, if we have  $f_{j} = \log (5x + 3)$ , we replace the argument via  $f_{j} = \log (u_{j,1})$  and introduce  $u_{j,1} = 5x + 3$ .

14 The slack variable constraints  $s \geq 0$  are equivalent to  $s \in \mathbb{R}^+$ , and thus satisfy the convex set constraint form  $\mathbf{x}^{(i)} \in S^{(i)}$ . Similarly, the inequality constraints  $f_{j}(\pmb{\epsilon}_{j}) \leq v_{j}$  and  $f_{j}(\pmb{\epsilon}_{j}) \geq v_{j}$  can also be viewed as convex set membership constraints for the pairs  $(\pmb{\epsilon}_{j}, v_{j})$ . This is why the function arguments must all be converted to simple variables.  
There is an additional step, set decomposition, which converts all set membership constraints into equality constraints and atomic nonlinearities. It is similar to linearization, and so is not covered in detail here.  
16 Second order cone programs are convex optimization problems with linear objectives  $\mathbf{q}^{\dagger}\mathbf{x}$ , affine equality constraints  $\mathbf{A}\mathbf{x} = \mathbf{b}$ , and second order cone constraints:

$$
\left\| \mathbf {C}_{i} \mathbf {x} + \mathbf {d}_{i} \right\| _{2} \leq \mathbf {e}_{i}^{\top} \mathbf {x} + f_{i}
$$

17 M. Grant and S. Boyd, "Graph Implementations for Nonsmooth Convex Programs," in Recent Advances in Learning and Control, 2008.

Consider again the product-free expression for  $x_{1} \in [0,\infty]$  and  $x_{2} \in [-\pi, 0]$ :

$$
\operatorname{pow} (3 + 2 \log (1 + x_{1}) - \sin (x_{2}), - 1)
$$

We first consider the innermost arguments,  $1 + x_{1}$  and  $x_{2}$ . These are affine, and thus need not be linearized.

The next larger expression  $3 + 2\log (1 + x_1) - \sin (x_2)$  is not affine. We introduce new variables and constraints, producing:

$$
3 + 2 v_{1} - v_{2}
$$

subject to  $\log (u_{1,1})\geq v_1$

$$
u_{1, 1} = 1 + x_{1}
$$

$$
\sin (x_{2}) \leq v_{2}
$$

We return to the top-level expression, now  $\mathrm{pow}(3 + 2v_1 - v_2, -1)$ . We similarly introduce a variables and a new constraint:

$$
v_{3}
$$

subject to  $\operatorname{pow}(u_{3,1}, -1) \leq v_3$

$$
u_{3, 1} = 3 + 2 v_{1} - v_{2}
$$

We have thus successfully linearized our expression by introducing 5 new variables and 5 new constraints.

Example 14.8. Linearizing a product-free expression that has already been verified to obey the disciplined convex programming rules.

Suppose we have a disciplined convex program that contains  $|x|$ . This atom is nonlinear, and does not fit the form accepted by conventional linear or quadratic solvers. However,  $|x|$  can be represented as the solution to a linear program:

$$
\begin{array}{r l r} & \min_{y} & y \\ | x | & \Rightarrow & \text{subjectto} \quad y \quad \geq x \\ & & y \quad \geq - x \end{array} \tag {14.13}
$$

A linearized disciplined convex program can incorporate this transformation. For example, if the original problem has a constraint  $|2x + 3| \leq 5$ , then its linearized form would be have a graph expansion as follows:

$$
\begin{array}{c c c} \underset{\mathbf {x}} {\text{minimize}} \dots & \underset{\mathbf {x}, v} {\text{minimize}} \dots \\ \vdots & & \vdots \\ v = | u | & \Rightarrow & v \geq u \\ u = 2 x + 3 & & v \geq - u \\ v + s = 5 & & u = 2 x + 3 \\ s \geq 0 & & v + s = 5 \\ & & s \geq 0 \end{array} \tag {14.14}
$$

Replacing  $|x|$  in this way has produced a linear program, and in so doing has removed the problematic nondifferentiable point. This transformation for  $|x|$  can be derived using properties of convex functions.

A convex function is only convex if its epigraph is a convex set, and a concave function is only concave if its hypograph is a concave set. The epigraph is the region above and including a function, whereas the hypograph is the region below and including a function:

$$
\operatorname{epi} f = \left\{\left(\mathbf {x}, y\right) \in \mathbb {R}^{n} \times \mathbb {R} \mid y \geq f (\mathbf {x}) \right\} \tag {14.15}
$$

$$
\mathbf {h y p o} f = \left\{\left(\mathbf {x}, y\right) \in \mathbb {R}^{n} \times \mathbb {R} \mid y \leq f (\mathbf {x}) \right\}
$$

Examples of such sets are given in figure 14.3.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e8815e74457cc1350d89afb05058c7c4402b956e7ffb676294ca0602c5e0bdc4.jpg)  
Figure 14.3. The blue region above the extended-valued convex function  $f(x) = x^{3}$  for  $x \geq 0$  is that function's epigraph. The red region below  $f(x) = \log x$  for  $x \geq 0$  is that function's hypograph.

We can thus replace a convex or concave function with a minimization or maximization problem based on its epigraph or hypograph, respectively:

$$
\begin{array}{l l} \text{forconvex} f, & f (\mathbf {x}) = \begin{array}{c c} \min_{y} & y \\ \text{subjectto} & (\mathbf {x}, y) \in \mathbf {e p i} f \end{array} \\ & \text{forconcave} f, & f (\mathbf {x}) = \begin{array}{c c} \max_{y} & y \\ \text{subjectto} & (\mathbf {x}, y) \in \mathbf {h y p o} f \end{array} \end{array} \tag {14.16}
$$

The epigraph of the real absolute value function  $f(x) = |x|$  is:

$$
\operatorname{epi} | x | = \{(x, y) \mid y \geq x, y \geq - x \} \tag {14.17}
$$

This epigraph is shown in figure 14.4.

Every atom in the library can have two types of implementations: a traditional implementation that provides an evaluation function, its derivatives, Hessian, etc., or a graph implementation, which describes the function as the solution to another DCP, typically in the standard form expected by a target solver. This process of replacing atoms with their graph implementations is called graph expansion.[18]

Many norms have convenient graph implementations. The norms in example 14.9 are presented to motivate disciplined convex programming. In some cases, we can recognize certain common optimization problem forms and apply similar transformations, such as the  $L_{2}$  norm in example 14.10.

Disciplined convex programming allows us to automatically select appropriate transforms to efficiently solve problems. While the user could transform the input problem themselves, they may be unaware of these techniques or may find it inconvenient to transform their problems by hand. The problems shown here are relatively simple, and it can be harder for a practitioner to recognize when to tailor a problem themselves. Graph implementations let users provide their problem in the most natural form relevant to them (e.g., with an  $L_{p}$ -norm), allowing for a solution to be found efficiently.

# 14.4 Solving

Once we have a problem in partitioned canonical form, and we have applied all necessary graph expansions to convert our disciplined convex program into a form required by our targeted solver, we can then pass our problem to the solver<sup>19</sup>

Figure 14.4. The epigraph for  $|x|$  can be formed by two linear inequalities. By minimizing  $y$  we recover the original function.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1ced159ce675509502cc49fff62c656870a2329d7796b93091689d81f7d59116.jpg)  
18 The "graph" in both names come from hypograph and epigraph.

19 We can apply techniques such as scaling the design variables to improve a solver's reliability. For an overview of scaling, see A. D. Belegundu and T. R. Chandrupatla, Optimization Concepts and Applications in Engineering, 2nd ed. Cambridge University Press, 2011.

Both  $L_{1}$  (Manhattan) norms and  $L_{\infty}$  (Chebyshev) norms can be converted into equivalent linear programs:

$$
\begin{array}{r c l} \| \mathbf {x} \| _{1} & \Rightarrow & \underset{\mathbf {v}} {\text{minimize}} \quad \mathbf {1}^{\top} \mathbf {v} \\ & & \text{subjectto} \quad - \mathbf {v} \leq \mathbf {x} \leq \mathbf {v} \end{array}
$$

$$
\| \mathbf {x} \| _{\infty} \Rightarrow \begin{array}{l l} \underset{v} {\text{minimize}} & v \\ \text{subjectto} & - v \mathbf {1} \leq \mathbf {x} \leq v \mathbf {1} \end{array}
$$

Again, such graph expansions change our original problem into a well-known form for which many solvers exist.

The more general  $L_{p}$ -norm for  $p \geq 1$  can also be transformed into another DCP:

$$
\| \mathbf {x} \| _{p} \quad \Rightarrow \quad \underset{\mathbf {v}} {\text{minimize}} \quad \mathbf {1}^{\top} \mathbf {v} \\ \text{subjectto} \quad | x_{i} | ^{p} \leq v_{i} \quad \text{for} i \text{in} 1: m \tag {14.18}
$$

This form can be optimized using interior point methods (section 10.10).

Example 14.9. Graph implementations for several norms. This example was adapted from M. Grant, S. Boyd, and Y. Ye, "Disciplined Convex Programming," Global Optimization: From Theory to Implementation, pp. 155-210, 2006.

If our problem is recognized to be a least squares problem, we can simplify by squaring our objective, thereby removing the square root:

$$
\underset{\mathbf {x}} {\operatorname{minimize}} \quad \| \mathbf {A} \mathbf {x} - \mathbf {b} \| _{2} \quad \Rightarrow \quad \underset{\mathbf {x}} {\operatorname{minimize}} \quad \| \mathbf {A} \mathbf {x} - \mathbf {b} \| _{2}^{2}
$$

The least squares problem has a known solution,  $\mathbf{x} = \left(\mathbf{A}^{\top}\mathbf{A}\right)^{-1}\mathbf{A}^{\top}\mathbf{b}$ , and can be solved efficiently as discussed in section 13.2.

Example 14.10. A graph implementation for  $L_{2}$  norm minimization problems.

to solve it. Many solver types exist and use techniques already covered in this text.

One common solution technique is to use interior point methods (section 10.10). A DCP can be solved with interior point methods if every atom is either twice differentiable, has a predefined barrier function, or has a graph implementation that is itself solvable with interior point methods.

Interior point methods move all equality constraints, other than affine constraints, into the objective function using a barrier function:

$$
\underset{\mathbf {x}} {\operatorname{minimize}} \quad \mathbf {c}^{\top} \mathbf {x} + d + \frac{1}{\rho} p_{\text{barrier}} (\mathbf {x}) \tag {14.19}
$$

$$
\text{subject} \quad \mathbf {A x} = \mathbf {b}
$$

The optimal solution approaches the solution of the original problem as  $\rho$  approaches infinity. Interior point methods such as algorithm 10.3 iteratively solve this formulation, increasing  $\rho$  as they go. Intermediate solutions remain feasible, and are thus in the interior of the feasible set.

Each iteration typically solves equation (14.19) with Newton's method from an initial point  $\mathbf{x}$ . Here, the search direction  $\Delta \mathbf{x}$  is obtained jointly with the dual parameters  $\mu$  by solving the augmented system:20

$$
\left[ \begin{array}{c c} \nabla^{2} p_{\text{barrier}} (\mathbf {x}) & \mathbf {A}^{\top} \\ \mathbf {A} & \mathbf {0} \end{array} \right] \left[ \begin{array}{l} \Delta \mathbf {x} \\ \boldsymbol {\mu} \end{array} \right] = - \left[ \begin{array}{c} \rho \mathbf {c} + \nabla p_{\text{barrier}} (\mathbf {x}) \\ \mathbf {A x} - \mathbf {b} \end{array} \right] = - \left[ \begin{array}{l} \mathbf {r}_{d} \\ \mathbf {r}_{p} \end{array} \right] \tag {14.20}
$$

The left hand side of the augmented system is symmetric but not invertible. In some cases we know that  $\nabla^2 p_{\mathrm{barrier}}(\mathbf{x})$  is positive definite,[21] and we can transform the problem. Here, we rearrange the top row to obtain

$$
\Delta \mathbf {x} = - \nabla^{2} p_{\text{barrier}} (\mathbf {x}) ^{- 1} \left(\mathbf {r}_{d} + \mathbf {A}^{\top} \boldsymbol {\mu}\right) \tag {14.21}
$$

which can be substituted into the second row to obtain

$$
\left(\mathbf {A}^{\top} \nabla^{2} p_{\text{barrier}} (\mathbf {x}) ^{- 1} \mathbf {A}\right) \boldsymbol {\mu} = \mathbf {r}_{p} - \mathbf {A} \nabla^{2} p_{\text{barrier}} (\mathbf {x}) ^{- 1} r_{d} \tag {14.22}
$$

Inverting an arbitrary  $n \times n$  matrix is  $O(n^{3})$ . Unfortunately, canonicalization introduces many additional variables into our problem, resulting in a large Hessian matrix  $\nabla^2 p_{\mathrm{barrier}}(\mathbf{x})$ . Fortunately, a DCP in partitioned canonical form is comprised of many independent atoms that depend on different subsets of our expanded design variables  $\mathbf{x}$ , resulting in a sparse Hessian matrix with block-diagonal

20 The augmented system enforces the KKT conditions of stationarity and feasibility. See exercise 14.7.

21 The Hessian matrix is positive definite for linear programs in standard form, for example.

structure:

$$
\left[ \begin{array}{c c c c} \nabla^{2} p_{\text{barrier}}^{(1)} (\mathbf {x}^{(1)}) & & & \\ & \nabla^{2} p_{\text{barrier}}^{(2)} (\mathbf {x}^{(2)}) & & \\ & & \ddots & \\ & & & \nabla^{2} p_{\text{barrier}}^{(k)} (\mathbf {x}^{(k)}) \end{array} \right] \tag {14.23}
$$

The inverse of a block-diagonal matrix is simply a block-diagonal matrix whose entries are the inverses of each block. By exploiting this structure, the inversion becomes extremely efficient.

When we cannot invert our Hessian, we solve the augmented system with an  $\mathbf{LDL}^{\top}$  factorization $^{22}$  with a lower triangular  $\mathbf{L}$  and a block-diagonal  $\mathbf{D}$ . This factorization can often be computed very efficiently, again leveraging block-diagonal structure. Additional tricks, including properties of selected barrier functions, can also be applied.

# 14.5 Summary

- A disciplined convex program is a type of convex optimization problem for which automated methods can verify convexity and transcribe it into a canonical form amenable to solution by existing solvers.  
- Convexity for a DCP requires that its expressions be product-free and that they follow sign, composition, and top-level production rules.  
- Canonicalization is the process of taking a DCP and producing a problem in partitioned canonical form.  
- Interior point methods are popular approaches for solving problems in partitioned canonical form and can typically be solved efficiently by exploiting block-diagonal structure.

# 14.6 Exercises

Exercise 14.1. We can use extended-valued functions to conveniently extend functions over a subset of  $\mathbb{R}^n$  to all of  $\mathbb{R}^n$ . This extension works by having the function produce an infinite value if it receives a output outside of its typical domain, thereby extending their range to include infinite values.

22 This factorization is covered in appendix C.7.2.

Show that if we have a convex function  $f$  defined over a convex set  $S$ , then its extended-value extension

$$
g (\mathbf {x}) = \left\{ \begin{array}{l l} f (\mathbf {x}) & \text{if } x \in \mathcal {S} \\ \infty & \text{otherwise} \end{array} \right.
$$

is also convex.

Solution: According to the definition given in appendix C.3, our extended-valued function  $g$  is convex over  $\mathbb{R}^n$  if, for all  $\mathbf{x}, \mathbf{y}$  in  $\mathbb{R}^n$  and for all  $\alpha$  in [0, 1],

$$
g (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) \leq \alpha g (\mathbf {x}) + (1 - \alpha) g (\mathbf {y})
$$

If both  $\mathbf{x}$  and  $\mathbf{y}$  are in  $S$ , then  $\alpha \mathbf{x} + (1 - \alpha)\mathbf{y} \in S$  because  $S$  is convex. Furthermore, we know

$$
f (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) \leq \alpha f (\mathbf {x}) + (1 - \alpha) f (\mathbf {y})
$$

is satisfied because  $f$  is convex.

If  $\mathbf{x} \notin S$  and  $\mathbf{y} \in S$ , then we get:

$$
g (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) \leq \alpha \cdot \infty + (1 - \alpha) f (\mathbf {y})
$$

which is  $g(\alpha \mathbf{x} + (1 - \alpha)\mathbf{y}) \leq \infty$  for  $\alpha > 0$  and  $f(\mathbf{y}) = f(\mathbf{y})$  when  $\alpha = 1$ , so holds.

Similarly, if  $\mathbf{x} \in S$  and  $\mathbf{y} \notin S$ , then we get:

$$
g (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) \leq \alpha f (\mathbf {x}) + (1 - \alpha) \cdot \infty
$$

which is  $g(\alpha \mathbf{x} + (1 - \alpha)\mathbf{y}) \leq \infty$  for  $\alpha < 1$  and  $f(\mathbf{x}) = f(\mathbf{x})$  when  $\alpha = 0$ , so holds.

Finally, if both  $\mathbf{x}$  and  $\mathbf{y}$  are not in  $S$ , then we get:

$$
g (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) \leq \alpha \cdot \infty + (1 - \alpha) \cdot \infty
$$

which is  $g(\alpha \mathbf{x} + (1 - \alpha)\mathbf{y})\leq \infty$  for  $0\leq \alpha \leq 1$  , so holds.

Exercise 14.2. The product-free expressions do not allow for products, so  $x \cdot x$  or  $x^2$  is not DCP-compliant. Show how squares could still be incorporated into disciplined convex programming.

Solution: We can introduce a new atom to support squares. Let us call this atom square  $(x)$ , and use it to replace all instances of  $x \cdot x$  or  $x^2$ . The atom is convex, increasing in  $x$  for  $x \geq 0$ , decreasing in  $x$  for  $x \leq 0$ , and has range  $\geq 0$ . It will then be treated just like other atoms, and we can successfully solve problems containing squares.

Exercise 14.3. Linearize the optimization problem from example 14.7:

$$
\begin{array}{l} \underset{x_{1}, x_{2}} {\text{minimize}} \quad \operatorname{pow} \left(a_{1} x_{1} + a_{2} x_{2} - b_{1}, 2\right) \\ \text{subject} \quad c_{1} x_{1} + c_{2} x_{2} = d \\ \sqrt{\min \left(x_{1} , x_{2}\right)} \geq 1 - \log (x_{2}) \\ \end{array}
$$

Solution: First we linearize the objective function, introducing  $v_{1}$  and  $u_{1,1}$ :

$$
\begin{array}{r c l} & \underset{x_{1}, x_{2}, v_{1}, u_{1, 1}} {\text{minimize}} & v_{1} \\ \operatorname{pow} (a_{1} x_{1} + a_{2} x_{2} - b_{1}, 2) & \Rightarrow & \text{subjectto} \quad u_{1, 1} = a_{1} x_{1} + a_{2} x_{2} - b_{1} \\ & & \operatorname{pow} (u_{1, 1}, 2) \leq v_{1} \end{array}
$$

The affine constraint does not need to be linearized, so we next linearize the left-hand side of the inequality constraint. We start by recursing in and linearizing  $\min(x_1, x_2)$ , introducing  $v_2, u_{2,1}$ , and  $u_{2,2}$ :

$$
\begin{array}{r c l} & \text{minimize} & v_{2} \\ \min (x_{1}, x_{2}) & \Rightarrow & \text{subjectto} \quad u_{2, 1} = x_{1} \\ & & u_{2, 2} = x_{2} \\ & & \min (u_{2, 1}, u_{2, 2}) \geq v_{2} \end{array}
$$

We then linearize  $\sqrt{v_2}$ , introducing  $v_{3}$  and  $u_{3,1}$ :

$$
\begin{array}{c c c} & \text{minimize} & v_{3} \\ \sqrt{v_{2}} & \Rightarrow & \text{subjectto} \quad u_{3, 1} = v_{2} \\ & & \sqrt{u_{3 , 1}} \geq v_{3} \end{array}
$$

We next linearize the right-hand side of the inequality constraint, introducing  $v_4$  and  $u_{4,1}$ :

$$
\begin{array}{r c l} & \text{minimize} & 1 - v_{4} \\ 1 - \log (x_{2}) & \Rightarrow & \text{subjectto} \quad u_{4, 1} = x_{2} \\ & & \log (u_{4, 1}) \geq v_{4} \end{array}
$$

We conclude by introducing a slack variable  $s \geq 0$  to make our top-level inequality constraint linear:

$$
v_{3} \geq 1 - v_{4} \quad \Rightarrow \quad v_{3} = 1 - v_{4} + s
$$

Our linearized problem is thus:

$$
\begin{array}{c} \text{minimize} \\ \mathbf {x}_{1: 2}, u_{1, 1}, \mathbf {u}_{2, 1: 2}, u_{3, 1}, u_{4, 1}, \mathbf {v}_{1: 4} \\ \text{subjectto} \end{array}
$$

$$
v_{1}
$$

$$
c_{1} x_{1} + c_{2} x_{2} = d
$$

$$
v_{3} = 1 - v_{4} + s
$$

$$
\sqrt{u_{3 , 1}} \geq v_{3}
$$

$$
\operatorname{pow} (u_{1, 1}, 2) \leq v_{1}
$$

$$
\min (u_{2, 1}, u_{2, 2}) \geq v_{2}
$$

$$
\log (u_{4, 1}) \geq v_{4}
$$

$$
s \geq 0
$$

$$
u_{1, 1} = a_{1} x_{1} + a_{2} x_{2} - b_{1}
$$

$$
u_{2, 1} = x_{1}
$$

$$
u_{2, 2} = x_{2}
$$

$$
u_{3, 1} = v_{2}
$$

$$
u_{4, 1} = x_{2}
$$

Exercise 14.4. Convert the graph-expanded  $p$ -norm problem in example 14.9 into an unconstrained problem using log barriers.[23] In doing so, please remove the absolute value.

Solution: We start with the problem,

$$
\underset{\mathbf {x}, \mathbf {v}} {\text{minimize}} \quad \mathbf {1}^{\top} \mathbf {v}
$$

subject to  $|\mathbf{a}_i^\top \mathbf{x} - b_i|^p\leq v_i$  for  $i$  in  $1:m$

we first exponentiate the inequalities by  $2 / p$  in order to drop the absolute value: $^{24}$

$$
\left| \mathbf {a}_{i}^{\top} \mathbf {x} - b_{i} \right| ^{p} \leq v_{i} \quad \Rightarrow \quad \left| \mathbf {a}_{i}^{\top} \mathbf {x} - b_{i} \right| ^{2} \leq v_{i}^{2 / p}
$$

We then move the right-hand side to the left to obtain the form  $g(\cdot) \leq 0$ :

$$
\underset{\mathbf {x}, \mathbf {v}} {\text{minimize}} \quad \mathbf {1}^{\top} \mathbf {v}
$$

subject to  $\left(\mathbf{a}_i^\top \mathbf{x} - b_i\right)^2 - v_i^{2/p} \leq 0$  for  $i$  in  $1:m$

We can then construct a log barrier for the constraints using equation (10.37):2523 Log barrier functions are given in equation (10.37).

24 We know that both sides of the inequality are nonnegative. We want to exponentiate with a positive value to avoid changing the sign. Since  $p \geq 1$ , we know that  $2 / p \geq 0$ .

25 Note that we use  $\infty$  to produce an extended-real value function.

$$
p_{\text{barrier}} (\mathbf {x}, \mathbf {v}) = - \sum_{i} \left\{ \begin{array}{l l} \log \left(v_{i}^{2 / p} - \left(\mathbf {a}_{i}^{\top} \mathbf {x} - b_{i}\right) ^{2}\right) & \text{if } (\mathbf {x}, \mathbf {v}) \text{isfeasiblewithrespecttoconstrainti} \\ \infty & \text{otherwise} \end{array} \right.
$$

Exercise 14.5. Consider the atom  $f(x) = \sqrt{x}$  for  $x \geq 0$ . Derive a graph implementation that avoids the nondifferentiability at  $x = 0$ .

Solution: Because we know  $\sqrt{x}$  is concave, we use its hypograph:

$$
\operatorname{hypo} \sqrt{x} = \left\{\left(x, y\right) \mid x \geq 0, y \leq \sqrt{x} \right\}
$$

We can equivalently express the hypograph as:

$$
\operatorname{hypo} \sqrt{x} = \left\{(x, y) \mid x \geq 0, \max (y, 0) ^{2} \leq x \right\}
$$

We can drop  $x \geq 0$  because it will automatically be satisfied when  $\max(y, 0)^2 \leq x$  is satisfied. We can drop the max because  $y^2$  is already nonnegative. These changes result in the following graph implementation:

$$
\begin{array}{r c l} \sqrt{x} & \Rightarrow & \underset{y} {\operatorname{argmax}} \qquad y \\ & & \text{subjectto} \quad y^{2} \leq x \end{array}
$$

The graph implementation is differentiable everywhere.

Exercise 14.6. Canonicalize the following convex problem and apply the graph implementations from example 14.9:

$$
\underset{x} {\text{minimize}} \quad \| \mathbf {A x} - \mathbf {b} \| _{1}
$$

$$
\text{subject} \quad \| \mathbf {C x} - \mathbf {d} \| _{1. 5} \leq 3
$$

where

$$
\mathbf {A} = \left[ \begin{array}{c c} 2 & - 1 \\ 1 & 3 \end{array} \right] \quad \mathbf {b} = \left[ \begin{array}{c} 4 \\ - 5 \end{array} \right]
$$

$$
\mathbf {C} = \left[ \begin{array}{c c} 0 & 1 \\ 3 & - 4 \end{array} \right] \quad \mathbf {d} = \left[ \begin{array}{c} - 1 \\ 0 \end{array} \right]
$$

Solution: The norms are both convex, and because they contain affine expressions, the overall expressions are convex.

Linearizing the problem produces:

$$
\begin{array}{l} \underset{x, v, s} {\text{minimize}} \quad v_{1} \\ \text{subject} v_{1} = \| \mathbf {A x} - \mathbf {b} \| _{1} \\ v_{2} = \left\| \mathbf {C x} - \mathbf {d} \right\| _{1. 5} \\ v_{2} + s = 3 \\ s \geq 0 \\ \end{array}
$$

We then apply a graph expansion for the  $L_{1}$  norm:

$$
\underset{\mathbf {x}, \mathbf {y}^{(1)}, \mathbf {v}, s} {\text{minimize}} \quad \mathbf {1}^{\top} \mathbf {y}^{(1)}
$$

subject to  $-\mathbf{y}^{(1)}\leq \mathbf{A}\mathbf{x} - \mathbf{b}\leq \mathbf{y}^{(1)}$

$$
v_{2} = \left\| \mathbf {C x} - \mathbf {d} \right\| _{1. 5}
$$

$$
v_{2} + s = 3
$$

$$
s \geq 0
$$

We then apply a graph expansion for the  $L_{p}$  norm:

minimize  $\mathbf{1}^{\top}\mathbf{y}^{(1)} + \mathbf{1}^{\top}\mathbf{y}^{(2)}$ $\mathbf{x},\mathbf{y}^{(1)},\mathbf{y}^{(2)},\mathbf{v},s$

subject to  $-\mathbf{y}^{(1)}\leq \mathbf{A}\mathbf{x} - \mathbf{b}\leq \mathbf{y}^{(1)}$

$$
\left| x_{2} + 1 \right| ^{1. 5} \leq y_{1}^{(2)}
$$

$$
\left| 3 x_{1} - 4 x_{2} \right| ^{1. 5} \leq y_{2}^{(2)}
$$

$$
\mathbf {1}^{\top} \mathbf {y}^{(2)} + s = 3
$$

$$
s \geq 0
$$

Exercise 14.7. The augmented system in equation (14.20) comes from applying Newton's method to equation (14.19). Derive the augmented system. Start by multiplying the objective function by  $\rho$  and then take the second-order Taylor approximation to obtain an objective function in terms of an offset  $\Delta x$  from the current design. The augmented system can then be obtained from the KKT conditions; the first row comes from stationarity and the second from the original constraint.

Solution: We start by multiplying the objective function by  $\rho$ :

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \rho \mathbf {c}^{\top} \mathbf {x} + \rho d + p_{\text{barrier}} (\mathbf {x})
$$

subject to  $\mathbf{A}\mathbf{x} = \mathbf{b}$

To apply Newton's method, we replace the original objective  $f$  with its second-order Taylor approximation about the current design  $x$ :

$$
\begin{array}{l} \hat {f} (\Delta \mathbf {x}) = f (\mathbf {x}) + \nabla f (\mathbf {x}) ^{\top} \Delta \mathbf {x} + \frac{1}{2} \Delta \mathbf {x}^{\top} \nabla^{2} f (\mathbf {x}) \Delta \mathbf {x} \\ = \rho \mathbf {c}^{\top} \mathbf {x} + \rho d + p_{\text{barrier}} (\mathbf {x}) + \left(\rho \mathbf {c} + \nabla p_{\text{barrier}} (\mathbf {x})\right) ^{\top} \Delta \mathbf {x} + \frac{1}{2} \Delta \mathbf {x}^{\top} \nabla^{2} p_{\text{barrier}} (\mathbf {x}) \Delta \mathbf {x} \\ \end{array}
$$

From the KKT conditions, we know that a solution must satisfy both  $\nabla \hat{f} (\Delta \mathbf{x}) + \mathbf{A}^{\top}\boldsymbol {\mu} = \mathbf{0}$  and  $\mathbf{A}(\mathbf{x} + \Delta \mathbf{x}) = \mathbf{b}$ . The first constraint works out to be:

$$
\begin{array}{l} \nabla \hat {f} (\Delta \mathbf {x}) + \mathbf {A}^{\top} \boldsymbol {\mu} = \mathbf {0} \\ \rho \mathbf {c} + \nabla p_{\text{barrier}} (\mathbf {x}) + \nabla^{2} p_{\text{barrier}} (\mathbf {x}) \Delta \mathbf {x} + \mathbf {A}^{\top} \boldsymbol {\mu} = \mathbf {0} \\ \nabla^{2} p_{\text{barrier}} (\mathbf {x}) \Delta \mathbf {x} + \mathbf {A}^{\top} \boldsymbol {\mu} = - (\rho \mathbf {c} + \nabla p_{\text{barrier}} (\mathbf {x})) \\ \end{array}
$$

If we include the second constraint we get:

$$
\begin{array}{l} \nabla^{2} p_{\text{barrier}} (\mathbf {x}) \Delta \mathbf {x} + \mathbf {A}^{\top} \boldsymbol {\mu} = - (\rho \mathbf {c} + \nabla p_{\text{barrier}} (\mathbf {x})) \\ \mathbf {A} (\mathbf {x} + \Delta \mathbf {x}) = \mathbf {b} \\ \end{array}
$$

which produces the augmented system:

$$
\left[ \begin{array}{c c} \nabla^{2} p_{\text{barrier}} (\mathbf {x}) & \mathbf {A}^{\top} \\ \mathbf {A} & \mathbf {0} \end{array} \right] \left[ \begin{array}{c} \Delta \mathbf {x} \\ \boldsymbol {\mu} \end{array} \right] = - \left[ \begin{array}{c} \rho \mathbf {c} + \nabla p_{\text{barrier}} (\mathbf {x}) \\ \mathbf {A x} - \mathbf {b} \end{array} \right]
$$

# 15 Multiobjective Optimization

Previous chapters have developed methods for optimizing single-objective functions, but this chapter is concerned with multiobjective optimization, or vector optimization, where we must optimize with respect to several objectives simultaneously. Engineering is often a tradeoff between cost, performance, and time-to-market, and it is often unclear how to prioritize different objectives. We will discuss various methods for transforming vector-valued objective functions to scalar-valued objective functions so that we can use the algorithms discussed in previous chapters to arrive at an optimum. In addition, we will discuss algorithms for identifying the set of design points that represent the best tradeoff between objectives, without having to commit to a particular prioritization of objectives. These design points can then be presented to experts who can then identify the most desirable design.<sup>1</sup>

# 15.1 Pareto Optimality

The notion of Pareto optimality is useful when discussing problems where there are multiple objectives. A design is Pareto optimal if it is impossible to improve in one objective without worsening at least one other objective. In multiobjective design optimization, we can generally focus our efforts on designs that are Pareto optimal without having to commit to a particular tradeoff between objectives. This section introduces some definitions and concepts that are helpful when discussing approaches to identifying Pareto-optimal designs.

1 Additional methods are surveyed in R. T. Marler and J. S. Arora, "Survey of Multi-Objective Optimization Methods for Engineering," Structural and Multidisciplinary Optimization, vol. 26, no. 6, pp. 369-395, 2004. For a textbook dedicated entirely to multiobjective optimization, see K. Miettinen, Nonlinear Multiobjective Optimization. Kluwer Academic Publishers, 1999.

# 15.1.1 Dominance

In single-objective optimization, two design points  $\mathbf{x}$  and  $\mathbf{x}'$  can be ranked objectively based on their scalar function values. The point  $\mathbf{x}'$  is better whenever  $f(\mathbf{x}')$  is less than  $f(\mathbf{x})$ .

In multiobjective optimization, our objective function  $f$  returns an  $m$ -dimensional vector of values  $\mathbf{y}$  when evaluated at a design point  $\mathbf{x}$ . The different dimensions of  $\mathbf{y}$  correspond to different objectives, sometimes also referred to as metrics or criteria. We can objectively rank two design points  $\mathbf{x}$  and  $\mathbf{x}'$  only when one is better in at least one objective and no worse in any other. That is,  $\mathbf{x}$  dominates  $\mathbf{x}'$  if and only if

$$
f_{i} (\mathbf {x}) \leq f_{i} \left(\mathbf {x}^{\prime}\right) \text{foriin1 :m}
$$

and  $f_{i}(\mathbf{x}) <   f_{i}(\mathbf{x}^{\prime})$  for some  $i$  (15.1)

as compactly implemented in algorithm 15.1.

$$
\text{dominates} (y, y^{\prime}) = \operatorname{all} (y. \leq y^{\prime}) \& \& \text{any} (y. <   y^{\prime})
$$

Figure 15.1 shows that in multiple dimensions there are regions with dominance ambiguity. This ambiguity arises whenever  $\mathbf{x}$  is better in some objectives and  $\mathbf{x}'$  is better in others. Several methods exist for resolving these ambiguities.

Algorithm 15.1. A method for checking whether  $\mathbf{x}$  dominates  $\mathbf{x}'$ , where  $\mathbf{y}$  is the vector of objective values for  $\mathbf{f}(\mathbf{x})$  and  $\mathbf{y}'$  is the vector of objective values for  $\mathbf{f}(\mathbf{x}')$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c57f3e1b771e7344e6185f86d3e714426173717c78ad1c0d23c216ffeba28b8a.jpg)  
Single Objective

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f0f0bc95de7088f4fca478b87c0a4af90c7a96c531464bd7c1125925520500a0.jpg)  
Multiple Objectives  
Figure 15.1. Design points can be objectively ranked in single-objective optimization but can be objectively ranked in multi-objective optimization only in some cases.

# 15.1.2 Pareto Frontier

In mathematics, an image of an input set through some function is the set of all possible outputs of that function when evaluated on elements of that input set. We will denote the image of  $\mathcal{X}$  through  $\mathbf{f}$  as  $\mathcal{Y}$ , and we will refer to  $\mathcal{Y}$  as the criterion space. Figure 15.2 shows examples of criterion space for problems with single and multiple objectives. As illustrated, the criterion space in single-objective optimization is one dimensional. All of the global optima share a single objective function value,  $y^{*}$ . In multiobjective optimization, the criterion space is  $m$ -dimensional, where  $m$  is the number of objectives. There is typically no globally best objective function value because there may be ambiguity when tradeoffs between objectives are not specified.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c877cfc1e7481093717594436b89bd98d39e77c65b28a8de2ce2c21a610c6e6c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/eee8058bacc1878509750e7458dacf76e2569d58435d6e3cdb02eeb548deaf16.jpg)  
Figure 15.2. The criterion space is the set of all objective values obtained by feasible design points. Well-posed problems have criterion spaces that are bounded from below, but they do not have to be bounded from above. The Pareto frontier is highlighted in dark blue.

In multiobjective optimization, we can define the notion of Pareto optimality. A design point  $\mathbf{x}$  is Pareto-optimal when no point dominates it. That is,  $\mathbf{x} \in \mathcal{X}$  is Pareto-optimal if there does not exist an  $\mathbf{x}' \in \mathcal{X}$  such that  $\mathbf{x}'$  dominates  $\mathbf{x}$ . The set of Pareto-optimal points forms the Pareto frontier. The Pareto frontier is valuable for helping decision-makers make design trade decisions as discussed in example 15.1. In two dimensions, the Pareto frontier is also referred to as a Pareto curve.

All Pareto-optimal points lie on the boundary of the criterion space. Some multi-objective optimization methods also find weakly Pareto-optimal points. Whereas Pareto-optimal points are those such that no other point improves at least one ob

jective, weakly Pareto-optimal points are those such that no other point improves all of the objectives (figure 15.3). That is,  $\mathbf{x} \in \mathcal{X}$  is weakly Pareto-optimal if there does not exist an  $\mathbf{x}' \in \mathcal{X}$  such that  $\mathbf{f}(\mathbf{x}') < \mathbf{f}(\mathbf{x})$ . Pareto-optimal points are also weakly Pareto optimal. Weakly Pareto-optimal points are not necessarily Pareto optimal.

Several methods discussed below use another special point. We define the utopia point to be the point in the objective space consisting of the component-wise optima:

$$
y_{i}^{\text{utopia}} = \underset{\mathbf {x} \in \mathcal {X}} {\operatorname{minimize}} f_{i} (\mathbf {x}) \tag {15.2}
$$

The utopia point is often not attainable; optimizing one component typically requires a tradeoff in another component.

# 15.1.3 Pareto Frontier Generation

There are several methods for generating Pareto frontiers. A naive approach is to sample design points throughout the design space and then to identify the nondominated points (algorithm 15.2). This approach is typically wasteful, leading to many dominated design points as shown in figure 15.4. In addition, this approach does not guarantee a smooth or correct Pareto frontier. The remainder of this chapter discusses more effective ways to generate Pareto frontiers.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d061cae718a27fc81cebaad09720d0953ad51e8c2aacff2939091ebd7d824192.jpg)  
Figure 15-3. Weakly Pareto-optimal points, shown in red, cannot be improved simultaneously in all objectives.

When constructing a collision avoidance system for aircraft, one must minimize both the collision rate and the alert rate. Although more alerts can result in preventing more collisions if the alerts are followed, too many alerts can result in pilots losing trust in the system and lead to decreased compliance with the system. Hence, the designers of the system must carefully trade alerts and collision risk.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2ff104b41c89a01832b50a7f7c92497cc8d7551c041dc88851168f3245d6a5c4.jpg)

Example 15.1. An approximate Pareto frontier obtained from evaluating many different design points for an aircraft collision avoidance system.

By varying the collision avoidance system's design parameters, we can obtain many different collision avoidance systems, but, as the figure shows, some of these will be better than others. A Pareto frontier can be extracted to help domain experts and regulators understand the effects that objective tradeoffs will have on the optimized system.

```julia
function naive_pareto(xs, ys)  
pareto_xs, pareto_ys = similar(xs, 0), similar(ys, 0)  
for (x,y) in zip(xs, ys)  
    if !any(dominates(y', y) for y' in ys)  
        push!(pareto_xs, x)  
        push!(pareto_ys, y)  
    end  
end  
return (pareto_xs, pareto_ys)  
end
```

Algorithm 15.2. A method for generating a Pareto frontier using randomly sampled design points xs and their multiobjective values ys. Both the Pareto-optimal design points and their objective values are returned.

# 15.2 Constraint Methods

Constraints can be used to cut out sections of the Pareto frontier and obtain a single optimal point in the criterion space. Constraints can be supplied either by the problem designer or automatically obtained based on an ordering of the objectives.

# 15.2.1 Constraint Method

The constraint method constrains all but one of the objectives. Here we choose  $f_{1}$  without loss of generality:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f_{1} (\mathbf {x})
$$

$$
\text{subject} f_{2} (\mathbf {x}) \leq c_{2}
$$

$$
f_{3} (\mathbf {x}) \leq c_{3} \tag {15-3}
$$

$$
\begin{array}{c} \vdots \\ \vdots \end{array}
$$

$$
\begin{array}{l} f_{m} (\mathbf {x}) \leq c_{m} \\ \mathbf {x} \in \mathcal {X} \\ \end{array}
$$

Given the vector  $\mathbf{c}$ , the constraint method produces a unique optimal point in the criterion space, provided that the constraints are feasible. The constraint method can be used to generate Pareto frontiers by varying  $\mathbf{c}$  as shown in figure 15.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/09783c0dbf2c75f6d6e832dd93327a935500b6cfd8eef01b49195ee1b7b4bfb2.jpg)  
Figure 15.4. Generating Pareto frontiers with naively scattered points is straightforward but inefficient and approximate.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5edc2722c1cbc88dd964ad5752c54ad5e948ca9948d099301f751ffe25594a20.jpg)  
Figure 15.5. The constraint method for generating a Pareto frontier. This method can identify points in the concave region of the Pareto frontier.

# 15.2.2 Lexicographic Method

The lexicographic method ranks the objectives in order of importance. A series of single-objective optimizations are performed on the objectives in order of importance. Each optimization problem includes constraints to preserve the optimality with respect to previously optimized objectives as shown in figure 15.6.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bbeb4d3913ee379e7a6e94f23857feca303e2dd1bfc19530a5e22b8669bcecc7.jpg)  
Figure 15.6. The lexicographic method for an optimization problem with three objectives.

Iterations are always feasible because the minimum point from the previous optimization is always feasible. The constraints could also be replaced with equalities, but inequalities are often easier for optimizers to enforce. In addition, if the optimization method used is not optimal, then subsequent optimizations may encounter better solutions that would otherwise be rejected. The lexicographic method is sensitive to the ordering of the objective functions.

# 15.3 Weight Methods

A designer can sometimes identify preferences between the objectives and encode these preferences as a vector of weights. In cases where the choice of weights is not obvious, we can generate a Pareto frontier by sweeping over the space of weights. This section also discusses a variety of alternative methods for transforming multiobjective functions into single-objective functions.

# 15.3.1 Weighted Sum Method

The weighted sum method (algorithm 15.3) uses a vector of weights  $\mathbf{w}$  to convert  $\mathbf{f}$  to a single objective  $f$ :2

$$
f (\mathbf {x}) = \mathbf {w}^{\top} \mathbf {f} (\mathbf {x}) \tag {15.4}
$$

where the weights are nonnegative and sum to 1. The weights can be interpreted as costs associated with each objective. The Pareto frontier can be extracted by varying  $\mathbf{w}$  and solving the associated optimization problem with the objective in equation (15.4). In two dimensions, we vary  $w_{1}$  from 0 to 1, setting  $w_{2} = 1 - w_{1}$ . This approach is illustrated in figure 15.7.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/06c8f54ca3a44ba566b913351e2a8cdf96e9542452239ec7cf96480b9c577e66.jpg)  
Figure 15.7. The weighted sum method used to generate a Pareto frontier. Varying the weights allows us to trace the Pareto frontier.

In contrast with the constraint method, the weighted sum method cannot obtain points in nonconvex regions of the Pareto frontier as shown in figure 15.8.

2 L. Zadeh, "Optimality and Non-Scalar-Valued Performance Criteria," IEEE Transactions on Automatic Control, vol. 8, no. 1, pp. 59-60, 1963.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8db6295b86553866757b897e9a7414e832e77b13459af0441c541981fa7cf0e2.jpg)  
Figure 15.8. The points in red are Pareto optimal but cannot be obtained using the weighted sum method.

A given set of weights forms a linear objective function with parallel contour lines marching away from the origin. If the feasible set bends away from the origin, it will have other Pareto optimal points on the boundary that cannot be recovered by minimizing equation (15.4).

```julia
function weight_pareto(f1, f2, npts)  
return [  
    optimize(x→w1*f1(x) + (1-w1)*f2(x))  
    for w1 in range(0,stop=1,length=npts)  
]  
end
```

Algorithm 15.3. The weighted sum method for generating a Pareto frontier, which takes objective functions f1 and f2 and number of Pareto points npts.

# 15.3.2 Goal Programming

Goal programming $^3$  is a method for converting a multiobjective function to a single-objective function by minimizing an  $L_p$  norm $^4$  between  $\mathbf{f}(\mathbf{x})$  and a goal point:

$$
\underset{\mathbf {x} \in \mathcal {X}} {\operatorname{minimize}} \left\| \mathbf {f} (\mathbf {x}) - \mathbf {y}^{\text{goal}} \right\| _{p} \tag {15.5}
$$

where the goal point is typically the utopia point. The equation above does not involve a vector of weights, but the other methods discussed in this chapter can be thought of as generalizations of goal programming. This approach is illustrated in figure 15.9.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c6754dd9cde8bda94514ae337fbf73afee015ea12e695c63fb69fa7a3e7f2443.jpg)  
Figure 15.9. Solutions to goal programming as the value for  $p$  is changed.

An overview is presented in D. Jones and M. Tamiz, Practical Goal Programming. Springer, 2010.  
4 The definition of  $L_{p}$ -norms is covered in appendix C.4. Goal programming typically uses  $p = 1$ .

# 15.3.3 Weighted Exponential Sum

The weighted exponential sum combines goal programming and the weighted sum method<sup>5</sup>

$$
f (\mathbf {x}) = \sum_{i = 1}^{m} w_{i} \left(f_{i} (\mathbf {x}) - y_{i}^{\text{goal}}\right) ^{p} \tag {15.6}
$$

where  $\mathbf{w}$  is a vector of positive weights that sum to 1 and  $p \geq 1$  is an exponent similar to that used in  $L_{p}$  norms. As before, zero-valued weights can result in weakly Pareto-optimal points.

The weighted exponential sum weighs each component of the distance between the solution point and the goal point in the criterion space. Increasing  $p$  increases the relative penalty of the largest coordinate deviation between  $\mathbf{f}(\mathbf{x})$  and the goal point. While portions of the Pareto-optimal set can be obtained by continuously varying  $p$ , we are not guaranteed to obtain the complete Pareto frontier, and it is generally preferable to vary  $\mathbf{w}$  using a constant  $p$ .

# 15.3.4 Weighted Min-Max Method

Using higher values of  $p$  with the weighted exponential sum objective tends to produce better coverage of the Pareto frontier because the distance contours are able to enter nonconvex regions of the Pareto frontier. The weighted min-max method, also called the weighted Tchebycheff method, is the limit as  $p$  approaches infinity:6

$$
f (\mathbf {x}) = \max_{i} \left[ w_{i} \left(f_{i} (\mathbf {x}) - y_{i}^{\text{goal}}\right) \right] \tag {15.7}
$$

The weighted min-max method can provide the complete Pareto-optimal set by scanning over the weights but will also produce weakly Pareto-optimal points. The method can be augmented to produce only the Pareto frontier

$$
f (\mathbf {x}) = \max_{i} \left[ w_{i} \left(f_{i} (\mathbf {x}) - y_{i}^{\text{goal}}\right) \right] + \rho \mathbf {f} (\mathbf {x}) ^{\top} \mathbf {y}^{\text{goal}} \tag {15.8}
$$

where  $\rho$  is a small positive scalar with values typically between 0.0001 and 0.01. The added term requires that all terms in  $\mathbf{y}_{\mathrm{goal}}$  be positive, which can be accomplished by shifting the objective function. By definition,  $\mathbf{f}(\mathbf{x}) \geq \mathbf{y}_{\mathrm{goal}}$  for all  $\mathbf{x}$ . Any weakly Pareto-optimal point will have  $\mathbf{f}(\mathbf{x})^{\top} \mathbf{y}_{\mathrm{goal}}$  larger than a strongly Pareto-optimal point closer to  $\mathbf{y}_{\mathrm{goal}}$ .

5 P. L. Yu, "Cone Convexity, Cone Extreme Points, and Nondominated Solutions in Decision Problems with Multiobjectives," Journal of Optimization Theory and Applications, vol. 14, no. 3, pp. 319-377, 1974.

The maximization can be removed by including an additional parameter  $\lambda$ :

$$
\underset{\mathbf {x}, \lambda} {\text{minimize}} \quad \lambda
$$

$$
\text{subject} \quad \mathbf {x} \in \mathcal {X}
$$

$$
\mathbf {w} \odot (\mathbf {f} (\mathbf {x}) - \mathbf {y}^{\text{goal}}) - \lambda \mathbf {1} \leq \mathbf {0}
$$

# 15.3.5 Exponential Weighted Criterion

The exponential weighted criterion<sup>7</sup> was motivated by the inability of the weighted sum method to obtain points on nonconvex portions of the Pareto frontier. It constructs a scalar objective function according to

$$
f (\mathbf {x}) = \sum_{i = 1}^{m} \left(e^{p w_{i}} - 1\right) e^{p f_{i} (\mathbf {x})} \tag {15.9}
$$

Each objective is individually transformed and reweighted. High values of  $p$  can lead to numerical overflow.

# 15.4 Multiobjective Population Methods

Population methods have also been applied to multiobjective optimization. We can adapt the standard algorithms to encourage populations to spread over the Pareto frontier.

# 15.4.1 Subpopulations

Population methods can divide their attention over several potentially competing objectives. The population can be partitioned into subpopulations, where each subpopulation is optimized with respect to different objectives. A traditional genetic algorithm, for example, can be modified to bias the selection of individuals for recombination toward the fittest individuals within each subpopulation. Those selected can form offspring with individuals from different subpopulations.

One of the first adaptations of population methods to multiobjective optimization is the vector evaluated genetic algorithm<sup>9</sup> (algorithm 15.4). Figure 15.10 shows how subpopulations are used in a vector evaluated genetic algorithm to maintain diversity over multiple objectives. The progression of a vector evaluated genetic algorithm is shown in figure 15.11.

# 15.4.2 Nondomination Ranking

One can compute naive Pareto frontiers using the individuals in a population. A design point that lies on the approximate Pareto frontier is considered better than a value deep within the criterion space. We can use nondomination ranking (algorithm 15.5) to rank individuals according to the following levels:

7 T.W. Athan and P.Y. Papalambros, "A Note on Weighted Criteria Methods for Compromise Solutions in Multi-Objective Optimization," Engineering Optimization, vol. 27, no. 2, pp. 155-176, 1996.

8 Population methods are covered in chapter 9.

9J.D.Schaffer, "Multiple Objective Optimization with Vector Evaluated Genetic Algorithms," in International Conference on Genetic Algorithms and Their Applications, 1985.

10 K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II," IEEE Transactions on Evolutionary Computation, vol. 6, no. 2, pp. 182-197, 2002.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6dcf3d4500089987a68fdbc46b6ee56807bf3a31dc3302bd65c64efa4e2963dd.jpg)  
Figure 15.10. Using subpopulations in a vector evaluated genetic algorithm.

```julia
function vector Evaluated_genetic_algorithm(f, population, k_max, S, C, M)  
m = length(f(population[1]))  
m_pop = length(population)  
m_subpop = m_pop ÷ m  
for k in 1 : k_max  
    ys = f(population)  
    parents = select(S, [y[1] for y in ys])[1:m_subpop]  
    for i in 2 : m  
        subpop = select(S, [y[i] for y in ys])[1:m_subpop]  
    append!(parents, subpop)  
end  
p = randperm(2m_pop)  
p_ind=i→parents[mod(p[i]-1,m_pop)+1][(p[i]-1)÷m_pop + 1]  
parents = [[p_ind(i), p_ind(i+1)] for i in 1 : 2 : 2m_pop]  
children = [crossover(C,population[p[1]],population[p[2]])) for p in parents]  
population = [mutate(M, c) for c in children]  
end  
return population  
end
```

Algorithm 15.4. The vector evaluated genetic algorithm, which takes a vector-valued objective function  $f$ , an initial population, number of iterations  $k_{\text{max}}$ , a SelectionMethod  $S$ , a CrossoverMethod  $C$ , and a MutationMethod  $M$ . The resulting population is returned.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/248a1d4988eb58e6445dc085652c780a923017bbcd7e72d3a82454fd128f7ea7.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9f910235824c29a408f741702fabb766bb71dbe7556c6462e211d2de91c8bcdf.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/774f39e56a79efafdd8a08c29842a2aadfbab50eb2de16ae45b010221fc30cdc.jpg)  
Figure 15.11. A vector evaluated genetic algorithm applied to the circle function defined in appendix B.8. The Pareto frontier is shown in blue.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a42e42268252443283f44a8e384cc16d3750425194d5578c7ac86718095c33f9.jpg)

Level 1. Nondominated individuals in the population.  
Level 2. Nondominated individuals except those in Level 1.  
Level 3. Nondominated individuals except those in Levels 1 or 2.

：

Level  $k$ . Nondominated individuals except those in Levels 1 to  $k - 1$ .

Level 1 is obtained by applying algorithm 15.2 to the population. Subsequent levels are generated by removing all previous levels from the population and then applying algorithm 15.2 again. This process is repeated until all individuals have been ranked. An individual's objective function value is proportional to its rank.

The nondomination levels for an example population are shown in figure 15.12.

# 15.4.3 Pareto Filters

Population methods can be augmented with a Pareto filter, which is a population that approximates the Pareto frontier. $^{11}$  The filter is typically updated with every generation (algorithm 15.7). Individuals in the population that are not dominated by any individuals in the filter are added. Any dominated points in the filter are removed. Individuals from the Pareto filter can be injected into the population, thereby reducing the chance that portions of the Pareto frontier are lost between generations.

11 H. Ishibuchi and T. Murata, "A Multi-Objective Genetic Local Search Algorithm and Its Application to Flowshop Scheduling," IEEE Transactions on Systems, Man, and Cybernetics, vol. 28, no. 3, pp. 392-403, 1998.

```txt
function non_domination_levels(ys)  
L, m = 0, length(ys)  
levels = zeros(Int, m)  
while minimum levels == 0  
L += 1  
for (i, y) in enumerate(ys)  
if levels[i] == 0 && ! any((levels[i] == 0 || levels[i] == L) && dominates(ys[i], y) for i in 1 : m)  
levels[i] = L  
end  
end  
return levels
```

Algorithm 15.5. A function for getting the nondomination levels of an array of multiobjective function evaluations, ys.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9efa2b9d6d4c5b460fc70f7e468f5132b38579bb03c70a17e6e5b2ed44702df7.jpg)  
Figure 15.12. The nondomination levels for a population. Darker levels have lower (better) rankings.

The filter often has a maximum capacity. $^{12}$  Filters that are overcapacity can be reduced by finding the closest pair of design points in the criterion space and removing one individual from that pair. This pruning method is implemented in algorithm 15.6. A Pareto filter obtained using a genetic algorithm is shown in figure 15.13.

Typically the size of the population.

```julia
function discard_closest_pair!(xs, ys)  
index, min_dist = 0, Inf  
for (i,y) in enumerate(ys)  
    for (j, y') in enumerate(ys[i+1:end])  
        dist = norm(y - y')  
    if dist < min_dist  
        index, min_dist = rand([i,j]), dist  
    end  
end  
deleteat!(xs, index)  
deleteat!(ys, index)  
return (xs, ys)  
end
```

Algorithm 15.6. The method discardclosest_pair! is used to remove one individual from a filter that is above capacity. The method takes the filter's list of design points xs and associated objective function values ys.

```txt
function update_pareto_filter!(filter_xs, filter_ys, xs, ys; capacity=length(xs))  
for (x,y) in zip(xs, ys)  
    if !any(dominates(y',y) for y' in filter_ys)  
        push!(filter_xs, x)  
        push!(filter_ys, y)  
    end  
end  
filter_xs, filter_ys = naive_pareto(filters_xs, filter_ys)  
while length(filters_xs) > capacity  
    discard_closest_pair!(filter_xs, filter_ys)  
end  
return (filter_xs, filter_ys)  
end
```

Algorithm 15.7. A method for updating a Pareto filter with design points filter_xs, corresponding objective function values filter_ys, a population with design points xs and objective function values ys, and filter capacity capacity which defaults to the population size.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e8407d0037150915b59c78244529f1025e278d0443aaabbd104deaf42a649b85.jpg)  
Figure 15.13. A Pareto filter used on the genetic algorithm in figure 15.11 to approximate the Pareto frontier.

# 15.4.4 Niche Techniques

The term niche refers to a focused cluster of points, typically in the criterion space, as shown in figure 15.14. Population methods can converge on a few niches, which limits their spread over the Pareto frontier. Niche techniques help encourage an even spread of points.

In fitness sharing,[13] shown in figure 15.15, an individual's objective values are penalized by a factor equal to the number of other points within a specified distance in the criterion space.[14] This scheme causes all points in a local region to share the fitness of the other points within the local region. Fitness sharing can be used together with nondomination ranking and subpopulation evaluation.

Equivalence class sharing can be applied to nondomination ranking. When comparing two individuals, the fitter individual is first determined based on the nondomination ranking. If they are equal, the better individual is the one with the fewest number of individuals within a specified distance in the criterion space.

Another niche technique has been proposed for genetic algorithms in which parents selected for crossover cannot be too close together in the criterion space. Selecting only nondominated individuals is also recommended.[15]

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b8c6323281cd02875a54bd67f075a2c3de46afcac08a13bda42fb4132577caf6.jpg)  
Figure 15.14. Two clear niches for a population in a two-dimensional criterion space.

$^{13}$  Fitness is inversely related to the objective being minimized.

14 D. E. Goldberg and J. Richardson, "Genetic Algorithms with Sharing for Multimodal Function Optimization," in International Conference on Genetic Algorithms, 1987.  
15 S. Narayanan and S. Azarm, "On Improving Multiobjective Genetic Algorithms for Design Optimization," Structural Optimization, vol. 18, no. 2-3, pp. 146-155, 1999.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/71024b76807dd4c9000628c279f8acb1d61b0d740a6bf666489c030582f20df1.jpg)  
Figure 15.15. The results of applying fitness sharing to the Pareto filter in figure 15.13, thereby significantly improving its coverage.

# 15.5 Preference Elicitation

Preference elicitation involves inferring a scalar-valued objective function from preferences of experts about the tradeoffs between objectives. There are many different ways to represent the scalar-valued objective function, but this section will focus on the weighted sum model where  $f(\mathbf{x}) = \mathbf{w}^{\top}\mathbf{f}(\mathbf{x})$ . Once we identify a suitable  $\mathbf{w}$ , we can use this scalar-valued objective function to find an optimal design.

# 15.5.1 Model Identification

A common approach for identifying the weight vector  $\mathbf{w}$  in our preference model involves asking experts to state their preference between two points  $\mathbf{a}$  and  $\mathbf{b}$  in the criterion space  $\mathcal{Y}$  (figure 15.16). Each of these points is the result of optimizing for a point on the Pareto frontier using an associated weight vector  $\mathbf{w}_{\mathbf{a}}$  and  $\mathbf{w}_{\mathbf{b}}$ . The expert's response is either a preference for  $\mathbf{a}$  or a preference for  $\mathbf{b}$ . There are other schemes for eliciting preference information, such as ranking points in the criterion space, but this binary preference query has been shown to pose minimal cognitive burden on the expert.[17]

16 This section overviews non-Bayesian approaches to preference elicitation. For Bayesian approaches, see: S. Guo and S. Sanner, "Real-Time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries," in International Conference on Artificial Intelligence and Statistics (AISTATS), 2010. J.R. Lepird, M.P. Owen, and M.J. Kochenderfer, "Bayesian Preference Elicitation for Multi-objective Engineering Design Optimization," Journal of Aerospace Information Systems, vol. 12, no. 10, pp. 634-645, 2015.

17 V. Conitzer, "Eliciting SinglePeaked Preferences Using Comparison Queries," Journal of Artificial Intelligence Research, vol. 35, pp. 161-191, 2009.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/49fc44e673486a868e5285c0cdcfea726ba0f0be4593a2f177c96f5eb54c04a8.jpg)  
a

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1ae3ccadc9c1fc6350954cd3ad219cfccab3995623f945aa1397572b224cb223.jpg)  
b  
Figure 15.16. Preference elicitation schemes often involve asking experts their preferences between two points in the criterion space.

Suppose the outcomes of the expert queries have resulted in a set of criterion pairs

$$
\left\{\left(\mathbf {a}^{(1)}, \mathbf {b}^{(1)}\right), \dots , \left(\mathbf {a}^{(n)}, \mathbf {b}^{(n)}\right) \right\} \tag {15.10}
$$

where  $\mathbf{a}^{(i)}$  is preferable to  $\mathbf{b}^{(i)}$  in each pair. For each of these preferences, the weight vector must satisfy

$$
\mathbf {w}^{\top} \mathbf {a}^{(i)} <   \mathbf {w}^{\top} \mathbf {b}^{(i)} \Longrightarrow \left(\mathbf {a}^{(i)} - \mathbf {b}^{(i)}\right) ^{\top} \mathbf {w} <   0 \tag {15.11}
$$

In order to be consistent with the data, the weight vector must satisfy

$$
\left\{ \begin{array}{l} \left(\mathbf {a}^{(i)} - \mathbf {b}^{(i)}\right) ^{\top} \mathbf {w} <   0 \text{for} i \text{in} 1: n \\ \mathbf {1}^{\top} \mathbf {w} = 1 \\ \mathbf {w} \geq \mathbf {0} \end{array} \right. \tag {15.12}
$$

Many different weight vectors could potentially satisfy the above equation. One approach is to choose a  $\mathbf{w}$  that best separates  $\mathbf{w}^{\top} \mathbf{a}^{(i)}$  from  $\mathbf{w}^{\top} \mathbf{b}^{(i)}$

$$
\underset{\mathbf {w}} {\text{minimize}} \quad \sum_{i = 1}^{n} \left(\mathbf {a}^{(i)} - \mathbf {b}^{(i)}\right) ^{\top} \mathbf {w}
$$

$$
\text{subject} \quad \left(\mathbf {a}^{(i)} - \mathbf {b}^{(i)}\right) ^{\top} \mathbf {w} <   0 \quad \text{for} i \text{in} 1: n \tag {15.13}
$$

$$
\mathbf {1}^{\top} \mathbf {w} = 1 \quad \mathbf {w} \geq \mathbf {0}
$$

It is often desirable to choose the next weight vector such that it minimizes the distance from the previous weight vector. We can replace the objective function in equation (15.13) with  $\| \mathbf{w} - \mathbf{w}^{(n)}\| _1$ , thereby ensuring that our new weight vector  $\mathbf{w}^{(n + 1)}$  is as close as possible to our current one.[18]

18 The previous weight vector may or may not be consistent with the added constraint  $(\mathbf{a}^{(n)} - \mathbf{b}^{(n)})^\top \mathbf{w} < 0$

# 15.5.2 Paired Query Selection

We generally want to choose the two points in the criterion space so that the outcome of the query is as informative as possible. There are many different approaches for such paired query selection, but we will focus on methods that attempt to reduce the space of weights consistent with expert responses, preference information supplied by a domain expert.

We will denote the set of weights consistent with expert responses as  $\mathcal{W}$ , which is defined by the linear constraints in equation (15.12). Because weights are bounded between 0 and 1, the feasible set is an enclosed region forming a convex polytope with finite volume. We generally want to reduce the volume of  $\mathcal{W}$  in as few queries as possible.

$Q$ -Eval<sup>19</sup>, shown in figure 15.17, is a greedy elicitation strategy that heuristically seeks to reduce the volume of  $\mathcal{W}$  as quickly as possible with each iteration. It chooses the query that comes closest to bisecting  $\mathcal{W}$  into two equal parts. The method operates on a finite sampling of Pareto-optimal design points. The procedure for choosing a query pair is:

1. Compute the prime analytic center  $\mathbf{c}$  of  $\mathcal{W}$ , which is the point that maximizes the sum of the logarithms of the distances between itself and the closest point on each nonredundant constraint in  $\mathcal{W}$ :

$$
\mathbf {c} = \underset{\mathbf {w} \in \mathcal {W}} {\arg \max } \sum_{i = 1}^{n} \ln \left(\left(\mathbf {b}^{(i)} - \mathbf {a}^{(i)}\right) ^{\top} \mathbf {w}\right) \tag {15.14}
$$

2. Compute the normal distance from the bisecting hyperplane between each pair of points and the center.  
3. Sort the design-point pairs in order of increasing distance.  
4. For each of the  $k$  hyperplanes closest to  $\mathbf{c}$ , compute the volume ratio of the two polytopes formed by splitting  $\mathcal{W}$  along the hyperplane.  
5. Choose the design-point pair with split ratio closest to 1.

The polyhedral method $^{20}$  works by approximating  $\mathcal{W}$  with a bounding ellipsoid centered at the analytic center of  $\mathcal{W}$  as shown in figure 15.18. Queries are designed to partition the bounding ellipsoid into approximately equal parts and to favor cuts that are perpendicular to the longest axis of the ellipsoid to reduce both uncertainty and to balance the breadth in each dimension.

19 V.S. Iyengar, J. Lee, and M. Campbell, "Q-EVAL: Evaluating Multiple Attribute Items Using Queries," in ACM Conference on Electronic Commerce, 2001.

20 D. Braziunas and C. Boutilier, "Elicitation of Factored Utilities," AI Magazine, vol. 29, no. 4, pp. 79-92, 2009.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/72f7e792cce415053e9f859e90a1a5edf95ce16a39c58dc98e25501845c85992.jpg)  
Set of Consistent Weights  $\mathcal{W}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6b1023701d276ab83395a4de03c93183a2bb4627696782f55c01c1ee991bbd56.jpg)  
Analytic Center and Bisecting Hyperplane

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2b1e35276f01a8c4c27896b197a661c13dbf20ae3e695fd3fe6f2477ed18e1ff.jpg)  
Two Regions under Query a vs. b  
Figure 15.17. The Q-Eval greedy elicitation strategy. The figure shows the initial set of weights  $\mathcal{W}$  consistent with previous preferences, a pair of weight vectors and their corresponding bisecting hyperplane, and the two polytopes formed by splitting along the bisecting hyperplane. The algorithm considers all possible pairs from a finite sampling of Pareto-optimal design points and chooses the query that most evenly splits  $\mathcal{W}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d93d1174846443462eacc504a86e4532eb922c609c471956b5ade569daea2a5d.jpg)  
Figure 15.18. The polyhedral method uses a bounding ellipsoid for  $\mathcal{W}$ .

# 15.5.3 Design Selection

The previous section discussed query methods that select query pairs for efficiently reducing the search space. After query selection is complete, one must still select a final design. This process is known as design selection.

One such method, decision quality improvement,[21] is based on the idea that if we have to commit to a particular weight, we should commit to the one for which the worst-case objective value is lowest:

$$
\mathbf {x}^{*} = \underset{\mathbf {x} \in \mathcal {X}} {\arg \min } \underset{\mathbf {w} \in \mathcal {W}} {\max } \mathbf {w}^{\top} \mathbf {f} (\mathbf {x}) \tag {15.15}
$$

This minimax decision is robust because it provides an upper bound on the objective value.

The minimax regret $^{22}$  instead minimizes the maximum amount of regret the user can have when selecting a particular design:

$$
\mathbf {x}^{*} = \underset{\mathbf {x} \in \mathcal {X}} {\arg \min } \underbrace{\underset{\mathbf {w} \in \mathcal {W} \mathbf {x}^{\prime} \in \mathcal {X}} {\max } \mathbf {w}^{\top} \mathbf {f} (\mathbf {x}) - \mathbf {w}^{\top} \mathbf {f} \left(\mathbf {x}^{\prime}\right)}_{\text{maximumregret}} \tag {15.16}
$$

where  $\mathbf{w}^{\top}\mathbf{f}(\mathbf{x}) - \mathbf{w}^{\top}\mathbf{f}(\mathbf{x}^{\prime})$  is the regret associated with choosing design  $\mathbf{x}$  instead of design  $\mathbf{x}^{\prime}$  under the preference weight vector  $\mathbf{w}$ . Minimax regret can be viewed as accounting for the decision system's uncertainty with respect to the designer's true utility function.

The minimax regret can be used as stopping criteria for preference elicitation strategies. We can terminate the preference elicitation procedure once the minimax regret drops below a certain threshold.

# 15.6 Summary

- Design problems with multiple objectives often involve trading performance between different objectives.  
- The Pareto frontier represents the set of potentially optimal solutions.  
- Vector-valued objective functions can be converted to scalar-valued objective functions using constraint-based or weight-based methods.  
- Population methods can be extended to produce individuals that span the Pareto frontier.

21 D. Braziunas and C. Boutilier, "Minimax Regret-Based Elicitation of Generalized Additive Utilities," in Conference on Uncertainty in Artificial Intelligence (UAI), 2007.

22C. Boutilier, R. Patrascu, P. Poupart, and D. Schuurmans, "Constraint-Based Optimization and Utility Elicitation Using the Minimax Decision Criterion," Artificial Intelligence, vol. 170, no. 8-9, pp. 686-713, 2006.

- Knowing the preferences of experts between pairs of points in the criterion space can help guide the inference of a scalar-valued objective function.

# 15.7 Exercises

Exercise 15.1. The weighted sum method is a very simple approach, and it is indeed used by engineers for multiobjective optimization. What is one disadvantage of the procedure when it is used to compute the Pareto frontier?

Solution: The weighted sum method cannot find Pareto-optimal points in nonconvex regions of the Pareto frontier.

Exercise 15.2. Why are population methods well-suited for multiobjective optimization?

Solution: Nonpopulation methods will identify only a single point in the Pareto frontier. The Pareto frontier is very valuable in informing the designer of the tradeoffs among a set of very good solutions. Population methods can spread out over the Pareto frontier and be used as an approximation of the Pareto frontier.

Exercise 15.3. Suppose you have the points  $\{[1,2],[2,1],[2,2],[1,1]\}$  in the criterion space and you wish to approximate a Pareto frontier. Which points are Pareto optimal with respect to the rest of the points? Are any weakly Pareto-optimal points?

Solution:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c04e9663c60c1c18e9074cbf16a569489bed936b72bbbaf07aae3449fe359e00.jpg)

The only Pareto-optimal point is [1, 1]. Both [1, 2] and [2, 1] are weakly Pareto optimal.

Exercise 15.4. Multiobjective optimization is not easily done with second-order methods. Why is this the case?

Solution: The "gradient" of a vector is a matrix. Second-order derivatives would require using tensors and solving a tensor equation for a search direction is often computationally burdensome.

Exercise 15.5. Consider a square criterion space  $\mathcal{V}$  with  $y_{1} \in [0,1]$  and  $y_{2} \in [0,1]$ . Plot the criterion space, indicate the Pareto-optimal points, and indicate the weakly Pareto optimal points.

Solution: The only Pareto-optimal point is  $\mathbf{y} = [0, 0]$ . The rest of the points on the bottom-left border are weakly Pareto-optimal.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d09ffd1677b9232c199380d81b24b387711cf803b0fa732b6cf44e30b2a49667.jpg)

Exercise 15.6. Enforcing  $\mathbf{w} \geq \mathbf{0}$  and  $\| \mathbf{w} \|_1 = 1$  in the weighted sum method is not sufficient for Pareto optimality. Give an example where coordinates with zero-valued weights find weakly Pareto-optimal points.

Solution: Consider the square criterion space from the previous question. Using  $\mathbf{w} = [0,1]$  assigns zero value to the first objective, causing the entire bottom edge of the criterion space to have equal value. As discussed above, only  $\mathbf{y} = [0,0]$  is Pareto optimal, the rest are weakly Pareto optimal.

Exercise 15.7. Provide an example where goal programming does not produce a Pareto-optimal point.

Solution: For example, if  $\mathbf{y}^{\mathrm{goal}}$  is in the criterion set, the goal programming objective will be minimized by  $\mathbf{y}^{\mathrm{goal}}$ . If  $\mathbf{y}^{\mathrm{goal}}$  is also not Pareto optimal, the solution will not be Pareto optimal either.

Exercise 15.8. Use the constraint method to obtain the Pareto curve for the optimization problem:

$$
\underset{x} {\text{minimize}} \left[ x^{2}, (x - 2) ^{2} \right]
$$

Solution: The constraint method constrains all but one objective. A Pareto curve can be generated by varying the constraints. If we constrain the first objective, each optimization problem has the form:

$$
\underset{x} {\text{minimize}} \quad (x - 2) ^{2}
$$

$$
\text{subject} \quad x^{2} \leq c
$$

The constraint can be satisfied only for  $c \geq 0$ . This allows  $x$  to vary between  $\pm \sqrt{c}$ . The first objective is optimized by minimizing the deviation of  $x$  from 2. Thus, for a given value of  $c$ , we obtain:

$$
x^{*} = \left\{ \begin{array}{l l} 2 & \text{if } c \geq 4 \\ \sqrt{c} & \text{if } c \in [ 0, 4) \\ \text{undefined} & \text{otherwise} \end{array} \right.
$$

The resulting Pareto curve is:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/05b1742b85287e400a47e14fe081fa26b84ae2e89af1299571e9575c4cd7cee6.jpg)

Exercise 15.9. Suppose we have a multiobjective optimization problem where the two objectives are as follows:

$$
f_{1} (x) = - (x - 2) \sin (x)
$$

$$
f_{2} (x) = - (x + 3) ^{2} \sin (x)
$$

With  $x \in \{-5, -3, -1, 1, 3, 5\}$ , plot the points in the criterion space. How many points are on the Pareto frontier?

Solution: The criterion space is the space of objective function values. The resulting plot is:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7cc5a3eb7b26b15a4993ddeb2991979ad6d3809aaf7168becf94b2f79e622b7d.jpg)

We find that four points are on the approximate Pareto frontier corresponding to our six sample points. The corresponding design points are  $x = \{-1, -3, 3, 1\}$ .

# 16 Sampling Plans

For many optimization problems, function evaluations can be quite expensive. For example, evaluating a hardware design may require a lengthy fabrication process, an aircraft design may require a wind tunnel test, and new deep learning hyperparameters may require a week of GPU training. A common approach for optimizing in contexts where evaluating design points is expensive is to build a surrogate model, which is a model of the optimization problem that can be efficiently optimized in lieu of the true objective function. Further evaluations of the true objective function can be used to improve the model. Fitting such models requires an initial set of points, ideally points that are space-filling; that is, points that cover the region as well as possible. This chapter covers different sampling plans for covering the search space when we have limited resources.<sup>1</sup>

# 16.1 Full Factorial

The full factorial sampling plan (algorithm 16.1) places a grid of evenly spaced points over the search space. This approach is easy to implement, does not rely on randomness, and covers the space, but it uses a large number of points. A grid of evenly spaced points is spread over the search space as shown in figure 16.1. Optimization over the points in a full factorial sampling plan is referred to as grid search.

The sampling grid is defined by a lower-bound vector  $\mathbf{a}$  and an upper-bound vector  $\mathbf{b}$  such that  $a_{i} \leq x_{i} \leq b_{i}$  for each component  $i$ . For a grid with  $m_{i}$  samples in the  $i$ th dimension, the nearest points are separated by a distance  $(b_{i} - a_{i}) / (m_{i} - 1)$ .

The full factorial method requires a sample count exponential in the number of dimensions. For  $n$  dimensions with  $m$  samples per dimension, we have  $m^n$

There are other references that discuss the topics in this chapter in greater detail. See, for example: G.E.P.Box,W.G.Hunter,and J.S. Hunter,Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building, 2nd ed. Wiley,2005.A.Dean,D.Voss, and D.Draguljic,Design and Analysis of Experiments, 2nd ed.Springer, 2017.D.C.Montgomery,Design and Analysis of Experiments.Wiley, 2017.

2 The full factorial method gets its name not from a factorial sample count (it is exponential) but from designing with two or more discrete factors. Here the factors are the  $m$  discretized levels associated with each variable.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b6ea5ae1582fd6bc0bde5890b47cc258c06acfda6177e1289a0dc4169c7291e8.jpg)  
Figure 16.1. Full factorial search covers the search space in a grid of points.

total samples. This exponential growth is far too high to be of practical use when there are more than a few variables. Even when full factorial sampling is able to be used, the grid points are generally forced to be quite coarse and therefore can easily miss small, local features of the optimization landscape.

```matlab
function samples_full_factorial(a, b, m)  
ranges = [range(a[i], stop=b[i], length=m[i]) for i in eachindex(a)]  
return collect.(collect(Iterators/product(ranges...)))  
end
```

Algorithm 16.1. A function for obtaining all sample locations for the full factorial grid. Here, a is a vector of variable lower bounds, b is a vector of variable upper bounds, and m is a vector of sample counts for each dimension.

# 16.2 Random Sampling

A straightforward alternative to full factorial sampling is random sampling, which simply draws  $m$  random samples over the design space using a pseudo-random number generator. To generate a random sample  $\mathbf{x}$ , we can sample each variable independently from a distribution. If we have bounds on the variables, such as  $a_{i} \leq x_{i} \leq b_{i}$ , a common approach is to use a uniform distribution over  $[a_{i}, b_{i}]$ , although other distributions may be used. For some variables, it may make sense to use a log-uniform distribution.<sup>3</sup> The samples of design points are uncorrelated with each other. The hope is that the randomness, on its own, will result in an adequate cover of the design space.

3 Some parameters, such as the step factor for deep neural networks, are best searched in log-space.

# 16.3 Uniform Projection Plans

Suppose we have a two-dimensional optimization problem discretized into an  $m \times m$  sampling grid as with the full factorial method, but, instead of taking all  $m^2$  samples, we want to sample only  $m$  positions. We could choose the samples at random, but not all arrangements are equally useful. We want the samples to be spread across the space, and we want the samples to be spread across each individual component.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8b81111bb12426ebbf214c24024aa74a60744bf84206bf5e37deda4d09c44aa8.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9a898805609366fbb9ba6d3a3543ab78326f74d3c3ac87224dc4d40112533e2f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8be71afd6d2cfadbe0f0c5a527a110207e36165b7ae6c9a71a14b851a7ea67f7.jpg)  
Figure 16.2. Several ways to choose  $m$  samples from a two-dimensional grid. We generally prefer sampling plans that cover the space and vary across each component.

A uniform projection plan is a sampling plan over a discrete grid where the distribution over each dimension is uniform. For example, in the rightmost sampling plan in figure 16.2, each row has exactly one entry and each column has exactly one entry.

A uniform projection plan with  $m$  samples on an  $m \times m$  grid can be constructed using an  $m$ -element permutation as shown in figure 16.3. There are therefore  $m!$  possible uniform projection plans. This set of possible projection plans grows quickly. Even for  $m = 5$ , this is already  $5! = 120$  possible plans. For  $m = 10$ , there are 3,628,800 plans.

Sampling with uniform projection plans is sometimes called Latin-hypercube sampling because of the connection to Latin squares (figure 16.4). A Latin square is an  $m \times m$  grid where each row contains each integer 1 through  $m$  and each column contains each integer 1 through  $m$ . Latin-hypercubes are a generalization to any number of dimensions.

Uniform projection plans for  $n$  dimensions can be constructed using a permutation for each dimension (algorithm 16.2).

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/353c5eac5df6537c3919549178e43e5629a622ce95a6dfb50fb64e67f9f344f8.jpg)  
Figure 16.3. Constructing a uniform projection plan using the permutation  $p = [4,2,1,3,5]$ .

<table><tr><td>4</td><td>1</td><td>3</td><td>2</td></tr><tr><td>1</td><td>4</td><td>2</td><td>3</td></tr><tr><td>3</td><td>2</td><td>1</td><td>4</td></tr><tr><td>2</td><td>3</td><td>4</td><td>1</td></tr></table>

Figure 16.4. A  $4 \times 4$  Latin square. A uniform projection plan can be constructed by choosing a value  $i \in \{1,2,3,4\}$  and sampling all cells with that value.

```javascript
function uniform_projection計劃(m，n)perms  $=$  [randperm(m）fori in1：n][perms[i][j]foriin1：n]forjin1：m]end
```

Algorithm 16.2. A function for constructing a uniform projection plan for an n-dimensional hypercube with m samples per dimension. It returns a vector of index vectors.

# 16.4 Stratified Sampling

Many sampling plans, including uniform projection and full factorial plans, are based on an  $m \times m$  grid. Such a grid, even if fully sampled, could miss important information due to systematic regularities as shown in figure 16.5. One method for providing an opportunity to hit every point is to use stratified sampling.

Stratified sampling modifies any grid-based sampling plan, including full factorial and uniform projection plans. Cells are sampled at a point chosen uniformly at random from within the cell rather than at the cell's center as shown in figure 16.6.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2dde6b97ec024f42a55d6a864c113beb57652950f5ffafd3d04dd5d0d4535ca9.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9992b71384dbad57bafa08c507ba636ff20cad819ff1fe75bad1bda7eea90321.jpg)  
Figure 16.5. Using an evenly spaced grid on a function with systematic regularities can miss important information.

# 16.5 Space-Filling Metrics

A good sampling plan fills the design space since the ability for a surrogate model to generalize from samples decays with the distance from those samples. Not all plans, even uniform projection plans, are equally good at covering the search space. For example, a grid diagonal (figure 16.7) is a uniform projection plan but only covers a narrow strip. This section discusses different space-filling metrics for measuring the degree to which a sampling plan  $X \subseteq \mathcal{X}$  fills the design space.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/86d177e39a61a7e6db54abf3c7b61b0c43c8e9bbe053785bf7c2421724ab70a9.jpg)  
Figure 16.6. Stratified sampling applied to a uniform projection plan.  
Figure 16.7. A uniform projection plan that is not space-filling.

# 16.5.1 Discrepancy

The ability of the sampling plan to fill a hyperrectangular design space  $\mathcal{H}$  can be measured by its discrepancy. If  $X$  has low discrepancy, then a randomly chosen subset of the design space should contain a fraction of samples proportional to the subset's volume relative to that of  $\mathcal{H}$ . The discrepancy associated with  $X$  is the maximum difference between the fraction of samples in a hyperrectangular subset  $H$  and the fraction of the subset's volume with respect to the volume of the design space:

$$
d (X) = \sup_{H \in \mathcal {H}} \left| \frac{\# (X \cap H)}{\# (X \cap \mathcal {H})} - \frac{\lambda (H)}{\lambda (\mathcal {H})} \right| \tag {16.1}
$$

where  $\# (X\cap H)$  is the number of points in  $X$  that lie in the hyperrectangle  $H$ . Note that  $\# (X\cap \mathcal{H})$  will be the number of points in  $X$ , as all points lie in the design space. The value  $\lambda (H)$  is the  $n$ -dimensional volume of the given hyperrectangle, which is the product of its side lengths. The term supremum is very similar to maximization but allows a solution to exist for problems where  $H$  merely approaches a particular rectangular subset, as seen in example 16.1.<sup>6</sup> Computing the discrepancy in this way for hyperrectangles can be difficult, and it can be even less straightforward for non-hyperrectangles.

# 16.5.2 Pairwise Distances

An alternative method for measuring how space space-filling a sampling plan is involves analyzing the pairwise distances between all points. Algorithm 16.3 computes the pairwise distances between all points in a sampling plan. Sampling plans that are more space-filling will tend to have larger pairwise distances.

We can compare two different sampling plans by sorting each set's pairwise distances in ascending order. As implemented in algorithm 16.4, the plan with the first pairwise distance that exceeds the other is considered more space-filling.

There are several methods for generating space-filling uniform projection plans. One method simply generates candidates at random and then uses the one that is most space-filling. Alternatively, we can produce candidates by repeatedly mutating a uniform projection plan in a way that preserves the uniform projection property (algorithm 16.5). We can also incorporate simulated annealing to search the space of sampling plans.

4 L. Kuipers and H. Niederreiter, Uniform Distribution of Sequences. Dover, 2012.  
In arbitrary dimensions, we can use the Lebesgue measure, which is a generalization of volume to any subset of  $n$ -dimensional Euclidean space. It is length in one-dimensional space, area in two-dimensional space, and volume in three-dimensional space.

This definition of discrepancy requires hyperrectangles. The notion of discrepancy can be extended to allow  $\mathcal{H}$  to include other sets, such as convex polytopes.

Consider the set:

$$
X = \left\{\left[ \frac{1}{5}, \frac{1}{5} \right], \left[ \frac{2}{5}, \frac{1}{5} \right], \left[ \frac{1}{10}, \frac{3}{5} \right], \left[ \frac{9}{10}, \frac{3}{10} \right], \left[ \frac{1}{50}, \frac{1}{50} \right], \left[ \frac{3}{5}, \frac{4}{5} \right] \right\}
$$

The discrepancy of  $X$  with respect to the unit square is determined by a rectangular subset  $H$  that either has very small area but contains very many points or has very large area and contains very few points.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5d571abedfc12cdb147205f3e2d8545b1f5db90e253134aba216ada949f32010.jpg)

The blue rectangle,  $x_1 \in \left[\frac{1}{10}, \frac{2}{5}\right]$ ,  $x_2 \in \left[\frac{1}{5}, \frac{3}{5}\right]$ , has a volume of 0.12 and contains 3 points. Its corresponding discrepancy measure is thus 0.38.

The purple rectangle,  $x_{1} \in \left[\frac{1}{10} + \epsilon, \frac{9}{10} - \epsilon\right]$ ,  $x_{2} \in \left[\frac{1}{5} + \epsilon, \frac{4}{5} - \epsilon\right]$ , produces an even higher discrepancy. As  $\epsilon$  approaches zero, the volume and the discrepancy approach 0.48 because the rectangle contains no points. Note that the limit was required, reflecting the need to use a supremum in the definition of discrepancy.

Example 16.1. Computing the discrepancy for a sampling plan over the unit square. The sizes of the rectangles are slightly exaggerated to clearly show which points they contain.

```julia
import LinearAlgebra: norm  
function pairwise_distance(X, p=2)  
    m = length(X)  
        [norm(X[i] - X[j], p) for i in 1: (m-1) for j in (i+1): m]  
end
```

Algorithm 16.3. A function for obtaining the list of pairwise distances between points in sampling plan  $\mathsf{X}$  using the  $L_{p}$  norm specified by  $\mathfrak{p}$ .

```matlab
function compare_samplingplans(A,B,p=2)  
pA = sort(pairwise_distances(A,p))  
pB = sort(pairwise_distances(B,p))  
for (dA,dB) in zip(pA,pB)  
    if dA < dB  
        return 1  
    elseif dA > dB  
        return -1  
    end  
end  
return 0
```

```julia
function mutate!(X)  
m, n = length(X), length(X[1])  
j = rand(1:n)  
i = randperm(m)[1:2]  
X[i[1]][j], X[i[2]][j] = X[i[2]][j], X[i[1]][j]  
return X  
end
```

# 16.5.3 Morris-Mitchell Criterion

The comparison scheme in section 16.5.2 typically results in a challenging optimization problem with many local minima. An alternative is to search for a plan that minimizes the Morris-Mitchell criterion (algorithm 16.6):7

$$
\Phi_{q} (X) = \left(\sum_{i} d_{i}^{- q}\right) ^{1 / q} \tag {16.2}
$$

where  $d_{i}$  is the  $i$ th pairwise distance between points in  $X$  and  $q > 0$  is a tunable parameter. Morris and Mitchell recommend optimizing:

$$
\underset{X} {\text{minimize}} \underset{q \in \{1, 2, 5, 10, 20, 50, 10 0 \}} {\text{maximize}} \Phi_{q} (X) \tag {16.3}
$$

```matlab
function phiq(X, q=1, p=2)  
    dists = pairwise_distance(X, p)  
    return sum(dists.^(-q))^1/q)  
end
```

Algorithm 16.4. A function for comparing the degree to which two sampling plans A and B are spacefilling using the  $L_{p}$  norm specified by p. The function returns -1 if A is more space-filling than B. It returns 1 if B is more space-filling than A. It returns 0 if they are equivalent.

Algorithm 16.5. A function for mutating uniform projection plan  $\mathsf{X}$  while maintaining its uniform projection property.

7 M. D. Morris and T.J. Mitchell, "Exploratory Designs for Computational Experiments," Journal of Statistical Planning and Inference, vol. 43, no. 3, pp. 381-402, 1995.

8 We can view  $\Phi_q(X)$  as the application of the  $L_{q}$  norm to a vector containing inverse pairwise distances from the points in  $X$ .

Algorithm 16.6. An implementation of the Morris-Mitchell criterion which takes a list of design points  $X$ , the criterion parameter  $q > 0$ , and a norm parameter  $p \geq 1$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/92688e6cdef3d22f5d1b210edfd305b32a513bc6d7aa1c22ea65eae2c27e8661.jpg)  
Figure 16.8 shows the Morris-Mitchell criterion evaluated for several randomly generated uniform projection plans.  
Figure 16.8. Uniform projection plans sorted from best to worst according to  $\Phi_1$ .

# 16.6 Space-Filling Subsets

In some cases, we have a set of points  $X$  and want to find a subset of points  $S \subset X$  that still maximally fills  $X$ . The need for identifying space-filling subsets of  $X$  arises in the context of multifidelity models. For example, suppose we used a sampling plan  $X$  to identify a variety of aircraft wing designs to evaluate using computational fluid dynamic models in simulation. We can choose only a subset of these design points  $S$  to build and test in a wind tunnel. We still want  $S$  to be space filling.

The degree to which  $S$  fills the design space can be quantified using the maximum distance between a point in  $X$  and the closest point in  $S$ . This metric generalizes to any two finite sets  $A$  and  $B$  (algorithm 16.7). We can use any  $L_{p}$

9 A.I.J. Forrester, A. Sobester, and A.J. Keane, "Multi-Fidelity Optimization via Surrogate Modelling," Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 463, no. 2088, pp. 3251-3269, 2007.

norm, but we typically use  $L_{2}$ , the Euclidean distance:

$$
d_{\max } (X, S) = \underset{\mathbf {x} \in X} {\operatorname{maximize}} \underset{\mathbf {s} \in S} {\operatorname{minimize}} \| \mathbf {s} - \mathbf {x} \| _{p} \tag {16.4}
$$

$$
\begin{array}{l} \min_{\text{dist}} (a, B, p) = \operatorname{minimum} (\operatorname{norm} (a - b, p) \text{for} b \text{in} B) \\ d_{-} \max (A, B, p = 2) = \operatorname{maximum} (\operatorname{min}_{\text{dist}} (a, B, p) \text{for} a \text{in} A) \end{array}
$$

A space-filling sampling plan is one that minimizes this metric. $^{10}$  Finding a space-filling sampling plan with  $m$  elements is an optimization problem

$$
\underset{S} {\text{minimize}} d_{\max } (X, S)
$$

$$
\text{subject} S \subseteq X \tag {16.5}
$$

$$
\# S = m
$$

Optimizing equation (16.5) is typically computationally intractable. A brute force approach would try all  $d! / m!(d - m)!$  size- $m$  subsets for a dataset of  $d$  design points. There are various heuristic strategies that are more efficient but may not be optimal.

- In greedy subset selection (algorithm 16.8), we initialize  $S$  to contain a random point from  $X$ . We then add the next best point that minimizes the distance metric. We continue adding points until  $S$  is of the desired size.  
- In the exchange subset selection (algorithm 16.9), we initialize  $S$  to a random subset of  $X$ . We repeatedly replace points that are in  $S$  with a different point in  $X$  that is not already in  $S$  to improve on the distance metric. We stop when we can no longer find an exchange that improves the metric.

Because both of these algorithms are initialized randomly, better results might be obtained through random restarts (section 4.6) and returning the best sampling plan. Figure 16.9 compares space-filling subsets obtained using greedy subset selection and exchange subset selection.

Algorithm 16.7. The set  $L_{p}$  distance metrics between two discrete sets, where A and B are lists of design points and p is the  $L_{p}$  norm parameter.  
10 We can also minimize the Morris-Mitchell criterion for  $S$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3ac5186171f2f9641231f76e64e30a6e10e1289843c298a6325230d788cf10cb.jpg)  
greedy

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/18347975c8aa18a6132dd7bfea9afffd2e0b0445171340d268c419f90c70d866.jpg)  
exchange  
$x_{1}$  
Figure 16.9. Space-filling subsets obtained with both greedy subset selection and exchange subset selection.

```julia
function greedy_subset_selection(X, m, d=d_max)  
S = [sample(X)]  
for i in 2 : m  
    x = argmin(x→d(X, vcat(S, [x))), setdiff(X, S))  
push!(S, x)  
end  
return S  
end
```

Algorithm 16.8. Greedy subset selection for finding m-element sampling plans that minimize a distance metric d for discrete set X.

```txt
function exchange subtotal_selection(X, m, d=d_max)  
S = sample(X, m, replace=false)  
δ, done = d(X, S), false  
while !done  
best_pair = (0,0)  
for i in 1 : m  
s = S[i]  
for (j, x) in enumerate(X)  
if !(x in S)  
S[i] = x  
δ' = d(X, S)  
if δ' < δ  
δ = δ'  
best_pair = (i, j)  
end  
S[i] = s  
end  
end  
done = best_pair == (0,0)  
if !done  
i, j = best_pair  
S[i] = X[j]  
end  
end  
return S
```

Algorithm 16.9. Exchange subset selection for finding m-element sampling plans that minimize a distance metric d for discrete set X.

# 16.7 Quasi-Random Sequences

Quasi-random sequences, $^{11}$  also called low-discrepancy sequences, are often used in the context of trying to approximate an integral over a multidimensional space:

$$
\int_{\mathcal {X}} f (\mathbf {x}) d \mathbf {x} \approx \frac{v}{m} \sum_{i = 1}^{m} f \left(\mathbf {x}^{(i)}\right) \tag {16.6}
$$

where each  $\mathbf{x}^{(i)}$  is sampled uniformly at random over the domain  $\mathcal{X}$  and  $v$  is the volume of  $\mathcal{X}$ . This approximation is known as Monte Carlo integration.

Rather than relying on random or pseudo-random numbers to generate integration points, quasi-random sequences are deterministic sequences that fill the space in a systematic manner so that the integral converges as fast as possible in the number of points  $m$ . These quasi-Monte Carlo methods have an error convergence of  $O(1 / m)$  as opposed to  $O(1 / \sqrt{m})$  for typical Monte Carlo integration, as shown in figure 16.10.

Quasi-random sequences are typically constructed for the unit  $n$ -dimensional hypercube,  $[0,1]^n$ . Any multidimensional function with bounds on each variable can be transformed into such a hypercube. There are various methods for generating quasi-random sequences. Figure 16.13 compares several methods against random sampling.

# 16.7.1 Additive Recurrence

Simple recurrence relations of the form:

$$
x^{(k + 1)} = x^{(k)} + c \pmod {1} \tag {16.7}
$$

produce space-filling sets provided that  $c$  is irrational. The value of  $c$  leading to the smallest discrepancy is

$$
c = 1 - \varphi = \frac{\sqrt{5} - 1}{2} \approx 0. 61 80 34 \tag {16.8}
$$

where  $\varphi$  is the golden ratio.13

We can construct a space-filling set over  $n$  dimensions using an additive recurrence sequence for each coordinate, each with its own value of  $c$ . The square roots of the primes are known to be irrational, and can thus be used to obtain different sequences for each coordinate:

$$
c_{1} = \sqrt{2}, \quad c_{2} = \sqrt{3}, \quad c_{3} = \sqrt{5}, \quad c_{4} = \sqrt{7}, \quad c_{5} = \sqrt{11}, \quad \ldots \qquad (16. 9)
$$

11C. Lemieux, Monte Carlo and Quasi-Monte Carlo Sampling. Springer, 2009.

Figure 16.10. The error from estimating  $\int_0^1\sin (10x)dx$  using Monte Carlo integration with random numbers from  $\mathcal{U}(0,1)$  and a Sobol sequence. The Sobol sequence, covered in section 16.7.3, converges faster.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/01ac777cb052cdb1f0ccceaa10daf6332fd25fefaa10b1c21e2e1f9d3025fff5.jpg)  
In Julia, a transformation from a hyperrectangle with lower-bounds a and upper bounds b to a unit hypercube is given by  $x \rightarrow f((x - a). / (b - a))$ .  
13 C. Schretter, L. Kobbelt, and P.-O. Dehaye, "Golden Ratio Sequences for Low-Discrepancy Sampling," Journal of Graphics Tools, vol. 16, no. 2, pp. 95-104, 2016.

Methods for additive recurrence are implemented in algorithm 16.10.

```txt
function filling_set_additive_recurrence(m; c=φ-1)  
x0 = rand()  
return [mod(x0 + k*c, 1) for k in 0:m-1]  
end  
function filling_set_additive_recurrence(m, n)  
ps = primes(max(ceil(Int, n*(log(n) + log(log(n))), 13))  
seqs = [filling_set_additive_recurrence(m, c=sqrt(p))  
for p in ps[1:n]]  
return [collect(x) for x in zip(seqs...)]  
end
```

# 16.7.2 Halton Sequence

The Halton sequence is a multidimensional quasi-random space-filling set. $^{14}$  The single-dimensional version, called van der Corput sequences, generates sequences where the unit interval is divided into powers of base  $b$ . For example,  $b = 2$  produces:

$$
X = \left\{\frac{1}{2}, \frac{1}{4}, \frac{3}{4}, \frac{1}{8}, \frac{5}{8}, \frac{3}{8}, \frac{7}{8}, \frac{1}{16}, \dots \right\} \tag {16.10}
$$

whereas  $b = 5$  produces:

$$
X = \left\{\frac{1}{5}, \frac{2}{5}, \frac{3}{5}, \frac{4}{5}, \frac{1}{25}, \frac{6}{25}, \frac{11}{25}, \dots \right\} \tag {16.11}
$$

The values for  $b = 2$  are shown in figure 16.11.

Multi-dimensional space-filling sequences use one van der Corput sequence for each dimension, each with its own base  $b$ . In order to be uncorrelated, the bases must be coprime, meaning that the only positive integer that divides them both is 1. Methods for constructing Halton sequences are implemented in algorithm 16.11.

Algorithm 16.10. Additive recurrence for constructing m-element filling sequences over n-dimensional unit hypercubes. The Primes package is used to generate the first  $n$  prime numbers, where the  $k$ th prime number is bounded by

$$
k (\log k + \log \log k)
$$

for  $k > 6$  , and primes(a) from Primes.jl returns all primes up to a.Note that 13 is the sixth prime number.

14J.H.Halton,“Algorithm 247: Radical-Inverse Quasi-Random Point Sequence,"Communications of the ACM,vol.7,no.12,pp.701- 702,1964.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7d73e318ee2429f327832e89f8be239f725524286cb25e780b30e12f2f056bb1.jpg)  
Figure 16.11. The first 8 values of the Halton sequence for  $b = 2$ .

```julia
function halton(i, b)  
result, f = 0.0, 1.0  
while i > 0  
    f = f / b;  
    result = result + f * mod(i, b)  
    i = floor(Int, i / b)  
end  
return result  
end  
filling_set_halton(m; b=2) = [halton(i, b) for i in 1: m]  
function filling_set_halton(m, n)  
bs = primes(max(ceil(Int, n*(log(n) + log(log(n))))}, 6))  
seqs = [filling_set_halton(m, b=b) for b in bs[1:n]]  
return [collect(x) for x in zip(seqs...)]  
end
```

For large primes, we can get correlation in the first few numbers. Such a correlation is shown in figure 16.12. Correlation can be avoided by the leaped Halton method, $^{15}$  which takes every  $p$ th point, where  $p$  is a prime different from all coordinate bases.

# 16.7.3 Sobol Sequences

Sobol sequences are quasi-random space-filling sequences for  $n$ -dimensional hypercubes. $^{16}$  They are generated by xoring the previous Sobol number with a set of direction numbers:

$$
X_{j}^{(i)} = X_{j}^{(i - 1)} \underline {{\vee}} v_{j}^{(k)} \tag {16.12}
$$

where  $v_{j}^{(k)}$  is the  $j$ th bit of the  $k$ th direction number. The symbol  $\underline{\vee}$  denotes the XOR operation, which returns true if and only if both inputs are different. Tables of good direction numbers have been provided by various authors. $^{17}$

A comparison of these and previous approaches is shown in figure 16.13. Some methods exhibit a clear underlying structure.

# 16.8 Summary

- Sampling plans are used to cover search spaces with a limited number of points.

Algorithm 16.11. Halton quasi-random m-element filling sequences over n-dimensional unit hypercubes, where b is the base. The bases bs must be coprime.

Figure 16.12. The Halton sequence with  $\mathbf{b} = [19,23]$  for which the first 18 samples are perfectly linearly correlated.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e6533c48e1f06c3227ffec9c6bd183aeab0d1e4356dd4ec6bd86da5ab0dcf507.jpg)  
15 L. Kocis and W.J. Whiten, "Computational Investigations of Low-Discrepancy Sequences," ACM Transactions on Mathematical Software, vol. 23, no. 2, pp. 266-294, 1997.  
16 Named for Russian mathematician Ilya Meyerovich Sobol (1926-). I.M. Sobol, "On the Distribution of Points in a Cube and the Approximate Evaluation of Integrals," USSR Computational Mathematics and Mathematical Physics, vol. 7, no. 4, pp. 86-112, 1967.  
17 The Sobol. jl package provides an implementation for up to 21,201 dimensions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c7d06fc1e848e7ef2921f93b138953d827277ce5436b1b30b5c3ae4ade58bb91.jpg)  
Figure 16.13. Space-filling sampling plans in two dimensions. Samples are colored according to the order in which they are sampled. The uniform projection plan was generated randomly.

- Full factorial sampling, which involves sampling at the vertices of a uniformly discretized grid, requires a number of points exponential in the number of dimensions.  
- Uniform projection plans, which project uniformly over each dimension, can be efficiently generated and can be optimized to be space filling.  
- Greedy local search and the exchange algorithm can be used to find a subset of points that maximally fill a space.  
- Quasi-random sequences are deterministic procedures by which space-filling sampling plans can be generated.

# 16.9 Exercises

Exercise 16.1. Filling a multidimensional space requires exponentially more points as the number of dimensions increases. To help build this intuition, determine the side lengths of an  $n$ -dimensional hypercube such that it fills half of the volume of the  $n$ -dimensional unit hypercube.

Solution: The one-dimensional unit hypercube is  $x \in [0,1]$ , and its volume is 1. In this case the required side length  $\ell$  is 0.5. The two-dimensional unit hypercube is the unit square  $x_{i} \in [0,1]$  for  $i \in \{1,2\}$ , which has a 2-dimensional volume, or area, of 1. The area of a square with side length  $\ell$  is  $\ell^2$ , so we solve:

$$
\ell^{2} = \frac{1}{2} \quad \Longrightarrow \quad \ell = \frac{\sqrt{2}}{2} \approx 0. 70 7
$$

An  $n$ -dimensional hypercube has volume  $\ell^n$ . We thus solve:

$$
\ell^{n} = \frac{1}{2} \quad \Longrightarrow \quad \ell = 2^{- 1 / n}
$$

The side length approaches one.

Exercise 16.2. Suppose that you sample randomly inside a unit sphere in  $n$  dimensions. Compute the probability that a randomly sampled point is within  $\epsilon$  distance from the surface of the sphere as  $n \to \infty$ . Hint: The volume of a sphere is  $C(n)r^n$ , where  $r$  is the radius and  $C(n)$  is a function of the dimension  $n$  only.

Solution: The probability that a randomly sampled point is within  $\epsilon$ -distance from the surface is just the ratio of the volumes. Thus:

$$
P \left(\| x \| _{2} > 1 - \epsilon\right) = 1 - P \left(\| x \| _{2} <   1 - \epsilon\right) = 1 - (1 - \epsilon) ^{n} \rightarrow 1
$$

as  $n\to \infty$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e3a60208d06ecedf1d8ad7fe86168e3755911844ee4f4d3fa3d91458b5fa6f4c.jpg)

Exercise 16.3. We have a sampling plan  $X = \{\mathbf{x}^{(1)},\ldots ,\mathbf{x}^{(10)}\}$ , where

$$
\mathbf {x}^{(i)} = [ \cos (2 \pi i / 10), \sin (2 \pi i / 10) ]
$$

Suppose we use the Morris-Mitchell criterion with an  $L_{2}$  norm with the parameter  $q$  is set to 2. In other words, we want to evaluate  $\Phi_2(X)$ . If we add [2,3] to each  $\mathbf{x}^{(i)}$ , will  $\Phi_2(X)$  change? Why or why not?

Solution: No. The Morris-Mitchell criterion is based entirely on pairwise distances. Shifting all of the points by the same amount does not change the pairwise distances and thus will not change  $\Phi_2(X)$ .

Exercise 16.4. Additive recurrence requires that the multiplicative factor  $c$  in equation (16.7) be irrational. Why can  $c$  not be rational?

Solution: A rational number can be written as a fraction of two integers  $a / b$ . It follows that the sequence repeats every  $b$  iterations:

$$
\begin{array}{l} x^{(k + 1)} = x^{(k)} + \frac{a}{b} \pmod {1} \\ x^{(k)} = x^{(0)} + k \frac{a}{b} \pmod {1} \\ = x^{(0)} + k \frac{a}{b} + a \pmod {1} \\ = x^{(0)} + (k + b) \frac{a}{b} \pmod {1} \\ = x^{(k + b)} \\ \end{array}
$$

# 17 Surrogate Models

The previous chapter discussed methods for producing a sampling plan. This chapter shows how to use these samples to construct models of the objective function that can be used in place of the real objective function. Such surrogate models are designed to be smooth and inexpensive to evaluate so that they can be efficiently optimized. The surrogate model can then be used to help direct the search for the optimum of the real objective function.

# 17.1 Fitting Surrogate Models

A surrogate model  $\hat{f}$  parameterized by  $\theta$  is designed to mimic the true objective function  $f$ . The parameters  $\theta$  can be adjusted to fit the model based on samples collected from  $f$ . An example surrogate model is shown in figure 17.1.

Suppose we have  $m$  design points

$$
X = \left\{\mathbf {x}^{(1)}, \mathbf {x}^{(2)}, \dots , \mathbf {x}^{(m)} \right\} \tag {17.1}
$$

and associated function evaluations

$$
\mathbf {y} = \left\{y^{(1)}, y^{(2)}, \dots , y^{(m)} \right\} \tag {17.2}
$$

For a particular set of parameters, the model will predict

$$
\hat {\mathbf {y}} = \left\{\hat {f}_{\boldsymbol {\theta}} \left(\mathbf {x}^{(1)}\right), \hat {f}_{\boldsymbol {\theta}} \left(\mathbf {x}^{(2)}\right), \dots , \hat {f}_{\boldsymbol {\theta}} \left(\mathbf {x}^{(m)}\right) \right\} \tag {17.3}
$$

Fitting a model to a set of points requires tuning the parameters to minimize the difference between the true evaluations and those predicted by the model, typically according to an  $L_{p}$  norm:

$$
\underset{\theta} {\text{minimize}} \| \mathbf {y} - \hat {\mathbf {y}} \| _{p} \tag {17.4}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/37e80381a84dc33fab382a6f8fadb828aad6ea11bbbe90c1c8a9161166ddd815.jpg)  
Figure 17.1. Surrogate models approximate the true objective function. The model is fitted to the evaluated design points but deviates farther away from them.

1 It is common to use the  $L_{2}$  norm. Minimizing this equation with an  $L_{2}$  norm is equivalent to minimizing the mean squared error at those data points.

Equation (17.4) penalizes the deviation of the model only at the data points. There is no guarantee that the model will continue to fit well away from observed data, and model accuracy typically decreases the farther we go from the sampled points.

This form of model fitting is called regression. A large body of work exists for solving regression problems, and it is extensively studied in machine learning. The rest of this chapter covers several popular surrogate models and algorithms for fitting surrogate models to data, and concludes with methods for choosing between types of models.

# 17.2 Linear Models

A simple surrogate model is the linear model, which has the form<sup>3</sup>

$$
\hat {f} = w_{0} + \mathbf {w}^{\top} \mathbf {x} \quad \theta = \left\{w_{0}, \mathbf {w} \right\} \tag {17.5}
$$

For an  $n$ -dimensional design space, the linear model has  $n + 1$  parameters, and thus requires at least  $n + 1$  samples to fit unambiguously.

Instead of having both  $\mathbf{w}$  and  $w_{0}$  as parameters, it is common to construct a single vector of parameters  $\theta = [w_0,\mathbf{w}]$  and prepend 1 to the vector  $\mathbf{x}$  to get

$$
\hat {f} = \boldsymbol {\theta}^{\top} \mathbf {x} \tag {17.6}
$$

Finding an optimal  $\theta$  requires solving a linear regression problem:

$$
\underset{\theta} {\text{minimize}} \| \mathbf {y} - \hat {\mathbf {y}} \| _{2}^{2} \tag {17.7}
$$

which is equivalent to solving

$$
\underset{\theta} {\text{minimize}} \| \mathbf {y} - \mathbf {X} \theta \| _{2}^{2} \tag {17.8}
$$

where  $\mathbf{X}$  is a design matrix formed from  $m$  data points

$$
\mathbf {X} = \left[ \begin{array}{c} \left(\mathbf {x}^{(1)}\right) ^{\top} \\ \left(\mathbf {x}^{(2)}\right) ^{\top} \\ \vdots \\ \left(\mathbf {x}^{(m)}\right) ^{\top} \end{array} \right] \tag {17.9}
$$

2 K. P. Murphy, Machine Learning: A Probabilistic Perspective. MIT Press, 2012.

3 This equation may seem familiar. It is the equation for a hyperplane.

```julia
function design_matrix(X)  
n, m = length(X[1]), length(X)  
return [j==0 ? 1.0 : X[i][j] for i in 1:m, j in 0:n]  
end  
function linear_regression(X, y)  
 $\theta = \text{design\_matrix}(X) \setminus y$   
return x →  $\theta \cdot [1; x]$   
end
```

Algorithm 17.1. A method for constructing a design matrix from a list of design points  $\mathbf{X}$  and a method for fitting a surrogate model using linear regression to a list of design points  $\mathbf{X}$  and a vector of objective function values  $y$ .

Algorithm 17.1 implements methods for computing a design matrix and for solving a linear regression problem. Several cases for linear regression are shown in figure 17.2.

Linear regression has an analytic solution

$$
\boldsymbol {\theta} = \mathbf {X}^{+} \mathbf {y} \tag {17.10}
$$

where  $\mathbf{X}^{+}$  is the Moore-Penrose pseudoinverse of  $\mathbf{X}$ , as covered in section 13.2.

# 17.3 Basis Functions

The linear model is a linear combination of the components of  $\mathbf{x}$ :

$$
\hat {f} (\mathbf {x}) = \theta_{1} x_{1} + \dots + \theta_{n} x_{n} = \sum_{i = 1}^{n} \theta_{i} x_{i} = \boldsymbol {\theta}^{\top} \mathbf {x} \tag {17.11}
$$

which is a specific example of a more general linear combination of basis functions

$$
\hat {f} (\mathbf {x}) = \theta_{1} b_{1} (\mathbf {x}) + \dots + \theta_{q} b_{q} (\mathbf {x}) = \sum_{i = 1}^{q} \theta_{i} b_{i} (\mathbf {x}) = \boldsymbol {\theta}^{\top} \mathbf {b} (\mathbf {x}) \tag {17.12}
$$

In the case of linear regression, the basis functions simply extract each component,  $b_{i}(\mathbf{x}) = x_{i}$ .

Any surrogate model represented as a linear combination of basis functions can be fit using regression:

$$
\underset{\theta} {\text{minimize}} \| \mathbf {y} - \mathbf {B} \theta \| _{2}^{2} \tag {17.13}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/535aa03723a6435a544df7ba0fcfe2061fda5ff36b95b364ce4f898ce8cd9d36.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3191e8f091a641343a979ee9b2a9a01868557b4c4b38aeca69744a32c8e1433e.jpg)  
Figure 17.2. Models resulting from linear regression, which minimizes the square vertical distance of the model from each point. The pseudoinverse produces a unique solution for any nonempty point configuration.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/332a417e3a35ce8c578332e8fdd9dcf8bafe5ad8fc93f1cb8590b3bd145bb74a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f987cb44ce63707a7aa3b62d53e38b850d7dd2b5b32ccec25b72c5640aad168d.jpg)

The bottom-left subfigure shows the model obtained for two repeated points, in this case,  $m = n + 1$ . Because the two entries are repeated, the matrix  $\mathbf{X}$  is singular. Although  $\mathbf{X}$  does not have an inverse in this case, the pseudoinverse produces a unique solution that passes between the two points.

where  $\mathbf{B}$  is the basis matrix formed from  $m$  data points:

$$
\mathbf {B} = \left[ \begin{array}{c} \mathbf {b} \left(\mathbf {x}^{(1)}\right) ^{\top} \\ \mathbf {b} \left(\mathbf {x}^{(2)}\right) ^{\top} \\ \vdots \\ \mathbf {b} \left(\mathbf {x}^{(m)}\right) ^{\top} \end{array} \right] \tag {17.14}
$$

The weighting parameters can be obtained using the pseudoinverse

$$
\boldsymbol {\theta} = \mathbf {B}^{+} \mathbf {y} \tag {17.15}
$$

Algorithm 17.2 implements this more general regression procedure.

```julia
using LinearAlgebra  
function regression(X, y, bases)  
B = [b(x) for x in X, b in bases]  
 $\theta = B \setminus y$   
return x → sum(θ[i] * bases[i](x) for i in eachindex(θ))  
end
```

Linear models cannot capture nonlinear relations. There are a variety of other families of basis functions that can represent more expressive surrogate models. The remainder of this section discusses a few common families.

# 17.3.1 Polynomial Basis Functions

Polynomial basis functions consist of a product of design vector components, each raised to a power. Linear basis functions are a special case of polynomial basis functions.

From the Taylor series expansion<sup>4</sup> we know that any infinitely differentiable function can be closely approximated by a polynomial of sufficient degree. We can construct these bases using algorithm 17.3.

In one dimension, a polynomial model of degree  $k$  has the form

$$
\hat {f} (x) = \theta_{0} + \theta_{1} x + \theta_{2} x^{2} + \theta_{3} x^{3} + \dots + \theta_{k} x^{k} = \sum_{i = 0}^{k} \theta_{i} x^{i} \tag {17.16}
$$

Hence, we have a set of basis functions  $b_{i}(x) = x^{i}$  for  $i$  ranging from 0 to  $k$ .

Algorithm 17.2. A method for fitting a surrogate model to a list of design points  $\mathsf{X}$  and corresponding objective function values  $\mathbf{y}$  using regression with basis functions contained in the bases array.

Covered in appendix C.2.

In two dimensions, a polynomial model of degree  $k$  has basis functions of the form

$$
b_{i j} (\mathbf {x}) = x_{1}^{i} x_{2}^{j} \text{for} i, j \in \{0, \dots , k \}, i + j \leq k \tag {17.17}
$$

Fitting a polynomial surrogate model is a regression problem, so a polynomial model is linear in higher dimensional space (figure 17.3). Any linear combination of basis functions can be viewed as linear regression in a higher dimensional space.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e1193ad16d9b09c6acbff920c6c826f95fa2f8d235c2b05fe612ea717f3be8a5.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/18f1b338368af01b8da9f72488a964aa11d05ad9aefaf4bb074633745bf3598a.jpg)  
Figure 17.3. A polynomial model is linear in higher dimensions. The function exists in the plane formed from its bases, but it does not occupy the entire plane because the terms are not independent.

```txt
polynomialbases_1d(i,k)  $= [x\to x[i]^{\wedge}p$  for p in 0:k]   
function polynomialbases(n,k) bases  $=$  [polynomialbases_1d(i,k)for i in 1:n] terms  $=$  Function[] for ks in Iterators.product([0:k for i in 1:n]...) if sum(ks)  $\leq k$  push!(terms,  $\mathbf{x}\rightarrow \mathrm{prod}(b[j + 1](x)$  for (j,b) in zip(ks,bases))) end end return terms
```

Algorithm 17.3. A method for constructing an array of polynomial basis functions up to a degree  $k$  for the  $i$ th component of a design point, and a method for constructing a list of  $n$ -dimensional polynomial bases for terms up to degree  $k$ .

# 17.3.2 Sinusoidal Basis Functions

Any continuous function over a finite domain can be represented using an infinite set of sinusoidal basis functions. A Fourier series can be constructed for any

5 The Fourier series is also exact for functions defined over the entire real line if the function is periodic.

integrable univariate function  $f$  on an interval  $[a,b]$

$$
f (x) = \frac{\theta_{0}}{2} + \sum_{i = 1}^{\infty} \theta_{i}^{(\sin)} \sin \left(\frac{2 \pi i x}{b - a}\right) + \theta_{i}^{(\cos)} \cos \left(\frac{2 \pi i x}{b - a}\right) \tag {17.18}
$$

where

$$
\theta_{0} = \frac{2}{b - a} \int_{a}^{b} f (x) d x \tag {17.19}
$$

$$
\theta_{i}^{(\sin)} = \frac{2}{b - a} \int_{a}^{b} f (x) \sin \left(\frac{2 \pi i x}{b - a}\right) d x \tag {17.20}
$$

$$
\theta_{i}^{(\cos)} = \frac{2}{b - a} \int_{a}^{b} f (x) \cos \left(\frac{2 \pi i x}{b - a}\right) d x \tag {17.21}
$$

Just as the first few terms of a Taylor series are used in polynomial models, so too are the first few terms of the Fourier series used in sinusoidal models. The bases for a single component over the domain  $x \in [a,b]$  are:

$$
\left\{ \begin{array}{l l} b_{0} (x) & = 1 / 2 \\ b_{i}^{(\sin)} (x) & = \sin \left(\frac{2 \pi i x}{b - a}\right) \\ b_{i}^{(\cos)} (x) & = \cos \left(\frac{2 \pi i x}{b - a}\right) \end{array} \right. \tag {17.22}
$$

We can combine the terms for multidimensional sinusoidal models in the same way we combine terms in polynomial models. Algorithm 17.4 can be used to construct sinusoidal basis functions. Several cases for sinusoidal regression are shown in figure 17.4.

# 17.3.3 Radial Basis Functions

A radial function<sup>6</sup>  $\psi$  is a function that depends only on the distance of a point from some center point  $\mathbf{c}$ , such that it can be written  $\psi(\mathbf{x},\mathbf{c}) = \psi(\|\mathbf{x} - \mathbf{c}\|) = \psi(r)$ . Radial functions are convenient basis functions because placing a radial function contributes a hill or valley to the function landscape. Some common radial basis functions are shown in figure 17.5.

Radial basis functions require specifying the center points. One approach when fitting radial basis functions to a set of data points is to use the data points as the centers. For a set of  $m$  points, one thus constructs  $m$  radial basis functions

$$
b_{i} (\mathbf {x}) = \psi \left(\left\| \mathbf {x} - \mathbf {x}^{(i)} \right\|\right) \quad \text{for} i \text{in} 1: m \tag {17.23}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9bf12105df8608b5a7b7440b2c1c91b425e4ad10d25b6000cd7f86e03390b2b4.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a7536f6e42608e5084a5d9e61d019c0ca934d2d6a9ffded9d1aa477e5fc9937f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7cc4583e3d79ba98b595c39ab7180fb2361764a7f6081ca6ff929271a6944cc2.jpg)

Figure 17.4. Fitting sinusoidal models to noisy points.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5ad57d98a430c0028bdd3f1eb92a8f6fc869350dd8d90f9cff9cb5877994e421.jpg)  
6 A comprehensive review is provided by M. D. Buhmann, Radial Basis Functions. Cambridge University Press, 2003.

```julia
function sinusoidalbases_1d(j,k,a,b) T=b[j]-a[j] bases  $=$  Function[x  $\rightarrow$  1/2] for i in 1:k push!(bases,  $\mathbf{x}\rightarrow \sin (2\pi *\mathbf{i}*\mathbf{x}[\mathbf{j}] / \mathbf{T}))$  push!(bases,  $\mathbf{x}\rightarrow \cos (2\pi *\mathbf{i}*\mathbf{x}[\mathbf{j}] / \mathbf{T}))$  end return bases   
end function sinusoidalbases(k,a,b) n=length(a) bases  $=$  [sinusoidalbases_1d(i,k,a,b)for i in 1:n] terms  $=$  Function[] for ks in Iteratorproduct([0:2k for i in 1:n]...) powers  $=$  [div  $(\mathrm{k +1},2)$  for k in ks] if sum(powers)  $\leq k$  push!(terms, xprod(b[j+1](x) for (j,b) in zip(ks,bases))) end end return terms   
end
```

Algorithm 17.5 can be used to construct radial basis functions with known center points. Surrogate models with different radial basis functions are shown in figure 17.6.

```txt
radial bases  $(\psi ,C,p = 2) = [x\to \psi (\mathrm{norm}(x - c,p))$  for  $c$  in C]
```

# 17.4 Fitting Noisy Objective Functions

Models fit using regression will pass as close as possible to every design point. When the objective function evaluations are noisy, complex models are likely to excessively contort themselves to pass through every point. However, smoother fits are often better predictors of the true underlying objective function.

The basis regression problem specified in equation (17.13) can be augmented to prefer smoother solutions. A regularization term is added in addition to the prediction error in order to give preference to solutions with lower weights. The resulting basis regression problem with  $L_{2}$  regularization<sup>7</sup> is:

Algorithm 17.4. The method sinusoidalbases_1d produces a list of basis functions up to degree k for the ith component of the design vector given lower bound a and upper bound b. The method sinusoidalbases produces all base function combinations up to degree k for lower-bound vector a and upper-bound vector b.

Algorithm 17.5. A method for obtaining a list of basis functions given a radial basis function  $\psi$ , a list of centers  $C$ , and an  $L_{p}$  norm parameter  $p$ .

7 Regression with  $L_{2}$  regularization is also known as ridge regression. Other  $L_{p}$ -norms, covered in appendix C.4, can be used as well. Using the  $L_{1}$  norm is known as the lasso, originally introduced in section 11.5.5. The lasso encourages sparse solutions with less influential component weights set to zero, which can be useful in identifying important basis functions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5c6006fe59293a712cb33e0c4ba4a196d2d180d7baf537849add232037126e3a.jpg)  
linear:  $r$  
Gaussian:  $e^{-r^2 / 2\sigma^2}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c7aeb07a644a7e6eb252ad6bd5c078cca154df0c92750dd37bfe3fb507fd15cd.jpg)  
cubic:  $r^3$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/acfbf4246f66dadd0448fff081b5bce175aaf06f1699de3c603c9587050c71d2.jpg)  
thin plate spline:  $r^2 \log r$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0bc15107157f1a75c6dd4cd9353b90440dcd32d054003f2ce0392ea5845e86a8.jpg)  
r

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b210e469d80ecb2cdf0c7f1f3a15880657a9fdd5cefcd6d4dcb20b5a67453a88.jpg)  
multiquadratic:  $(r^2 +\sigma^2)^{\frac{1}{2}}$  
r

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/59018cff26b9226a1977afff203f0fb3d72980b4d55a8db60152f76fcfb89c6c.jpg)  
inverse multiquadratic:  $(r^2 +\sigma^2)^{-\frac{1}{2}}$  
r  
Figure 17.5. Several radial basis functions.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7d5250e0ce283133dd7433c4f6c45c4b120dae3a9c57d9896acf7ec242c0908a.jpg)  
x  
Figure 17.6. Several different Gaussian radial basis functions used to fit  $x \sin(5x)$  based on four noise-free samples.

$$
\underset{\theta} {\text{minimize}} \quad \| \mathbf {y} - \mathbf {B} \theta \| _{2}^{2} + \lambda \| \theta \| _{2}^{2} \tag {17.24}
$$

where  $\lambda \geq 0$  is a smoothing parameter, with  $\lambda = 0$  resulting in no smoothing.

The optimal parameter vector is given by:

$$
\boldsymbol {\theta} = \left(\mathbf {B}^{\top} \mathbf {B} + \lambda \mathbf {I}\right) ^{- 1} \mathbf {B}^{\top} \mathbf {y} \tag {17.25}
$$

where  $\mathbf{I}$  is the identity matrix. The matrix  $(\mathbf{B}^{\top}\mathbf{B} + \lambda \mathbf{I})$  is not always invertible if  $\lambda = 0$ . However, we can always produce an invertible matrix with a positive  $\lambda$ .

Algorithm 17.6 implements regression with  $L_{2}$  regularization. Surrogate models with different radial basis functions fit to noisy samples are shown in figure 17.7.

```matlab
function regression(x, y, bases,  $\lambda$ )  
B = [b(x) for x in X, b in bases]  
 $\theta = (B'B + \lambda*I)\backslash B'y$   
return x → sum(θ[i] * bases[i](x) for i in eachindex(θ))  
end
```

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/97fe9032c844d86412aa87955d3f3dc59cdac37e2b5de9372932c45d1109f4cc.jpg)

This optimization problem can be viewed as a multiobjective optimization problem (chapter 15) with the two objectives being minimize prediction error and minimize the sum of squared components of  $\mathbf{x}$ . We create a scalar objective function for this multiobjective problem using the weighted sum method from section 15.3.1.

Algorithm 17.6. A method for regression in the presence of noise, where  $\lambda$  is a smoothing term. It returns a surrogate model fitted to a list of design points  $X$  and corresponding objective function values  $y$  using regression with basis functions bases.

Figure 17.7. Several different Gaussian radial basis functions used to fit  $x\sin (5x)$  with zero mean, standard deviation 0.1 error based on ten noisy samples and radial basis function  $\psi = \exp (-5r^2)$ .

# 17.5 Model Selection

So far, we have discussed how to fit a particular model to data. This section explains how to select which model to use. We generally want to minimize generalization error, which is a measure of the error of the model on the full design space, including points that may not be included in the data used to train the model. One way to measure generalization error is to use the expected squared error of its predictions:

$$
\epsilon_{\text{gen}} = \mathbb {E}_{\mathbf {x} \sim \mathcal {X}} \left[ \left(f (\mathbf {x}) - \hat {f} (\mathbf {x})\right) ^{2} \right] \tag {17.26}
$$

Of course, we cannot calculate this generalization error exactly because it requires knowing the function we are trying to approximate. It may be tempting to estimate the generalization error of a model from the training error. One way to measure training error is to use the mean squared error (MSE) of the model evaluated on the  $m$  samples used for training:

$$
\epsilon_{\text{train}} = \frac{1}{m} \sum_{i = 1}^{m} \left(f \left(\mathbf {x}^{(i)}\right) - \hat {f} \left(\mathbf {x}^{(i)}\right)\right) ^{2} \tag {17.27}
$$

However, performing well on the training data does not necessarily correspond to low generalization error. Complex models may reduce the error on the training set, but they may not provide good predictions in other points in the design space as illustrated in example 17.1.<sup>9</sup>

This section discusses several methods for estimating generalization error. These methods train and test on subsets of the data with the help of algorithm 17.7. Although we train on subsets of the data when estimating the generalization error, once we have decided which model to use, we can train on the full dataset.

```julia
struct TrainTest
    train # training indices
    test # testing indices
end
function train_and Validate(X, y, tt, fit, metric)
    model = fit(X[tt.train], y[tt.train])
    return metric(model, X[tt.test], y[tt.test])
end
```

9 A major theme in machine learning is balancing model complexity to avoid overfitting the training data. K.P. Murphy, Machine Learning: A Probabilistic Perspective. MIT Press, 2012.

Algorithm 17.7. A method for training a model and then validating it on a metric. Here, train and test are lists of indices into the training data,  $X$  is a list of design points,  $y$  is the vector of corresponding function evaluations,  $tt$  is a train-test partition, fit is a model fitting function, and metric evaluates a model on test data.

Consider fitting polynomials of varying degrees to evaluations of the objective function

$$
f (x) = x / 10 + \sin (x) / 4 + \exp \left(- x^{2}\right)
$$

Below we plot polynomial surrogate models of varying degrees using the same nine evaluations evenly spaced over  $[-4, 4]$ . The training and generalization error are shown as well, where generalization is calculated over  $[-5, 5]$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f3328834a63d50bdd32860d7d5c7d42bc4b777aa0c7abe5b880355fab4296e13.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c32b641df4a892bd2a6ca1041d5ec91abcc6831efaae716ec0dde95859bb6cb2.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7cc4110700e4f41dc63495945ebc3ac79d5929be4b5baac606af6b50771cb666.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b001fce4fd53b3b56bb849b063be48e239bdf9a6724f0fc992d164475f57281c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/aac76a6cd6729368b4a6ed99759cad28bbee47d7efae40681d11463aa5463aed.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e533d1d1b19263b563a36b5d2126dcbc109f4d54a763553ceab6e08b5437260e.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b9ffeb6bbd73bad0350be4f01452c86566979f86707e2edfb2e5ec69713428e8.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fda0cec71265d5802c195ce113b2423638dc874a1d5cc395d2cf1683a75c22e8.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/331f6c09d8767c89b7e6edc8f07f53c2e472be3da3605f631539c29203b572b8.jpg)

The plot shows that the generalization error is high for both very low and high values of  $k$ , and that training error decreases as we increase the polynomial degree. The high-degree polynomials are particularly poor predictors for designs outside  $[-4, 4]$ .

Example 17.1. A comparison of training and generalization error as the degree of a polynomial surrogate model is varied.

# 17.5.1 Holdout

train

test

train(·) test(f,·) generalization error estimate

A simple approach to estimating the generalization error is the holdout method, which partitions the available data into a test set  $\mathcal{D}_h$  with  $h$  samples and a training set  $\mathcal{D}_t$  consisting of all remaining  $m - h$  samples as shown in figure 17.8. The training set is used to fit model parameters. The held out test set is not used during model fitting, and can thus be used to estimate the generalization error. Different split ratios are used, typically ranging from  $50\%$  train,  $50\%$  test to  $90\%$  train,  $10\%$  test, depending on the size and nature of the dataset. Using too few samples for training can result in poor fits (figure 17.9), whereas using too many will result in poor generalization estimates.

The holdout error for a model  $\hat{f}$  fit to the training set is

$$
\epsilon_{\text{holdout}} = \frac{1}{h} \sum_{(\mathbf {x}, y) \in \mathcal {D}_{h}} \left(y - \hat {f} (\mathbf {x})\right) ^{2} \tag {17.28}
$$

```julia
function holdout_partition(m, h=div(m,2))  
p = randperm(m)  
train = p[(h+1):m]  
holdout = p[1:h]  
return TrainTest(train, holdout)  
end
```

Even if the partition ratio is fixed, the holdout error will depend on the particular train-test partition chosen. Choosing a partition at random (algorithm 17.8) will only give a point estimate. In random subsampling (algorithm 17.9), we apply the holdout method multiple times with randomly selected train-test partitions. The estimated generalization error is the mean over all runs. $^{10}$  Because the validation sets are chosen randomly, this method does not guarantee that we validate on all of the data points.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/df298d6f198d2fecfdd9ac773c6cdc93804a180a42bd4fd97706b04db449539b.jpg)  
Figure 17.8. The holdout method (left) partitions the data into train and test sets.  
Figure 17.9. Poor train-test splits can result in poor model performance.

Algorithm 17.8. A method for randomly partitioning m data samples into training and holdout sets, where h samples are assigned to the holdout set.

10 The standard deviation over all runs can be used to estimate the standard deviation of the estimated generalization error.

```matlab
function random_subsampling(X, y, fit, metric; h=div(length(X), 2), k_max=10) m = length(X) mean(train_and Validate(X, y, holdout_partition(m, h), fit, metric) for k in 1 : k_max) end
```

Algorithm 17.9. The random subsampling method used to obtain mean and standard deviation estimates for model generalization error using k_max runs of the holdout method.

# 17.5.2 Cross Validation

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d4e5e5ab5f200956d0ed81a557e90e19530ef1902bfbf8878e83f6aa15b56b9d.jpg)  
Figure 17.10. Cross-validation partitions the data into equally sized sets. Each set is the holdout set once. Here we show 5-fold cross-validation.

```latex
train(··） test(  $\hat{f},\bullet)$   generalization error estimate   
train(··） test(  $\hat{f},\bullet)$   generalization error estimate   
train(··） test(  $\hat{f},\bullet)$   generalization error estimate   
train(··） test(  $\hat{f},\bullet)$   generalization error estimate   
train(··） test(  $\hat{f},\bullet)$   generalization error estimate   
generalization error  $\mu$  and  $\sigma$
```

Using a train-test partition can be wasteful because our model tuning can take advantage only of a segment of our data. Better results are often obtained by using  $k$ -fold cross validation. $^{11}$  Here, the original dataset  $\mathcal{D}$  is randomly partitioned into  $k$  sets  $\mathcal{D}_1, \ldots, \mathcal{D}_k$  of equal, or approximately equal, size, as shown in figure 17.10 and implemented in algorithm 17.10. We then train  $k$  models, one on each subset of  $k - 1$  sets, and we use the withheld set to estimate the generalization error. The cross-validation estimate of generalization error is the mean generalization error over all folds. $^{12}$

$$
\epsilon_{\text{cross -validation}} = \frac{1}{k} \sum_{i = 1}^{k} \epsilon_{\text{cross -validation}}^{(i)} \tag {17.29}
$$

$$
\epsilon_{\text{cross -validation}}^{(i)} = \frac{1}{| \mathcal {D}_{\text{test}}^{(i)} |} \sum_{(\mathbf {x}, y) \in \mathcal {D}_{\text{test}}^{(i)}} \left(y - \hat {f}^{(i)} (\mathbf {x})\right) ^{2} \tag {17.30}
$$

where  $\epsilon_{\mathrm{cross -validation}}^{(i)}$  and  $\mathcal{D}_{\mathrm{test}}^{(i)}$  are the cross-validation estimate and the withheld test set, respectively, for the  $i$ th fold.

Also known as rotation estimation.

12 As with random subsampling, an estimate of variance can be obtained from the standard deviation over folds.

```matlab
function k_fold CROSS_validation_sets(m, k)  
    perm = randperm(m)  
    sets = TrainTest[]  
    for i = 1:k  
        validate = perm[i:k:m];  
        train = permsetdiff(1:m, i:k:m)]  
        push!(sets, TrainTest(train, validate))  
    end  
    return sets  
end  
function cross_validationestimate(X, y, sets, fit, metric)  
    mean(train_and Validate(X, y, tt, fit, metric)  
            for tt in sets)  
    end
```

Cross-validation also depends on the particular data partition. An exception is leave-one-out cross-validation with  $k = m$ , which has a deterministic partition. It trains on as much data as possible, but it requires training  $m$  models. $^{13}$  Averaging over all  $\binom{m}{m/k}$  possible partitions, known as complete cross-validation, is often too expensive. While one can average multiple cross-validation runs, it is more common to average the models from a single cross-validation partition.

Cross-validation is demonstrated in example 17.2.

Algorithm 17.10. The method k_fold CROSS validation sets constructs the sets needed for k-fold cross validation on m samples, with  $k \leq m$ . The method cross_validation Estimate computes the mean of the generalization error estimate by training and validating on the list of train-validate sets contained in sets. The other variables are the list of design points X, the corresponding objective function values y, a function fit that trains a surrogate model, and a function metric that evaluates a model on a data set.

13 M. Stone, "Cross-Validatory Choice and Assessment of Statistical Predictions," Journal of the Royal Statistical Society, vol. 36, no. 2, pp. 111-147, 1974.

# 17.5.3 The Bootstrap

The bootstrap method<sup>14</sup> uses multiple bootstrap samples, which consist of  $m$  indices into a dataset of size  $m$  independently chosen uniformly at random. The indices are chosen with replacement, so some indices may be chosen multiple times and some indices may not be chosen at all as shown in figure 17.11. The bootstrap sample is used to fit a model that is then evaluated on the original training set. A method for obtaining bootstrap samples is given in algorithm 17.11.

If  $b$  bootstrap samples are made, then the bootstrap estimate of the generalization error is the mean of the corresponding generalization error estimates  $\epsilon_{\mathrm{test}}^{(1)}, \ldots, \epsilon_{\mathrm{test}}^{(b)}$ :

$$
\begin{array}{l} \epsilon_{\mathrm{boot}} = \frac{1}{b} \sum_{i = 1}^{b} \epsilon_{\mathrm{test}}^{(i)} (17.31) \\ = \frac{1}{m} \sum_{j = 1}^{m} \frac{1}{b} \sum_{i = 1}^{b} \left(y^{(j)} - \hat {f}^{(i)} \left(\mathbf {x}^{(j)}\right)\right) ^{2} (17.32) \\ \end{array}
$$

where  $\hat{f}^{(i)}$  is the model fit to the  $i$ th bootstrap sample. The bootstrap method is implemented in algorithm 17.12.

The bootstrap error in equation (17.31) tests models on data points to which they were fit. The leave-one-out bootstrap estimate removes this source of bias by only evaluating fitted models to withheld data:

$$
\epsilon_{\text{leave -one -out -boot}} = \frac{1}{m} \sum_{j = 1}^{m} \frac{1}{c_{- j}} \sum_{i = 1}^{b} \left\{ \begin{array}{l l} \left(y^{(j)} - \hat {f}^{(i)} \left(\mathbf {x}^{(j)}\right)\right) ^{2} & \text{ifjthindexwasnotintheithbootstrapsample} \\ 0 & \text{otherwise} \end{array} \right. \tag {17.33}
$$

where  $c_{-j}$  is the number of bootstrap samples that do not contain index  $j$ . The leave-one-out bootstrap method is implemented in algorithm 17.13.

The probability of a particular index not being in a bootstrap sample is:

$$
\left(1 - \frac{1}{m}\right) ^{m} \approx e^{- 1} \approx 0. 36 8 \tag {17.34}
$$

so a bootstrap sample is expected to have on average  $0.632m$  distinct indices from the original dataset.

14 B. Efron, "Bootstrap Methods: Another Look at the Jackknife," The Annals of Statistics, vol. 7, pp. 1-26, 1979.

Suppose we want to fit a noisy objective function using radial basis functions with the noise hyperparameter  $\lambda$  (section 17.4). We can use cross validation to determine  $\lambda$ . We are given ten samples from our noisy objective function. In practice, the objective function will be unknown, but this example uses

$$
f (x) = \sin (2 x) \cos (10 x) + \epsilon / 10
$$

where  $x \in [0,1]$  and  $\epsilon$  is random noise with zero mean and unit variance,  $\epsilon \sim \mathcal{N}(0,1)$ .

```txt
Random.seed!(0)  
f = x→sin(2x)*cos(10x)  
X = rand(10)  
y = f.(X) + rand(length(X))/10
```

We will use three folds assigned randomly:

```python
sets = k_fold CROSS_validation_sets(length(X), 3)
```

Next, we implement our metric. We use the mean squared error:

```txt
metric  $=$  (f, X, y)  $\rightarrow$  begin  
m = length(X)  
return sum((f(X[i]) - y[i])^2 for i in m)/m  
end
```

We now loop through different values of  $\lambda$  and fit different radial basis functions. We will use the Gaussian radial basis. Cross validation is used to obtain the MSE for each value:

```txt
$\lambda s = 10.^{\wedge}$  range(-4，stop=2，length=101)   
es  $=$  []   
basis  $=$  r  $\rightarrow$  exp(-5r^2)   
for  $\lambda$  in  $\lambda s$  fit  $\equiv$  (X，y)  $\rightarrow$  regression(X，y，radialbases(basis,X)，λ) push!(es, cross_validationestimate(X，y，sets，fit，metric)[1])   
end
```

The resulting curve has a minimum at  $\lambda \approx 0.2$ .

Example 17.2. Cross validation used to fit a hyperparameter.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3baa902498c914815e3e0ddbe737fb6c4ad4c456b4b5b62855bb35e2a59edd99.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fef13750e55c829256697035746998e64ada6ee7eaceaa033fe73635aeb2550f.jpg)  
Figure 17.11. A single bootstrap sample consists of  $m$  indices into the dataset sampled with replacement. A bootstrap sample is used to train a model, which is evaluated on the full dataset to obtain an estimate of the generalization error.

```python
bootstrap_sets(m, b) = [TrainTestrand(1:m, m), 1:m) for i in 1:b]
```

```txt
function bootstrap Estimate(X, y, sets, fit, metric) mean(train_and Validate(X, y, tt, fit, metric) for tt in sets) end
```

Algorithm 17.11. A method for obtaining b bootstrap samples, each for a data set of size m.

Algorithm 17.12. A method for computing the bootstrap generalization error estimate by training and validating on the list of train-validate sets contained in sets. The other variables are the list of design points  $X$ , the corresponding objective function values  $y$ , a function fit that trains a surrogate model, and a function metric that evaluates a model on a data set.

```txt
function leave_one_outiestroopestimate(X，y,sets，fit，metric) m，b=length(X)，length(setst）  $\varepsilon = 0.0$  models  $=$  [fit(X[tt.train]，y[tt.train])for tt in sets] forjin1:m c=0  $\delta = 0.0$  fori in1:b ifjneats[i].train c+=1  $\delta + =$  metric/models[i]，[X[j]，[y[j]]） end end  $\varepsilon + = \delta /\mathrm{c}$  end return  $\varepsilon /m$    
end
```

Algorithm 17.13. A method for computing the leave-one-out bootstrap generalization error estimate using the train-validate sets sets. The other variables are the list of design points  $X$ , the corresponding objective function values  $y$ , a function fit that trains a surrogate model, and a function metric that evaluates a model on a data set.

Unfortunately, the leave-one-out bootstrap estimate introduces a new bias due to the varying test set sizes. The 0.632 bootstrap estimate<sup>15</sup> (algorithm 17.14) alleviates this bias:

$$
\epsilon_{0. 63 2 \text{-boot}} = 0. 63 2 \epsilon_{\text{leave -one -out -boot}} + 0. 36 8 \epsilon_{\text{boot}} \tag {17.35}
$$

```txt
function bootstrap_632 Estimate(X, y, sets, fit, metric)  
models = [fit(X[tt.train], y[tt.train]) for tt in sets]  
 $\epsilon$ -loob = leave_one_out.bootstrap Estimate(X, y, sets, fit, metric)  
 $\epsilon$ -boot = bootstrap Estimate(X, y, sets, fit, metric)  
return 0.632  $\epsilon$ -loob + 0.368  $\epsilon$ -boot  
end
```

Several generalization estimation methods are compared in example 17.3.

Consider ten evenly spread samples of  $f(x) = x^{2} + \epsilon / 2$  over  $x \in [-3, 3]$ , where  $\epsilon$  is zero-mean, unit-variance Gaussian noise. We would like to test several different generalization error estimation methods when fitting a linear model to this data. Our metric is the root mean squared error, which is the square root of the mean squared error.

The methods used are the holdout method with eight training samples, five-fold cross validation, and the bootstrap methods each with ten bootstrap samples. Each method was fitted 100 times and the resulting statistics are shown below.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/81dab451bb8e16efa0bf0f13b86828a047b01f11536de5e6f35cbe3b7318ee90.jpg)

15 The 0.632 bootstrap estimate was introduced in B. Efron, "Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation," Journal of the American Statistical Association, vol. 78, no. 382, pp. 316-331, 1983. A variant, the  $0.632+$  bootstrap estimate, was introduced in B. Efron and R. Tibshirani, "Improvements on Cross-Validation: The .632+ Bootstrap Method," Journal of the American Statistical Association, vol. 92, no. 438, pp. 548-560, 1997.

Algorithm 17.14. A method for obtaining the 0.632 bootstrap estimate for data points  $X$ , objective function values  $y$ , number of bootstrap samples  $b$ , fitting function fit, and metric function metric.

Example 17.3. A comparison of generalization error estimation methods. The vertical lines in the box and whisker plots indicate the minimum, maximum, first and third quartiles, and median of every generalization error estimation method among 50 trials.

# 17.6 Multifidelity Surrogate Models

We use the word fidelity to describe the level of detail or accuracy of a model or a dataset. For some applications, high-fidelity data is expensive and time consuming to obtain in comparison to low-fidelity data, which can be much less expensive to obtain, but also less accurate. In the context of aircraft design, high-fidelity data might be obtained from a full-scale flight test, whereas low-fidelity data might be obtained from a fluid dynamics simulator. This section discusses how to use both low-fidelity and high-fidelity data to produce a multifidelity surrogate model.

In this section, we assume we have two datasets, a low-fidelity dataset  $X_{\ell}$  and a high-fidelity dataset  $X_{h}$ . Typically, the size of  $X_{\ell}$  is much larger than the size of  $X_{h}$ . Associated with these datasets are different evaluation functions,  $f_{\ell}(\mathbf{x})$  and  $f_{h}(\mathbf{x})$ . We assume that  $f_{\ell}(\mathbf{x})$  is a noisy approximation of  $f_{h}(\mathbf{x})$ . We want to use both datasets to produce a surrogate model  $\hat{f}_{h}(\mathbf{x})$  that is a good approximation of  $f_{h}(\mathbf{x})$ .

There are many methods for constructing  $\hat{f}_h(\mathbf{x})$  from  $X_{\ell}$  and  $X_{h}$ . One approach is to train a low-fidelity surrogate model  $\hat{f}_{\ell}(\mathbf{x})$  using  $X_{\ell}$  and then use  $X_{h}$  to find a transformation that makes  $\hat{f}_{\ell}(\mathbf{x})$  match  $f_{h}(\mathbf{x})$  as closely as possible. This transformation can take different forms, such as an affine transformation defined by scaling and offset parameters. An affine transformation would lead to a multifidelity model with the form:

$$
\hat {f}_{h} (\mathbf {x}) = a_{0} + a_{1} \hat {f}_{\ell} (\mathbf {x}) \tag {17.36}
$$

where  $a_0$  and  $a_1$  are free parameters in the model. This transformation is just another surrogate model, and the parameters can be fit using the high fidelity data just as in section 17.2.

Of course, an affine model might be inadequate to capture the differences between  $f_{\ell}(\mathbf{x})$  and  $f_{h}(\mathbf{x})$ . One way to address this is to construct a discrepancy model  $\hat{\delta} (\mathbf{x})$  that captures the differences between  $\hat{f}_h(\mathbf{x})$  and  $f_{h}(\mathbf{x})$ . This discrepancy model could be constructed using any of the methods discussed earlier in this chapter.

There are many extensions to the multifidelity surrogate model framework:16

- Multifidelity surrogate models where  $f_{\ell}$  and  $f_{h}$  do not have identical design vectors. In such cases, data from the low-fidelity model can be projected into the high-fidelity space.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/af9c73fa9510e2a56f65f6ea1889938442836ecd6f48b9f9c88a64854256ade0.jpg)  
Figure 17.12. A high-fidelity surrogate model  $\hat{f}_h(x)$  produced using many noisy low-fidelity samples and a small number of noisy high-fidelity samples.

16 For a broad overview, see Q. Zhou, M. Zhao, J. Hu, and M. Ma, Multi-Fidelity Surrogates: Modeling, Optimization and Applications. Springer, 2023.

- Cases where  $f_{\ell}$  and  $f_{h}$  do not measure directly comparable outputs. For example, test kitchens may be able to predict or measure objective qualities of a new dish like sweetness and salinity ( $f_{\ell}$ ), but the true higher-cost objective is based on how much their customers enjoy the resulting dish ( $f_{h}$ ). A common strategy is to build a mapping between outputs using samples of both  $f_{\ell}$  and  $f_{h}$  from the same design vectors.  
- Nonhierarchical datasets where  $f_{h}$  is not necessarily higher-fidelity than  $f_{\ell}$ , and multiple models of similar, or unknown, fidelity must be combined. One approach is to construct a surrogate model for each data source, and then to combine predictions from each model.

# 17.7 Summary

- Surrogate models are function approximations that can be optimized instead of the true, potentially expensive objective function.  
- Many surrogate models can be represented using a linear combination of basis functions.  
- Model selection involves a bias-variance tradeoff between models with low complexity that cannot capture important trends and models with high complexity that overfit to noise.  
- Generalization error can be estimated using techniques such as holdout,  $k$ -fold cross validation, and the bootstrap.  
- Multifidelity surrogate models can be constructed to use multiple data sources with varying fidelity.

# 17.8 Exercises

Exercise 17.1. Derive an expression satisfied by the optimum of the regression problem equation (17.8) by setting the gradient to zero. Do not invert any matrices. The resulting relation is called the normal equation.

Solution: The linear regression objective function is

$$
\left\| \mathbf {y} - \mathbf {X} \boldsymbol {\theta} \right\| _{2}^{2}
$$

We take the gradient and set it to zero:

$$
\nabla (\mathbf {y} - \mathbf {X} \theta) ^{\top} (\mathbf {y} - \mathbf {X} \theta) = - 2 \mathbf {X}^{\top} (\mathbf {y} - \mathbf {X} \theta) = \mathbf {0}
$$

which yields the normal equation

$$
\mathbf {x}^{\top} \mathbf {x} \boldsymbol {\theta} = \mathbf {x}^{\top} \mathbf {y}
$$

Exercise 17.2. When would we use a more descriptive model, for example, with polynomial features, versus a simpler model like linear regression?

Solution: As a general rule, more descriptive models should be used when more data are available. If only few samples are available such models are prone to overfitting, and a simpler model (with fewer degrees of freedom) should be used.

Exercise 17.3. A linear regression problem of the form in equation (17.8) is not always solved analytically, and optimization techniques are used instead. Why is this the case?

Solution: The model at hand may have a very large number of parameters. In such case, the resulting linear system will be too large and will require memory that grows quadratically with the parameter space. Iterative procedures like stochastic gradient descent require memory linear in the size of the parameter space and are sometimes the only viable solution.

Exercise 17.4. Suppose we evaluate our objective function at four points: 1, 2, 3, and 4, and we get back 0, 5, 4, and 6. We want to fit a polynomial model  $f(x) = \sum_{i=0}^{k} \theta_i x^i$ . Compute the leave-one-out cross validation estimate of the mean squared error as  $k$  varies between 0 and 4. According to this metric, what is the best value for  $k$ , and what are the best values for the elements of  $\theta$ ?

Solution: The leave-one-out cross-validation estimate is obtained by running  $k$ -fold cross validation with  $k$  equal to the number of samples in  $X$ . This means we must run 4-fold cross validation for each polynomial degree.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/01f52c792fd35774abc2c440fac7871eb8bafba01ab623d6d8a665bec4a8c0f8.jpg)

The lowest mean squared error is obtained for a linear model,  $k = 1$ . We fit a new linear model on the complete dataset to obtain our parameters:

```julia
X = [[1], [2], [3], [4]]  
y = [0, 5, 4, 6]  
bases = polynomialbases(1, 1)  
B = [b(x) for x in X, b in bases]  
 $\theta = B \setminus y$   
@show  $\theta$ $\theta = [-0.500000000000001, 1.700000000000002]$
```

Exercise 17.5. Suppose you have an aircraft collision avoidance system that you wish to optimize to minimize the frequency of near mid-air collisions, and you have a simulator that can simulate your collision avoidance system on generated aircraft encounters. You can run the simulator many times to get an accurate assessment of the collision avoidance system's performance. You can also run the simulator fewer times on more dangerous scenarios to get a quicker, albeit less accurate, assessment.

Suppose your high-fidelity simulator evaluations execute a million flight hours worth of simulations, of which only  $1\%$  are safety-critical<sup>17</sup>. Suppose also that your low-fidelity simulator runs only a thousand flight hours worth of simulations, but they are heavily biased such that  $100\%$  of them are safety-critical.

Notionally describe  $\mathbf{x}$ ,  $f_{\ell}$ ,  $f_{h}$ ,  $X_{\ell}$ ,  $X_{h}$ , and our optimization objective. If you were to construct a multifidelity surrogate model using equation (17.36), what would you expect  $a_0$  and  $a_1$  to be?

Solution: Here we tune the parameters of an aircraft collision avoidance system  $\mathbf{x}$  in order to minimize the frequency of near mid-air collisions. The entries of  $\mathbf{x}$  determine the behavior of the collision avoidance system.

17 Scenarios that are not safety-cirtical are unlikely to result in a near mid-air collision.

We have access to a simulator  $\mathfrak{sim}$ , which typically takes a scenario description  $\mathbf{s}$  containing initial conditions and descriptions of scenario-related events, and then simulates the behavior of two or more aircraft and the parameterized aircraft collision avoidance system to produce a trajectory,  $\mathfrak{sim}(\mathbf{x},\mathbf{s}) \to \tau$ . We quantify the performance of the collision avoidance system by evaluating whether the resulting trajectory includes a near mid-air collision, has_nmac(τ). Our optimization objective is to minimize the frequency of near mid-air collisions under some distribution of scenarios  $S$ :

$$
\underset{\mathbf {x}} {\text{minimize}} \underset{\mathbf {s} \in S} {\mathbb {E}} [ \operatorname{has}_{-} \operatorname{nmac} (\operatorname{sim} (\mathbf {x}, \mathbf {s})) ]
$$

The functions  $f_{\ell}$  and  $f_{h}$  both notionally have the form:

```julia
function objective(x, S, m)  
n_nmacs = 0  
for i in 1:m  
    s = rand(S)  
     $\tau$  = simulate(x, s)  
    n_nmacs += has_nmac( $\tau$ )  
end  
return n_nmacs / m  
end
```

They differ in both the distribution of scenarios  $S$  and the number of samples  $m$ , with  $f_{h}$  simulating 1,000 more scenarios than  $f_{\ell}$  assuming scenario durations are roughly uniform in length. The scenario distribution for  $f_{\ell}$  is heavily biased toward safety-critical scenarios.

The low-fidelity function  $f_{\ell}$  is thus a thousand times cheaper to evaluate than  $f_{h}$ . We can afford to sweep over the design space using  $f_{\ell}$ , producing a comparably large  $X_{\ell}$ . In comparison,  $X_{h}$  will have far fewer evaluations. Both of these sets will consist of (design, estimated collision frequency) tuples, though estimated collision frequencies using  $f_{\ell}$  will be much larger due to the scenario biasing.

In constructing our multifidelity surrogate model, we expect to have to scale down the frequency of near mid-air collisions obtained from the low-fidelity simulator in order to estimate the true safety risk. In this case, we would expect  $a_0$  to be roughly 0.01 to account for the adjustment in criticality. We do not need a bias term, so  $a_1$  would likely be close to zero.

# 18 Probabilistic Surrogate Models

The previous chapter discussed how to construct surrogate models from evaluated design points. When using surrogate models for the purpose of optimization, it is often useful to quantify our confidence in the predictions of these models. One way to quantify our confidence is by taking a probabilistic approach to surrogate modeling. A common probabilistic surrogate model is the Gaussian process, which represents a probability distribution over functions. This chapter will explain how to use Gaussian processes to infer a distribution over the values of different design points given the values of previously evaluated design points. We will discuss how to incorporate gradient information as well as noisy measurements of the objective function. Since the predictions made by a Gaussian process are governed by a set of parameters, we will discuss how to infer these parameters directly from data.

# 18.1 Gaussian Distribution

Before introducing Gaussian processes, we will first review some relevant properties of the multivariate Gaussian distribution, often also referred to as the multivariate normal distribution. An  $n$ -dimensional Gaussian distribution is parameterized by its mean  $\mu$  and its covariance matrix  $\Sigma$ . The probability density at  $\mathbf{x}$  is

$$
\mathcal {N} (\mathbf {x} \mid \boldsymbol {\mu}, \boldsymbol {\Sigma}) = (2 \pi) ^{- n / 2} | \boldsymbol {\Sigma} | ^{- 1 / 2} \exp \left(- \frac{1}{2} (\mathbf {x} - \boldsymbol {\mu}) ^{\top} \boldsymbol {\Sigma}^{- 1} (\mathbf {x} - \boldsymbol {\mu})\right) \tag {18.1}
$$

Figure 18.1 shows contour plots of the density functions with different covariance matrices. Covariance matrices are always positive semidefinite.

A value sampled from a Gaussian is written

$$
\mathbf {x} \sim \mathcal {N} (\boldsymbol {\mu}, \boldsymbol {\Sigma}) \tag {18.2}
$$

The univariate Gaussian distribution is discussed in appendix C.8.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/97f9f63b7b7c5c9dddb57d4a661d54c9feab5d70ea7adae3ac7db5368ccb2819.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0194b1aa94511cf6c16f69b68e0a54aa2f63865a6a9a490b9d0533931703ba38.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/008428bdd46b318ede414f18ab511ccf25689413fa6792ad38d0bf1a7e390c9d.jpg)  
Figure 18.1. Multivariate Gaussians with different covariance matrices.

Two jointly Gaussian random variables  $\mathbf{a}$  and  $\mathbf{b}$  can be written

$$
\left[ \begin{array}{l} \mathbf {a} \\ \mathbf {b} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{l} \mu_{\mathbf {a}} \\ \mu_{\mathbf {b}} \end{array} \right], \left[ \begin{array}{l l} \mathbf {A} & \mathbf {C} \\ \mathbf {C}^{\top} & \mathbf {B} \end{array} \right]\right) \tag {18.3}
$$

The marginal distribution<sup>2</sup> for a vector of random variables is given by its corresponding mean and covariance

$$
\mathbf {a} \sim \mathcal {N} \left(\mu_{\mathrm{a}}, \mathbf {A}\right) \quad \mathbf {b} \sim \mathcal {N} \left(\mu_{\mathrm{b}}, \mathbf {B}\right) \tag {18.4}
$$

The conditional distribution for a multivariate Gaussian also has a convenient closed-form solution:

$$
\mathbf {a} \mid \mathbf {b} \sim \mathcal {N} \left(\mu_{\mathbf {a} | \mathbf {b}}, \Sigma_{\mathbf {a} | \mathbf {b}}\right) \tag {18.5}
$$

$$
\mu_{\mathbf {a} \mid \mathbf {b}} = \mu_{\mathbf {a}} + \mathbf {C B}^{- 1} (\mathbf {b} - \mu_{\mathbf {b}}) \tag {18.6}
$$

$$
\boldsymbol {\Sigma}_{\mathbf {a} \mid \mathbf {b}} = \mathbf {A} - \mathbf {C B}^{- 1} \mathbf {C}^{\top} \tag {18.7}
$$

Example 18.1 illustrates how to extract the marginal and conditional distributions from a multivariate Gaussian.

2 The marginal distribution is the distribution of a subset of the variables when the rest are integrated, or marginalized, out. For a distribution over two variables  $a$  and  $b$  the marginal distribution over  $a$  is:

$$
p (a) = \int p (a, b) d b
$$

We have a joint Gaussian distribution over two variables  $x_{1}$  and  $x_{2}$ :

$$
\left[ \begin{array}{c} x_{1} \\ x_{2} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{c} 0 \\ 1 \end{array} \right], \left[ \begin{array}{c c} 3 & 1 \\ 1 & 2 \end{array} \right]\right)
$$

The marginal distribution for  $x_{1}$  is  $\mathcal{N}(0,3)$ , and the marginal distribution for  $x_{2}$  is  $\mathcal{N}(1,2)$ .

The conditional distribution for  $x_{1}$  given  $x_{2} = 2$  is

$$
\begin{array}{l} \mu_{x_{1} \mid x_{2} = 2} = 0 + 1 \cdot 2^{- 1} \cdot (2 - 1) = 0. 5 \\ \Sigma_{x_{1} \mid x_{2} = 2} = 3 - 1 \cdot 2^{- 1} \cdot 1 = 2. 5 \\ x_{1} \mid (x_{2} = 2) \sim \mathcal {N} (0. 5, 2. 5) \\ \end{array}
$$

Example 18.1. Marginal and conditional distributions for a multivariate Gaussian.

# 18.2 Gaussian Processes

In the previous chapter, we approximated the objective function  $f$  using a surrogate model function  $\hat{f}$  fitted to previously evaluated design points. A special type of surrogate model known as a Gaussian process allows us not only to predict  $f$  but also to quantify our uncertainty in that prediction using a probability distribution.<sup>3</sup>

A Gaussian process is a distribution over functions. For any finite set of points  $X = \{\mathbf{x}^{(1)},\dots ,\mathbf{x}^{(|X|)}\}$ , the associated function evaluations  $\left\{y_{1},\ldots ,y_{|X|}\right\}$  are distributed according to:

$$
\left[ \begin{array}{c} y_{1} \\ \vdots \\ y_{| X |} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{c} m (\mathbf {x}^{(1)}) \\ \vdots \\ m (\mathbf {x}^{(| X |)}) \end{array} \right], \left[ \begin{array}{c c c} k (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) & \dots & k (\mathbf {x}^{(1)}, \mathbf {x}^{(| X |)}) \\ \vdots & \ddots & \vdots \\ k (\mathbf {x}^{(| X |)}, \mathbf {x}^{(1)}) & \dots & k (\mathbf {x}^{(| X |)}, \mathbf {x}^{(| X |)}) \end{array} \right]\right) \tag {18.8}
$$

where  $m(\mathbf{x})$  is a mean function and  $k(\mathbf{x},\mathbf{x}^{\prime})$  is the covariance function, or kernel. The mean function can represent prior knowledge about the function. The kernel controls the smoothness of the functions. Methods for constructing the mean vector and covariance matrix using mean and covariance functions are given in algorithm 18.1.

3 A more extensive introduction to Gaussian processes is provided by C.E. Rasmussen and C.K.I. Williams, Gaussian Processes for Machine Learning. MIT Press, 2006. The GaussianProcesses.jl.jl package provides some implementation. J. Fairbrother, C. Nemeth, M. Rischard, J. Brea, and T. Pinder, "GaussianProcesses.jl: A Nonparametric Bayes Package for the Julia Language," Journal of Statistical Software, vol. 102, no. 1, 2022.

4 The mean function produces the expectation:

$$
m (\mathbf {x}) = \mathbb {E} [ f (\mathbf {x}) ]
$$

and the covariance function produces the covariance:

$$
k \left(\mathbf {x}, \mathbf {x}^{\prime}\right) =
$$

$$
\mathbb {E} \left[ (f (\mathbf {x}) - m (\mathbf {x})) (f (\mathbf {x}^{\prime}) - m (\mathbf {x}^{\prime})) \right]
$$

$$
\begin{array}{l} \mu (X, m) = [ m (x) f o r x i n X ] \\ \Sigma (X, k) = [ k (x, x^{\prime}) f o r x i n X, x^{\prime} i n X ] \\ K (X, X^{\prime}, k) = [ k (x, x^{\prime}) f o r x i n X, x^{\prime} i n X^{\prime} ] \end{array}
$$

A common kernel function is the squared exponential kernel, where

$$
k \left(x, x^{\prime}\right) = \exp \left(- \frac{\left\| \mathbf {x} - \mathbf {x}^{\prime} \right\| ^{2}}{2 \ell^{2}}\right) \tag {18.9}
$$

The parameter  $\ell$  corresponds to what is called the characteristic length-scale, which can be thought of as the distance we have to travel in design space until the objective function value changes significantly. Hence, larger values of  $\ell$  result in smoother functions. Figure 18.2 shows functions sampled from a Gaussian process with a zero-mean function and a squared exponential kernel with different characteristic length-scales.

Algorithm 18.1. The function  $\mu$  for constructing a mean vector given a list of design points and a mean function  $\mathfrak{m}$ , and the function  $\Sigma$  for constructing a covariance matrix given one or two lists of design points and a covariance function  $\mathbf{k}$ .

5 A mathematical definition of characteristic length-scale is provided by C.E. Rasmussen and C.K.I. Williams, Gaussian Processes for Machine Learning. MIT Press, 2006.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2d7ea5a947e3f101afb12901359fa42aaee5bd116917e53f243077aa5b7e612a.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e09df86cd5f692dc79ae2b9e54e59d88b57482bccab7cdbe1de027528d7fc256.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b154fdd00fd9576856eb1089b108d4afc95e9589728c3b609118d4a5521c981c.jpg)  
Figure 18.2. Functions sampled from Gaussian processes with squared exponential kernels.

There are many other kernel functions besides the squared exponential. Several are shown in figure 18.3. Many kernel functions use  $r$ , which is the distance between  $\mathbf{x}$  and  $\mathbf{x}'$ . Usually the Euclidean distance is used. The Matérn kernel uses the gamma function  $\Gamma$ , implemented by gamma from the Special Functions.jl package, and  $K_{\nu}(x)$  is the modified Bessel function of the second kind, implemented by besselk(v,x). The neural network kernel augments each design vector with a 1 for ease of notation:  $\bar{\mathbf{x}} = [1, x_1, x_2, \ldots]$  and  $\bar{\mathbf{x}}' = [1, x_1', x_2', \ldots]$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4ab555e0a1093839e05db92de3ba1668b18fd08a61fe10a8d2a845ed17380eb5.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7e8008cd4215be16caf939e50d1d04c587f30d3b0b090bce4ecbe67bf28b8924.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/73dce0372099d67a2186165df26fe2ba8d6a8a3d8a52f2bf4e34a4bb6888b27b.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f81d83cc9831425cab8cb33ba8fe6049796b555fa945c244ad2a13192b3191f2.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4e1529df77381a7835ae015fd6d3f8376b12d1acc98f85f27102b9db5a47137f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/117ade2577cca7db6f7c80eca5168c50c59f7556a59a4bf0b678e7aff5f98ca4.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8555f559a69cae6da487acbb60557ca7535fa7a079fb013c3418c41c76eacdc3.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cdef894e625e78a1909cbbdff4f6ad19383d7c2390f0ceccabed6b84d3b47b25.jpg)  
Figure 18.3. Functions sampled from Gaussian processes with different kernel functions. Shown functions are for  $\sigma_0^2 = \sigma_d^2 = \ell = 1$ ,  $p = 2$ ,  $\gamma = \nu = \alpha = 0.5$ , and  $\boldsymbol{\Sigma} = \mathbf{I}$ . In addition,  $r = \| \mathbf{x} - \mathbf{x}'\|$ .

This chapter will focus on examples of Gaussian processes with single-dimensional design spaces for ease of plotting. However, Gaussian processes can be defined over multidimensional design spaces, as illustrated in figure 18.4.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1a705f6e29c66e5ddd20171551a991792abdfae01f8f138d5efc987d669e6596.jpg)  
$\ell = 1 / 2$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/662a902ba51c285a6de787255e3c3047a4f59200d41797b500e63b8bc42abad4.jpg)  
$\ell = 1$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8716fc898dafe788340c5cb84f17ce245667233c32b8f3cd82565bafb04a65d5.jpg)  
$\ell = 2$  
Figure 18.4. Functions sampled from a Gaussian process with zero-mean and squared-exponential kernels over a two-dimensional design space.

As we will see in section 18.5, Gaussian processes can also incorporate prior independent noise variance, denoted  $\nu$ . A Gaussian process is thus defined by mean and covariance functions, prior design points and their function evaluations, and a noise variance. The associated type is given in algorithm 18.2.

```julia
mutable struct GaussianProcess  
m # mean  
k # covariance function  
X # design points  
y # objective values  
v # noise variance  
end
```

Algorithm 18.2. A Gaussian process is defined by a mean function  $\mathfrak{m}$ , a covariance function  $\mathbf{k}$ , sampled design vectors  $\mathbf{x}$  and their corresponding values  $\mathbf{y}$ , and a noise variance  $\nu$ .

# 18.3 Prediction

Gaussian processes are able to represent distributions over functions using conditional probabilities. Suppose we already have a set of points  $X$  and the corresponding  $y$ , but we wish to predict the values  $\hat{y}$  at points  $X^*$ . The joint distribution

is

$$
\left[ \begin{array}{l} \hat {\mathbf {y}} \\ \mathbf {y} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{l} \mathbf {m} \left(X^{*}\right) \\ \mathbf {m} (X) \end{array} \right], \left[ \begin{array}{l l} \mathbf {K} \left(X^{*}, X^{*}\right) & \mathbf {K} \left(X^{*}, X\right) \\ \mathbf {K} (X, X^{*}) & \mathbf {K} (X, X) \end{array} \right]\right) \tag {18.10}
$$

In the equation above, we use the functions  $\mathbf{m}$  and  $\mathbf{K}$ , which are defined as follows:

$$
\mathbf {m} (X) = \left[ m \left(\mathbf {x}^{(1)}\right), \dots , m \left(\mathbf {x}^{(| X |)}\right) \right] \tag {18.11}
$$

$$
\mathbf {K} (X, X^{\prime}) = \left[ \begin{array}{c c c} k \left(\mathbf {x}^{(1)}, \mathbf {x}^{\prime (1)}\right) & \dots & k \left(\mathbf {x}^{(1)}, \mathbf {x}^{\prime \left(| X^{\prime} |\right)}\right) \\ \vdots & \ddots & \vdots \\ k \left(\mathbf {x}^{\left(| X |\right)}, \mathbf {x}^{\prime (1)}\right) & \dots & k \left(\mathbf {x}^{\left(| X |\right)}, \mathbf {x}^{\prime \left(| X^{\prime} |\right)}\right) \end{array} \right] \tag {18.12}
$$

The conditional distribution is given by

In the language of Bayesian statistics, the posterior distribution is the distribution of possible unobserved values conditioned on observed values.

$$
\hat {\mathbf {y}} \mid \mathbf {y} \sim \mathcal {N} \left(\underbrace{\mathbf {m} (X^{*}) + \mathbf {K} (X^{*} , X) \mathbf {K} (X , X) ^{- 1} (\mathbf {y} - \mathbf {m} (X))}_{\text{mean}}, \underbrace{\mathbf {K} (X^{*} , X^{*}) - \mathbf {K} (X^{*} , X) \mathbf {K} (X , X) ^{- 1} \mathbf {K} (X , X^{*})}_{\text{covariance}}\right) \tag {18.13}
$$

Note that the covariance does not depend on  $\mathbf{y}$ . This distribution is often referred to as the posterior distribution. A method for computing and sampling from the posterior distribution defined by a Gaussian process is given in algorithm 18.3.

```matlab
function mvrand  $(\mu ,\Sigma ,$  inflation  $= 1e - 6$  N  $=$  MvNormal  $(\mu ,\Sigma +$  inflation\*I) return rand(N)   
end   
Base.rand(GP,X)  $=$  mvrand  $(\mu (X,\mathrm{GP.m}),\Sigma (X,\mathrm{GP.k}))$
```

The predicted mean can be written as a function of  $\mathbf{x}$ :

$$
\begin{array}{l} \hat {\mu} (\mathbf {x}) = m (\mathbf {x}) + \mathbf {K} (\mathbf {x}, X) \mathbf {K} (X, X) ^{- 1} (\mathbf {y} - \mathbf {m} (X)) (18.14) \\ = m (\mathbf {x}) + \boldsymbol {\theta}^{\top} \mathbf {K} (X, \mathbf {x}) (18.15) \\ \end{array}
$$

where  $\theta = \mathbf{K}(X,X)^{-1}(\mathbf{y} - \mathbf{m}(X))$  can be computed once and reused for different values of  $\mathbf{x}$ . Notice the similarity to the surrogate models in the previous chapter. The value of the Gaussian process beyond the surrogate models discussed previously is that it also quantifies our uncertainty in our predictions.

The variance of the predicted mean can also be obtained as a function of  $\mathbf{x}$ :

$$
\hat {v} (\mathbf {x}) = \mathbf {K} (\mathbf {x}, \mathbf {x}) - \mathbf {K} (\mathbf {x}, X) \mathbf {K} (X, X) ^{- 1} \mathbf {K} (X, \mathbf {x}) \tag {18.16}
$$

Algorithm 18.3. The function mvnr and samples from a multivariate Gaussian with an added inflation factor to prevent numerical issues. The method rand samples a Gaussian process GP at the given design points in matrix X.

In some cases, it is more convenient to formulate equations in terms of the standard deviation, which is the square root of the variance:

$$
\hat {\sigma} (\mathbf {x}) = \sqrt{\hat {v} (\mathbf {x})} \tag {18.17}
$$

The standard deviation has the same units as the mean. From the standard deviation, we can compute the  $95\%$  confidence region, which is an interval containing  $95\%$  of the probability mass associated with the distribution over  $y$  given  $\mathbf{x}$ . For a particular  $\mathbf{x}$ , the  $95\%$  confidence region is given by  $\hat{\mu}(\mathbf{x}) \pm 1.96\hat{\sigma}(\mathbf{x})$ . One may want to use a confidence level different from  $95\%$ , but we will use  $95\%$  for the plots in this chapter. Figure 18.5 shows a plot of a confidence region associated with a Gaussian process fit to four function evaluations.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2a34850ff77e7b9fdf2db6d54b00c6f9bfb085adea41376536ba9252e119d9b1.jpg)  
Figure 18.5. A Gaussian process using the squared exponential kernel and its  $95\%$  confidence interval. Uncertainty increases the farther we are from a data point. The expected function value approaches zero as we move far away from the data point.

# 18.4 Gradient Measurements

Gradient observations can be incorporated into Gaussian processes in a manner consistent with the existing Gaussian process machinery.7 The Gaussian process is extended to include both the function value and its gradient:

$$
\left[ \begin{array}{l} \mathbf {y} \\ \nabla \mathbf {y} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{l} \mathbf {m}_{f} \\ \mathbf {m}_{\nabla} \end{array} \right], \left[ \begin{array}{l l} \mathbf {K}_{f f} & \mathbf {K}_{f \nabla} \\ \mathbf {K}_{\nabla f} & \mathbf {K}_{\nabla \nabla} \end{array} \right]\right) \tag {18.18}
$$

7 For an overview, see for example A. O'Hagan, "Some Bayesian Numerical Analysis," Bayesian Statistics, vol. 4, J.M. Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith, eds., pp. 345-363, 1992.

where  $\mathbf{y} \sim \mathcal{N}\big(\mathbf{m}_f, \mathbf{K}_{ff}\big)$  is a traditional Gaussian process,  $\mathbf{m}_{\nabla}$  is a mean function for the gradient,  ${}^8\mathbf{K}_{f\nabla}$  is the covariance matrix between function values and gradients,  $\mathbf{K}_{\nabla f}$  is the covariance matrix between function gradients and values, and  $\mathbf{K}_{\nabla \nabla}$  is the covariance matrix between function gradients.

These covariance matrices are constructed using covariance functions. The linearity of Gaussians causes these covariance functions to be related:

$$
k_{f f} \left(\mathbf {x}, \mathbf {x}^{\prime}\right) = k \left(\mathbf {x}, \mathbf {x}^{\prime}\right) \tag {18.19}
$$

$$
k_{\nabla f} \left(\mathbf {x}, \mathbf {x}^{\prime}\right) = \nabla_{\mathbf {x}} k \left(\mathbf {x}, \mathbf {x}^{\prime}\right) \tag {18.20}
$$

$$
k_{f \nabla} (\mathbf {x}, \mathbf {x}^{\prime}) = \nabla_{\mathbf {x}^{\prime}} k (\mathbf {x}, \mathbf {x}^{\prime}) \tag {18.21}
$$

$$
k_{\nabla \nabla} \left(\mathbf {x}, \mathbf {x}^{\prime}\right) = \nabla_{\mathbf {x}} \nabla_{\mathbf {x}^{\prime}} k \left(\mathbf {x}, \mathbf {x}^{\prime}\right) \tag {18.22}
$$

Example 18.2 uses these relations to derive the higher-order covariance functions for a particular kernel.

Consider the squared exponential covariance function

$$
k_{f f} (\mathbf {x}, \mathbf {x}^{\prime}) = \exp \left(- \frac{1}{2} \| \mathbf {x} - \mathbf {x}^{\prime} \| ^{2}\right)
$$

We can use equations (18.19) to (18.22) to obtain the other covariance functions necessary for using Gaussian processes with gradient information:

$$
k_{\nabla f} \left(\mathbf {x}, \mathbf {x}^{\prime}\right) _{i} = - \left(\mathbf {x}_{i} - \mathbf {x}_{i}^{\prime}\right) \exp \left(- \frac{1}{2} \left\| \mathbf {x} - \mathbf {x}^{\prime} \right\| ^{2}\right)
$$

$$
k_{\nabla \nabla} (\mathbf {x}, \mathbf {x}^{\prime}) _{i j} = - \Big ((i = j) - (\mathbf {x}_{i} - \mathbf {x}_{i}^{\prime}) (\mathbf {x}_{j} - \mathbf {x}_{j}^{\prime}) \Big) \exp \left(- \frac{1}{2} \| \mathbf {x} - \mathbf {x}^{\prime} \| ^{2}\right)
$$

As a reminder, Boolean expressions, such as  $(i = j)$ , return 1 if true and 0 if false.

8 Like the mean of the function value,  $\mathbf{m}_{\nabla}$  is often zero.

Example 18.2. Deriving covariance functions for a Gaussian process with gradient observations.

Prediction can be accomplished in the same manner as with a traditional Gaussian process. We first construct the joint distribution

$$
\left[ \begin{array}{l} \hat {\mathbf {y}} \\ \mathbf {y} \\ \nabla \mathbf {y} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{l} \mathbf {m}_{f} \left(X^{*}\right) \\ \mathbf {m}_{f} (X) \\ \mathbf {m}_{\nabla} (X) \end{array} \right], \left[ \begin{array}{l l l} \mathbf {K}_{f f} \left(X^{*}, X^{*}\right) & \mathbf {K}_{f f} \left(X^{*}, X\right) & \mathbf {K}_{f \nabla} \left(X^{*}, X\right) \\ \mathbf {K}_{f f} (X, X^{*}) & \mathbf {K}_{f f} (X, X) & \mathbf {K}_{f \nabla} (X, X) \\ \mathbf {K}_{\nabla f} (X, X^{*}) & \mathbf {K}_{\nabla f} (X, X) & \mathbf {K}_{\nabla \nabla} (X, X) \end{array} \right]\right) \tag {18.23}
$$

For a Gaussian process over  $n$ -dimensional design vectors given  $m$  pairs of function and gradient evaluations and  $\ell$  query points, the covariance blocks have the following dimensions:

$$
\ell \times \ell \qquad \ell \times m \qquad \ell \times n m
$$

$$
m \times \ell \quad m \times m \quad m \times n m \tag {18.24}
$$

$$
n m \times \ell n m \times m n m \times n m
$$

Example 18.3 constructs such a covariance matrix.

Suppose we have evaluated a function and its gradient at two locations,  $\mathbf{x}^{(1)}$  and  $\mathbf{x}^{(2)}$ , and we wish to predict the function value at  $\hat{\mathbf{x}}$ . We can infer the joint distribution over  $\hat{\mathbf{y}}, \mathbf{y}$ , and  $\nabla \mathbf{y}$  using a Gaussian process. The covariance matrix is:

$$
\left[ \begin{array}{l l l l l l l} k_{f f} (\hat {\mathbf {x}}, \hat {\mathbf {x}}) & k_{f f} (\hat {\mathbf {x}}, \mathbf {x}^{(1)}) & k_{f f} (\hat {\mathbf {x}}, \mathbf {x}^{(2)}) & k_{f \nabla} (\hat {\mathbf {x}}, \mathbf {x}^{(1)}) _{1} & k_{f \nabla} (\hat {\mathbf {x}}, \mathbf {x}^{(1)}) _{2} & k_{f \nabla} (\hat {\mathbf {x}}, \mathbf {x}^{(2)}) _{1} & k_{f \nabla} (\hat {\mathbf {x}}, \mathbf {x}^{(2)}) _{2} \\ k_{f f} (\mathbf {x}^{(1)}, \hat {\mathbf {x}}) & k_{f f} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) & k_{f f} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) & k_{f \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{1} & k_{f \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{2} & k_{f \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{1} & k_{f \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{2} \\ k_{f f} (\mathbf {x}^{(2)}, \hat {\mathbf {x}}) & k_{f f} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) & k_{f f} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) & k_{f \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{1} & k_{f \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{2} & k_{f \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{1} & k_{f \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{2} \\ k_{\nabla f} (\mathbf {x}^{(1)}, \hat {\mathbf {x}}) _{1} & k_{\nabla f} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{1} & k_{\nabla f} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{1} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{11} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{12} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{11} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{12} \\ k_{\nabla f} (\mathbf {x}^{(1)}, \hat {\mathbf {x}}) _{2} & k_{\nabla f} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{2} & k_{\nabla f} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{2} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{21} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(1)}) _{22} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{21} & k_{\nabla \nabla} (\mathbf {x}^{(1)}, \mathbf {x}^{(2)}) _{22} \\ k_{\nabla f} (\mathbf {x}^{(2)}, \hat {\mathbf {x}}) _{1} & k_{\nabla f} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{1} & k_{\nabla f} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{1} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{11} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{12} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{11} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{12} \\ k_{\nabla f} (\mathbf {x}^{(2)}, \hat {\mathbf {x}}) _{2} & k_{\nabla f} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{2} & k_{\nabla f} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{2} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{21} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(1)}) _{22} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{21} & k_{\nabla \nabla} (\mathbf {x}^{(2)}, \mathbf {x}^{(2)}) _{22}. \end{array} \right]
$$

Example 18.3. Constructing the covariance matrix for a Gaussian process with gradient observations.

The conditional distribution follows the same Gaussian relations as in equation (18.13):

$$
\hat {\mathbf {y}} \mid \mathbf {y}, \nabla \mathbf {y} \sim \mathcal {N} (\boldsymbol {\mu}_{\nabla}, \boldsymbol {\Sigma}_{\nabla}) \tag {18.25}
$$

where:

$$
\boldsymbol {\mu}_{\nabla} = \mathbf {m}_{f} \left(X^{*}\right) + \left[ \begin{array}{l} \mathbf {K}_{f f} (X, X^{*}) \\ \mathbf {K}_{\nabla f} (X, X^{*}) \end{array} \right] ^{\top} \left[ \begin{array}{l l} \mathbf {K}_{f f} (X, X) & \mathbf {K}_{f \nabla} (X, X) \\ \mathbf {K}_{\nabla f} (X, X) & \mathbf {K}_{\nabla \nabla} (X, X) \end{array} \right] ^{- 1} \left[ \begin{array}{l} \mathbf {y} - \mathbf {m}_{f} (X) \\ \nabla \mathbf {y} - \mathbf {m}_{\nabla} (X) \end{array} \right] \tag {18.26}
$$

$$
\boldsymbol {\Sigma}_{\nabla} = \mathbf {K}_{f f} (X^{*}, X^{*}) - \left[ \begin{array}{l} \mathbf {K}_{f f} (X, X^{*}) \\ \mathbf {K}_{\nabla f} (X, X^{*}) \end{array} \right] ^{\top} \left[ \begin{array}{l l} \mathbf {K}_{f f} (X, X) & \mathbf {K}_{f \nabla} (X, X) \\ \mathbf {K}_{\nabla f} (X, X) & \mathbf {K}_{\nabla \nabla} (X, X) \end{array} \right] ^{- 1} \left[ \begin{array}{l} \mathbf {K}_{f f} (X, X^{*}) \\ \mathbf {K}_{\nabla f} (X, X^{*}) \end{array} \right] \tag {18.27}
$$

The regions obtained when including gradient observations are compared to those without gradient observations in figure 18.6.

Figure 18.6. Gaussian processes with and without gradient information using squared exponential kernels. Incorporating gradient information can significantly reduce the confidence intervals.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/10521e70970f9ecf49fdb730b3dcb818bbe2389ac61d7c0236f0ef98c3b9dca8.jpg)  
confidence interval without gradient  
confidence interval with gradient  
predicted mean without gradient  
predicted mean with gradient  
fit points  
true function

# 18.5 Noisy Measurements

So far we have assumed that the objective function  $f$  is deterministic. In practice, however, evaluations of  $f$  may include measurement noise, experimental error, or numerical roundoff.

We can model noisy evaluations as  $y = f(\mathbf{x}) + z$ , where  $f$  is deterministic but  $z$  is zero-mean Gaussian noise,  $z \sim \mathcal{N}(0,\nu)$ . The variance of the noise  $\nu$  can be adjusted to control the uncertainty.

The new joint distribution is:

$$
\left[ \begin{array}{l} \hat {\mathbf {y}} \\ \mathbf {y} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{l} \mathbf {m} \left(X^{*}\right) \\ \mathbf {m} (X) \end{array} \right], \left[ \begin{array}{l l} \mathbf {K} \left(X^{*}, X^{*}\right) & \mathbf {K} \left(X^{*}, X\right) \\ \mathbf {K} (X, X^{*}) & \mathbf {K} (X, X) + \nu \mathbf {I} \end{array} \right]\right) \tag {18.28}
$$

with conditional distribution:

$$
\hat {\mathbf {y}} \mid \mathbf {y}, v \sim \mathcal {N} \left(\boldsymbol {\mu}^{*}, \boldsymbol {\Sigma}^{*}\right) \tag {18.29}
$$

$$
\boldsymbol {\mu}^{*} = \mathbf {m} \left(X^{*}\right) + \mathbf {K} \left(X^{*}, X\right) \left(\mathbf {K} (X, X) + \nu \mathbf {I}\right) ^{- 1} (\mathbf {y} - \mathbf {m} (X)) \tag {18.30}
$$

$$
\boldsymbol {\Sigma}^{*} = \mathbf {K} \left(X^{*}, X^{*}\right) - \mathbf {K} \left(X^{*}, X\right) \left(\mathbf {K} (X, X) + v \mathbf {I}\right) ^{- 1} \mathbf {K} \left(X, X^{*}\right) \tag {18.31}
$$

As the equations above show, accounting for Gaussian noise is straightforward and the posterior distribution can be computed analytically. Figure 18.7 shows a noisy Gaussian process. Algorithm 18.4 implements prediction for Gaussian processes with noisy measurements.

9 The techniques covered in section 17.5 can be used to tune the variance of the noise.

Figure 18.7. A noisy Gaussian process using a squared exponential kernel.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9cac255eec8205f1854eb0ec5fd03be7cef6b090a17b6a3ede5437b77f399357.jpg)  
confidence region  
true objective function  
predicted function mean  
fit points

```julia
function predict(GP, X_pred)  
m, k, v = GP.m, GP.k, GP.v  
tmp = K(X_pred, GP.X, k) / (K(GP.X, GP.X, k) + v*I)  
 $\mu_{P} = \mu(X\_pred, m) + tmp*(GP.y - \mu(GP.X, m))$   
S = K(X_pred, X_pred, k) - tmp*K(GP.X, X_pred, k)  
v_p = diag(S).+ eps() # eps prevents numerical issues  
return ( $\mu_{P}, v_{P}$ )  
end
```

Algorithm 18.4. A method for obtaining the predicted means and standard deviations in  $f$  under a Gaussian process. The method takes a Gaussian process GP and a list of points X_pred at which to evaluate the prediction. It returns the mean and variance at each evaluation point.

# 18.6 Fitting Gaussian Processes

The choice of kernel and parameters has a large effect on the form of the Gaussian process between evaluated design points. Kernels and their parameters can be chosen using cross validation introduced in the previous chapter. Instead of minimizing the squared error on the test data, we maximize the likelihood of the data. $^{10}$  That is, we seek the parameters  $\theta$  that maximize the probability of the function values,  $p(\mathbf{y} \mid X, \theta)$ . The likelihood of the data is the probability that the observed points were drawn from the model. Equivalently, we can maximize the log likelihood, which is generally preferable because multiplying small probabilities in the likelihood calculation can produce extremely small values. Given a dataset  $\mathcal{D}$  with  $n$  entries, the log likelihood is given by

10 Alternatively, we could maximize the pseudolikelihood as discussed by C.E. Rasmussen and C.K.I. Williams, Gaussian Processes for Machine Learning. MIT Press, 2006.

$$
\log p (\mathbf {y} \mid X, \nu , \theta) = - \frac{n}{2} \log 2 \pi - \frac{1}{2} \log | \mathbf {K}_{\theta} (X, X) + \nu \mathbf {I} | - \frac{1}{2} (\mathbf {y} - \mathbf {m}_{\theta} (X)) ^{\top} \left(\mathbf {K}_{\theta} (X, X) + \nu \mathbf {I}\right) ^{- 1} (\mathbf {y} - \mathbf {m}_{\theta} (X)) \tag {18.32}
$$

where the mean and covariance functions are parameterized by  $\theta$ .

Let us assume a zero mean such that  $\mathbf{m}_{\theta}(X) = \mathbf{0}$  and  $\theta$  refers only to the parameters for the Gaussian process covariance function. We can arrive at a maximum likelihood estimate by gradient ascent. The gradient is then given by

$$
\frac{\partial}{\partial \boldsymbol {\theta}_{j}} \log p (\mathbf {y} \mid X, \boldsymbol {\theta}) = \frac{1}{2} \mathbf {y}^{\top} \mathbf {K}^{- 1} \frac{\partial \mathbf {K}}{\partial \boldsymbol {\theta}_{j}} \mathbf {K}^{- 1} \mathbf {y} - \frac{1}{2} \operatorname{tr} \left(\boldsymbol {\Sigma}_{\boldsymbol {\theta}}^{- 1} \frac{\partial \mathbf {K}}{\partial \boldsymbol {\theta}_{j}}\right) \tag {18.33}
$$

where  $\pmb{\Sigma}_{\theta} = \mathbf{K}_{\theta}(X,X) + \nu \mathbf{I}$ . Above, we use the matrix derivative relations

$$
\frac{\partial \mathbf {K}^{- 1}}{\partial \theta_{j}} = - \mathbf {K}^{- 1} \frac{\partial \mathbf {K}}{\partial \theta_{j}} \mathbf {K}^{- 1} \tag {18.34}
$$

$$
\frac{\partial \log | \mathbf {K} |}{\partial \boldsymbol {\theta}_{j}} = \operatorname{tr} \left(\mathbf {K}^{- 1} \frac{\partial \mathbf {K}}{\partial \boldsymbol {\theta}_{j}}\right) \tag {18.35}
$$

where  $\operatorname{tr}(\mathbf{A})$  denotes the trace of a matrix  $\mathbf{A}$ , defined to be the sum of the elements on the main diagonal.

# 18.7 Summary

- Gaussian processes are probability distributions over functions.

- The choice of kernel affects the smoothness of the functions sampled from a Gaussian process.  
- The multivariate normal distribution has analytic conditional and marginal distributions.  
- We can compute the mean and standard deviation of our prediction of an objective function at a particular design point given a set of past evaluations.  
- We can incorporate gradient observations to improve our predictions of the objective value and its gradient.  
- We can incorporate measurement noise into a Gaussian process.  
- We can fit the parameters of a Gaussian process using maximum likelihood.

# 18.8 Exercises

Exercise 18.1. Gaussian processes will grow in complexity during the optimization process as more samples accumulate. How can this be an advantage over models based on regression?

Solution: Gaussian processes are nonparametric, whereas linear regression models are parametric. This means that the number of degrees of freedom of the model grows with the amount of data, allowing the Gaussian process to maintain a balance between bias and variance during the optimization process.

Exercise 18.2. How does the computational complexity of prediction with a Gaussian process increase with the number of data points  $m$ ?

Solution: Obtaining the conditional distribution of a Gaussian process requires solving equation (18.13). The most expensive operation is inverting the  $m \times m$  matrix  $\mathbf{K}(X, X)$  which is  $O(m^3)$ .

Exercise 18.3. Consider the function  $f(x) = \sin (x) / (x^2 + 1)$  over  $[-5,5]$ . Plot the  $95\%$  confidence bounds for a Gaussian process with derivative information fitted to the evaluations at  $\{-5, -2.5, 0, 2.5, 5\}$ . What is the maximum standard deviation in the predicted distribution within  $[-5,5]$ ? How many function evaluations, evenly spaced over the domain, are needed such that a Gaussian process without derivative information achieves the same maximum predictive standard deviation?

Assume zero-mean functions and noise-free observations, and use the covariance functions:

$$
k_{f f} (x, x^{\prime}) = \exp \left(- \frac{1}{2} \| x - x^{\prime} \| _{2}^{2}\right)
$$

$$
k_{\nabla f} (x, x^{\prime}) = \left(x^{\prime} - x\right) \exp \left(- \frac{1}{2} \| x - x^{\prime} \| _{2}^{2}\right)
$$

$$
k_{\nabla \nabla} (x, x^{\prime}) = ((x - x^{\prime}) ^{2} - 1) \exp (- \frac{1}{2} \| x - x^{\prime} \| _{2}^{2})
$$

Solution: The derivative of  $f$  is

$$
\frac{(x^{2} + 1) \cos (x) - 2 x \sin (x)}{(x^{2} + 1) ^{2}}
$$

Below we plot the predictive distribution for Gaussian processes with and without derivative information. The maximum standard deviation in the predicted distribution over  $[-5,5]$  for the Gaussian process with derivative information is approximately 0.377 at  $x \approx \pm 3.8$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a1a87814fa41d68fa9a5a2ad3dda1ccff9e0445a3c082c2c912336ee29566423.jpg)

confidence interval without derivative  
confidence interval with derivative  
predicted mean without derivative  
predicted mean with derivative  
fitted points  
true function

Incorporating derivative information significantly decreases the confidence interval because more information is available to inform the prediction. Below we plot the maximum standard deviation in the predicted distribution over  $[-5, 5]$  for Gaussian processes without derivative information with a varying number of evenly spaced evaluations. At least eight points are needed in order to outperform the Gaussian process with derivative information.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/46b6f904627ab84c817044fdd9e1a02bbee1b5e2d41b3f10140eec6234c61b92.jpg)

Exercise 18.4. Derive the relation  $k_{f\nabla}(\mathbf{x},\mathbf{x}^{\prime})_i = \mathrm{cov}\Big(f(\mathbf{x}),\frac{\partial}{\partial x_i^{\prime}} f(\mathbf{x}^{\prime})\Big) = \frac{\partial}{\partial x_i^{\prime}} k_{ff}(\mathbf{x},\mathbf{x}^{\prime}).$

Solution: This can be derived according to:

$$
\begin{array}{l} k_{f \nabla} (\mathbf {x}, \mathbf {x}^{\prime}) _{i} = \operatorname{cov} \left(f (\mathbf {x}), \frac{\partial}{\partial x_{i}^{\prime}} f (\mathbf {x}^{\prime})\right) \\ = \mathbb {E} \left[ \left(f (\mathbf {x}) - \mathbb {E} [ f (\mathbf {x}) ]\right) \left(\frac{\partial}{\partial x_{i}^{\prime}} f (\mathbf {x}^{\prime}) - \mathbb {E} \left[ \frac{\partial}{\partial x_{i}^{\prime}} f (\mathbf {x}^{\prime}) \right]\right) \right] \\ = \mathbb {E} \left[ \left(f (\mathbf {x}) - \mathbb {E} [ f (\mathbf {x}) ]\right) \left(\frac{\partial}{\partial x_{i}^{\prime}} f (\mathbf {x}^{\prime}) - \frac{\partial}{\partial x_{i}^{\prime}} \mathbb {E} [ f (\mathbf {x}^{\prime}) ]\right) \right] \\ = \mathbb {E} \left[ \left(f (\mathbf {x}) - \mathbb {E} [ f (\mathbf {x}) ]\right) \frac{\partial}{\partial x_{i}^{\prime}} \left(f (\mathbf {x}^{\prime}) - \mathbb {E} [ f (\mathbf {x}^{\prime}) ]\right) \right] \\ = \frac{\partial}{\partial x_{i}^{\prime}} \mathbb {E} \left[ (f (\mathbf {x}) - \mathbb {E} [ f (\mathbf {x}) ]) \left(f (\mathbf {x}^{\prime}) - \mathbb {E} [ f (\mathbf {x}^{\prime}) ]\right) \right] \\ = \frac{\partial}{\partial x_{i}^{\prime}} \operatorname{cov} \left(f (\mathbf {x}), f \left(\mathbf {x}^{\prime}\right)\right) \\ = \frac{\partial}{\partial x_{i}^{\prime}} k_{f f} (\mathbf {x}, \mathbf {x}^{\prime}) \\ \end{array}
$$

where we have used  $\mathbb{E}\left[\frac{\partial}{\partial x} f\right] = \frac{\partial}{\partial x}\mathbb{E}[f]$ . We can convince ourselves that this is true:

$$
\begin{array}{l} \mathbb {E} \left[ \frac{\partial}{\partial x} f \right] = \mathbb {E} \left[ \lim_{h \rightarrow 0} \frac{f (x + h) - f (x)}{h} \right] \\ = \lim_{h \rightarrow 0} \mathbb {E} \left[ \frac{f (x + h) - f (x)}{h} \right] \\ = \lim_{h \rightarrow 0} \frac{1}{h} (\mathbb {E} [ f (x + h) ] - \mathbb {E} [ f (x) ]) \\ = \frac{\partial}{\partial x} \mathbb {E} [ f (x) ] \\ \end{array}
$$

provided that the objective function is differentiable.

Exercise 18.5. Suppose we have a multivariate Gaussian distribution over two variables  $a$  and  $b$ . Show that the variance of the conditional distribution over  $a$  given  $b$  is no greater than the variance of the marginal distribution over  $a$ . Does this make intuitive sense?

Solution: Let us write the joint Gaussian distribution as:

$$
\left[ \begin{array}{c} a \\ b \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{c} \mu_{a} \\ \mu_{b} \end{array} \right], \left[ \begin{array}{c c} \nu_{a} & \nu_{c c} \\ \nu_{c c} & \nu_{b} \end{array} \right]\right)
$$

The marginal distribution over  $a$  is  $\mathcal{N}(\mu_a, \nu_a)$ , which has variance  $\nu_a$ . The conditional distribution for  $a$  has variance  $\nu_a - \nu_c^2 / \nu_b$ . We know  $\nu_b$  must be positive in order for the original covariance matrix to be positive definite. Thus,  $\nu_c^2 / \nu_b$  is positive and  $\nu_a - \nu_c^2 / \nu_b \leq \nu_a$ .

It is intuitive that the conditional distribution has no greater variance than the marginal distribution because the conditional distribution incorporates more information about  $a$ . If  $a$  and  $b$  are correlated, then knowing the value of  $b$  informs us about the value of  $a$  and decreases our uncertainty.

Exercise 18.6. Suppose we observe many outliers while sampling, that is, we observe samples that do not fall within the confidence interval given by the Gaussian process. This means the probabilistic model we chose is not appropriate. What can we do?

Solution: We can tune the parameters to our kernel function or switch kernel functions using generalization error estimation or by maximizing the likelihood of the observed data.

Exercise 18.7. Equation (18.10) provides the joint distribution over observed evaluations  $\mathbf{y}$  at points  $X$  and the evaluations  $\hat{\mathbf{y}}$  at new points  $X^{*}$ . Suppose we wish to produce a multifidelity surrogate model (section 17.6) over both low-fidelity evaluations  $\mathbf{y}_{\ell}$  at points  $X_{\ell}$  and high-fidelity evaluations  $\mathbf{y}_h$  at points  $X_{h}$  using a single Gaussian process. We assume the low- and high-fidelity points are in a common design space. Modify equation (18.10) to provide a joint distribution over  $\hat{\mathbf{y}}_h, \mathbf{y}_{\ell}$ , and  $\mathbf{y}_h$ , and then write the predicted mean as a function of  $\mathbf{x}$ . Use the following three kernel functions to capture the expected covariance between designs:

-  $k_{\ell \ell}(\mathbf{x},\mathbf{x}^{\prime})$  for the covariance between two low-fidelity measurements,  
-  $k_{hh}(\mathbf{x},\mathbf{x}^{\prime})$  for the covariance between two high-fidelity measurements, and  
-  $k_{\ell h}(\mathbf{x}, \mathbf{x}')$  for the covariance between a low-fidelity and a high-fidelity measurement.

Solution: We begin by writing a joint distribution over our low-fidelity and high-fidelity data:

$$
\left[ \begin{array}{c} \mathbf {y}_{\ell} \\ \mathbf {y}_{h} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{c} \mathbf {m}_{\ell} (X_{\ell}) \\ \mathbf {m}_{h} (X_{h}) \end{array} \right], \left[ \begin{array}{c c} \mathbf {K}_{\ell \ell} (X_{\ell}, X_{\ell}) & \mathbf {K}_{\ell h} (X_{\ell}, X_{h}) \\ \mathbf {K}_{h \ell} (X_{h}, X_{\ell}) & \mathbf {K}_{h h} (X_{h}, X_{h}) \end{array} \right]\right)
$$

The mean functions  $m_{\ell}(\mathbf{x})$  and  $m_h(\mathbf{x})$  represent our mean predictions for the low- and high-fidelity values in the absence of nearby supporting data, and can typically be obtained using priors or by fitting (non-probabilistic) surrogate models.

We can then expand this joint distribution to include high-fidelity values we wish to predict  $\hat{y}_h$  at points  $X^*$ :

$$
\left[ \begin{array}{c} \hat {\mathbf {y}}_{h} \\ \mathbf {y}_{\ell} \\ \mathbf {y}_{h} \end{array} \right] \sim \mathcal {N} \left(\left[ \begin{array}{c} \mathbf {m}_{h} (X^{*}) \\ \mathbf {m}_{\ell} (X_{\ell}) \\ \mathbf {m}_{h} (X_{h}) \end{array} \right], \left[ \begin{array}{c c c} \mathbf {K}_{h h} (X^{*}, X^{*}) & \mathbf {K}_{\ell h} (X^{*}, X_{\ell}) & \mathbf {K}_{h h} (X^{*}, X_{h}) \\ \mathbf {K}_{\ell h} (X_{\ell}, X^{*}) & \mathbf {K}_{\ell \ell} (X_{\ell}, X_{\ell}) & \mathbf {K}_{\ell h} (X_{\ell}, X_{h}) \\ \mathbf {K}_{h h} (X_{h}, X^{*}) & \mathbf {K}_{\ell h} (X_{h}, X_{\ell}) & \mathbf {K}_{h h} (X_{h}, X_{h}) \end{array} \right]\right)
$$

This joint distribution is analogous to equation (18.10), and very closely reflects equation (18.23). Notice that because we are inferring high-fidelity data, we treat the inferred measurements as high-fidelity with respect to the mean and kernel functions.

The conditional distribution has a mean:

$$
\mathbf {m}_{h} (X^{*}) + \left[ \begin{array}{c c} \mathbf {K}_{\ell h} (X^{*}, X_{\ell}) & \mathbf {K}_{h h} (X^{*}, X_{h}) \end{array} \right] \left[ \begin{array}{c c} \mathbf {K}_{\ell \ell} (X_{\ell}, X_{\ell}) & \mathbf {K}_{\ell h} (X_{\ell}, X_{h}) \\ \mathbf {K}_{\ell h} (X_{h}, X_{\ell}) & \mathbf {K}_{h h} (X_{h}, X_{h}) \end{array} \right] ^{- 1} \left(\left[ \begin{array}{c} \mathbf {y}_{\ell} \\ \mathbf {y}_{h} \end{array} \right] - \left[ \begin{array}{c} \mathbf {m}_{\ell} (X_{\ell}) \\ \mathbf {m}_{h} (X_{h}) \end{array} \right]\right)
$$

The predicted high-fidelity mean as a function of  $\mathbf{x}$  is then given by:

$$
\hat {\mu}_{h} (\mathbf {x}) = \mathbf {m}_{h} (\mathbf {x}) + \left[ \begin{array}{l l} \mathbf {K}_{\ell h} (\mathbf {x}, X_{\ell}) & \mathbf {K}_{h h} (\mathbf {x}, X_{h}) \end{array} \right] \left[ \begin{array}{l l} \mathbf {K}_{\ell \ell} (X_{\ell}, X_{\ell}) & \mathbf {K}_{\ell h} (X_{\ell}, X_{h}) \\ \mathbf {K}_{h \ell} (X_{h}, X_{\ell}) & \mathbf {K}_{h h} (X_{h}, X_{h}) \end{array} \right] ^{- 1} \left[ \begin{array}{l} \mathbf {y}_{\ell} - \mathbf {m}_{\ell} (X_{\ell}) \\ \mathbf {y}_{h} - \mathbf {m}_{h} (X_{h}) \end{array} \right]
$$

Exercise 18.8. Fit a Gaussian process to the following data

$$
x s = \left[ - 0. 8, \quad - 0. 6, \quad - 0. 4, \quad - 0. 2, \quad 0. 0, \quad 0. 2, \quad 0. 4, \quad 0. 6, \quad 0. 8, \quad 1. 0 \right]
$$

$$
y s = \left[ - 0. 46 3, - 0. 14 8, 0. 27 7, 0. 51 2, 0. 42 5, 0. 05 2, - 0. 40 4, - 0. 64 4, - 0. 99 6, - 0. 36 0 \right]
$$

using a mean function based on a 5th-order polynomial fit and a squared exponential kernel with  $\ell = 0.25$ . Plot the mean prediction of your model and its  $95\%$  confidence interval.

Solution: Linear regression with 5th-order polynomial bases on the given data produces the weights:

$$
\theta = [ 0. 41 9, - 0. 99 1, - 3. 30 8, 1. 32 4, 2. 47 1, - 0. 29 8 ]
$$

We then construct our Gaussian process using the mean function

$$
m (x) = \theta_{1} + \theta_{2} x + \theta_{3} x^{2} + \theta_{4} x^{3} + \theta_{5} x^{4} + \theta_{6} x^{5}
$$

and a squared exponential kernel with  $\ell = 0.25$ :

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5695cd298cecf601f97644ca282665477a2f4b4802997fc6857c5698937226d8.jpg)

Exercise 18.9. Suppose the Gaussian Process in exercise 18.8 was fit on data from a low-fidelity dataset, but we also had additional data from a high-fidelity dataset:

$$
x s = [ - 0. 7, \quad 0. 5, \quad 0. 9 ]
$$

$$
y s = [ - 0. 22 0, - 0. 73 7, - 1. 44 9 ]
$$

Extend that Gaussian process into a multifidelity Gaussian process by incorporating separate estimates for the high-fidelity data. Again use linear regression to determine a mean function  $m_h(x)$ , but use  $m_{\ell}(x)$  as one of the bases in addition to a 2nd-order polynomial:

$$
m_{h} (x) = \theta_{1} m_{\ell} (x) + \theta_{2} + \theta_{3} x + \theta_{4} x^{2}
$$

Under this scheme, the high-fidelity model is explicitly correlated with the low-fidelity model through the learned weight  $\theta_{1}$ . We have  $k_{\ell h}(x,x^{\prime}) = \theta_{1}k_{\ell \ell}(x,x^{\prime})$ . Similarly, the covariance of the high-fidelity function is at least  $k_{hh}(x,x^{\prime}) = \theta_{1}^{2}k_{\ell \ell}(x,x^{\prime})$ .

Use your mean functions and exercise 18.7 to construct a multifidelity Gaussian process using squared exponential kernels with  $\ell = 0.25$ . Choose a suitable scaling for the high-fidelity kernel function.

Solution: Linear regression to learn the high-fidelity basis weights yields:

$$
\theta h i = [ 0. 25 6, - 0. 11 2, - 0. 53 6, - 0. 82 0 ]
$$

Below we show a multifidelity Gaussian process using these mean functions. The low-fidelity kernel  $k_{\ell \ell}$  is a squared exponential kernel with  $\ell = 0.25$ , and the cross-term kernels are thus:

$$
k_{\ell h} (\mathbf {x}, \mathbf {x}^{\prime}) = k_{h \ell} (\mathbf {x}, \mathbf {x}^{\prime}) = 0. 25 6 k_{\ell \ell} (\mathbf {x}, \mathbf {x}^{\prime})
$$

It is common practice to represent the high-fidelity estimates in multifidelity surrogate optimization as adjustments to a low-fidelity model. In this context, the additional bases are called a corrector function.

We used a high-fidelity kernel of  $k_{hh}(\mathbf{x},\mathbf{x}^{\prime}) = 0.1k_{\ell \ell}(\mathbf{x},\mathbf{x}^{\prime})$ . Varying the scaling term will change the confidence intervals.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/067e0ae27ecf5ff721802b2522e7e6fa2482fab254e03528df467cfb0272a474.jpg)

We can see how the high-fidelity model uses information from the low-fidelity model to bend upwards in the middle. It does not directly lie on the low-fidelity points, and even learns to lie offset from the low-fidelity model on the right side of the plot.

Exercise 18.10. Consider model selection for the function evaluation pairs  $(x,y)$ :

$$
\{(1, 0), (2, - 1), (3, - 2), (4, 1), (5, 0) \}
$$

Use leave-one-out cross-validation to select the kernel that maximizes the likelihood of predicting the withheld pair given a Gaussian process over the other pairs in the fold. Assume zero mean with no noise. Select from the kernels:

$$
\exp (- \| x - x^{\prime} \|) \qquad \exp (- \| x - x^{\prime} \| ^{2}) \qquad (1 + \| x - x^{\prime} \|) ^{- 1} \qquad (1 + \| x - x^{\prime} \| ^{2}) ^{- 1} \qquad (1 + \| x - x^{\prime} \|) ^{- 2}
$$

Solution: Maximizing the product of the likelihoods is equivalent to maximizing the sum of the log likelihoods. Here are the log likelihoods of the third point given the other points using each kernel:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0ff741e7356ee526da8250c0fd81689a5cd41a11c1f43c09490b114ca33ef979.jpg)

Computing these values over all five folds yields the total log likelihoods:

$$
\exp (- \| x - x^{\prime} \|) \rightarrow - 8. 68 8
$$

$$
\exp \left(- \| x - x^{\prime} \| ^{2}\right)\rightarrow - 9. 01 0
$$

$$
\left(1 + \left\| x - x^{\prime} \right\|\right) ^{- 1} \rightarrow - 9. 57 9
$$

$$
(1 + \| x - x^{\prime} \| ^{2}) ^{- 1} \rightarrow - 10. 19 5
$$

$$
(1 + \| x - x^{\prime} \|) ^{- 2} \rightarrow - 8. 08 8
$$

It follows that the kernel that maximizes the leave-one-out cross-validated likelihood is the rational quadratic kernel  $(1 + \| x - x' \|)^{-2}$ .

# 19 Surrogate Optimization

The previous chapter explained how to use a probabilistic surrogate model, in particular a Gaussian process, to infer probability distributions over the true objective function. These distributions can be used to guide an optimization process toward better design points.<sup>1</sup> This chapter outlines several common techniques for choosing which design point to evaluate next. The techniques we discuss here greedily optimize various metrics.<sup>2</sup> We will also discuss how surrogate models can be used to optimize an objective measure in a safe manner.

# 19.1 Prediction-Based Exploration

In prediction-based exploration, we select the minimizer of the surrogate function. An example of this approach is the quadratic fit search that we discussed earlier in section 3.5. With quadratic fit search, we use a quadratic surrogate model to fit the last three bracketing points and then select the point at the minimum of the quadratic function.

If we use a Gaussian process surrogate model, prediction-based optimization has us select the minimizer of the mean function

$$
\mathbf {x}^{(m + 1)} = \underset{\mathbf {x} \in \mathcal {X}} {\arg \min } \hat {\mu} (\mathbf {x}) \tag {19.1}
$$

where  $\hat{\mu}(\mathbf{x})$  is the predicted mean of a Gaussian process at a design point  $\mathbf{x}$  based on the previous  $m$  design points. The process is illustrated in figure 19.1.

Prediction-based optimization does not take uncertainty into account, and new samples can be generated very close to existing samples. Sampling at locations where we are already confident in the objective value is a waste of function evaluations.

<sup>1</sup>A. Forrester, A. Sobester, and A. Keane, Engineering Design via Surrogate Modelling: A Practical Guide. Wiley, 2008.

An alternative to greedy optimization is to frame the problem as a partially observable Markov decision process and plan ahead some number of steps as outlined by M. Toussaint, "The Bayesian Search Game," in Theory and Principled Methods for the Design of Metaheuristics, Y. Borenstein and A. Moraglio, eds. Springer, 2014, pp. 129-144. See also R. Lam, K. Willcox, and D. H. Wolpert, "Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach," in Advances in Neural Information Processing Systems (NIPS), 2016.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ed67df89a2d167263f4f60b87f673da69e6b6380905aed208b8ee72de60308b4.jpg)  
Figure 19.1. Prediction-based optimization selects the point that minimizes the mean of the objective function.

# 19.2 Error-Based Exploration

Error-based exploration seeks to increase confidence in the true function. A Gaussian process can tell us both the mean and standard deviation at every point. A large standard deviation indicates low confidence, so error-based exploration samples at design points with maximum uncertainty.

The next sample point is:

$$
x^{(m + 1)} = \underset{\mathbf {x} \in \mathcal {X}} {\arg \max } \hat {\sigma} (\mathbf {x}) \tag {19.2}
$$

where  $\hat{\sigma}(\mathbf{x})$  is the standard deviation of a Gaussian process at a design point  $\mathbf{x}$  based on the previous  $m$  design points. The process is illustrated in figure 19.2.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/de0941bd5f7c28bf2f9a5b2171a71dc993eb06c1b630776e5d45d1bf8daf8f69.jpg)  
Figure 19.2. Error-based exploration selects a point with maximal uncertainty.

Gaussian processes are often defined over all of  $\mathbb{R}^n$ . Optimization problems with unbounded feasible sets will always have high uncertainty far away from sampled points, making it impossible to become confident in the true underlying function over the entire domain. Error-based exploration must thus be constrained to a closed region.

# 19.3 Lower Confidence Bound Exploration

While error-based exploration reduces the uncertainty in the objective function overall, its samples are often in regions that are unlikely to contain a global minimum. Lower confidence bound exploration trades off between greedy minimization employed by prediction-based optimization and uncertainty reduction employed by error-based exploration. The next sample minimizes the lower confidence bound of the objective function

$$
L B (\mathbf {x}) = \hat {\mu} (\mathbf {x}) - \alpha \hat {\sigma} (\mathbf {x}) \tag {19.3}
$$

where  $\alpha \geq 0$  is a constant that controls the trade-off between exploration and exploitation. Exploration involves minimizing uncertainty, and exploitation involves minimizing the predicted mean. We have prediction-based optimization with  $\alpha = 0$ , and we have error-based exploration as  $\alpha$  approaches  $\infty$ . The process is illustrated in figure 19.3.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5ff0628781ccfb9db24401acefb5329d9142f76c770bffd95b87a2c9974ecb47.jpg)  
Figure 19.3. Lower confidence bound exploration trades off between minimizing uncertainty and minimizing the predicted function.

# 19.4 Probability of Improvement Exploration

We can sometimes obtain faster convergence by selecting the design point that maximizes the chance that the new point will be better than the samples we have seen so far. The improvement for a function sampled at  $\mathbf{x}$  producing  $y = f(\mathbf{x})$  is

$$
I (y) = \left\{ \begin{array}{l l} y_{\min } - y & \text{if } y <   y_{\min } \\ 0 & \text{otherwise} \end{array} \right. \tag {19.4}
$$

where  $y_{\mathrm{min}}$  is the minimum value sampled so far.

The probability of improvement at points where  $\hat{\sigma} > 0$  is

$$
\begin{array}{l} P (y <   y_{\min }) = \int_{- \infty}^{y_{\min }} \mathcal {N} (y \mid \hat {\mu}, \hat {\sigma}^{2}) d y (19.5) \\ = \Phi \left(\frac{y_{\min } - \hat {\mu}}{\hat {\sigma}}\right) (19.6) \\ \end{array}
$$

where  $\Phi$  is the standard normal cumulative distribution function (see appendix C.8). This calculation (algorithm 19.1) is shown in figure 19.4. Figure 19.5 illustrates this process. When  $\hat{\sigma} = 0$ , which occurs at points where we have noiseless measurements, the probability of improvement is zero.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0995cda651f16200623fa293db9babc87ba327967deb31c1c6f6ea6bf70d1075.jpg)  
true predicted fit points  
Figure 19.4. The probability of improvement is the probability that evaluating a particular point will yield a better result than the best so far. This figure shows the probability density function predicted at a query point, with the shaded region below  $y_{\mathrm{min}}$  corresponding to the probability of improvement.

prob_of_improvement(y_min,  $\mu ,\sigma) =$  cdf(Normal  $(\mu ,\sigma)$  ，y_min)

Algorithm 19.1. Computing the probability of improvement for a given best  $y$  value y_min, mean  $\mu$ , and variance  $\nu$ .

# 19.5 Expected Improvement Exploration

Optimization is concerned with finding the minimum of the objective function. While maximizing the probability of improvement will tend to decrease the objective function over time, it does not improve very much with each iteration.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f2f6acafee7126767e655ec532c348a3b5f5e192baf96c870570af62ca19c99d.jpg)  
Figure 19.5. Maximizing the probability of improvement selects samples most likely to produce lower objective point values.

We can focus our exploration of points that maximize our expected improvement over the current best function value.

Through a substitution

$$
z = \frac{y - \hat {\mu}}{\hat {\sigma}} \quad y_{\min }^{\prime} = \frac{y_{\min } - \hat {\mu}}{\hat {\sigma}} \tag {19.7}
$$

we can write the improvement in equation (19.4) as

$$
I (y) = \left\{ \begin{array}{l l} \hat {\sigma} \left(y_{\min }^{\prime} - z\right) & \text{if } z <   y_{\min }^{\prime} \text{and} \hat {\sigma} > 0 \\ 0 & \text{otherwise} \end{array} \right. \tag {19.8}
$$

where  $\hat{\mu}$  and  $\hat{\sigma}$  are the predicted mean and standard deviation at the sample point  $\mathbf{x}$ .

We can calculate the expected improvement using the distribution predicted by the Gaussian process:

$$
\begin{array}{l} \mathbb {E} [ I (y) ] = \hat {\sigma} \int_{- \infty}^{y_{\min }^{\prime}} \left(y_{\min }^{\prime} - z\right) \mathcal {N} (z \mid 0, 1) d z (19.9) \\ = \hat {\sigma} \left[ y_{\min }^{\prime} \int_{- \infty}^{y_{\min }^{\prime}} \mathcal {N} (z \mid 0, 1) d z - \int_{- \infty}^{y_{\min }^{\prime}} z \mathcal {N} (z \mid 0, 1) d z \right] (19.10) \\ = \hat {\sigma} \left[ y_{\min }^{\prime} P \left(z \leq y_{\min }^{\prime}\right) + \mathcal {N} \left(y_{\min }^{\prime} \mid 0, 1\right) - \underbrace{\mathcal {N} (- \infty \mid 0 , 1)}_{= 0} \right] (19.11) \\ = \left(y_{\min } - \hat {\mu}\right) P \left(y \leq y_{\min }\right) + \hat {\sigma}^{2} \mathcal {N} \left(y_{\min } \mid \hat {\mu}, \hat {\sigma}^{2}\right) (19.12) \\ \end{array}
$$

Figure 19.6 illustrates this process using algorithm 19.2.

```javascript
function expected_improvement(y_min,  $\mu ,\sigma$  p_imp  $=$  prob_of_improvement(y_min,  $\mu ,\sigma$  ) p_ymin  $=$  pdf(Normal(  $\mu ,\sigma$  ),y_min) return (y_min -  $\mu$  \*p_imp  $^+$ $\sigma^{\wedge}2^{*}p_{-}$  ymin end
```

Algorithm 19.2. Computing the expected improvement for a given best  $y$  value y_min, mean  $\mu$ , and standard deviation  $\sigma$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d4a66417f8ea86424abf51ef8a264d8bfa3dc7820be57cda343ccde89552c5a5.jpg)  
Figure 19.6. Maximizing the expected improvement selects samples that are likely to improve the lower bound by as much as possible.

# 19.6 Safe Optimization

In some contexts, it may be costly to evaluate points that are deemed unsafe, which may correspond to low performing or infeasible points. Problems such as the in-flight tuning of the controller of a drone or safe movie recommendations require safe exploration—searching for an optimal design point while carefully avoiding sampling an unsafe design.

This section outlines the SafeOpt algorithm, which addresses a class of safe exploration problems. We sample a series of design points  $\mathbf{x}^{(1)},\ldots ,\mathbf{x}^{(m)}$  in pursuit of a minimum but without  $f(\mathbf{x}^{(i)})$  exceeding a critical safety threshold  $y_{\mathrm{max}}$ . In addition, we receive only noisy measurements of the objective function, where the noise is zero-mean with variance  $\nu$ . Such an objective function and its associated safe regions are shown in figure 19.7.

The SafeOpt algorithm uses Gaussian process surrogate models for prediction. At each iteration, we fit a Gaussian process to the noisy samples from  $f$ . After the

Figure 19.7. SafeOpt solves safe exploration problems that minimize  $f$  while remaining within safe regions defined by maximum objective function values.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/368ddaeddab7e9f3dffedaef8970aa887f3437973a989d49b628e1a4df3b671d.jpg)  
3 Y. Sui, A. Gotovos, J. Burdick, and A. Krause, "Safe Exploration for Optimization with Gaussian Processes," in International Conference on Machine Learning (ICML), vol. 37, 2015.

ith sample, SafeOpt calculates the upper and lower confidence bounds:

$$
u_{i} (\mathbf {x}) = \hat {\mu}_{i - 1} (\mathbf {x}) + \sqrt{\beta \hat {v}_{i - 1} (\mathbf {x})} \tag {19.13}
$$

$$
\ell_{i} (\mathbf {x}) = \hat {\mu}_{i - 1} (\mathbf {x}) - \sqrt{\beta \hat {v}_{i - 1} (\mathbf {x})} \tag {19.14}
$$

where larger values of  $\beta$  yield wider confidence regions. Such bounds are shown in figure 19.8.

Figure 19.8. An illustration of functions based on the predictions of a Gaussian process used by SafeOpt.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/737c3d22bf835a406b7f2a660fb297a77a6ceabe2f3bf7c29b6a739d04ba217d.jpg)  
confidence region  
true  
predicted  
-  $\ell (x)$  
$u(x)$  
- fit points

The Gaussian process predicts a distribution over  $f(\mathbf{x})$  for any design point. Being Gaussian, these predictions can provide only a probabilistic guarantee of safety up to an arbitrary factor:4

$$
P (f (\mathbf {x}) \leq y_{\max }) = \Phi \left(\frac{y_{\max } - \hat {\mu} (\mathbf {x})}{\sqrt{\hat {v} (\mathbf {x})}}\right) \geq P_{\text{safe}} \tag {19.15}
$$

The predicted safe region  $S$  consists of the design points that provide a probability of safety greater than the required level  $P_{\mathrm{safe}}$ , as illustrated in figure 19.9. The safe region can also be defined in terms of Lipschitz upper bounds constructed from upper bounds evaluated at previously sampled points.

SafeOpt chooses a safe sample point that balances the desire to localize a reachable minimizer of  $f$  and to expand the safe region. The set of potential minimizers of  $f$  is denoted  $\mathcal{M}$  (figure 19.10), and the set of points that will potentially lead to the expansion of the safe regions is denoted  $\mathcal{E}$  (figure 19.11). To trade off exploration and exploitation, we choose the design point  $\mathbf{x}$  with the largest predictive variance among both sets  $\mathcal{M}$  and  $\mathcal{E}$ .

The set of potential minimizers consists of the safe points whose lower confidence bound is lower than the lowest upper bound:

$$
\mathcal {M}_{i} = \left\{\mathbf {x} \in \mathcal {S}_{i} \mid \ell_{i} (\mathbf {x}) \leq \min_{\mathbf {x}^{\prime} \in \mathcal {S}_{i}} u_{i} \left(\mathbf {x}^{\prime}\right) \right\} \tag {19.16}
$$

4 Note the similarity to the probability of improvement.

For a variation of this algorithm, see F. Berkenkamp, A. P. Schoellig, and A. Krause, "Safe Controller Optimization for Quadrotors with Gaussian Processes," in IEEE International Conference on Robotics and Automation (ICRA), 2016.

Figure 19.9. The safety regions (green) predicted by a Gaussian process.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/864876ea0452943661f96db7ed59c533404397ef9000273f3c8bf10698fefb5b.jpg)  
confidence interval  
objective function  $f$  
fit points  
- safety threshold  
estimated safe region  $S$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dc5430b319a92ced84891b3412025ccc92e3925f4981a3da94effe1f174335f1.jpg)  
Figure 19.10. The potential minimizers are the safe points whose lower bounds are lower than the best, safe upper bound.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4dd1a6c3725d0d1044113d4c966dca01590f31ade3ab54a2dba3dfdcfd1aa48e.jpg)  
estimated safe region  $s$  
potential minimizers  $\mathcal{M}$  
confidence region  
objective function  
fit points  
— safety threshold  
best upper bound

At step  $i$ , the set of potential expanders  $\mathcal{E}_i$  consists of the safe points that, if added to the Gaussian process, optimistically assuming the lower bound, produce a posterior distribution with a larger safe set. The potential expanders naturally lie near the boundary of the safe region.

Figure 19.11. The set of potential expanders.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a02a0062ca778d9614b4487cd27864176f6d7479766bef500a60f7249c47902f.jpg)  
estimated safe region  $\mathcal{S}$  
expected expanders  $\mathcal{E}$  
confidence region  
objective function  
- fit points  
— safety threshold

Given an initial safe point $^6$ $\mathbf{x}^{(1)}$ , SafeOpt chooses the design point among sets  $\mathcal{M}$  and  $\mathcal{E}$  with the greatest uncertainty, as quantified by the width  $w_{i}(x) = u(x) - \ell(x)$ :

$$
\begin{array}{l} x^{(i)} = \arg \max w_{i} (\mathbf {x}) \tag {19.17} \\ \mathbf {x} \in \bar {\mathcal {M}}_{i} \bigcup \mathcal {E}_{i} \\ \end{array}
$$

SafeOpt proceeds until a termination condition is met. It is common to run the algorithm for a fixed number of iterations or until the maximum width is less than a set threshold.

Maintaining sets in multidimensional spaces can be computationally challenging. SafeOpt assumes a finite design space  $\mathcal{X}$  that can be obtained with a sampling method applied over the continuous search domain. Increasing the density of the finite design space leads to more accurate results with respect to the continuous space, but it takes longer per iteration.

SafeOpt is implemented in algorithm 19.3, and calls algorithm 19.4 to update the predicted confidence intervals; algorithm 19.5 to compute the safe, minimizer, and expander regions; and algorithm 19.6 to select a query point. The progression of SafeOpt is shown for one dimension in figure 19.12, and for two dimensions in figure 19.13.

SafeOpt cannot guarantee safety if it is not initialized with at least one point that it knows is safe.

```julia
function safe_opt(GP, X, i, f, y_max;  $\beta = 3.0$ , k_max=10) push!(GP, X[i], f(X[i])) # make first observation  
m = length(X)  
u, l = fill(Inf, m), fill(-Inf, m)  
S, M, E = falses(m), falses(m), falses(m)  
for k in 1: k_max  
update_confidence_intervals!(GP, X, u, l,  $\beta$ )  
compute_sets!(GP, S, M, E, X, u, l, y_max,  $\beta$ )  
i = safeopt_query_point(M, E, u, l)  
i != 0 | | break  
push!(GP, X[i], f(X[i]))  
end  
# return the best point  
update_confidence_intervals!(GP, X, u, l,  $\beta$ )  
S := u <= y_max  
if any(S)  
u_best, i_best = findmin(u[S])  
i_best = findfirst(isequal(i_best), cumsum(S))  
return (u_best, i_best)  
else  
return (NaN, 0)  
end
```

Algorithm 19.3. The SafeOpt algorithm applied to an empty Gaussian process GP, a finite design space X, index of initial safe point i, objective function f, and safety threshold y_max. The optional parameters are the confidence scalar  $\beta$  and the number of iterations k_max. A tuple containing the best safe upper bound and its index in X is returned.

```matlab
function update_confidence_intervals!(GP, X, u, l, β)  
 $\mu_{P}, \nu_{P} = \text{predict}(GP, X)$   
u :=  $\mu_{P} + \text{sqrt}(\beta * \nu_{P})$   
l :=  $\mu_{P} - \text{sqrt}(\beta * \nu_{P})$   
return (u, l)  
end
```

Algorithm 19.4. A method for updating the lower and upper bounds used in SafeOpt, which takes the Gaussian process GP, the finite search space  $X$ , the upper and lower-bound vectors  $\mathsf{u}$  and  $\mathsf{l}$ , and the confidence scalar  $\beta$ .

```txt
function compute_sets!(GP,S,M,E,X,u,l,y_max,β) fill!(M,false) fill!(E,false) # safe set S.=u.≤y_max if any(S) #potential minimizers M[S]=l[S].<minimum(u[S]) #maximum width (in M) w_max=maximum(u[M]-l[M]) #expanders - skip values in M or those with  $w\leq w_{-}\max$  E.=S.&.~M#skip points in M if any(E) E[E].= maximum(u[E]-l[E]).>w_max for(i,e) in enumerate(E) if e&&u[i]-l[i]>w_max push!(GP,X[i],l[i])  $\mu_{\mathrm{p}},\nu_{\mathrm{p}} =$  predict(GP,X[.~S]) pop!(GP) E[i]=any(  $\mu_{\mathrm{p}}+$  sqrt.(β*v_p).≥y_max) if E[i];w_max=u[i]-l[i];end end end end end return (S,M,E) end
```

Algorithm 19.5. A method for updating the safe S, minimizer M, and expander E sets used in SafeOpt. The sets are all Boolean vectors indicating whether the corresponding design point in X is in the set. The method also takes the Gaussian process GP, the upper and lower bounds u and l, respectively, the safety threshold y_max, and the confidence scalar  $\beta$ .

```julia
function safeopt_query_point(M, E, u, l)  
ME = M . | E  
if any(ME)  
    v = argmax(u[ME] - l[ME])  
    return findfirst(isequal(v), cumsum(ME))  
else  
    return 0  
end
```

Algorithm 19.6. A method for obtaining the next query point in SafeOpt. The index of the point in  $\mathsf{X}$  with the greatest width is returned.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/36266852f7f577cce92ba7c1b21325dfd56c69ea91dfaa647f1f3961be7297cb.jpg)  
estimated safe region potential minimizers potential expanders  
Figure 19.12. The first eight iterations of SafeOpt on a univariate function. SafeOpt can never reach the global optimum on the right-hand side because it requires crossing an unsafe region. We can only hope to find the global minima in our locally reachable safe region.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/51a17a0e9e85cec9c3fe352098db03bb25f69b685759d23d55c85c05822469b7.jpg)  
Figure 19.13. SafeOpt applied to the flower function (appendix B.4) with  $y_{\mathrm{max}} = 2$ , a Gaussian process mean of  $\mu (\mathbf{x}) = 2.5$ , variance  $\nu = 0.01, \beta = 10, a 51 \times 51$  uniform grid over the search space, and an initial point  $x^{(1)} = [-2.04, 0.96]$ . The color indicates the value of the upper bound, the cross indicates the safe point with the lowest upper bound, and the white contour line is the estimated safe region.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d65bffaf013f032a86d9689526e158060b09bd3ba822c9b398dd9ac2baf5beed.jpg)  
The objective function with the true safe region outlined in white:

# 19.7 Summary

- Gaussian processes can be used to guide the optimization process using a variety of strategies that use estimates of quantities such as the lower confidence bound, probability of improvement, and expected improvement.  
- Some problems do not allow for the evaluation of unsafe designs, in which case we can use safe exploration strategies that rely on Gaussian processes.

# 19.8 Exercises

Exercise 19.1. Give an example in which prediction-based optimization fails.

Solution: Prediction-based optimization with Gaussian processes can repeatedly sample the same point. Suppose we have a zero-mean Gaussian process and we start with a single point  $\mathbf{x}^{(1)}$ , which gives us some  $y^{(1)}$ . The predicted mean has a single global minimizer at  $\mathbf{x}^{(1)}$ . Prediction-based optimization will continue to sample at  $\mathbf{x}^{(1)}$ .

Exercise 19.2. What is the main difference between lower confidence bound exploration and error-based exploration in the context of optimization?

Solution: Error-based exploration wastes effort in reducing the variance and does not actively seek to minimize the function.

Exercise 19.3. We have a function  $f(x) = (x - 2)^{2} / 40 - 0.5$  with  $x \in [-5, 5]$ , and we have evaluation points at  $-1$  and  $1$ . Assume we use a Gaussian process surrogate model with a zero-mean function, and a squared exponential kernel  $\exp(-r^2 / 2)$ , where  $r$  is the Euclidean distance between two points. Which value for  $x$  would we evaluate next if we were maximizing probability of improvement? Which value for  $x$  would we evaluate next if we were maximizing expected improvement?

Solution: The Gaussian process looks like this:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8f0ee70df6fcc09474e7db55fdeccaac0f9bcee0c2e7dabf5c3a5e4d92af2266.jpg)

The probability of improvement and expected improvement look like:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/59cd24abff08e058ab9604d364c54ad5fcc027a96d0aa1139dcdfb1b1b1a8f64.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/09f8b8dc127fd8f9921e9284fae05ac2fa2f8bb9c9443c5585ad568c93e28fc4.jpg)

The maximum probability of improvement is at  $x = 0.84$  for  $P = 0.504$ .

The maximum expected improvement is at  $x = 2.78$  for  $E = 0.22$ .

# 20 Optimization under Uncertainty

Previous chapters assumed that the optimization objective is to minimize a deterministic function of our design points. In many engineering tasks, however, there may be uncertainty in the objective function or the constraints. Uncertainty may arise due to a number of factors, such as model approximations, imprecision, and fluctuations of parameters over time. This chapter covers a variety of methods for accounting for uncertainty in our optimization to enhance robustness. $^{1}$

# 20.1 Uncertainty

Uncertainty in the optimization process can arise for a variety of reasons. There may be irreducible uncertainty, which is inherent to the system, such as background noise, varying material properties, and quantum effects. These uncertainties cannot be avoided and our design should accommodate them. There may also be epistemic uncertainty, which is uncertainty caused by a subjective lack of knowledge by the designer. This uncertainty can arise from approximations in the model used when formulating the design problem and errors introduced by numerical solution methods.

Accounting for these various forms of uncertainty is critical to ensuring robust designs. In this chapter, we will use  $\mathbf{z} \in \mathcal{Z}$  to represent a vector of random values. We want to minimize  $f(\mathbf{x},\mathbf{z})$ , but we do not have control over  $\mathbf{z}$ . Feasibility depends on both the design vector  $\mathbf{x}$  and the uncertain vector  $\mathbf{z}$ . This chapter introduces the feasible set over  $\mathbf{x}$  and  $\mathbf{z}$  pairs as  $\mathcal{F}$ . We have feasibility if and only if  $(\mathbf{x},\mathbf{z}) \in \mathcal{F}$ . We will use  $\mathcal{X}$  as the design space, which may include potentially infeasible designs depending on the value of  $\mathbf{z}$ .

Optimization with uncertainty was briefly introduced in section 18.5 in the context of using a Gaussian process to represent an objective function inferred

1 Additional references include: H.-G. Beyer and B. Sendhoff, "Robust Optimization—A Comprehensive Survey," Computer Methods in Applied Mechanics and Engineering, vol. 196, no. 33, pp. 3190-3218, 2007. G.-J. Park, T.-H. Lee, K.H. Lee, and K.-H. Hwang, "Robust Design: An Overview," AIAA Journal, vol. 44, no. 1, pp. 181-191, 2006.

2 This form of uncertainty is sometimes called aleatory uncertainty or random uncertainty.  
3 Epistemic uncertainty is also called reducible uncertainty.  
4 The statistician George Box famously wrote: All models are wrong; some models are useful. G.E.P.Box, W.G.Hunter, and J.S.Hunter, Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building, 2nd ed. Wiley, 2005. P. 440.

from noisy measurements. We had  $f(\mathbf{x},z) = f(\mathbf{x}) + z$  with the additional assumption that  $z$  comes from a zero-mean Gaussian distribution.<sup>5</sup> Uncertainty may be incorporated into the evaluation of a design point in other ways. For example, if we had noise in the input to the objective function,<sup>6</sup> we might have  $f(\mathbf{x},\mathbf{z}) = f(\mathbf{x} + \mathbf{z})$ . In general,  $f(\mathbf{x},\mathbf{z})$  can be a complex, nonlinear function of  $\mathbf{x}$  and  $\mathbf{z}$ . In addition,  $\mathbf{z}$  may not come from a Gaussian distribution; in fact, it may come from a distribution that is not known.

Figure 20.1 demonstrates how the degree of uncertainty can affect our choice of design. For simplicity,  $x$  is a scalar and  $z$  is selected from a zero-mean Gaussian distribution. We assume that  $z$  corresponds to noise in the input to  $f$ , and so  $f(x,z) = f(x + z)$ . The figure shows the expected value of the objective function for different levels of noise. The global minimum without noise is  $a$ . However, aiming for a design near  $a$  can be risky since it lies within a steep valley, making it rather sensitive to noise. Even with low noise, it may be better to choose a design near  $b$ . Designs near  $c$  can provide even greater robustness to larger amounts of noise. If the noise is very high, the best design might even fall between  $b$  and  $c$ , which corresponds to a local maximum in the absence of noise.

There are a variety of different ways to account for uncertainty in optimization. We will discuss both set-based uncertainty and probabilistic uncertainty.<sup>7</sup>

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2f123bc167cd5960df4ffc564c280089ec9bf78758e511a49d7d0d85536ea5db.jpg)

Here, the two-argument version of  $f$  takes as input the design point and random vector, but the single-argument version of  $f$  represents a deterministic function of the design point without noise.  
For example, there may be variability in the manufacturing of our design.

7 Other approaches for representing uncertainty include Dempster-Shafer theory, fuzzy-set theory, and possibility theory, which are beyond the scope of this book.

Figure 20.1. The global minimum at  $a$  in the noiseless case is sensitive to noise. Other design points may be more robust depending on the anticipated level of noise.

# 20.2 Set-Based Uncertainty

Set-based uncertainty approaches assume that  $\mathbf{z}$  belongs to a set  $\mathcal{Z}$ , but these approaches make no assumptions about the relative likelihood of different points within that set. The set  $\mathcal{Z}$  can be defined in different ways. One way is to define intervals for each component of  $\mathcal{Z}$ . Another way is to define  $\mathcal{Z}$  by a set of inequality constraints,  $\mathbf{g}(\mathbf{x},\mathbf{z})\leq \mathbf{0}$ , similar to what was done for the design space  $\mathcal{X}$  in chapter 10. In general, we can have  $\mathcal{Z}$  depend on  $\mathbf{x}$ , but we will omit this potential dependency in our discussion for simplicity.

# 20.2.1 Minimax

In problems with set-based uncertainty, we often want to minimize the maximum possible value of the objective function. Such a minimax approach solves the optimization problem

$$
\underset{\mathbf {x} \in \mathcal {X}} {\text{minimize}} \underset{\mathbf {z} \in \mathcal {Z}} {\text{maximize}} f (\mathbf {x}, \mathbf {z}) \tag {20.1}
$$

In other words, we want to find an  $\mathbf{x}$  that minimizes  $f$ , assuming the worst-case value for  $\mathbf{z}$ .

This optimization is equivalent to defining a modified objective function

$$
f_{\mathrm{mod}} (\mathbf {x}) = \underset{\mathbf {z} \in \mathcal {Z}} {\text{maximize}} f (\mathbf {x}, \mathbf {z}) \tag {20.2}
$$

and then solving

$$
\underset{\mathbf {x} \in \mathcal {X}} {\text{minimize}} f_{\mathrm{mod}} (\mathbf {x}) \tag {20.3}
$$

Example 20.1 shows this optimization on a univariate problem and illustrates the effect of different levels of uncertainty.

In problems where we have feasibility constraints, our optimization problem becomes

$$
\underset{\mathbf {x} \in \mathcal {X}} {\text{minimize}} \underset{\mathbf {z} \in \mathcal {Z}} {\text{maximize}} f (\mathbf {x}, \mathbf {z}) \text{subjectto} (\mathbf {x}, \mathbf {z}) \in \mathcal {F} \text{and} (\mathbf {x}, \mathbf {z}^{\prime}) \in \mathcal {F} \text{for all} \mathbf {z}^{\prime} \tag {20.4}
$$

Example 20.2 shows the effect of applying minimax on the space of feasible design points when there are constraints.

Also called the robust counterpart approach or robust regularization.

Consider the objective function

$$
f (x, z) = f (x + z) = f (\tilde {x}) = \left\{ \begin{array}{l l} - \tilde {x} & \text{if } \tilde {x} \leq 0 \\ \tilde {x}^{2} & \text{otherwise} \end{array} \right.
$$

where  $\tilde{x} = x + z$ , with a set-based uncertainty region  $z \in [-\epsilon, \epsilon]$ . The minimax approach is a minimization problem over the modified objective function  $f_{\mathrm{mod}}(x) = \mathrm{maximize}_{z \in [-\epsilon, \epsilon]} f(x, z)$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/27bbb683611f73279c828b802505c3a2aee2446e20a091db6e1a32655e5620c5.jpg)

The figure above shows  $f_{\mathrm{mod}}(x)$  for several different values of  $\epsilon$ . The minimum for  $\epsilon = 0$  coincides with the minimum of  $f(x,0)$ . As  $\epsilon$  is increased, the minimum first shifts right as  $x$  increases faster than  $x^2$  and then shifts left as  $x^2$  increases faster than  $x$ . The robust minimizer does not generally coincide with the minimum of  $f(x,0)$ .

Example 20.1. Example of a minimax approach to optimization under set-based uncertainty.

Consider an uncertain feasible set in the form of a rotated ellipse, where  $(\mathbf{x},z)\in \mathcal{F}$  if and only if  $z\in [0,\pi /2]$  and

$$
\left(x_{1} \cos z + x_{2} \sin z\right) ^{2} + \left(x_{1} \sin z - x_{2} \cos z\right) ^{2} / 16 \leq 1
$$

When  $z = 0$ , the major axis of the ellipse is vertical. Increasing values of  $z$  slowly rotates it counter clockwise to horizontal at  $z = \pi / 2$ . The figure below shows the vertical and horizontal ellipses and the set of all points that are feasible for at least one  $z$  in blue.

A minimax approach to optimization should consider only design points that are feasible under all values of  $z$ . The set of designs that are always feasible are given by the intersection of all ellipses formed by varying  $z$ . This set is outlined in red.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ebe0cc9752232d051972a6b1283f7636cff1cbcacd66d88943b18ffddb50fb5e.jpg)

Example 20.2. The minimax approach applied to uncertainty in the feasible set.

# 20.2.2 Information-Gap Decision Theory

Instead of assuming the uncertainty set  $\mathcal{Z}$  is fixed, an alternative approach known as information-gap decision theory parameterizes the uncertainty set by a nonnegative scalar gap parameter  $\epsilon$ . The gap controls the volume of the parameterized set  $\mathcal{Z}(\epsilon)$  centered at some nominal value  $\bar{\mathbf{z}} = \mathcal{Z}(0)$ . One way to define  $\mathcal{Z}(\epsilon)$  is as a hypersphere of radius  $\epsilon$  centered at a nominal point  $\bar{\mathbf{z}}$ :

$$
\mathcal {Z} (\epsilon) = \left\{\mathbf {z} \mid \| \mathbf {z} - \bar {\mathbf {z}} \| _{2} \leq \epsilon \right\} \tag {20.5}
$$

Figure 20.2 illustrates this definition in two dimensions.

By parameterizing the uncertainty set, we avoid committing to a particular uncertainty set. Uncertainty sets that are too large sacrifice the quality of the solution, and uncertainty sets that are too small sacrifice robustness. Design points that remain feasible for larger gaps are considered more robust.

In information-gap decision theory, we try to find the design point that allows for the largest gap while preserving feasibility. This design point can be obtained by solving the following optimization problem:

$$
\mathbf {x}^{*} = \underset{\mathbf {x} \in \mathcal {X}} {\arg \max } \underset{\epsilon \in [ 0, \infty)} {\operatorname{maximize}} \left\{ \begin{array}{l l} \epsilon & \text{if } (\mathbf {x}, \mathbf {z}) \in \mathcal {F} \text{for all} \mathbf {z} \in \mathcal {Z} (\epsilon) \\ 0 & \text{otherwise} \end{array} \right. \tag {20.6}
$$

This optimization focuses on finding designs that ensure feasibility in the presence of uncertainty. In fact, equation (20.6) does not explicitly include the objective function  $f$ . However, we can incorporate the constraint that  $f(\mathbf{x},\mathbf{z})$  be no greater than some threshold  $y_{\mathrm{max}}$ . Such performance constraints can help us avoid excessive risk aversion. Figure 20.3 and example 20.3 illustrate the application of information-gap decision theory.

# 20.3 Probabilistic Uncertainty

Models of probabilistic uncertainty use distributions over a set  $\mathcal{Z}$ . Probabilistic uncertainty models provide more information than set-based uncertainty models, allowing the designer to account for the probability of different outcomes of a design. These distributions can be defined using expert knowledge or learned from data. Given a distribution  $p$  over  $\mathcal{Z}$ , we can infer a distribution over the output of  $f$  using methods that will be discussed in chapter 21. In general, the

9F.M. Hemez and Y. Ben-Haim, "Info-Gap Robustness for the Correlation of Tests and Simulations of a Non-Linear Transient," Mechanical Systems and Signal Processing, vol. 18, no. 6, pp. 1443-1467, 2004.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/44cab17c63f4e9be91351adc4cd9cf99e4957f87eee2ebad62df998168365fec.jpg)  
Figure 20.2. A parametrized uncertainty set  $\mathcal{Z}(\epsilon)$  in the form of a hypersphere.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2451ed8a35d744124a0445c9b47d95dd958db5b9a47ba79c3a0266de161b9c7e.jpg)  
Figure 20.3. Information-gap decision theory applied to an objective function with additive noise  $f(\tilde{\mathbf{x}})$  with  $\tilde{\mathbf{x}} = \mathbf{x} + \mathbf{z}$  and a circular uncertainty set

$$
\mathcal {Z} (\epsilon) = \left\{\mathbf {z} \mid \| z \| _{2} \leq \epsilon \right\}
$$

The design  $\mathbf{x}^*$  is optimal under information-gap decision theory as it allows for the largest possible  $\epsilon$

Consider the robust optimization of  $f(x,z) = \tilde{x}^2 + 6e^{-\tilde{x}^2}$  with  $\tilde{x} = x + z$  subject to the constraint  $\tilde{x} \in [-2,2]$  with the uncertainty set  $\mathcal{Z}(\epsilon) = [-\epsilon, \epsilon]$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1bdbead258c3f1ff56ee651a18b38842e8f35674ed835522b435b402c65fb98a.jpg)

Applying information-gap decision theory to this problem results in a maximally sized uncertainty set and a design centered in the suboptimal region of the objective function. Applying an additional constraint on the maximum objective function value,  $f(x,z) \leq 5$ , allows the same approach to find a design with better noise-free performance. The blue lines indicate the worst-case objective function value for a given uncertainty parameter  $\epsilon$ .

Example 20.3. We can mitigate excessive risk aversion by applying a constraint on the maximum acceptable objective function value when applying information-gap decision theory.

distribution  $p$  may depend on  $\mathbf{x}$ , but we will omit this dependency for simplicity. This section outlines five different metrics for converting this distribution into a scalar value given a particular design  $\mathbf{x}$ . We can then optimize with respect to these metrics. $^{10}$

We can also combine elements of the previous section on set-based uncertainty with probabilistic uncertainty. For example, we may be uncertain about the distribution  $p$ , in which case we can consider a set of possible probability distributions  $\mathcal{P}$ . In this case, we can optimize over the worst-case distribution in  $\mathcal{P}$ . This approach is known as distributionally robust optimization.[11]

# 20.3.1 Expected Value

One way to convert the distribution output by  $f$  into a scalar value is to use the expected value or mean. The expected value is the average output that we can expect when considering all outputs of  $f(\mathbf{x},\mathbf{z})$  for all  $\mathbf{z} \in \mathcal{Z}$  and their corresponding probabilities. The expected value as a function of the design point  $\mathbf{x}$  is

$$
\mathbb {E}_{\mathbf {z} \sim p} [ f (\mathbf {x}, \mathbf {z}) ] = \int_{\mathcal {Z}} f (\mathbf {x}, \mathbf {z}) p (\mathbf {z}) d \mathbf {z} \tag {20.7}
$$

The expected value does not necessarily correspond to the objective function without noise, as illustrated in example 20.4.

Computing the integral in equation (20.7) analytically may not be possible. One may approximate that value using sampling or a variety of other more sophisticated techniques discussed in chapter 21.

# 20.3.2 Variance

Besides optimizing with respect to the expected value of the function, we may also be interested in choosing design points whose value is not overly sensitive to uncertainty. $^{12}$  Such regions can be quantified using the variance of  $f$ :

$$
\begin{array}{l} \operatorname{Var} [ f (\mathbf {x}, \mathbf {z}) ] = \mathbb {E}_{\mathbf {z} \sim p} \left[ \left(f (\mathbf {x}, \mathbf {z}) - \mathbb {E}_{\mathbf {z} \sim p} [ f (\mathbf {x}, \mathbf {z}) ]\right) ^{2} \right] (20.8) \\ = \int_{\mathcal {Z}} f (\mathbf {x}, \mathbf {z}) ^{2} p (\mathbf {z}) d \mathbf {z} - \mathbb {E}_{\mathbf {z} \sim p} [ f (\mathbf {x}, \mathbf {z}) ] ^{2} (20.9) \\ \end{array}
$$

Further discussion of various metrics can be found in A. Shapiro, D. Dentcheva, and A. Ruszczyński, Lectures on Stochastic Programming: Modeling and Theory, 2nd ed. SIAM, 2014.  
11 A. Shapiro, "Distributionally Robust Stochastic Programming," SIAM Journal on Optimization, vol. 27, no. 4, pp. 2258-2275, 2017.

Sometimes designers seek plateau-like regions where the output of the objective function is relatively constant, such as producing materials with consistent performance or scheduling trains such that they arrive at a consistent time.

One common model is to apply zero-mean Gaussian noise to the function output,  $f(\mathbf{x}, \mathbf{z}) = f(\mathbf{x}) + \mathbf{z}$ , as was the case with Gaussian processes in chapter 19. The expected value is equivalent to the noise-free case:

$$
\mathbb {E}_{\mathbf {z} \sim \mathcal {N} (\mathbf {0}, \boldsymbol {\Sigma})} [ f (\mathbf {x}) + \mathbf {z} ] = \mathbb {E}_{\mathbf {z} \sim \mathcal {N} (\mathbf {0}, \boldsymbol {\Sigma})} [ f (\mathbf {x}) ] + \mathbb {E}_{\mathbf {z} \sim \mathcal {N} (\mathbf {0}, \boldsymbol {\Sigma})} [ \mathbf {z} ] = f (\mathbf {x})
$$

It is also common to add noise directly to the design vector,  $f(\mathbf{x},\mathbf{z}) = f(\mathbf{x} + \mathbf{z}) = f(\tilde{\mathbf{x}})$ . In such cases the expected value is affected by the variance of zero-mean Gaussian noise.

Consider minimizing the expected value of  $f(\tilde{x}) = \sin(2\tilde{x}) / \tilde{x}$  with  $\tilde{x} = x + z$  for  $z$  drawn from a zero-mean Gaussian distribution  $\mathcal{N}(0, \nu)$ . Increasing the variance increases the effect that the local function landscape has on a design.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8c26fc959f960eea3ec8e9f7a5e26d88fe97e5ccae882828cb4599e38fdb49c1.jpg)

The plot above shows that changing the variance affects the location of the optima.

Example 20.4. The expected value of an uncertain objective function depends on how the uncertainty is incorporated into the objective function.

We call design points with large variance sensitive and design points with small variance robust. Examples of sensitive and robust points are shown in figure 20.4. We are typically interested in good points as measured by their expected value that are also robust. Managing the trade-off between the expected objective function value and the variance is a multiobjective optimization problem (see examples 20.5 and 20.6), and we can use techniques discussed in chapter 15.

# 20.3.3 Statistical Feasibility

An alternative metric against which to optimize is statistical feasibility. Given  $p(\mathbf{z})$ , we can compute the probability a design point  $\mathbf{x}$  is feasible:

$$
P (\mathbf {x} \in \mathcal {F}) = \int_{\mathcal {Z}} ((\mathbf {x}, \mathbf {z}) \in \mathcal {F}) p (\mathbf {z}) d \mathbf {z} \tag {20.10}
$$

This probability can be estimated through sampling. If we are also interested in ensuring that the objective value does not exceed a certain threshold, we can incorporate a constraint  $f(\mathbf{x}, \mathbf{z}) \leq y_{\max}$  as is done with information-gap decision theory. Unlike the expected value and variance metrics, we want to maximize this metric.

# 20.3.4 Value at Risk

The value at risk (VaR) is a risk measure corresponding to the best objective value that can be guaranteed with probability  $\alpha$ . We can write this definition mathematically in terms of the cumulative distribution function, denoted  $\Phi(y)$ , over the random output of the objective function. The probability that the outcome is less than or equal to  $y$  is given by  $\Phi(y)$ . VaR with confidence  $\alpha$  is the minimum value of  $y$  such that  $\Phi(y) \geq \alpha$ . This definition is equivalent to the  $\alpha$  quantile of a probability distribution. An  $\alpha$  close to 1 is sensitive to unfavorable outliers, $^{13}$  whereas an  $\alpha$  close to 0 is overly optimistic and close to the best possible outcome.

# 20.3.5 Conditional Value at Risk

The conditional value at risk (CVaR) is another risk measure closely related to the value at risk. $^{14}$  CVaR is the expected value of the top  $1 - \alpha$  quantile of the probability distribution over the output. This quantity is illustrated in figure 20.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1612e9d81f5a2a8838219048ea20a804cdfdc6ff44687fe208b50de7677da9ab.jpg)  
Figure 20.4. Probabilistic approaches produce probability distributions over the model output. Design points can be sensitive or robust to uncertainty. The blue regions show how the distribution over a normally distributed design is affected by the objective function.

13 Setting  $\alpha = 1$  when using VaR is equivalent to a minimax approach with set-based uncertainty where the optimization is done assuming the worst-case outcome.

14 The conditional value at risk is also known as the mean excess loss, mean shortfall, and tail value at risk. R. T. Rockafellar and S. Uryasev, "Optimization of Conditional Value-at-Risk," Journal of Risk, vol. 2, pp. 21-42, 2000. It is also a kind of coherent risk measure, which means that it satisfies some additional mathematical properties. Another coherent risk measure, not discussed here, is the entropic value at risk. A. Ahmadi-Javid, "Entropic Value-At-Risk: A New Coherent Risk Measure," Journal of Optimization Theory and Applications, vol. 155, no. 3, pp. 1105-1123, 2011.

Consider the objective function  $f(x, z) = x^{2} + z$ , with  $z$  drawn from a Gamma distribution that depends on  $x$ . We can construct a function  $\text{dist}(x)$  that returns a Gamma distribution from the Distributions.jl package:

$$
\operatorname{dist} (x) = \operatorname{Gamma} (2 / (1 + \operatorname{abs} (x)), 2)
$$

This distribution has mean  $4 / (1 + |x|)$  and variance  $8 / (1 + |x|)$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/29a193b9b5506ecf6d27e692c178756893429761d71374c33514d2bd631549f2.jpg)

We can find a robust optimizer that minimizes both the expected value and the variance. Minimizing with respect to the expected value, ignoring the variance, produces two minima at  $x \approx \pm 0.695$ . Incorporating a penalty for the variance shifts these minima away from the origin. The figure below shows objective functions of the form  $\alpha \mathbb{E}[y \mid x] + (1 - \alpha) \sqrt{\operatorname{Var}[y \mid x]}$  for  $\alpha \in [0,1]$  along with their associated minima.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/543b5c89d78cf39b546ce31983c085563210edb9b95f06ccbc48e197659ddd9a.jpg)

Example 20.5. Considering both the expected value and the variance in optimization under uncertainty.

The mean-variance tradeoff often arises in finance, where there is uncertainty in return on investments. Suppose we have a budget  $b$  available for investment and a design  $\mathbf{x}$  is a stock portfolio, with  $x_{i}$  being the amount of money allocated to the  $i$ th stock. Stock returns  $\mathbf{z}$  are modeled as Gaussian with  $\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ . The return of a portfolio is then

$$
y = f (\mathbf {x}, \mathbf {z}) = x_{1} z_{1} + x_{2} z_{2} + \dots = \mathbf {x}^{\top} \mathbf {z}
$$

which is also Gaussian with

$$
y \sim \mathcal {N} (\mathbf {x}^{\top} \boldsymbol {\mu}, \mathbf {x}^{\top} \boldsymbol {\Sigma} \mathbf {x})
$$

We wish to maximize the expected return while minimizing its variance. If we weight the expected return by  $w$  and variance by  $1 - w$ , we arrive at the following problem:

minimize

$$
- w \mathbf {x}^{\top} \boldsymbol {\mu} + (1 - w) \mathbf {x}^{\top} \boldsymbol {\Sigma} \mathbf {x}
$$

subject to

$$
\mathbf {x}^{\top} \mathbf {1} = b
$$

$$
x \geq 0
$$

This formulation is known as Markowitz portfolio optimization. The first constraint enures that we allocate our full budget. The last constraint ensures we do not hold negative quantities of stock. This problem is a quadratic program (chapter 13). The figure below shows how the optimal portfolio changes based on  $w$  in a sample problem.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f962fc1fd0e220558c165fc1a9f2d63bd1d7767df32c565bf0ba91b568f8a542.jpg)

Example 20.6. Considering both the expected value and the variance in portfolio optimization with

$$
\mu = \left[ \begin{array}{c c c} 0. 26 & 0. 08 & 0. 74 \end{array} \right]
$$

and

$$
\boldsymbol {\Sigma} = \left[ \begin{array}{c c c} 0. 21 & 0. 03 & 0. 01 \\ 0. 03 & 0. 06 & 0. 04 \\ 0. 01 & 0. 04 & 0. 94 \end{array} \right]
$$

Markowitz portfolio optimization is named for the American economist Harry Max Markowitz (1927-2023). H. Markowitz, "Portfolio Selection," Journal of Finance, vol. 7, no. 1, pp. 77-91, 1952.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4a3e533f6dff5e4b53f8c8d534a8e3c2f465d2901069a82a03fe755607486f21.jpg)  
Figure 20.5. CVaR and VaR for a particular level  $\alpha$ . CVaR is the expected value of the top  $1 - \alpha$  quantile, whereas the VaR is the lowest objective function value over the same quantile.

CVaR has some theoretical and computational advantages over VaR. CVaR is less sensitive to estimation errors in the distribution over the objective output. For example, if the cumulative distribution function is flat in some intervals, then VaR can jump with small changes in  $\alpha$ . In addition, VaR does not account for costs beyond the  $\alpha$  quantile, which is undesirable if there are rare outliers with very poor objective values.[15]

# 20.4 Summary

- Uncertainty in the optimization process can arise due to errors in the data, the models, or the optimization method itself.  
- Accounting for these sources of uncertainty is important in ensuring robust designs.  
- Optimization with respect to set-based uncertainty includes the minimax approach that assumes the worst-case and information-gap decision theory that finds a design robust to a maximally sized uncertainty set.  
- Probabilistic approaches typically minimize the expected value, the variance, risk of infeasibility, value at risk, conditional value at risk, or a combination of these.

# 20.5 Exercises

Exercise 20.1. Suppose we have zero-mean Gaussian noise in the input such that  $f(x,z) = f(x + z)$ . Consider the three points  $a, b$ , and  $c$  in the figure below:

15 For an overview of properties, see G.C. Pflug, "Some Remarks on the Value-at-Risk and the Conditional Value-at-Risk," in Probabilistic Constrained Optimization: Methodology and Applications, S.P. Uryasev, ed. Springer, 2000, pp. 272-281. and R.T. Rockafellar and S. Uryasev, "Conditional Value-at-Risk for General Loss Distributions," Journal of Banking and Finance, vol. 26, pp. 1443-1471, 2002.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/01b20d7b30cd1a75201240cce9cd58f132af543015a2b0ab0760eeace267d607.jpg)

Which design point is best if we are minimizing the expected value minus the standard deviation?

Solution: The objective is to minimize  $\mathbb{E}_z[f(x + z)] - \sqrt{\mathrm{Var}_z[f(x + z)]}$ . The first term, corresponding to the mean, is minimized at design point  $a$ . The second term, corresponding to the standard deviation, is also maximized at design point  $a$  because perturbations to the design at that location cause large variations in the output. The optimal design is thus  $x^{*} = a$ .

Exercise 20.2. Optima, such as the one depicted in figure 20.6, often lie on a constraint boundary and are thus sensitive to uncertainties that could cause them to become infeasible. One approach to overcome uncertainty with respect to feasibility is to make the constraints more stringent, reducing the size of the feasible region as shown in figure 20.7.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9f3f0b7a9821abcd515e6732b5fc7e905203c30adbea5f4a5d18973c518847bf.jpg)  
Figure 20.7. Applying more stringent constraints during optimization prevents designs from being too close to the true feasibility boundary.

It is common to rewrite constraints of the form  $g(\mathbf{x}) \leq g_{\max}$  to  $\gamma g(\mathbf{x}) \leq g_{\max}$ , where  $\gamma > 1$  is a factor of safety. Optimizing such that the constraint values stay below  $g_{\max} / \gamma$  provides an additional safety buffer.

Consider a beam with a square cross section thought to fail when the stresses exceed  $\sigma_{\mathrm{max}} = 1$ . We wish to minimize the cross section  $f(x) = x^{2}$ , where  $x$  is the cross section length. The stress in the beam is also a function of the cross section length  $g(x) = x^{-2}$ . Plot the probability that the optimized design does not fail as the factor of safety varies from 1 to 2:

Uncertainty in maximum stress,  $g(x,z) = x^{-2} + z$  
- Uncertainty in construction tolerance,  $g(x,z) = (x + z)^{-2}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/faf8d59764e3a7f438fc7d5d47f258fb8d21a15d4231c3fa7a1fbb1dea741436.jpg)  
Figure 20.6. Optima with active constraints are often sensitive to uncertainty.

- Uncertainty in material properties,  $g(x,z) = (1 + z)x^{-2}$

where  $z$  is zero-mean noise with variance 0.01.

Solution: The deterministic optimization problem is:

$$
\underset{x} {\text{minimize}} \quad x^{2}
$$

$$
\text{subject} \quad \gamma x^{- 2} \leq 1
$$

The optimal cross-section length as a function of the factor of safety is  $x = \sqrt{\gamma}$ . We can thus substitute  $\sqrt{\gamma}$  for the cross-section length in each uncertainty formulation and evaluate the probability that the design does not fail. Note that all designs have a 50% chance of failure when the factor of safety is one, due to the symmetry of the normal distribution.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/40469efb2bae940738ea443af7192cb7f7886a5016e51778f498bf0f471725f9.jpg)

Exercise 20.3. The six-sigma method is a special case of statistical feasibility in which a production or industrial process is improved until its assumed Gaussian output violates design requirements only with outliers that exceed six standard deviations. This requirement is fairly demanding, as is illustrated in figure 20.8.

Consider the optimization problem

$$
\underset{\mathbf {x}} {\text{minimize}}
$$

subject to

$$
x_{1}
$$

$$
e^{x_{1}} \leq x_{2} + z \leq 2 e^{x_{1}}
$$

with  $z\sim \mathcal{N}(0,1)$ . Find the optimal design  $x^{*}$  such that  $(x,z)$  is feasible for all  $|z|\leq 6$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/cbba4da2b118dbbe63ff0317793c173c792c52bc7461586b78f2b5a281b390d1.jpg)  
Figure 20.8. Statistical feasibility can be met either by shifting the objective function mean away from the feasibility boundary or by reducing the variance of the objective function.

Solution: The figure on the right shows the noise-free feasible region. Without noise, the optimum lies with  $x_{1}$  infinitely negative. We have noise and have chosen not to accept any outliers with magnitude greater than 6. Such outliers occur approximately  $1.973 \times 10^{-7\%}$  of the time.

The feasible region for  $x_{2}$  lies between  $e^{x_{1}}$  and  $2e^{x_{1}}$ . The noise is symmetric, so the most robust choice for  $x_{2}$  is  $1.5e^{x_{1}}$ .

The width of the feasible region for  $x_{2}$  is  $e^{x_{1}}$ , which increases with  $x_{1}$ . The objective function increases with  $x_{1}$  as well, so the optimal  $x_{1}$  is the lowest such that the width of the feasible region is at least 12. This results in  $x_{1} = \ln 12 \approx 2.485$  and  $x_{2} = 18$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/257aa1c3967c296828a302da91b795e489c064818bde40fee7c0ef2c73253eb0.jpg)

# 21 Uncertainty Propagation

As discussed in the previous chapter, probabilistic approaches to optimization under uncertainty model some of the inputs to the objective function as a probability distribution. This chapter discusses how to propagate known input distributions to estimate quantities associated with the output distribution, such as the mean and variance of the objective function. There are a variety of approaches to uncertainty propagation, some based on mathematical concepts such as Monte Carlo, the Taylor series approximation, orthogonal polynomials, and Gaussian processes. $^{1}$  These approaches differ in the assumptions they make and the quality of their estimates.

# 21.1 Sampling Methods

The mean and variance of the objective function at a particular design point can be approximated using Monte Carlo integration, which approximates the integral using  $m$  samples  $\mathbf{z}^{(1)},\ldots ,\mathbf{z}^{(m)}$ , from the distribution  $p$  over  $\mathcal{Z}$ . These estimates are also called the sample mean and sample variance:

$$
\mathbb {E}_{\mathbf {z} \sim p} [ f (\mathbf {z}) ] \approx \hat {\mu} = \frac{1}{m} \sum_{i = 1}^{m} f \left(\mathbf {z}^{(i)}\right) \tag {21.1}
$$

$$
\operatorname{Var}_{\mathbf {z} \sim p} [ f (\mathbf {z}) ] \approx \hat {v} = \left(\frac{1}{m} \sum_{i = 1}^{m} f \left(\mathbf {z}^{(i)}\right) ^{2}\right) - \hat {\mu}^{2} \tag {21.2}
$$

In the equation above, and for the rest of this chapter, we drop  $\mathbf{x}$  from  $f(\mathbf{x},\mathbf{z})$  for notational convenience, but the dependency on  $\mathbf{x}$  still exists. For each new design point  $\mathbf{x}$  in our optimization process, we recompute the mean and variance.

<sup>1</sup>A variety of different uncertainty propagation methods are discussed by R. Ghanem, D. Higdon, and H. Owhadi, eds., Handbook of Uncertainty Quantification. Springer, 2017.

2 Alternatively, quasi Monte Carlo integration can be used to produce estimates with faster convergence as discussed in chapter 16.

A desirable property of this sampling-based approach is that  $p$  does not need to be known exactly. We can obtain samples directly from simulation or real-world experiments. A potential limitation of this approach is that many samples may be required before there is convergence to a suitable estimate. The variance of the sample mean for a normally distributed  $f$  is  $\mathrm{Var}[\hat{\mu}] = \nu / m$ , where  $\nu$  is the true variance of  $f$ . Thus, doubling the number of samples  $m$  tends to halve the variance of the sample mean.

# 21.2 Taylor Approximation

Another way to estimate  $\hat{\mu}$  and  $\hat{v}$  is to use the Taylor series approximation for  $f$  at a fixed design point  $\mathbf{x}$ . For the moment, we will assume that the  $n$  components of  $\mathbf{z}$  are uncorrelated and have finite variance. We will denote the mean of the distribution over  $\mathbf{z}$  as  $\mu$  and the variances of the individual components of  $\mathbf{z}$  as  $\nu$ . The following is the second-order Taylor series approximation of  $f(\mathbf{z})$  at the point  $\mathbf{z} = \boldsymbol{\mu}$ :

$$
\hat {f} (\mathbf {z}) = f (\boldsymbol {\mu}) + \sum_{i = 1}^{n} \frac{\partial f}{\partial z_{i}} (z_{i} - \mu_{i}) + \frac{1}{2} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \frac{\partial^{2} f}{\partial z_{i} \partial z_{j}} (z_{i} - \mu_{i}) (z_{j} - \mu_{j}) \qquad (21. 3)
$$

From this approximation, we can analytically compute estimates of the mean and variance of  $f$ :

$$
\hat {\mu} = f (\boldsymbol {\mu}) + \frac{1}{2} \sum_{i = 1}^{n} \left. \frac{\partial^{2} f}{\partial z_{i}^{2}} v_{i} \right| _{\mathbf {z} = \boldsymbol {\mu}} \tag {21.4}
$$

$$
\hat {v} = \sum_{i = 1}^{n} \left(\frac{\partial f}{\partial z_{i}}\right) ^{2} v_{i} + \frac{1}{2} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \left(\frac{\partial^{2} f}{\partial z_{i} \partial z_{j}}\right) ^{2} v_{i} v_{j} \Bigg | _{\mathbf {z} = \boldsymbol {\mu}} \tag {21.5}
$$

The higher-order terms can be neglected to obtain a first-order approximation:

$$
\hat {\mu} = f (\boldsymbol {\mu}) \quad \hat {v} = \sum_{i = 1}^{n} \left(\frac{\partial f}{\partial z_{i}}\right) ^{2} v_{i} \Bigg | _{\mathbf {z} = \boldsymbol {\mu}} \tag {21.6}
$$

We can relax the assumption that the components of  $\mathbf{z}$  are uncorrelated, but it makes the mathematics more complex. In practice, it can be easier to transform the random variables so that they are uncorrelated. We can transform a vector of  $n$  correlated random variables  $\mathbf{c}$  with covariance matrix  $\mathbf{C}$  into  $m$  uncorrelated

3 For a derivation of the mean and variance of a general function of  $n$  random variables, see H. Benaroya and S.M. Han, Probability Models in Engineering and Science. Taylor & Francis, 2005.  
4 If the components of  $\mathbf{z}$  are uncorrelated, then the covariance matrix is diagonal and  $\mathbf{v}$  would be the vector composed of the diagonal elements.

random variables  $\mathbf{z}$  by multiplying by an orthogonal  $m \times n$  matrix  $\mathbf{T}$  containing the eigenvectors corresponding to the  $m$  largest eigenvalues of  $\mathbf{C}$ . We have  $\mathbf{z} = \mathbf{Tc}$ .

The Taylor approximation method is implemented in algorithm 21.1. First- and second-order approximations are compared in example 21.1.

```julia
using ForwardDiff  
function taylor_approx(f,  $\mu$ ,  $\nu$ , secondorder=false)  
 $\mu$ hat = f( $\mu$ )  
 $\nabla = (z \rightarrow$  ForwardDiffgradient(f, z))( $\mu$ )  
 $\nu$ hat =  $\nabla \cdot 2 \cdot \nu$   
if secondorder  
 $\mathsf{H} = (\mathsf{z} \rightarrow$  ForwardDiff.hessian(f, z))( $\mu$ )  
 $\mu$ hat += (diag(H)  $\cdot \nu$ )/2  
 $\nu$ hat +=  $\nu \cdot (\mathsf{H} \cdot 2 \star \nu)$ /2  
end  
return ( $\mu$ hat,  $\nu$ hat)  
end
```

It is also common to scale the outputs such that the covariance matrix becomes the identity matrix. This process is known as whitening. J.H. Friedman, "Exploratory Projection Pursuit," Journal of the American Statistical Association, vol. 82, no. 397, pp. 249-266, 1987.

Algorithm 21.1. A method for automatically computing the Taylor approximation of the mean and variance of objective function  $f$  at design point  $x$  with noise mean vector  $\mu$  and variance vector  $\nu$ . The Boolean parameter secondorder controls whether the first- or second-order approximation is computed.

# 21.3 Polynomial Chaos

Polynomial chaos is a method for fitting a polynomial to evaluations of  $f(\mathbf{z})$  and using the resulting surrogate model to estimate the mean and variance. We will begin this section by discussing how polynomial chaos is used in the univariate case. We will then generalize the concept to multivariate functions and show how to obtain estimates of the mean and variance by integrating the function represented by our surrogate model.

# 21.3.1 Univariate

In one dimension, we approximate  $f(z)$  with a surrogate model consisting of  $k$  polynomial basis functions,  $b_{1},\ldots ,b_{k}$ :

$$
f (z) \approx \hat {f} (z) = \sum_{i = 1}^{k} \theta_{i} b_{i} (z) \tag {21.7}
$$

In contrast with the Monte Carlo methods discussed in section 21.1, our samples of  $z$  do not have to be randomly drawn from  $p$ . In fact, it may be desirable to obtain samples using one of the sampling plans discussed in chapter 16. We will discuss how to obtain the basis coefficients in section 21.3.2.

Consider the objective function  $f(x,z) = \sin (x + z_1)\cos (x + z_2)$ , where  $z_{1}$  and  $z_{2}$  are zero-mean Gaussian noise with variances 0.1 and 0.2, respectively.

The first and second partial derivatives of  $f$  with respect to the  $zs$  are

$$
\begin{array}{l} \frac{\partial f}{\partial z_{1}} = \cos (x + z_{1}) \cos (x + z_{2}) \quad \frac{\partial^{2} f}{\partial z_{2}^{2}} = - \sin (x + z_{1}) \cos (x + z_{2}) \\ \frac{\partial f}{\partial z_{2}} = - \sin (x + z_{1}) \sin (x + z_{2}) \quad \frac{\partial^{2} f}{\partial z_{1} \partial z_{2}} = - \cos (x + z_{1}) \sin (x + z_{2}) \\ \frac{\partial^{2} f}{\partial z_{1}^{2}} = - \sin (x + z_{1}) \cos (x + z_{2}) \\ \end{array}
$$

which allow us to construct the Taylor approximation using equation (21.4) and equation (21.5):

$$
\begin{array}{l} \hat {\mu} (x) = 0. 85 \sin (x) \cos (x) \\ \hat {v} (x) = 0. 1 \cos^{4} (x) + 0. 2 \sin^{4} (x) + 0. 04 5 \sin^{2} (x) \cos^{2} (x) \\ \end{array}
$$

We can use taylor_approx for a given  $x$  using:

```python
taylor_approx(z  $\rightarrow$  sin(x+z[1])*cos(x+z[2]), [0,0], [0.1,0.2])

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/548d7c13e63216abc456b1dcaa48b957f2e0d48efbc0b4b7b5498cb51bf958fd.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/51ed83e0e9a976c8aadadd1d1649cf5920de1b53f9c31b4cd16c2ac72697b380.jpg)

true first-order Taylor approx. second-order Taylor approx.

Example 21.1. The Taylor approximation applied to a univariate design problem with two-dimensional Gaussian noise.

We can use the surrogate model  $\hat{f}$  to estimate the mean:

$$
\begin{array}{l} \hat {\mu} = \mathbb {E} [ \hat {f} ] (21.8) \\ = \int_{\mathcal {Z}} \hat {f} (z) p (z) d z (21.9) \\ = \int_{\mathcal {Z}} \sum_{i = 1}^{k} \theta_{i} b_{i} (z) p (z) d z (21.10) \\ = \sum_{i = 1}^{k} \theta_{i} \int_{\mathcal {Z}} b_{i} (z) p (z) d z (21.11) \\ = \theta_{1} \int_{\mathcal {Z}} b_{1} (z) p (z) d z + \dots + \theta_{k} \int_{\mathcal {Z}} b_{k} (z) p (z) d z (21.12) \\ \end{array}
$$

We can also estimate the variance:

$$
\begin{array}{l} \hat {v} = \mathbb {E} \left[ \left(\hat {f} - \mathbb {E} [ \hat {f} ]\right) ^{2} \right] (21.13) \\ = \mathbb {E} \left[ \hat {f}^{2} - 2 \hat {f} \mathbb {E} [ \hat {f} ] + \mathbb {E} [ \hat {f} ] ^{2} \right] (21.14) \\ = \mathbb {E} \left[ \hat {f}^{2} \right] - \mathbb {E} \left[ \hat {f} \right] ^{2} (21.15) \\ = \int_{\mathcal {Z}} \hat {f} (z) ^{2} p (z) d z - \mu^{2} (21.16) \\ = \int_{\mathcal {Z}} \sum_{i = 1}^{k} \sum_{j = 1}^{k} \theta_{i} \theta_{j} b_{i} (z) b_{j} (z) p (z) d z - \mu^{2} (21.17) \\ = \int_{\mathcal {Z}} \left(\sum_{i = 1}^{k} \theta_{i}^{2} b_{i} (z) ^{2} + 2 \sum_{i = 2}^{k} \sum_{j = 1}^{i - 1} \theta_{i} \theta_{j} b_{i} (z) b_{j} (z)\right) p (z) d z - \mu^{2} (21.18) \\ = \sum_{i = 1}^{k} \theta_{i}^{2} \int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z + 2 \sum_{i = 2}^{k} \sum_{j = 1}^{i - 1} \theta_{i} \theta_{j} \int_{\mathcal {Z}} b_{i} (z) b_{j} (z) p (z) d z - \mu^{2} (21.19) \\ \end{array}
$$

The mean and variance can be efficiently computed if the basis functions are chosen to be orthogonal under  $p$ . Two basis functions  $b_{i}$  and  $b_{j}$  are orthogonal with respect to a probability density  $p(z)$  if

$$
\int_{\mathcal {Z}} b_{i} (z) b_{j} (z) p (z) d z = 0 \quad \text{if } i \neq j \tag {21.20}
$$

If the chosen basis functions are all orthogonal to one another and the first basis function is  $b_{1}(z) = 1$ , the mean is:

$$
\begin{array}{l} \hat {\mu} = \theta_{1} \int_{\mathcal {Z}} b_{1} (z) p (z) d z + \theta_{2} \int_{\mathcal {Z}} b_{2} (z) p (z) d z + \dots + \theta_{k} \int_{\mathcal {Z}} b_{k} (z) p (z) d z (21.21) \\ = \theta_{1} \int_{\mathcal {Z}} b_{1} (z) ^{2} p (z) d z + \theta_{2} \int_{\mathcal {Z}} b_{1} (z) b_{2} (z) p (z) d z + \dots + \theta_{k} \int_{\mathcal {Z}} b_{1} (z) b_{k} (z) p (z) d z (21.22) \\ = \theta_{1} \int_{\mathcal {Z}} p (z) d z + 0 + \dots + 0 (21.23) \\ = \theta_{1} (21.24) \\ \end{array}
$$

Similarly, the variance is:

$$
\begin{array}{l} \hat {v} = \sum_{i = 1}^{k} \theta_{i}^{2} \int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z + 2 \sum_{i = 2}^{k} \sum_{j = 1}^{i - 1} \theta_{i} \theta_{j} \int_{\mathcal {Z}} b_{i} (z) b_{j} (z) p (z) d z - \mu^{2} (21.25) \\ = \sum_{i = 1}^{k} \theta_{i}^{2} \int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z - \mu^{2} (21.26) \\ = \theta_{1}^{2} \int_{\mathcal {Z}} b_{1} (z) ^{2} p (z) d z + \sum_{i = 2}^{k} \theta_{i}^{2} \int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z - \theta_{1}^{2} (21.27) \\ = \sum_{i = 2}^{k} \theta_{i}^{2} \int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z (21.28) \\ \end{array}
$$

The mean thus falls immediately from fitting a surrogate model to the observed data, and the variance can be very efficiently computed given the values  $\int_{\mathcal{Z}} b_i(z)^2 p(z) dz$  for a choice of basis functions and probability distribution. Example 21.2 uses these procedures to estimate the mean and variance with different sample sizes.

Polynomial chaos approximates the function using  $k$ th degree orthogonal polynomial basis functions with  $i$  in  $1: k + 1$  and  $b_{1} = 1$ . All orthogonal polynomials satisfy the recurrence relation:

$$
b_{i + 1} (z) = \left\{ \begin{array}{l l} \left(z - \alpha_{i}\right) b_{i} (z) & \text{for} i = 1 \\ \left(z - \alpha_{i}\right) b_{i} (z) - \beta_{i} b_{i - 1} (z) & \text{for} i > 1 \end{array} \right. \tag {21.29}
$$

with  $b_{1}(z) = 1$  and weights

$$
\alpha_{i} = \frac{\int_{\mathcal {Z}} z b_{i} (z) ^{2} p (z) d z}{\int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z} \tag {21.30}
$$

$$
\beta_{i} = \frac{\int_{\mathcal {Z}} b_{i} (z) ^{2} p (z) d z}{\int_{\mathcal {Z}} b_{i - 1} (z) ^{2} p (z) d z} \tag {21.31}
$$

6 Integrals of this form can be efficiently computed using Gaussian quadrature, covered in appendix C.9.

Consider optimizing the (unknown) objective function

$$
f (x, z) = 1 - e^{- (x + z - 1) ^{2}} - 2 e^{- (x + z - 3) ^{2}}
$$

with  $z$  known to be drawn from a zero-mean unit-Gaussian distribution.

The objective function, its true expected value, and estimated expected values with different sample counts are plotted below. The estimated expected value is computed using third-order Hermite polynomials.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/da2147f97baf687edaa76dac813d9614b7b36214290d3cbdf52620a1bcc38803.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4b2dac3d59693d7eda2137c0a0ca8268491109da1276bdcc889b611356767fce.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ccf61572f484e23f54397173466ad4c36d5ce1153fb166193ff98f6d0710bed2.jpg)

Example 21.2. Estimating the expected value of an unknown objective function using polynomial chaos. Hermite polynomials are defined in table 21.1.

The recurrence relation can be used to generate the basis functions. Each basis function  $b_{i}$  is a polynomial of degree  $i - 1$ . The basis functions for several common probability distributions are given in table 21.1, can be generated using the methods in algorithm 21.2, and are plotted in figure 21.1. Example 21.3 illustrates the effect the polynomial order has on the estimates of the mean and variance.

<table><tr><td>Distribution</td><td>Domain</td><td>Density</td><td>Name</td><td>Recursive Form</td><td>Closed Form</td></tr><tr><td>Uniform</td><td>[-1,1]</td><td>1/2</td><td>Legendre</td><td>Le_k(x) = 1/2^k! \frac{d^k}{dx^k} [ (x^2 - 1)^k ]</td><td>bi(x) = ∑j=0i-1 (i-1)/j (−i-2)/j (1-x/2)^j</td></tr><tr><td>Exponential</td><td>[0,∞)</td><td>e^-x</td><td>Laguerre</td><td>\(\frac{d}{dx}\mathrm{La}_k(x)=\left(\frac{d}{dx}-1\right)\mathrm{La}_{k-1}\)</td><td>bi(x) = ∑j=0i-1 (i-1)/j (−1)^j x^j</td></tr><tr><td>Unit Gaussian</td><td>(−∞,∞)</td><td>\(\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\)</td><td>Hermite</td><td>H_k(x) = xH_{k-1} - \(\frac{d}{dx}\mathrm{H}_{k-1}\)</td><td>bi(x) = \(\sum_{j=0}^{\lfloor(i-1)/2\rfloor}(i-1)!\frac{(-1)^{\frac{i-1}{2}-j}}{(2j)!(\frac{i-1}{2}-j)!}(2x)^{2j}\)</td></tr></table>

Basis functions for arbitrary probability density functions and domains can be constructed both analytically and numerically. The Stieltjes algorithm (algorithm 21.3) generates orthogonal polynomials using the recurrence relation in equation (21.29). Example 21.4 shows how the polynomial order affects the estimates of the mean and variance.

# 21.3.2 Coefficients

The coefficients  $\theta_{1},\ldots ,\theta_{k}$  in equation (21.7) can be inferred in two different ways. The first way is to fit the values of the samples from  $\mathcal{Z}$  using the linear regression method discussed in section 17.3. The second way is to exploit the orthogonality of the basis functions, producing an integration term amenable to Gaussian quadrature.

Table 21.1. Orthogonal polynomial basis functions for several common probability distributions.

7 The polynomials can be scaled by a nonzero factor. It is convention to set  $b_{1}(x) = 1$ .  
T.J. Stieltjes, "Quelques Recherches sur la Theorie des Quadratures Dites Mécaniques," Annales Scientifiques de l'École Normale Supérieure, vol. 1, pp. 409-426, 1884. in French. An overview in English is provided by W. Gautschi, Orthogonal Polynomials: Computation and Approximation. Oxford University Press, 2004.

import Polynomials: Polynomial, derivative, integrate  
```matlab
function legendre(i)  
n = i-1  
p = Polynomial([-1,0,1])^n  
for i in 1 : n  
    p = derivative(p)  
end  
return p / (2^n * factorial(n))  
end  
function laguerre(i)  
p = Polynomial([1])  
for j in 2 : i  
    p = integrate(derivative(p) - p) + 1  
end  
return p  
end  
function hermite(i)  
p = Polynomial([1])  
x = Polynomial([0,1])  
for j in 2 : i  
    p = x*p - derivative(p)  
end  
return p
```

Algorithm 21.2. Methods for constructing polynomial orthogonal basis functions, where  $\mathbf{i}$  indicates the construction of  $b_{i}$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bf028b06edd5af3e19ade7af02bb51d39254d2066ef51321abb673a8b47d2196.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/100a126fec07b8f566eb9c415d342befecb08847090bc196a24d6625ff93bfa8.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/02865cbe3ffcebe653f1dd4083d05b27b2eb99fe9f7377b29d3b77ce41e62de3.jpg)  
Figure 21.1. Orthogonal basis functions for uniform, exponential, and unit Gaussian distributions.

Consider the function  $f(z) = \sin(\pi z)$  with input  $z$  drawn from a uniform distribution over the domain  $[-1, 1]$ . The true mean and variance can be computed analytically:

$$
\mu = \int_{a}^{b} f (z) p (z) d z = \int_{- 1}^{1} \sin (\pi z) \frac{1}{2} d z = 0 \tag {21.32}
$$

$$
\nu = \int_{a}^{b} f (z) ^{2} p (z) d z - \mu^{2} = \int_{- 1}^{1} \sin^{2} (\pi z) \frac{1}{2} d z - 0 = \frac{1}{2} \tag {21.33}
$$

Suppose we have five samples of  $f$  at  $z = \{-1, -0.2, 0.3, 0.7, 0.9\}$ . We can fit a Legendre polynomial to the data to obtain our surrogate model  $\hat{f}$ . Polynomials of different degrees yield:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/86d70ca93bc3328637a81fb4c2d13793bf71089a35fabce880d0bff8a19f92c8.jpg)

```txt
true  $i = 1$ $\mu = +0.268$ $\nu = 0.000$ $i = 2$ $\mu = +0.207$ $\nu = 0.063$ $i = 3$ $\mu = +0.170$ $\nu = 0.075$ $i = 4$ $\mu = -0.056$ $\nu = 0.573$ $i = 5$ $\mu = -0.136$ $\nu = 0.757$
```

```matlab
function orthogonal_recurrence(bs, p, dom,  $\epsilon = 1\mathrm{e} - 6$    
i  $=$  length(bs)   
c1  $=$  quadgk(z  $\rightarrow$  z\*bs[i](z)\*2\*p(z), dom...，atol=ε)[1]   
c2  $=$  quadgk(z  $\rightarrow$  bs[i](z)\*2\*p(z), dom...，atol=ε)[1]   
 $\alpha = c1 / c2$    
if i > 1   
c3  $=$  quadgk(z  $\rightarrow$  bs[i-1](z)\*2\*p(z)，dom...，atol=ε)[1]   
β  $=$  c2/c3   
return Polynomial([-α, 1])*bs[i] - β\*bs[i-1]   
else   
return Polynomial([-α, 1])*bs[i]   
end
```

Example 21.3. Legendre polynomials used to estimate the mean and variance of a function with a uniformly distributed input.

Algorithm 21.3. The Stieltjes algorithm for constructing the next polynomial basis function  $b_{i+1}$  according to the orthogonal recurrence relation, where  $\mathsf{bs}$  contains  $\{b_1, \ldots, b_i\}$ ,  $\mathsf{p}$  is the probability distribution, and  $\mathsf{dom}$  is a tuple containing a lower and upper bound for  $z$ . The optional parameter  $\epsilon$  controls the absolute tolerance of the numerical integration. We make use of the Polynomials.jl package.

Consider the function  $f(z) = \sin(\pi z)$  with input  $z$  drawn from a truncated Gaussian distribution with mean 3 and variance 1 over the domain [2, 5]. The true mean and variance are:

$$
\mu = \int_{a}^{b} f (z) p (z) d z = \int_{2}^{5} \sin (\pi z) p (z) d z \approx 0. 10 4
$$

$$
\nu = \int_{a}^{b} f (z) ^{2} p (z) d z - \mu^{2} = \int_{2}^{5} \sin^{2} (\pi z) p (z) d z - 0. 10 4^{2} \approx 0. 49 5
$$

where the probability density of the truncated Gaussian is:

$$
p (z) = \left\{ \begin{array}{l l} \frac{\mathcal {N} (z | 3 , 1)}{\int_{2}^{5} \mathcal {N} (\tau | 3 , 1)   d \tau} & \text{if } z \in [ 2, 5 ] \\ 0 & \text{otherwise} \end{array} \right.
$$

Suppose we have five samples of  $f$  at  $z = \{2.1, 2.5, 3.3, 3.9, 4.7\}$ . We can fit orthogonal polynomials to the data to obtain our surrogate model  $\hat{f}$ . Polynomials of different degrees yield:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c3aaae1bb4ad6b05a6de9f65aa2e4496a7be84747e69e1b42f35783d532dbabb.jpg)  
Z

true  
$i = 1\quad \mu = +0.200\quad \nu = -0.000$  
$i = 2\quad \mu = +0.204\quad \nu = 0.001$  
$i = 3\quad \mu = -0.036\quad \nu = 0.159$  
$i = 4\quad \mu = +0.037\quad \nu = 0.273$  
$i = 5\quad \mu = +0.055\quad \nu = 0.478$

Example 21.4. Legendre polynomials constructed using the Stieltjes method to estimate the mean and variance of a function with a random variable input.

We multiply each side of equation (21.7) by the  $j$ th basis and our probability density function and integrate:

$$
f (z) \approx \sum_{i = 1}^{k} \theta_{i} b_{i} (z) \tag {21.34}
$$

$$
\begin{array}{l} \int_{\mathcal {Z}} f (z) b_{j} (z) p (z) d z \approx \int_{\mathcal {Z}} \left(\sum_{i = 1}^{k} \theta_{i} b_{i} (z)\right) b_{j} (z) p (z) d z (21.35) \\ = \sum_{i = 1}^{k} \theta_{i} \int_{\mathcal {Z}} b_{i} (z) b_{j} (z) p (z) d z (21.36) \\ = \theta_{j} \int_{\mathcal {Z}} b_{j} (z) ^{2} p (z) d z (21.37) \\ \end{array}
$$

where we made use of the orthogonality property from equation (21.20).

It follows that the  $j$ th coefficient is:

$$
\theta_{j} = \frac{\int_{\mathcal {Z}} f (z) b_{j} (z) p (z) d z}{\int_{\mathcal {Z}} b_{j} (z) ^{2} p (z) d z} \tag {21.38}
$$

The denominator of equation (21.38) typically has a known analytic solution or can be inexpensively precomputed. Calculating the coefficient thus primarily requires solving the integral in the numerator, which can be done numerically using Gaussian quadrature.<sup>9</sup>

# 21.3.3 Multivariate

Polynomial chaos can be applied to functions with multiple random inputs. Multivariate basis functions over  $m$  variables are constructed as a product over univariate orthogonal polynomials:

$$
b_{i} (\mathbf {z}) = \prod_{j = 1}^{m} b_{a_{j}} \left(z_{j}\right) \tag {21.39}
$$

where  $\mathbf{a}$  is an assignment vector that assigns the  $a_{j}$ th basis function to the  $j$ th random component. This basis function construction is demonstrated in example 21.5.

A common method for constructing multivariate basis functions is to generate univariate orthogonal polynomials for each random variable and then to construct a multivariate basis function for every possible combination. $^{10}$  This procedure

9 Gaussian quadrature is implemented in QuadGK.jl via the quadgk function, and is covered in appendix C.9. Quadrature rules can also be obtained using the eigenvalues and eigenvectors of a tri-diagonal matrix formed using the coefficients  $\alpha_{i}$  and  $\beta_{i}$  from equations (21.30) and (21.31). G.H. Golub and J.H. Welsch, "Calculation of Gauss Quadrature Rules," Mathematics of Computation, vol. 23, no. 106, pp. 221-230, 1969.

Here the number of multivariate exponential basis functions grows exponentially in the number of variables.

Consider a three-dimensional polynomial chaos model of which one of the multidimensional basis functions is  $b(\mathbf{z}) = b_{3}(z_{1})b_{1}(z_{2})b_{3}(z_{3})$ . The corresponding assignment vector is  $\mathbf{a} = [3,1,3]$ .

is implemented in algorithm 21.4. Constructing basis functions in this manner assumes that the variables are independent. Interdependence can be resolved using the same transformation discussed in section 21.2.

```matlab
function polynomial Chaosbases(bases1d) bases  $= []$  for a in Iterators.product(bases1d...) push!(bases,  $\mathbf{z}\rightarrow \mathrm{prod}(\mathbf{b}(\mathbf{z}[\mathbf{i}])$  for (i,b) in enumerate(a))) end return bases   
end
```

A multivariate polynomial chaos approximation with  $k$  basis functions is still a linear combination of terms

$$
f (\mathbf {z}) \approx \hat {f} (\mathbf {z}) = \sum_{i = 1}^{k} \theta_{i} b_{i} (\mathbf {z}) \tag {21.40}
$$

where the mean and variance can be computed using the equations in section 21.3.1, provided that  $b_{1}(\mathbf{z}) = 1$ .

# 21.4 Bayesian Monte Carlo

Gaussian processes, covered in chapter 19, are probability distributions over functions. They can be used as surrogates for stochastic objective functions. We can incorporate prior information, such as the expected smoothness of the objective function, in a process known as Bayesian Monte Carlo or Bayes-Hermite Quadrature.

Consider a Gaussian process fit to several points with the same value for the design point  $\mathbf{x}$  but different values for the uncertain point  $\mathbf{z}$ . The Gaussian process obtained is a distribution over functions based on the observed data. When obtaining the expected value through integration, we must consider the expected value of the functions in the probability distribution represented by the

Example 21.5. Constructing a multivariate polynomial chaos basis function using equation (21.39).

Algorithm 21.4. A method for constructing multivariate basis functions where bases1d contains lists of univariate orthogonal basis functions for each random variable.

Gaussian process  $p(\hat{f})$ :

$$
\begin{array}{l} \mathbb {E}_{\mathbf {z} \sim p} [ f ] \approx \mathbb {E}_{\hat {f} \sim p (\hat {f})} \left[ \mathbb {E}_{\mathbf {z} \sim p} \left[ \hat {f} \right] \right] (21.41) \\ = \int_{\hat {\mathcal {F}}} \left(\int_{\mathcal {Z}} \hat {f} (\mathbf {z}) p (\mathbf {z}) d \mathbf {z}\right) p (\hat {f}) d \hat {f} (21.42) \\ = \int_{\mathcal {Z}} \left(\int_{\hat {\mathbf {f}}} \hat {f} (\mathbf {z}) p (\hat {f}) d \hat {f}\right) p (\mathbf {z}) d \mathbf {z} (21.43) \\ = \int_{\mathcal {Z}} \hat {\mu} (\mathbf {z}) p (\mathbf {z}) d \mathbf {z} (21.44) \\ \end{array}
$$

where  $\hat{\mu} (\mathbf{z})$  is the predicted mean under the Gaussian process and  $\hat{\mathcal{F}}$  is the space of functions. The variance of the estimate is

$$
\begin{array}{l} \operatorname{Var}_{\mathbf {z} \sim p} [ f ] \approx \operatorname{Var}_{\hat {f} \sim p (\hat {f})} \left[ \mathbb {E}_{\mathbf {z} \sim p} [ \hat {f} ] \right] (21.45) \\ = \int_{\hat {f}} \left(\int_{\mathcal {Z}} \hat {f} (\mathbf {z}) p (\mathbf {z}) d \mathbf {z} - \int_{\mathcal {Z}} \mathbb {E}_{\hat {f} \sim p (\hat {f})} [ \hat {f} \left(\mathbf {z}^{\prime}\right) ] p \left(\mathbf {z}^{\prime}\right) d \mathbf {z}^{\prime}\right) ^{2} p (\hat {f}) d \hat {f} (21.46) \\ = \int_{\mathcal {Z}} \int_{\mathcal {Z}} \int_{\hat {\mathbf {F}}} \left[ \hat {f} (\mathbf {z}) - \mathbb {E}_{\hat {f} \sim p (\hat {f})} [ \hat {f} (\mathbf {z}) ] \right] \left[ \hat {f} \left(\mathbf {z}^{\prime}\right) - \mathbb {E}_{\hat {f} \sim p (\hat {f})} [ \hat {f} \left(\mathbf {z}^{\prime}\right) ] \right] p (\hat {f}) d \hat {f} p (\mathbf {z}) p \left(\mathbf {z}^{\prime}\right) d \mathbf {z} d \mathbf {z}^{\prime} (21.47) \\ = \int_{\mathcal {Z}} \int_{\mathcal {Z}} \operatorname{Cov} \left(\hat {f} (\mathbf {z}), \hat {f} \left(\mathbf {z}^{\prime}\right)\right) p (\mathbf {z}) p \left(\mathbf {z}^{\prime}\right) d \mathbf {z} d \mathbf {z}^{\prime} (21.48) \\ \end{array}
$$

where Cov is the posterior covariance under the Gaussian process:

$$
\operatorname{Cov} \left(\hat {f} (\mathbf {z}), \hat {f} \left(\mathbf {z}^{\prime}\right)\right) = k \left(\mathbf {z}, \mathbf {z}^{\prime}\right) - k (\mathbf {z}, Z) \mathbf {K} (Z, Z) ^{- 1} k (Z, \mathbf {z}^{\prime}) \tag {21.49}
$$

where  $Z$  contains the observed inputs.

Analytic expressions exist for the mean and variance for the special case where  $\mathbf{z}$  is Gaussian.11 Under a Gaussian kernel,

$$
k \left(\mathbf {x}, \mathbf {x}^{\prime}\right) = \exp \left(- \frac{1}{2} \sum_{i = 1}^{n} \frac{\left(x_{i} - x_{i}^{\prime}\right) ^{2}}{w_{i}^{2}}\right) \tag {21.50}
$$

the mean for Gaussian uncertainty  $\mathbf{z}\sim \mathcal{N}(\boldsymbol{\mu}_{\mathbf{z}},\boldsymbol{\Sigma}_{\mathbf{z}})$  is

$$
\mathbb {E}_{\mathbf {z} \sim p} [ f ] \approx \mathbb {E}_{\hat {f} \sim p (\hat {f})} \left[ \mathbb {E}_{\mathbf {z} \sim p} [ \hat {f} ] \right] = \mathbf {q}^{\top} \mathbf {K}^{- 1} \mathbf {y} \tag {21.51}
$$

with

$$
q_{i} = \left| \mathbf {W}^{- 1} \boldsymbol {\Sigma}_{\mathbf {z}} + \mathbf {I} \right| ^{- 1 / 2} \exp \left(- \frac{1}{2} \left(\boldsymbol {\mu}_{\mathbf {z}} - \mathbf {z}^{(i)}\right) ^{\top} \left(\boldsymbol {\Sigma}_{\mathbf {z}} + \mathbf {W}\right) ^{- 1} \left(\boldsymbol {\mu}_{\mathbf {z}} - \mathbf {z}^{(i)}\right)\right) \tag {21.52}
$$

It is also required that the covariance function obey the product correlation rule, that is, it can be written as the product of a univariate positive-definite function  $r$ :

$$
k (\mathbf {x}, \mathbf {x}^{\prime}) = \prod_{i = 1}^{n} r \left(x_{i} - x_{i}^{\prime}\right)
$$

Analytic results exist for polynomial kernels and mixtures of Gaussians. C.E. Rasmussen and Z. Ghahramani, "Bayesian Monte Carlo," in Advances in Neural Information Processing Systems (NIPS), 2003.

where  $\mathbf{W} = \mathrm{diag}[w_1^2, \ldots, w_n^2]$ , and we have constructed our Gaussian process using samples  $(\mathbf{z}^{(i)}, y_i)$  for  $i$  in  $1:m$ .<sup>12</sup>

The variance is

$$
\operatorname{Var}_{\mathbf {z} \sim p} [ f ] = | 2 \mathbf {W}^{- 1} \boldsymbol {\Sigma}_{\mathbf {z}} + \mathbf {I} | ^{- 1 / 2} - \mathbf {q}^{\top} \mathbf {K}^{- 1} \mathbf {q} \tag {21.53}
$$

Even when the analytic expressions are not available, there are many problems for which numerically evaluating the expectation is sufficiently inexpensive that the Gaussian process approach is better than a Monte Carlo estimation.

Bayesian Monte Carlo is implemented in algorithm 21.5 and is worked out in example 21.6.

```julia
function bayesian_monte_carlo(GP, w,  $\mu z$ ,  $\Sigma z$ )  
W = Matrix(Diagonal(w.^2))  
invK = inv(K(GP.X, GP.X, GP.k))  
q = [exp(-((z -  $\mu z$ ) * (inv(W +  $\Sigma z$ ) * (z -  $\mu z$ )) / 2) for z in GP.X]  
q *= (det(W  $\Sigma z + I$ )^(-0.5)  
 $\mu = q^{\prime} * \text{invK} * \text{GP}.y$ $\nu = (\text{det}(2W \backslash \Sigma z + I))^{\wedge}(-0.5) - (q^{\prime} * \text{invK} * q)[1]$   
return ( $\mu$ ,  $\nu$ )  
end
```

12 See A. Girard, C.E. Rasmussen, J.Q. Candela, and R. Murray-Smith, "Gaussian Process Priors with Uncertain Inputs—Application to Multiple-Step Ahead Time Series Forecasting," in Advances in Neural Information Processing Systems (NIPS), 2003.

Algorithm 21.5. A method for obtaining the Bayesian Monte Carlo estimate for the expected value of a function under a Gaussian process  $\mathbf{G}$  with a Gaussian kernel with weights  $w$ , where the variables are drawn from a normal distribution with mean  $\mu z$  and covariance  $\Sigma z$ .

# 21.5 Summary

- The expected value and variance of the objective function are useful when optimizing problems involving uncertainty, but computing these quantities reliably can be challenging.  
- One of the simplest approaches is to estimate the moments using sampling in a process known as Monte Carlo integration.  
- Other approaches, such as the Taylor approximation, use knowledge of the objective function's partial derivatives.  
- Polynomial chaos is a powerful uncertainty propagation technique based on orthogonal polynomials.  
- Bayesian Monte Carlo uses Gaussian processes to efficiently arrive at the moments with analytic results for Gaussian kernels.

Consider again estimating the expected value and variance of  $f(x, z) = \sin (x + z_1) \cos (x + z_2)$ , where  $z_1$  and  $z_2$  are zero-mean Gaussian noise with variances 1 and 1/2, respectively:  $\mu_{\mathbf{z}} = [0, 0]$  and  $\Sigma_{\mathbf{z}} = \mathrm{diag}([1, 1/2])$ .

We use Bayesian Monte Carlo with a Gaussian kernel with unit weights  $\mathbf{w} = [1,1]$  for  $x = 0$  with samples  $Z = \{[0,0],[1,0],[-1,0],[0,1],[0,-1]\}$ .

We compute:

$$
\mathbf {W} = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right]
$$

$$
\mathbf {K} = \left[ \begin{array}{c c c c c} 1 & 0. 60 7 & 0. 60 7 & 0. 60 7 & 0. 60 7 \\ 0. 60 7 & 1 & 0. 13 5 & 0. 36 8 & 0. 36 8 \\ 0. 60 7 & 0. 13 5 & 1 & 0. 36 8 & 0. 36 8 \\ 0. 60 7 & 0. 36 8 & 0. 36 8 & 1 & 0. 13 5 \\ 0. 60 7 & 0. 36 8 & 0. 36 8 & 0. 13 5 & 1 \end{array} \right]
$$

$$
\mathbf {q} = \left[ 0. 57 7, 0. 45 0, 0. 45 0, 0. 41 7, 0. 41 7 \right]
$$

$$
\mathbb {E}_{\mathbf {Z} \sim p} [ f ] = 0. 0
$$

$$
\operatorname{Var}_{\mathbf {z} \sim p} [ f ] = 0. 32 7
$$

Below we plot the expected value as a function of  $x$  using the same approach with ten random samples of  $\mathbf{z}$  at each point.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/d9f1244ae56fa40b83bbc5e1b30a054d43be227d0ae6512f42f0112fed633f8a.jpg)

Example 21.6. An example of using Bayesian Monte Carlo to estimate the expected value and variance of a function.

The figure compares the Bayesian Monte Carlo method to the sample mean for estimating the expected value of a function. The same randomly sampled  $\mathbf{z}$  values generated for each evaluated  $x$  were input to each method.

# 21.6 Exercises

Exercise 21.1. Suppose we draw a sample from a univariate Gaussian distribution. What is the probability that our sample falls within one standard deviation of the mean  $(x\in [\mu -\sigma ,\mu +\sigma ])$ ? What is the probability that our sample is less than one standard deviation above the mean  $(x < \mu +\sigma)$ ?

Solution: The cumulative distribution function can be used to calculate these values:

```julia
julia> using Distributions  
julia> N = Normal(0,1);  
julia> cdf(N, 1) - cdf(N, -1)  
0.6826894921370861  
julia> cdf(N, 1)  
0.841344746068543
```

Thus, our sample falls within one standard deviation of the mean about  $68.3\%$  of the time and is less than one standard deviation above the mean  $84.1\%$  of the time.

Exercise 21.2. Let  $x^{(1)}, x^{(2)}, \ldots, x^{(m)}$  be a random sample of independent, identically distributed values of size  $m$  from a distribution with mean  $\mu$  and variance  $\nu$ . Show that the variance of the sample mean  $\operatorname{Var}(\hat{\mu})$  is  $\nu / m$ .

Solution: We begin by substituting in the definition of the sample mean:

$$
\begin{array}{l} \operatorname{Var} (\hat {\mu}) = \operatorname{Var} \left(\frac{x^{(1)} + x^{(2)} + \cdots + x^{(m)}}{m}\right) \\ = \operatorname{Var} \left(\frac{1}{m} x^{(1)} + \frac{1}{m} x^{(2)} + \dots + \frac{1}{m} x^{(m)}\right) \\ \end{array}
$$

The variance of the sum of two independent variables is the sum of the variances of the two variables. It follows that:

$$
\begin{array}{l} \operatorname{Var} (\hat {\mu}) = \operatorname{Var} \left(\frac{1}{m} x^{(1)}\right) + \operatorname{Var} \left(\frac{1}{m} x^{(2)}\right) + \dots + \operatorname{Var} \left(\frac{1}{m} x^{(m)}\right) \\ = \frac{1}{m^{2}} \operatorname{Var} \left(x^{(1)}\right) + \frac{1}{m^{2}} \operatorname{Var} \left(x^{(2)}\right) + \dots + \frac{1}{m^{2}} \operatorname{Var} \left(x^{(m)}\right) \\ = \frac{1}{m^{2}} (\nu + \nu + \dots + \nu) \\ = \frac{1}{m^{2}} (m v) \\ = \frac{\nu}{m} \\ \end{array}
$$

Exercise 21.3. Derive the recurrence relation equation (21.29) that is satisfied by all orthogonal polynomials.

Solution: The three-term recurrence relation for orthogonal polynomials is central to their construction and use. A key to the derivation is noticing that a multiple of  $z$  can be shifted from one basis to the other:

$$
\int_{\mathcal {Z}} \left(z b_{i} (z)\right) b_{j} (z) p (z) d z = \int_{\mathcal {Z}} b_{i} (z) \left(z b_{j} (z)\right) p (z) d z
$$

We must show that

$$
b_{i + 1} (z) = \left\{ \begin{array}{l l} (z - \alpha_{i}) b_{i} (z) & \text{for} i = 1 \\ (z - \alpha_{i}) b_{i} (z) - \beta_{i} b_{i - 1} (z) & \text{for} i > 1 \end{array} \right.
$$

produces orthogonal polynomials.

We notice that  $b_{i+1} - zb_i$  is a polynomial of degree at most  $i$ , allowing us to write it as a linear combination of the first  $i$  orthogonal polynomials:

$$
b_{i + 1} (z) - z b_{i} (z) = - \alpha_{i} b_{i} (z) - \beta_{i} b_{i - 1} (z) + \sum_{j = 0}^{i - 2} \gamma_{i j} b_{j} (z)
$$

for constants  $\alpha_{i},\beta_{i}$  , and  $\gamma_{ij}$

Multiplying both sides by  $b_{i}$  and  $p$  and then integrating yields:

$$
\int_{\mathcal {Z}} (b_{i + 1} (z) - z b_{i} (z)) b_{i} (z) p (z) d z = \int_{\mathcal {Z}} \left(- \alpha_{i} b_{i} (z) - \beta_{i} b_{i - 1} (z) + \sum_{j = 0}^{i - 2} \gamma_{i j} b_{j} (z)\right) b_{i} (z) p (z) d z
$$

$$
\begin{array}{l} \int_{\mathcal {Z}} b_{i + 1} (z) b_{i} (z) p (z) d z - \int_{\mathcal {Z}} z b_{i} (z) b_{i} (z) p (z) d z = - \int_{\mathcal {Z}} \alpha_{i} b_{i} (z) b_{i} (z) p (z) d z - \\ - \int_{\mathcal {Z}} \beta_{i} b_{i - 1} (z) b_{i} (z) p (z) d z + \\ + \sum_{j = 0}^{i - 2} \int_{\mathcal {Z}} \gamma_{i j} b_{j} (z) b_{i} (z) p (z) d z \\ - \int_{\mathcal {Z}} z b_{i}^{2} (z) p (z) d z = - \alpha_{i} \int_{\mathcal {Z}} b_{i}^{2} (z) p (z) d z \\ \end{array}
$$

producing our expression for  $\alpha_{i}$ :

$$
\alpha_{i} = \frac{\int_{\mathcal {Z}} z b_{i}^{2} (z) p (z) d z}{\int_{\mathcal {Z}} b_{i}^{2} (z) p (z) d z}
$$

The expression for  $\beta_{i}$  with  $i\geq 1$  is obtained instead by multiplying both sides by  $b_{i - 1}$  and  $p$  and then integrating.

Multiplying both sides by  $b_{k}$ , with  $k < i - 1$ , similarly produces:

$$
- \int_{\mathcal {Z}} z b_{i} (z) b_{k} (z) p (z) d z = \gamma_{i k} \int_{\mathcal {Z}} b_{k}^{2} (z) p (z) d z
$$

The shift property can be applied to yield:

$$
\int_{\mathcal {Z}} z b_{i} (z) b_{k} (z) p (z) d z = \int_{\mathcal {Z}} b_{i} (z) \left(z b_{k} (z)\right) p (z) d z = 0
$$

as  $z b_{k}(z)$  is a polynomial of at most order  $i - 1$ , and, by orthogonality, the integral is zero. It follows that all  $\gamma_{ik}$  are zero, and the three term recurrence relation is established.

Exercise 21.4. Suppose we have fitted a polynomial chaos model of an objective function  $f(\mathbf{x}, \mathbf{z})$  for a particular design point  $\mathbf{x}$  using  $m$  evaluations with  $\mathbf{z}^{(1)}, \ldots, \mathbf{z}^{(m)}$ . Derive an expression for estimating the partial derivative of the polynomial chaos coefficients with respect to a design component  $x_i$ .

Solution: We can derive a gradient approximation using the partial derivative of  $f$  with respect to a design component  $x_{i}$ :

$$
\frac{\partial}{\partial x_{i}} f (\mathbf {x}, \mathbf {z}) \approx b_{1} (\mathbf {z}) \frac{\partial}{\partial x_{i}} \theta_{1} (\mathbf {x}) + \dots + b_{k} (\mathbf {z}) \frac{\partial}{\partial x_{i}} \theta_{k} (\mathbf {x})
$$

If we have  $m$  samples, we can write these partial derivatives in matrix form:

$$
\left[ \begin{array}{c} \frac{\partial}{\partial x_{i}} f (\mathbf {x}, \mathbf {z}^{(1)}) \\ \vdots \\ \frac{\partial}{\partial x_{i}} f (\mathbf {x}, \mathbf {z}^{(m)}) \end{array} \right] \approx \left[ \begin{array}{c c c} b_{1} (\mathbf {z}^{(1)}) & \dots & b_{k} (\mathbf {z}^{(1)}) \\ \vdots & \ddots & \vdots \\ b_{1} (\mathbf {z}^{(m)}) & \dots & b_{k} (\mathbf {z}^{(m)}) \end{array} \right] \left[ \begin{array}{c} \frac{\partial}{\partial x_{i}} \theta_{1} (\mathbf {x}) \\ \vdots \\ \frac{\partial}{\partial x_{i}} \theta_{k} (\mathbf {x}) \end{array} \right]
$$

We can solve for approximations of  $\frac{\partial}{\partial x_i}\theta_1(\mathbf{x}),\ldots ,\frac{\partial}{\partial x_i}\theta_k(\mathbf{x})$  using the pseudoinverse:

$$
\left[ \begin{array}{c} \frac{\partial}{\partial x_{i}} \theta_{1} (\mathbf {x}) \\ \vdots \\ \frac{\partial}{\partial x_{i}} \theta_{k} (\mathbf {x}) \end{array} \right] \approx \left[ \begin{array}{c c c} b_{1} (\mathbf {z}^{(1)}) & \dots & b_{k} (\mathbf {z}^{(1)}) \\ \vdots & \ddots & \vdots \\ b_{1} (\mathbf {z}^{(m)}) & \dots & b_{k} (\mathbf {z}^{(m)}) \end{array} \right] ^{+} \left[ \begin{array}{c} \frac{\partial}{\partial x_{i}} f (\mathbf {x}, \mathbf {z}^{(1)}) \\ \vdots \\ \frac{\partial}{\partial x_{i}} f (\mathbf {x}, \mathbf {z}^{(m)}) \end{array} \right]
$$

Exercise 21.5. Consider an objective function  $f(\mathbf{x}, \mathbf{z})$  with design variables  $\mathbf{x}$  and random variables  $\mathbf{z}$ . As discussed in chapter 20, optimization under uncertainty often involves minimizing a linear combination of the estimated mean and variance:

$$
f_{\mathrm{mod}} (\mathbf {x}, \mathbf {z}) = \alpha \hat {\mu} (\mathbf {x}) + (1 - \alpha) \hat {v} (\mathbf {x})
$$

How can one use polynomial chaos to estimate the gradient of  $f_{\mathrm{mod}}$  with respect to a design variable  $\mathbf{x}$ ?

Solution: The estimated mean and variance have coefficients which depend on the design variables:

$$
\begin{array}{l} \hat {\mu} (\mathbf {x}) = \theta_{1} (\mathbf {x}) \\ \hat {v} (\mathbf {x}) = \sum_{i = 2}^{k} \theta_{i}^{2} (\mathbf {x}) \int_{\mathcal {Z}} b_{i} (\mathbf {z}) ^{2} p (\mathbf {z}) d \mathbf {z} \\ \end{array}
$$

The partial derivative of  $f_{\mathrm{mod}}$  with respect to the  $i$ th design component is

$$
\frac{\partial}{\partial x_{i}} f_{\mathrm{mod}} (\mathbf {x}) = \alpha \frac{\partial \theta_{1} (\mathbf {x})}{\partial x_{i}} + 2 (1 - \alpha) \sum_{i = 2}^{k} \theta_{i} (\mathbf {x}) \frac{\partial \theta_{i} (\mathbf {x})}{x_{i}} \int_{\mathcal {Z}} b_{i} (\mathbf {z}) ^{2} p (\mathbf {z}) d \mathbf {z}
$$

This computation requires the gradient of the coefficients with respect to  $\mathbf{x}$ , which is estimated in exercise 21.4.

# 22 Discrete Optimization

Previous chapters have focused on optimizing problems involving design variables that are continuous. Many problems, however, have design variables that are naturally discrete, such as manufacturing problems involving mechanical components that come in fixed sizes or navigation problems involving choices between discrete paths. A discrete optimization problem has constraints such that the design variables must be chosen from a discrete set. Some discrete optimization problems have infinite design spaces, and others are finite.<sup>1</sup> Even for finite problems, where we could in theory enumerate every possible solution, it is generally not computationally feasible to do so in practice. This chapter discusses both exact and approximate approaches to solving discrete optimization problems that avoid enumeration. Many of the methods covered earlier, such as simulated annealing and genetic programming, can easily be adapted for discrete optimization problems, but we will focus this chapter on categories of techniques we have not yet discussed.

Discrete optimization constrains the design to be integral. Consider the problem:

$$
\underset{x} {\text{minimize}} \quad x_{1} + x_{2}
$$

$$
\text{subject} \quad \| \mathbf {x} \| \leq 2
$$

$$
\mathbf {x} \mathrm{isintegral}
$$

The optimum in the continuous case is  $\mathbf{x}^{*} = [-\sqrt{2}, -\sqrt{2}]$  with a value of  $y = -2\sqrt{2} \approx -2.828$ . If  $x_{1}$  and  $x_{2}$  are constrained to be integer-valued, then the best we can do is to have  $y = -2$  with  $\mathbf{x}^{*} \in \{[-2,0], [-1, -1], [0, -2]\}$ .

1 Discrete optimization with a finite design space is sometimes referred to as combinatorial optimization. For a review, see B. Korte and J. Vygen, Combinatorial Optimization: Theory and Algorithms, 5th ed. Springer, 2012.

Example 22.1. Discrete versions of problems constrain the solution, often resulting in worse solutions than their continuous counterparts.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1467f0f3ed1883d1456431797f0492a4bfdddc66e3051f7ed6283f2e78d2362a.jpg)

# 22.1 Integer Programs

An integer program is a linear program $^2$  with integral constraints. By integral constraints, we mean that the design variables must come from the set of integers. $^3$  Integer programs are sometimes referred to as integer linear programs to emphasize the assumption that the objective function and constraints are linear.

An integer program in standard form is expressed as:

$$
\underset{x} {\text{minimize}} \quad \mathbf {c}^{\top} x
$$

$$
\text{subject} \quad \mathbf {A x} \leq \mathbf {b} \tag {22.1}
$$

$$
x \geq 0
$$

$$
\mathbf {x} \in \mathbb {Z}^{n}
$$

where  $\mathbb{Z}^n$  is the set of  $n$ -dimensional integral vectors. As with linear programs,  $\mathbf{c} \in \mathbb{R}^n$ ,  $\mathbf{A} \in \mathbb{R}^{m \times n}$ , and  $\mathbf{b} \in \mathbb{R}^m$ .

Like linear programs, integer programs are often solved in equality form. Transforming an integer program to equality form often requires adding additional slack variables  $\mathbf{s}$  that do not need to be integral. Thus, the equality form for integral programs is:

$$
\underset{\mathbf {x}, \mathbf {s}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad \mathbf {A x} + \mathbf {s} = \mathbf {b}
$$

$$
\mathbf {x} \geq \mathbf {0} \tag {22.2}
$$

$$
s \geq 0
$$

$$
\mathbf {x} \in \mathbb {Z}^{n}
$$

More generally, a mixed integer program (algorithm 22.1) includes both continuous and discrete design components. Such a problem, in equality form, is expressed as:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad \mathbf {A x} = \mathbf {b} \tag {22.3}
$$

$$
x \geq 0
$$

$$
\mathbf {x}_{\mathcal {D}} \in \mathbb {Z}^{| \mathcal {D} |}
$$

where  $\mathcal{D}$  is a set of indices corresponding to the design variables that are constrained to be discrete. Here,  $\mathbf{x} = [\mathbf{x}_{\mathcal{D}}, \mathbf{x}_{\mathcal{C}}]$ , where  $\mathbf{x}_{\mathcal{D}}$  represents the vector of discrete design variables and  $\mathbf{x}_{\mathcal{C}}$  the vector of continuous design variables.

See chapter 12.

3 Integer programming is a very mature field, with applications in operations research, communications networks, task scheduling, and other disciplines. Modern solvers, such as Gurobi and CPLEX, can routinely handle problems with millions of variables. There are packages for Julia that provide access to Gurobi, CPLEX, and a variety of other solvers.

```julia
mutable struct MixedIntegerProgram
A # minimize c·x
b # subject to Ax = b
c # x ≥ 0
D # x[D] ∈ Z^D
end
```

# 22.2 Rounding

A common approach to discrete optimization is to relax the constraint that the design points must come from a discrete set. The advantage of this relaxation is that we can use techniques, like gradient descent or linear programming, that take advantage of the continuous nature of the objective function to direct the search. After a continuous solution is found, the design variables are rounded to the nearest discrete design. An alternative is randomized rounding, where the design variables are rounded probabilistically up or down to a discrete design.<sup>4</sup>

There are potential issues with rounding. Rounding might result in an infeasible design point, as shown in figure 22.1. Even if rounding results in a feasible point, it may be far from optimal, as shown in figure 22.2. The addition of the discrete constraint will typically worsen the objective value as illustrated in example 22.1. However, for some problems, we can show the relaxed solution is close to the optimal discrete solution.

We can solve integer programs using rounding by removing the integer constraint, solving the corresponding linear program, or LP, and then rounding the solution to the nearest integer. This method is implemented in algorithm 22.2.

```txt
relax(MIP) = LinearProgram(MIP.A, MIP.b, MIP.c)  
round_ip(MIP) = round.Int, minimize_lp(relax(MIP)).x)
```

We can show that rounding the continuous solution for a constraint  $\mathbf{A}\mathbf{x} \leq \mathbf{b}$  when  $\mathbf{A}$  is integral is never too far from the optimal integral solution. A bound can be derived based on the determinants of the submatrices of  $\mathbf{A}$ , where a submatrix is a matrix obtained by deleting rows and/or columns of another matrix. If  $\mathbf{x}_c^*$  is an optimal solution of the LP with  $m \times n$  matrix  $\mathbf{A}$ , then there exists an optimal discrete solution  $\mathbf{x}_d^*$  with  $\left\| \mathbf{x}_c^* - \mathbf{x}_d^* \right\|_{\infty}$  less than or equal to  $n$  times the maximum absolute value of the determinants of the submatrices of  $\mathbf{A}$ .

Algorithm 22.1. A mixed integer linear program type that reflects equation (22.3). Here,  $\mathsf{D}$  is the set of discrete design indices.

4 P. Raghavan and C. D. Tompson, "Randomized Rounding: A Technique for Provably Good Algorithms and Algorithmic Proofs," Combinatorica, vol. 7, no. 4, pp. 365-374, 1987.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/27bd7d96f77207b3c18006b96a6fe26643d253984dfc067b00fe3ca7190e1111.jpg)  
Figure 22.1. Rounding can produce an infeasible design point.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/ca5bdf65d25b1972e800ef61969eb19449821ddf59237c01af147f244ede303d.jpg)  
Figure 22.2. The nearest feasible discrete design may be significantly worse than the best feasible discrete design.

Algorithm 22.2. Relaxing a mixed integer linear program into a linear program and solving a mixed integer linear program by rounding.

5 W. Cook, A. M. Gerards, A. Schrijver, and E. Tardos, "Sensitivity Theorems in Integer Linear Programming," Mathematical Programming, vol. 34, no. 3, pp. 251-264, 1986.

The vector  $\mathbf{c}$  need not be integral for an LP to have an optimal integral solution because the feasible region is purely determined by  $\mathbf{A}$  and  $\mathbf{b}$ . Some approaches use the dual formulation for the LP, which has a feasible region dependent on  $\mathbf{c}$ , in which case having an integral  $\mathbf{c}$  is also required.

In the special case of totally unimodular integer programs, where  $\mathbf{A}$ ,  $\mathbf{b}$ , and  $\mathbf{c}$  have all integer entries and  $\mathbf{A}$  is totally unimodular, the simplex algorithm is guaranteed to return an integer solution. A matrix is totally unimodular if the determinant of every square submatrix is 0, 1, or  $-1$ . If a totally unimodular matrix is invertible, then its inverse is also integral. In fact, every vertex solution of a totally unimodular integer program is integral.

Several matrices and their total unimodularity are discussed in example 22.2. Methods for determining whether a matrix or an integer linear program are totally unimodular are given in algorithm 22.3.

Consider the following matrices:

$$
\left[ \begin{array}{c c c} 1 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & - 1 \end{array} \right] \qquad \left[ \begin{array}{c c c} 1 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{array} \right] \qquad \left[ \begin{array}{c c c c c} - 1 & - 1 & 0 & 0 & 0 \\ 1 & 0 & - 1 & - 1 & 0 \\ 0 & 1 & 1 & 0 & - 1 \end{array} \right]
$$

The left matrix is not totally unimodular because

$$
\left| \begin{array}{c c} 1 & 1 \\ 1 & - 1 \end{array} \right| = - 2
$$

The other two matrices are totally unimodular.

# 22.3 Cutting Planes

The cutting plane method is an exact method for solving mixed integer programs when  $\mathbf{A}$  is not totally unimodular. Modern practical methods for solving integer programs use branch and cut algorithms that combine the cutting plane method with the branch and bound method, discussed in the next section. The cutting plane method works by solving the relaxed LP and then adding linear constraints that result in an optimal solution to the original problem.

Example 22.2. Examples of totally unimodular matrices.

$^{6}$ R.E. Gomory, "An Algorithm for Integer Solutions to Linear Programs," Recent Advances in Mathematical Programming, vol. 64, pp. 269-302, 1963.  
7 M. Padberg and G. Rinaldi, "A Branch-and-Cut Algorithm for the Resolution of Large-Scale Symmetric Traveling Salesman Problems," SIAM Review, vol. 33, no. 1, pp. 60-100, 1991.

```julia
isint(x,  $\epsilon = 1\mathrm{e} - 10$  )  $=$  abs(round(x)-x) $\leq \epsilon$    
function is_totally_unimodular(A::Matrix) # all entries must be in [0,1,-1] if any(a  $\notin (0, - 1,1)$  for a in A) return false end # brute force check every subdeterminant r,c  $=$  size(A) for i in 1 : min(r,c) for a in combinations(1:r,i) for b in combinations(1:c,i) B  $=$  A[a,b] if det(B)  $\notin (0, - 1,1)$  return false end end end end return true end function is_totally_unimodular(MIP) return is_totally_unimodular(MIP.A)&& all(isint,MIP.b)&&all(isint,MIP.c) end
```

We begin the cutting method with a solution  $\mathbf{x}_c^*$  to the relaxed LP that is a vertex of  $\mathbf{A}\mathbf{x} = \mathbf{b}$ . If the  $\mathcal{D}$  components in  $\mathbf{x}_c^*$  are integral, then it is also an optimal solution to the original mixed integer program, and we are done. If the  $\mathcal{D}$  components in  $\mathbf{x}_c^*$  are not integral, we find a hyperplane with  $\mathbf{x}_c^*$  on one side and all feasible discrete solutions on the other. This cutting plane is an additional linear constraint to exclude  $\mathbf{x}_c^*$ . The augmented LP is then solved for a new  $\mathbf{x}_c^*$ .

Each iteration of algorithm 22.4 introduces cutting planes that make nonintegral components of  $\mathbf{x}_c^*$  infeasible while preserving the feasibility of the nearest integral solutions and the rest of the feasible set. The integer program modified with these cutting plane constraints is solved for a new relaxed solution. Figure 22.3 illustrates this process.

We wish to add constraints that cut out nonintegral components of  $\mathbf{x}_c^*$ . For an LP in equality form with constraint  $\mathbf{A}\mathbf{x} = \mathbf{b}$ , recall from section 12.2.1 that we can partition a vertex solution  $\mathbf{x}_c^*$  to arrive at

$$
\mathbf {A}_{\mathcal {B}} \mathbf {x}_{\mathcal {B}}^{*} + \mathbf {A}_{\mathcal {V}} \mathbf {x}_{\mathcal {V}}^{*} = \mathbf {b} \tag {22.4}
$$

where  $\mathbf{x}_{\mathcal{V}}^{*} = \mathbf{0}$ . The nonintegral components of  $\mathbf{x}_c^*$  will thus occur only in  $\mathbf{x}_{\mathcal{B}}^{*}$ .

Algorithm 22.3. Methods for determining whether matrices A or mixed integer programs MIP are totally unimodular. The method isint returns true if the given value is integral. A more efficient method for determining total unimodularity is provided by K. Truemper, "A Decomposition Theory for Matroids. V. Testing of Matrix Total Unimodularity," Journal of Combinatorial Theory, Series B, vol. 49, no. 2, pp. 241-281, 1990.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/709c4f656ef78f0014109253b7bf5b2810840e7ea33eb9b6e02f7dc2e676581d.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/01fa6eefa53f8e354c260f2edfd4f868e1cf9190b07fce0958115531d7b05946.jpg)  
Figure 22.3. The cutting plane method introduces constraints until the solution to the LP is integral. The cutting plane is shown as a red line on the top plot. The feasible region of the augmented LP is on the bottom plot.

```matlab
function cuttingPlane(MIP)  
frac(x) = x - floor(x)  
LP = relax(MIP)  
x, b_inds = minimize_lp(LP)  
n_orig = length(x)  
v_inds = setdiff(1:n_orig, b_inds)  
D = copy(MIP.D)  
while !all(isint(x[i]) for i in D)  
AB, AV = LP.A(:,b_inds], LP.A(:,v_inds]  
Abar = AB\AV  
for i in D  
if !isint(x[i])  
b = findfirst(isequal(i), b_inds)  
A2 = [LP.A zeros(size(LP.A,1)); zeros(1,size(LP.A,2)+1)]  
A2[end,end] = 1  
A2[end,v_inds] = -frac.(Abar[b,:])  
b2 = [LP.b; -frac{x[i]])]  
c2 = [LP.c; 0]  
LP = LinearProgram(A2,b2,c2)  
end  
end  
x, b_inds = minimize_lp(LP)  
v_inds = setdiff(eachindex(x), b_inds)  
end  
return x[1:n_orig]  
end
```

Algorithm 22.4. The cutting plane method solves a given mixed integer program MIP and returns an optimal design vector. An error is thrown if no feasible solution exists. The helper function frac returns the fractional part of a number, and the implementation for minimize_lp, algorithm 12.5, has been adjusted to return the basic and nonbasic indices b_inds and v_inds along with an optimal design x.

We will now create a constraint for design index  $b \in \mathcal{B} \cap \mathcal{D}$  where  $x_{c}^{*}$  is nonintegral. Equation (22.4) holds for any feasible  $\mathbf{x}$ . Multiplying by  $\mathbf{A}_{\mathcal{B}}^{-1}$  produces:

$$
\mathbf {x}_{\mathcal {B}} + \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {A}_{\mathcal {V}} \mathbf {x}_{\mathcal {V}} = \mathbf {A}_{\mathcal {B}}^{- 1} \mathbf {b} \tag {22.5}
$$

or equivalently

$$
\mathbf {x}_{\mathcal {B}} + \bar {\mathbf {A}} \mathbf {x}_{\mathcal {V}} = \bar {\mathbf {b}} \tag {22.6}
$$

where  $\bar{\mathbf{A}} = \mathbf{A}_{\mathcal{B}}^{-1}\mathbf{A}_{\mathcal{V}}$  and  $\bar{\mathbf{b}} = \mathbf{A}_{\mathcal{B}}^{-1}\mathbf{b}$

We can expand equation (22.6) for component  $b$  into its integral and fractional parts:

$$
x_{b} + \sum_{v \in \mathcal {V}} \left(\bar {A}_{b v} + \left\lfloor \bar {A}_{b v} \right\rfloor - \left\lfloor \bar {A}_{b v} \right\rfloor\right) x_{v} = \bar {b}_{b} + \left\lfloor \bar {b}_{b} \right\rfloor - \left\lfloor \bar {b}_{b} \right\rfloor \tag {22.7}
$$

and we move all integers to the left-hand side:9

$$
x_{b} - \underbrace{\left\lfloor \bar {b}_{b} \right\rfloor}_{\text{integer}} + \sum_{v \in \mathcal {V}} \underbrace{\left(\left\lfloor \bar {A}_{b v} \right\rfloor\right)}_{\text{integer}} x_{v} = \underbrace{\bar {b}_{b} - \left\lfloor \bar {b}_{b} \right\rfloor}_{\text{fraction}} - \sum_{v \in \mathcal {V}} \underbrace{\left(\bar {A}_{b v} - \left\lfloor \bar {A}_{b v} \right\rfloor\right)}_{\text{fraction}} x_{v} \tag {22.8}
$$

The right side contains the fractional terms. It must be less than 1, since  $0 \leq \bar{b}_b - \left\lfloor \bar{b}_b \right\rfloor < 1$  and the remaining term is negative. The left side forces the right to be an integer, so:

$$
\bar {b}_{b} - \left\lfloor \bar {b}_{b} \right\rfloor - \sum_{v \in \mathcal {V}} \left(\bar {A}_{b v} - \left\lfloor \bar {A}_{b v} \right\rfloor\right) x_{v} \leq 0 \tag {22.9}
$$

will hold for all integers in the feasible set and will not hold for  $\mathbf{x}_c^*$ .

We can introduce this new cutting plane constraint to cut out our relaxed solution while preserving the feasibility of integer solutions in the feasible set. The constraint is written in equality form using a new integral slack variable  $x_{k}$ :

$$
x_{k} + \sum_{v \in \mathcal {V}} \left(\left\lfloor \bar {A}_{b v} \right\rfloor - \bar {A}_{b v}\right) x_{v} = \left\lfloor \bar {b}_{b} \right\rfloor - \bar {b}_{b} \tag {22.10}
$$

Using equation (22.5) with  $\mathbf{x}_{\mathcal{V}}^{*} = \mathbf{0}$ , we can replace  $\bar{b}_b$  with  $x_b^*$  to get

$$
x_{k} + \sum_{v \in \mathcal {V}} \left(\left\lfloor \bar {A}_{b v} \right\rfloor - \bar {A}_{b v}\right) x_{v} = \left\lfloor x_{b}^{*} \right\rfloor - x_{b}^{*} \tag {22.11}
$$

Each iteration of algorithm 22.4 thus increases the number of constraints and the number of variables until solving the LP produces an integral solution. Only the components corresponding to the original design variables are returned.

The cutting plane method is used to solve a simple integer linear program in example 22.3.

Note that  $\lfloor x\rfloor$  , or floor of  $x$  ,rounds  $x$  down to the nearest integer.

9 The floor of a number produces an integer, and the solution to our integer program requires that  $x$  be integral, so all entries on the left side will be integers, and the left side will evaluate to an integer.

Consider the integer program:

$$
\begin{array}{l} \underset{\mathbf {x}} {\text{minimize}} 2 x_{1} + x_{2} + 3 x_{3} \\ \text{subject} \quad \left[ \begin{array}{c c c} 0. 5 & - 0. 5 & 1. 0 \\ 2. 0 & 0. 5 & - 1. 5 \end{array} \right] \mathbf {x} = \left[ \begin{array}{c} 2. 5 \\ - 1. 5 \end{array} \right] \\ \mathbf {x} \geq \mathbf {0} \quad \mathbf {x} \in \mathbb {Z}^{3} \\ \end{array}
$$

The relaxed solution is  $\mathbf{x}^{*}\approx [0.818,0,2.091]$ , yielding:

$$
\mathbf {A}_{\mathcal {B}} = \left[ \begin{array}{c c} 0. 5 & 1 \\ 2 & - 1. 5 \end{array} \right] \quad \mathbf {A}_{\mathcal {V}} = \left[ \begin{array}{c} - 0. 5 \\ 0. 5 \end{array} \right] \quad \bar {\mathbf {A}} = \left[ \begin{array}{c} - 0. 09 1 \\ - 0. 45 5 \end{array} \right] \quad \bar {\mathbf {b}} = \left[ \begin{array}{c} 0. 81 8 \\ 2. 09 1 \end{array} \right]
$$

From equation (22.11), the constraint for  $x_{1}$  with slack variable  $x_{4}$  is:

$$
\begin{array}{l} x_{4} + (\lfloor - 0. 09 1 \rfloor - (- 0. 09 1)) x_{2} = \lfloor 0. 81 8 \rfloor - 0. 81 8 \\ x_{4} - 0. 90 9 x_{2} = - 0. 81 8 \\ \end{array}
$$

The constraint for  $x_{3}$  with slack variable  $x_{5}$  is:

$$
\begin{array}{l} x_{5} + (\lfloor - 0. 45 5 \rfloor - (- 0. 45 5)) x_{2} = \lfloor 2. 09 1 \rfloor - 2. 09 1 \\ x_{5} - 0. 54 5 x_{2} = - 0. 09 1 \\ \end{array}
$$

The modified integer program has:

$$
\mathbf {A} = \left[ \begin{array}{c c c c c} 0. 5 & - 0. 5 & 1 & 0 & 0 \\ 2 & 0. 5 & - 1. 5 & 0 & 0 \\ 0 & - 0. 90 9 & 0 & 1 & 0 \\ 0 & - 0. 54 5 & 0 & 0 & 1 \end{array} \right] \quad \mathbf {b} = \left[ \begin{array}{c} 2. 5 \\ - 1. 5 \\ - 0. 81 8 \\ - 0. 09 1 \end{array} \right] \quad \mathbf {c} = \left[ \begin{array}{c} 2 \\ 1 \\ 3 \\ 0 \\ 0 \end{array} \right]
$$

Solving the modified LP, we get  $\mathbf{x}^{*} \approx [0.9, 0.9, 2.5, 0.0, 0.4]$ . Since this point is not integral, we repeat the procedure with constraints:

$$
x_{6} - 0. 9 x_{4} = - 0. 9 \quad x_{7} - 0. 9 x_{4} = - 0. 9
$$

$$
x_{8} - 0. 5 x_{4} = - 0. 5 \quad x_{9} - 0. 4 x_{4} = - 0. 4
$$

and solve a third LP to obtain:  $\mathbf{x}^{*} = [1,2,3,1,1,0,0,0,0]$  with a final solution of  $\mathbf{x}_i^* = [1,2,3]$ .

Example 22.3. The cutting plane method used to solve an integer program.

# 22.4 Branch and Bound

One method for finding the global optimum of a discrete problem, such as an integer program, is to enumerate all possible solutions. The branch and bound<sup>10</sup> method guarantees that an optimal solution is found without having to evaluate all possible solutions. Many commercial integer program solvers use ideas from both the cutting plane method and branch and bound.<sup>11</sup> The method gets its name from the branch operation that partitions the solution space<sup>12</sup> and the bound operation that computes a lower bound for a partition.

Algorithm 22.5 uses a priority queue, which is a data structure that associates priorities with elements in a collection. We can add an element and its priority value to a priority queue using the enqueue operation. We can remove the element with the minimum priority value using the.dequeue operation.

The algorithm begins with a priority queue containing a single LP relaxation of the original mixed integer program. Associated with that LP is a solution  $\mathbf{x}_c^*$  and objective value  $y_c = \mathbf{c}^\top \mathbf{x}_c^*$ . The objective value serves as a lower bound on the solution and thus serves as the LP's priority in the priority queue. At each iteration of the algorithm, we check whether the priority queue is empty. If it is not empty, we deque the LP with the lowest priority value. If the solution associated with that element has the necessary integral components, then we keep track of whether it is the best integral solution found so far.

If the dequeued solution has one or more components in  $\mathcal{D}$  that are nonintegral, we choose from  $\mathbf{x}_c^*$  a component that is farthest from an integer value. Suppose this component corresponds to the  $i$ th design variable. We branch by considering two new LPs, each one created by adding one of the following constraints to the dequeued LP: $^{13}$

$$
x_{i} \leq \left\lfloor x_{i, c}^{*} \right\rfloor \quad \text{or} \quad x_{i} \geq \left\lceil x_{i, c}^{*} \right\rceil \tag {22.12}
$$

as shown in figure 22.4. Example 22.4 demonstrates this process.

We compute the solution associated with these two LPs, which provide lower bounds on the value of the original mixed integer program. If either solution lowers the objective value when compared to the best integral solution seen so far, it is placed into the priority queue. Not placing solutions already known to be inferior to the best integral solution seen thus far allows branch and bound to prune the search space. The process continues until the priority queue is empty,

10 Branch and bound is a general method that can be applied to many kinds of discrete optimization problems, but we will focus here on how it can be used for integer programming. A.H. Land and A.G.Doig, "An Automatic Method of Solving Discrete Programming Problems," Econometrica, vol. 28, no.3,pp.497-520,1960.  
11 J. E. Mitchell, "Branch-And-Cut Algorithms for Combinatorial Optimization Problems," in Handbook of Applied Optimization, P. M. Pardalos and M. G. C. Resende, eds., Oxford University Press, 2002, pp. 65-77.  
12 The subsets are typically disjoint, but this is not required. For branch and bound to work, at least one subset must have an optimal solution. D.A. Bader, W.E. Hart, and C.A. Phillips, "Parallel Algorithm Design for Branch and Bound," in Tutorials on Emerging Methodologies and Applications in Operations Research, H.J. Greenberg, ed., Kluwer Academic Press, 2004.  
13 Note that  $\lceil x\rceil$  , or ceiling of  $x$  - rounds  $x$  up to the nearest integer.

```julia
function minimize_lp_and_y(LP) try  $x =$  minimize_lp(LP).x return  $(x,x\cdot LP.c)$  catch return (fill(NaN, length(LP.c)), Inf) end   
end   
function branch_and_bound(MIP) LP  $=$  relax(MIP) x, y  $=$  minimize_lp_and_y(LP) best, n  $=$  (x=deepcopy(x), y=Inf), length(x) Q  $=$  PriorityQueue() enqueue!(Q, (LP,x,y), y)   
while !isEmpty(Q) LP, x, y  $=$  dequeue!(Q) if any(isnan.(x)) || all(isint(x[i]) for i in MIP.D) if y  $<$  best.y best  $=$  (x=x[1:n],y=y)   
end   
else i  $=$  argmax(abs(x[i] - round(x[i])) for i in MIP.D) # x_i  $\leq$  floor(x_i) A,b,c  $=$  LP.A,LP.b, LP.c A2  $=$  [A zeros(size(A,1)); [j==i for j in 1:size(A,2)]' 1] b2,c2  $=$  [b; floor(x[i]]],[c;0] LP2  $=$  LinearProgram(A2,b2,c2) x2,y2  $=$  minimize_lp_and_y(LP2) if y2  $\leq$  best.y enqueue!(Q,(LP2,x2,y2),y2)   
end # x_i  $\geq$  ceil(x_i) A2  $=$  [A zeros(size(A,1)); [j==i for j in 1:size(A,2)]' -1] b2,c2  $=$  [b;ceil(x[i]]],[c;0] LP2  $=$  LinearProgram(A2,b2,c2) x2,y2  $=$  minimize_lp_and_y(LP2) if y2  $\leq$  best.y enqueue!(Q,(LP2,x2,y2),y2)   
end   
end   
return best.x   
end
```

Algorithm 22.5. The branch and bound algorithm for solving a mixed integer program MIP. The helper method solves an LP and returns both the solution and its value. An infeasible LP produces a NaN solution and an Inf value. More sophisticated implementations will drop variables whose solutions are known in order to speed computation. The PriorityQueue type is provided by the DataStructures.jl package.

Consider a relaxed solution  $\mathbf{x}_c^* = [3,2.4,1.2,5.8]$  for an integer program with  $\mathbf{c} = [-1, - 2, - 3, - 4]$ . The lower bound is

$$
y \geq \mathbf {c}^{\top} \mathbf {x}_{c}^{*} = - 34. 6
$$

We branch on a nonintegral coordinate of  $\mathbf{x}_c^*$ , typically the one farthest from an integer value. In this case, we choose the first nonintegral coordinate,  $x_{2,c}^*$ , which is 0.4 from the nearest integer value. We then consider two new LPs, one with  $x_2\leq 2$  as an additional constraint and the other with  $x_{2}\geq 3$  as an additional constraint.

Example 22.4. An example of a single application of the branching step in branch and bound.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/df08dbc7c77ab876e110dfed9507042935c4693baf3758ba9e61ce15589b4928.jpg)

and we return the best feasible integral solution. Example 22.5 shows how branch and bound can be applied to a small integer program.

We can use branch and bound to solve the integer program in example 22.3. As before, the relaxed solution is  $\mathbf{x}_c^* = [0.818, 0, 2.09]$ , with a value of 7.909. We branch on the first component, resulting in two integer programs, one with  $x_1 \leq 0$  and one with  $x_1 \geq 1$ :

$$
\mathbf {A}_{\text{left}} = \left[ \begin{array}{c c c c} 0. 5 & - 0. 5 & 1 & 0 \\ 2 & 0. 5 & - 1. 5 & 0 \\ 1 & 0 & 0 & 1 \end{array} \right]
$$

$$
\mathbf {b}_{\text{left}} = \left[ \begin{array}{c} 2. 5 \\ - 1. 5 \\ 0 \end{array} \right] \qquad \mathbf {c}_{\text{left}} = \left[ \begin{array}{c} 2 \\ 1 \\ 3 \\ 0 \end{array} \right]
$$

$$
\mathbf {A}_{\text{right}} = \left[ \begin{array}{c c c c} 0. 5 & - 0. 5 & 1 & 0 \\ 2 & 0. 5 & - 1. 5 & 0 \\ 1 & 0 & 0 & - 1 \end{array} \right]
$$

$$
\mathbf {b}_{\text{right}} = \left[ \begin{array}{c} 2. 5 \\ - 1. 5 \\ 1 \end{array} \right] \qquad \mathbf {c}_{\text{right}} = \left[ \begin{array}{c} 2 \\ 1 \\ 3 \\ 0 \end{array} \right]
$$

The left LP with  $x_{1} \leq 0$  is infeasible. The right LP with  $x_{1} \geq 1$  has a relaxed solution,  $\mathbf{x}_c^* = [1,2,3,0]$ , and a value of 13. We have thus obtained our integral solution,  $\mathbf{x}_i^* = [1,2,3]$ .

# 22.5 Dynamic Programming

Dynamic programming $^{14}$  is a technique that can be applied to problems with optimal substructure and overlapping subproblems. A problem has optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. Figure 22.5 shows an example.

A problem with overlapping subproblems solved recursively will encounter the same subproblem many times. Instead of enumerating exponentially many potential solutions, dynamic programming either stores subproblem solutions, and thereby avoids having to recompute them, or recursively builds the optimal solution in a single pass. Problems with recurrence relations often have overlapping subproblems. Figure 22.6 shows an example.

Example 22.5. Applying branch and bound to solve an integer programming problem.

14 The term dynamic programming was chosen by Richard Bellman to reflect the time-varying aspect of the problems he applied it to and to avoid the sometimes negative connotations words like research and mathematics had. He wrote, "I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities." R. Bellman, Eye of the Hurricane: An Autobiography. World Scientific, 1984. p. 159.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f8e0e4a314c6fe4d9c0b3c56d3378c8a1bbed0ec0c45ab3953a7b66c8b3d45d9.jpg)  
Figure 22.5. Shortest path problems have optimal substructure because if the shortest path from any  $a$  to  $c$  passes through  $b$ , then the subpaths  $a \to b$  and  $b \to c$  are both shortest paths. The blue boxes are obstacles that the path must avoid.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/0f049602f7b2a9f2957354446b32efd65fc49faaa252b319d4c33fb68db8df63.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/348bf0a5b1645db3592e8ed53965a4e7a6afbdd1477db86e0ef2b3606964d5a7.jpg)  
Figure 22.6. We can compute the  $n$ th term of the Padovan sequence,  $P_{n} = P_{n - 2} + P_{n - 3}$ , with  $P_0 = P_1 = P_2 = 1$  by recursing through all subterms (left). A more efficient approach is to compute subterms once and reuse their values in subsequent calculations by exploiting the problem's overlapping substructure (right).

Dynamic programming can be implemented either top-down or bottom-up, as demonstrated in algorithm 22.6. The top-down approach begins with the desired problem and recurses down to smaller and smaller subproblems. Subproblem solutions are stored so that when we are given a new subproblem, we can either retrieve the computed solution or solve and store it for future use.[15] The bottom-up approach starts by solving the smaller subproblems and uses their solutions to obtain solutions to larger problems.

```txt
function padovan_topdown(n, P=Dict())
if !haskey(P, n)
P[n] = n < 3 ? 1 :
		padovan_topdown(n-2,P) + padovan_topdown(n-3,P)
end
return P[n]
end
function padovan_bottomup(n)
P = Dict(0=>1,1=>1,2=>1)
for i in 3 : n
	 $\mathrm{P}\left\lbrack  \mathrm{i}\right\rbrack   = \mathrm{P}\left\lbrack  {\mathrm{i} - 2}\right\rbrack   + \mathrm{P}\left\lbrack  {\mathrm{i} - 3}\right\rbrack$ 
end
return P[n]
end
```

15 Storing subproblem solutions in this manner is called memoization.

Algorithm 22.6. Computing the Padovan sequence using dynamic programming, with both the top-down and bottom-up approaches.

The knapsack problem is a well-known combinatorial optimization problem that often arises in resource allocation.<sup>16</sup> Suppose we are packing our knapsack for a trip, but we have limited space and want to pack the most valuable items. There are several variations of the knapsack problem. In the 0-1 knapsack problem, we have  $n$  items, with the  $i$ th item having integral weight  $w_{i} > 0$  and value

16 The knapsack problem is an integer program with a single constraint, but it can be efficiently solved using dynamic programming.

$v_{i}$ . The design vector  $\mathbf{x}$  consists of binary values that indicate whether an item is packed. The total weight cannot exceed our integral capacity  $w_{\mathrm{max}}$ , and we seek to maximize the total value of packed items. Hence, we have the following optimization problem:

$$
\underset{\mathbf {x}} {\text{minimize}} - \sum_{i = 1}^{n} v_{i} x_{i}
$$

$$
\text{subject} \quad \sum_{i = 1}^{n} w_{i} x_{i} \leq w_{\max } \tag {22.13}
$$

$$
x_{i} \in \{0, 1 \} \text{for} i \text{in} 1: n
$$

There are  $2^{n}$  possible design vectors, which makes direct enumeration for large  $n$  intractable. However, we can use dynamic programming. The 0-1 knapsack problem has optimal substructure and overlapping subproblems. Suppose we have already solved the knapsack problems involving the first  $i - 1$  items and all capacities up to  $w_{\mathrm{max}}$ . Now consider a larger problem with one additional item, item  $i$  with weight  $w_{i}$  and value  $v_{i}$ . The optimal solution will either include or exclude the new item.

- If it is not worth including the new item, the solution will have the same value as a knapsack with  $i - 1$  items and capacity  $w_{\max}$ .  
- If it is worth including the new item, the solution will have the value of a knapsack with  $i - 1$  items and capacity  $w_{\max} - w_i$  plus the value of the new item  $v_i$ .

We can define a recurrence relation knapsack  $(i, w_{\max})$  that returns the sum of the values of the items in an optimally packed knapsack when considering the first  $i$  items with a remaining capacity of  $w_{\max}$ . This recurrence relation can be written as follows:

$$
\operatorname{knapsack} (i, w_{\max }) = \left\{ \begin{array}{l l} 0 & \text{if } i = 0 \\ \operatorname{knapsack} (i - 1, w_{\max }) & \text{if } w_{i} > w_{\max } \\ \max \left\{ \begin{array}{l l} \operatorname{knapsack} (i - 1, w_{\max }) & (\text{discardnewitem}) \\ \operatorname{knapsack} (i - 1, w_{\max } - w_{i}) + v_{i} & (\text{includenewitem}) \end{array} \right. & \text{otherwise} \end{array} \right. \tag {22.14}
$$

Algorithm 22.7 provides an implementation.

```julia
function knapsack(v, w, w_max)  
n = length(v)  
y = Dict((0, j) => 0.0 for j in 0: w_max)  
for i in 1 : n  
    for j in 0 : w_max  
        y[i, j] = w[i] > j ? y[i-1, j] :  
            max(y[i-1, j],  
                y[i-1, j-w[i]] + v[i])  
end  
end  
# recover solution  
x, j = falsely(n), w_max  
for i in n: -1: 1  
    if w[i] ≤ j && y[i, j] - y[i-1, j-w[i]] == v[i]  
        # the ith element is in the knapsack  
        x[i] = true  
        j -= w[i]  
end  
end  
return x
```

Algorithm 22.7. A method for solving the 0-1 knapsack problem with item values v, integral item weights w, and integral capacity w_max. Recovering the design vector from the cached solutions requires additional iteration.

# 22.6 Ant Colony Optimization

Ant colony optimization<sup>17</sup> is a stochastic method for optimizing paths through graphs. This method was inspired by some ant species that wander randomly in search of food, leaving pheromone trails as they go. Other ants that stumble upon a pheromone trail are likely to start following it, thereby reinforcing the trail's scent. Pheromones slowly evaporate over time, causing unused trails to fade. Short paths, with stronger pheromones, are traveled more often and thus attract more ants. Thus, short paths create positive feedback that lead other ants to follow and further reinforce the shorter path.

Basic shortest path problems, such as the shortest paths found by ants between the ant hill and sources of food, can be efficiently solved using dynamic programming. Ant colony optimization has been used to find near-optimal solutions to the traveling salesman problem, a much more difficult problem in which we want to find the shortest path that passes through each node of the graph exactly once. Ant colony optimization has also been used to route multiple vehicles, find optimal locations for factories, and fold proteins.[18] The algorithm is stochastic in nature and is thus resistant to changes to the problem over time, such as traffic delays

17 M. Dorigo, V. Maniezzo, and A. Colorni, "Ant System: Optimization by a Colony of Cooperating Agents," IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 26, no. 1, pp. 29-41, 1996.

18 These and other applications are discussed in these references: M. Manfrin, "Ant Colony Optimization for the Vehicle Routing Problem," Ph.D. dissertation, Université Libre de Bruxelles, 2004. T. Stützle, "MAX-MIN Ant System for Quadratic Assignment Problems," Technical University Darmstadt, Tech. Rep., 1997. and A. Shmygelska, R. Aguirre-Hernández, and H. H. Hoos, "An Ant Colony Algorithm for the 2D HP Protein Folding Problem," in International Workshop on Ant Algorithms (ANTS), 2002.

changing effective edge lengths in the graph or networking issues that remove edges entirely.

Ants move stochastically based on the attractiveness of the edges available to them. The attractiveness of transition  $i \rightarrow j$  depends on the pheromone level and an optional prior factor:

$$
A (i \rightarrow j) = \tau (i \rightarrow j) ^{\alpha} \eta (i \rightarrow j) ^{\beta} \tag {22.15}
$$

where  $\alpha$  and  $\beta$  are exponents for the pheromone level  $\tau$  and prior factor  $\eta$ , respectively. For problems involving shortest paths, we can set the prior factor to the inverse edge length  $\ell(i \to j)$  to encourage the traversal of shorter paths:  $\eta(i \to j) = 1 / \ell(i \to j)$ . A method for computing the edge attractiveness is given in algorithm 22.8.

Suppose an ant is at node  $i$  and can transition to any of the nodes  $j \in \mathcal{J}$ . The set of successor nodes  $\mathcal{J}$  contains all valid outgoing neighbors.[20] Sometimes edges are excluded, such as in the traveling salesman problem where ants are prevented from visiting the same node twice. It follows that  $\mathcal{J}$  is dependent on both  $i$  and the ant's history.

```matlab
function edge attractsness(G,  $\tau$ ,  $\eta$ ;  $\alpha = 1$ ,  $\beta = 5$ )  
A = Dict()  
for i in 1: nv(G)  
neighbors = outneighbors(G, i)  
for j in neighbors  
v =  $\tau[(i,j)]^{\wedge} \alpha * \eta[(i,j)]^{\wedge} \beta$   
A[(i,j)] = v  
end  
end  
return A
```

The probability of edge transition  $i\rightarrow j$  is:

$$
P (i \rightarrow j) = \frac{A (i \rightarrow j)}{\sum_{j^{\prime} \in \mathcal {J}} A (i \rightarrow j^{\prime})} \tag {22.16}
$$

Ants affect subsequent generations by depositing pheromones. There are several methods for modeling pheromone deposition. One approach is to deposit pheromones after a path is completed.[21] Ants that do not find a path do not deposit pheromones. For shortest path problems, a successful ant that has established a path of length  $\ell$  deposits  $1 / \ell$  pheromones on each edge it traversed.

19 Dorigo, Maniezzo, and Colorni recommend  $\alpha = 1$  and  $\beta = 5$ .

20 The outgoing neighbors of a node  $i$  are all nodes  $j$  such that  $i \rightarrow j$  is in the graph. In an undirected graph, the neighbors and the outgoing neighbors are identical.

Algorithm 22.8. A method for computing the edge attractiveness table given a graph  $G$ , pheromone levels  $\tau$ , prior edge weights  $\eta$ , pheromone exponent  $\alpha$ , and prior exponent  $\beta$ .

21 M. Dorigo, G. Di Caro, and L. M. Gambardella, "Ant Algorithms for Discrete Optimization," Artificial Life, vol. 5, no. 2, pp. 137-172, 1999.

```julia
function run_ant(G, lengths,  $\tau$  A,x_best,y_best)   
 $\mathbf{x} = [1]$    
while length(x)  $<  \mathrm{nv}(G)$    
i  $=$  x[end] neighbors  $=$  setdiff(outneighbors(G,i),x) if isempty(neighbors)# ant got stuck return (x=x_best,y=y_best)   
end as  $= [A[(i,j)]$  for j in neighbors] push!(x, neighbors[sample(Weights(as))])   
end   
l  $=$  sum(lengths[(x[i-1],x[i]]] for i in 2:length(x))   
for i in 2:length(x)   
 $\tau [(x[i - 1],x[i])]\stackrel {+}{=}\frac{1}{l}$    
end   
if l  $<  y_{-}$  best return  $(x = x,y = l)$    
else return  $(x = x\_ best$  ,y=y_best)   
end
```

Algorithm 22.9. A method for simulating a single ant on a traveling salesman problem in which the ant starts at the first node and attempts to visit each node exactly once. Pheromone levels are increased at the end of a successful tour. The parameters are the graph G, edge lengths lengths, pheromone levels  $\tau$ , edge attractiveness A, the best solution found thus far x_best, and its value y_best.

Ant colony optimization also models pheromone evaporation, which naturally occurs in the real world. Modeling evaporation helps prevent the algorithm from prematurely converging to a single, potentially suboptimal, solution. Pheromone evaporation is executed at the end of each iteration after all ant simulations have been completed. Evaporation decreases the pheromone level of each transition by a factor of  $1 - \rho$ , with  $\rho \in [0,1]$ .22

For  $m$  ants at iteration  $k$ , the effective pheromone update is

$$
\tau (i \rightarrow j) ^{(k + 1)} = (1 - \rho) \tau (i \rightarrow j) ^{(k)} + \sum_{a = 1}^{m} \frac{1}{\ell^{(a)}} ((i \rightarrow j) \in \mathcal {P}^{(a)}) \tag {22.17}
$$

22 It is common to use  $\rho = 1 / 2$

where  $\ell^{(a)}$  is the path length and  $\mathcal{P}^{(a)}$  is the set of edges traversed by ant  $a$ .

Ant colony optimization is implemented in algorithm 22.10, with individual ant simulations using algorithm 22.9. Figure 22.7 shows ant colony optimization on a traveling salesman problem.

```matlab
function ant_colony_optimization(G, lengths;  
m = 1000, k_max=100,  $\alpha = 1.0$ ,  $\beta = 5.0$ ,  $\rho = 0.5$ ,  
n = Dict((e.src, e.dst) => 1/lengths[(e.src, e.dst)]  
    for e in edges(G))  
     $\tau =$  Dict((e.src, e.dst) => 1.0 for e in edges(G))  
best = (x=[], y=Inf)  
for k in 1 : k_max  
    A = edge_attractiveness(G,  $\tau$ , n,  $\alpha = \alpha$ ,  $\beta = \beta$ )  
    for (e,v) in  $\tau$ $\tau[e] = (1 - \rho)*v$   
    end  
    for ant in 1 : m  
        best = run_ant(G, lengths,  $\tau$ , A, best.x, best.y)  
end  
return best.x
```

Algorithm 22.10. Ant colony optimization, which takes a directed or undirected graph G from Graphs.jl and a dictionary of edge tuples to path lengths lengths. Ants start at the first node in the graph. Optional parameters include the number of ants per iteration m, the number of iterations k_max, the pheromone exponent  $\alpha$  the prior exponent  $\beta$ , the evaporation scalar  $\rho$ , and a dictionary of prior edge weights n.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e925eed3077bffaf566d1499a08f4f3516108361654771438cac78ffe111e54c.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/712f703b771a38c5a55f5a1875f0dbbf2d4c54cfd997816cfc03be14a955b3aa.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b42c54441457157aeeb966e56347a1822fca6dbc806c0c25335478869b3bbb58.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6d0483f8571774c2c594df9bd944ee100e4d004f249a108dd87f365544c0a465.jpg)  
Figure 22.7. Ant colony optimization used to solve a traveling salesman problem on a directed graph using 50 ants per iteration. Path lengths are the Euclidean distances. Color opacity corresponds to pheromone level.

# 22.7 Summary

- Discrete optimization problems require that the design variables be chosen from discrete sets.  
- Relaxation, in which the continuous version of the discrete problem is solved, is by itself an unreliable technique for finding an optimal discrete solution but is central to more sophisticated algorithms.  
- Many combinatorial optimization problems can be framed as an integer program, which is a linear program with integer constraints.  
- Both the cutting plane and branch and bound methods can be used to solve integer programs efficiently and exactly. The branch and bound method is quite general and can be applied to a wide variety of discrete optimization problems.  
- Dynamic programming is a powerful technique that exploits optimal overlapping substructure in some problems.  
- Ant colony optimization is a nature-inspired algorithm that can be used for optimizing paths in graphs.

# 22.8 Exercises

Exercise 22.1. A Boolean satisfiability problem, often abbreviated SAT, requires determining whether a Boolean design exists that causes a Boolean-valued objective function to output true. SAT problems were the first to be proven to belong to the difficult class of NP-complete problems.[23] This means that SAT is at least as difficult as all other problems whose solutions can be verified in polynomial time.

Consider the Boolean objective function:

$$
f (\mathbf {x}) = x_{1} \wedge (x_{2} \vee \neg x_{3}) \wedge (\neg x_{1} \vee \neg x_{2})
$$

Find an optimal design using enumeration. How many designs must be considered for an  $n$ -dimensional design vector in the worst case?

Solution: Enumeration tries all designs. Each component can either be true or false, thus resulting in  $2^{n}$  possible designs in the worst case. This problem has  $2^{3} = 8$  possible designs.

23S. Cook, "The Complexity of Theorem-Proving Procedures," in ACM Symposium on Theory of Computing, 1971.

```latex
$\begin{array}{rl} & {\mathsf{f}(\mathbf{x}) = (\mathsf{\nabla}x[1]\mid \mid \mathsf{x}[3])\& \& (\mathsf{x}[2]\mid \mid \mathsf{\nabla}x[3])\& \& (\mathsf{\nabla}x[1]\mid \mid \mathsf{\nabla}x[2])}\\ & {\mathrm{using~IterTools}}\\ & {\mathrm{for~x~in~Iterators~-product([true,false],~[true,false],~[true,false])}}\\ & {\mathrm{if~f(x)}}\\ & {\mathrm{@show(x)}}\\ & {\mathrm{break}}\\ & {\mathrm{end}}\\ & {\mathrm{end}}\\ & {\mathrm{x = (false,~true,~true)}} \end{array}$
```

Exercise 22.2. Formulate the problem in exercise 22.1 as an integer linear program. Can any Boolean satisfiability problem be formulated as an integer linear program?

Solution: Because the Boolean satisfiability problem simply seeks a valid solution, we set  $\mathbf{c}$  to zero in the objective. For the constraints,  $\mathbf{x}$  is constrained to be nonnegative and integral as with all integer linear programs. In addition, we let 1 correspond to true and 0 correspond to false and introduce the constraint  $\mathbf{x} \leq \mathbf{1}$ .

The  $\wedge$  "and" statements divide  $f$  into separate Boolean expressions, each of which must be true. We convert the expressions to linear constraints:

$$
x_{1} \Rightarrow x_{1} \geq 1
$$

$$
x_{2} \vee \neg x_{3} \Longrightarrow x_{2} + (1 - x_{3}) \geq 1
$$

$$
\neg x_{1} \lor \neg x_{2} \Rightarrow (1 - x_{1}) + (1 - x_{2}) \geq 1
$$

where each expression must be satisfied  $(\geq 1)$  and a negated variable  $\neg x_{i}$  is simply  $1 - x_{i}$ . The resulting integer linear program is:

$$
\begin{array}{l} \underset{x} {\text{minimize}} \quad 0 \\ \text{subject} \quad x_{1} \geq 1 \\ x_{2} - x_{3} \geq 0 \\ - x_{1} - x_{2} \geq - 1 \\ \mathbf {x} \in \mathbb {N}^{3} \\ \end{array}
$$

This approach can be used to transform any Boolean satisfiability problem into an integer linear program.

Exercise 22.3. Why are we interested in totally unimodular matrices? Furthermore, why does every totally unimodular matrix contain only entries that are  $0,1$ , or  $-1$ ?

Solution: Integer programs for which  $\mathbf{A}$  is totally unimodular and  $\mathbf{b}$  is integral can be solved exactly using the simplex method. A matrix is totally unimodular if every square nonsingular submatrix is unimodular. A single matrix entry is a square submatrix. The determinant of a  $1 \times 1$  matrix is the absolute value of its single entry. A single-entry submatrix is only unimodular if it has a determinant of  $\pm 1$ , which occurs only for entries of  $\pm 1$ . The single-entry submatrix can also be nonsingular, which allows for 0. No other entries are permitted, and thus every totally unimodular matrix contains only entries that are  $0, 1$ , or  $-1$ .

Exercise 22.4. This chapter solved the 0-1 knapsack problem using dynamic programming. Show how to apply branch and bound to the 0-1 knapsack problem, and use your approach to solve the knapsack problem with values  $\mathbf{v} = [9,4,2,3,5,3]$ , and weights  $\mathbf{w} = [7,8,4,5,9,4]$  with capacity  $w_{\max} = 20$ .

Solution: The branch and bound method requires that we can perform the branching and bounding operations on our design.[24] The decisions being made in 0-1 knapsack are whether or not to include each item. Each item therefore represents a branch; either the item is included or it is excluded.

A tree is constructed for every such enumeration according to:

24 P.J. Kolesar, "A Branch and Bound Algorithm for the Knapsack Problem," Management Science, vol. 13, no. 9, pp. 723-735, 1967.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6341a39b855d79499df0af6fdd1559d3de9d3e9b4635cfe25123591e7f284a02.jpg)

Each node represents a subproblem in which certain items have already been included or excluded. The subproblem has the associated decision variables, values, and weights removed, and the capacity is effectively reduced by the total weight of the included items.

The branch and bound method avoids constructing the entire tree using information from the bounding operation. A bound can be constructed by solving a relaxed version of the knapsack subproblem. This fractional knapsack problem allows fractional values of items to be allocated,  $0 \leq x_{i} \leq 1$ .

The relaxed knapsack problem can be efficiently solved with a greedy approach. Items are added one at a time by selecting the next item with the greatest ratio of value to weight. If there is enough remaining capacity, the item is fully assigned with  $x_{i} = 1$ . If not, a fractional value is assigned such that the remaining capacity is saturated and all remaining items have  $x_{i} = 0$ .

We begin by branching on the first item. The subtree with  $x_{1} = 0$  has the subproblem:

$$
\underset{x_{2: 6}} {\text{minimize}} - \sum_{i = 2}^{6} v_{i} x_{i}
$$

$$
\text{subject} \quad \sum_{i = 2}^{6} w_{i} x_{i} \leq 20
$$

whereas the subtree with  $x_{1} = 1$  has the subproblem:

$$
\underset{\mathbf {x}_{2: 6}} {\text{minimize}} \quad - 9 - \sum_{i = 2}^{6} v_{i} x_{i}
$$

$$
\text{subject} \quad \sum_{i = 2}^{6} w_{i} x_{i} \leq 13
$$

We can construct a lower bound for both subtrees using the greedy approach. We sort the remaining items by value to weight:

<table><tr><td>item:</td><td>6</td><td>4</td><td>5</td><td>3</td><td>2</td></tr><tr><td>ratio:</td><td>3/4</td><td>3/5</td><td>5/9</td><td>2/4</td><td>4/8</td></tr><tr><td></td><td>0.75</td><td>0.6</td><td>0.556</td><td>0.5</td><td>0.5</td></tr></table>

For the subtree with  $x_{1} = 0$ , we fully allocate items 6, 4, and 5. We then partially allocate item 3 because we have remaining capacity 2, and thus set  $x_{3} = 2 / 4 = 0.5$ . The lower bound is thus  $-(3 + 5 + 3 + 0.5 \cdot 2) = -12$ .

For the subtree with  $x_{1} = 1$ , we allocate items 6 and 4 and partially allocate item 5 to  $x_{5} = 4 / 9$ . The lower bound is thus  $-(3 + 5 + (4 / 9) \cdot 3) \approx -18.333$ .

The subtree with  $x_{1} = 1$  has the better lower bound, so the algorithm continues by splitting that subproblem. The final solution is  $\mathbf{x} = [1,0,0,0,1,1]$ .

Exercise 22.5. Discrete variables are often used to control whether or not a constraint applies. Consider using a binary variable  $z$  to control whether one of two inequality constraints applies:

$$
\underset{\mathbf {x}, z} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad \mathbf {a}_{0}^{\top} \mathbf {x} \leq b_{0} \quad \text{when} z = 0
$$

$$
\mathbf {a}_{1}^{\top} \mathbf {x} \leq b_{1} \quad \text{when} z = 1
$$

$$
\mathrm{D} x \leq \mathrm{c}
$$

$$
x \geq 0
$$

$$
z \in \{0, 1 \}
$$

We multiply by  $z$  or  $(1 - z)$  to zero out the left-hand side when appropriate:

$$
\underset{\mathbf {x}, z} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x}
$$

$$
\text{subject} \quad z \mathbf {a}_{0}^{\top} \mathbf {x} \leq b_{0}
$$

$$
\begin{array}{l} (1 - z) \mathbf {a}_{1}^{\top} \mathbf {x} \leq b_{1} \\ D x \leq c \\ x \geq 0 \\ z \in \{0, 1 \} \\ \end{array}
$$

Unfortunately, these products prevent the overall optimization problem from being a mixed-integer linear program. Show that there is an alternative formulation by which the binary variable can be incorporated that results in a mixed-integer linear program. Assume that there is a large scalar value  $M$  that exceeds the maximum values of  $\mathbf{a}_0^\top \mathbf{x} - b_0$  and  $\mathbf{a}_1^\top \mathbf{x} - b_1$  for feasible values of  $\mathbf{x}$ .

Solution: We can use the large scalar  $M$  to ensure that each constraint is satisfied based on  $z$ :

$$
\begin{array}{l} \underset{\mathbf {x}, z} {\text{minimize}} \quad \mathbf {c}^{\top} \mathbf {x} \\ \text{subject} \quad \mathbf {a}_{0}^{\top} \mathbf {x} \leq b_{0} + M z \\ \mathbf {a}_{1}^{\top} \mathbf {x} \leq b_{1} + M \cdot (1 - z) \\ D x \leq c \\ x \geq 0 \\ z \in \{0, 1 \} \\ \end{array}
$$

When  $z = 0$ , the first constraint behaves as normal and the second constraint is automatically satisfied, as  $M$  drives up the right-hand side. When  $z = 1$ , the opposite occurs. The added terms are linear in  $z$ , so the resulting constraints are linear, and the overall problem is a mixed-integer program. This technique is called the big  $M$  method.

Exercise 22.6. Consider a job shop scheduling problem with three jobs that each require completing four phases in a prescribed order on each of four different machines:

$$
\text{job} 1: \text{machines} 1 \rightarrow 2 \rightarrow 3 \rightarrow 4
$$

$$
\text{job} 2: \text{machines} 2 \rightarrow 3 \rightarrow 4 \rightarrow 1
$$

$$
\text{job} 3: \text{machines} 3 \rightarrow 4 \rightarrow 1 \rightarrow 2
$$

Each job phase has a duration:

$$
\text{job} 1: \text{durations} [ 2, 3, 4, 1 ]
$$

$$
\begin{array}{l} \text{job2 :durations [2 ,2 ,5 ,3 ]} \end{array}
$$

$$
j o b 3: d u r a t i o n s [ 1, 2, 5, 3 ]
$$

The objective is to minimize the time  $T$  required to complete all jobs by optimizing the start time of each job phase and the binary precedences, where

-  $t_{j,p}$  is the start time of phase  $p$  of job  $j$  
-  $z_{j,k,m}$  is a binary value that is 1 if job  $j$  precedes job  $k \neq j$  on machine  $m$ .

Formulate this problem as a mixed integer program, including  $T$  as a design variable, and enforce that:

- Job phase start times are nonnegative.  
- Job phases must start after their previous phases end.  
- If job  $j$  precedes job  $k$  on machine  $m$ , then job  $j$  must start after job  $k$  finishes on machine  $m$ .  
- One job proceeds the other for every job pair  $j \neq k$ .  
- The total time to complete all jobs comes after the last job ends.

Solution: We can enforce that job phase start times are nonnegative with the constraint:

$$
t \geq 0
$$

We can enforce that job phase  $p$  starts after the previous phase  $p - 1$  ends with:

$$
t_{j, p} \geq t_{j, p - 1} + \Delta t_{j, p - 1}
$$

where  $\Delta t_{j,p}$  is the duration of phase  $p$  of job  $j$ . We include 9 such constraints for the 3 subsequent phase pairs for the 3 jobs.

We must enforce that job  $j$  starts after job  $k$  on machine  $m$  if job phase  $j$  precedes  $k$ . These constraints are added for the specific job phases that occur on the specified machine. For example, for machine 1, we must ensure that job 1 phase 1 precedes job 2 phase 4 when  $z_{1,2,1} = 1$ . We formulate this using the big  $M$  method as:

$$
t_{2, 4} \geq t_{1, 1} + \Delta t_{1, 1} - M \times (1 - z_{1, 2, 1})
$$

where  $M$  is a very large scalar value. The big  $M$  method effectively removes the constraint when job 1 phase 1 does not precedes job 2 phase 4, while keeping the constraint linear. We include 24 such constraints for the 6 ordered job pairs across the 4 machines.

We can enforce that one job precedes the other in each job pair  $(j \neq k)$  and each machine  $m$  with

$$
z_{j, k, m} + z_{k, j, m} = 1
$$

We include 12 such constraints for the 3 unordered job pairs and 4 machines.

Finally, we enforce that the total time to complete all jobs  $T$  comes after the last job ends by enforcing that it comes after every job  $j$ :

$$
T \geq t_{j, 4} + \Delta t_{j, 4}
$$

We include 3 such constraints for the 3 jobs.

Exercise 22.7. Consider the following solution to the job shop scheduling problem from the previous question:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7bd553b16c903cf059c5d4a38ae9cb908d8c037d798a879dff7266ef7dc0ac60.jpg)

Job 1 is red, job 2 is green, and job 3 is blue. Although this is the optimal solution according to our objective, does this solution have any undesired properties? If yes, how could they be averted?

Solution: The solution above is an optimal solution output by a mixed integer program solver that achieves the minimum total duration of 12. While this solution is optimal for our problem formulation, this schedule does not complete each job phase as soon as possible.

One way to remedy this issue is to solve the problem a second time starting from the current design, holding the precedences  $\mathbf{z}$  fixed, but now minimizing all phase start times:

$$
\text{minimize} \left(\sum_{j = 1}^{3} \sum_{p = 1}^{4} t_{j, p}\right)
$$

Solving this second optimization problem yields:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6f107bd11a13166fb5fb393467cafe804972eef25b8150b79a93893d0115e2af.jpg)

# 23 Expression Optimization

Previous chapters discussed optimization over a fixed set of design variables. For many problems, the number of variables is unknown, such as in the optimization of graphical structures or computer programs. $^{1}$  Designs in these contexts can often be represented by expressions generated by a grammar. This chapter discusses ways to make the search of optimal designs more efficient by accounting for the grammatical structure of the design space.

# 23.1 Grammars

An expression can be represented by a tree of symbols. For example, the mathematical expression  $x + \ln 2$  can be represented using the tree in figure 23.1 consisting of the symbols  $+$ ,  $x$ ,  $\ln$ , and 2. Grammars specify the rules for generating valid expressions, thereby inducing constraints on the space of possible expressions.

A grammar is represented by a set of production rules. These rules involve symbols as well as types. A type can be interpreted as a set of expression trees. A production rule represents a possible expansion of a type into an expression involving symbols or types. $^2$  If a rule expands only to symbols, then it is called terminal because it cannot be expanded further. An example of a nonterminal rule is  $\mathbb{R} \mapsto \mathbb{R} + \mathbb{R}$ , which means that the type  $\mathbb{R}$  can consist of elements of the set  $\mathbb{R}$  added to elements in the set  $\mathbb{R}$ . $^3$

We can generate an expression from a grammar by starting with a start type and then recursively applying different production rules. We stop when the tree contains only symbols. Figure 23.2 illustrates this process for the expression  $x + \ln 2$ . An application to natural language expressions is shown in example 23.1.

The number of possible expressions allowed by a grammar can be infinite. Example 23.2 shows a grammar that allows for infinitely many valid expressions.

1 S. Gulwani, "Automating String Processing in Spreadsheets Using Input-Output Examples," in ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), 2011. J. R. Koza, F. H. Bennett, D. Andre, M. A. Keane, and F. Dunlap, "Automated Synthesis of Analog Electrical Circuits by Means of Genetic Programming," IEEE Transactions on Evolutionary Computation, vol. 1, no. 2, pp. 109-128, 1997.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1e09da415a7fbd0df8676970fa590664c43c65855f527871d8679cf0b9e83c15.jpg)  
Figure 23.1. The expression  $x + \ln 2$  represented as a tree.

2 Types are sometimes referred to as nonterminal symbols (or simply nonterminals) because they can be expanded into a sequence of terminal symbols or terminals according to the production rules defining the grammar.  
3 This chapter focuses on context-free grammars, but other forms exist. See L. Kallmeyer, Parsing Beyond Context-Free Grammars. Springer, 2010.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a4a726abbca63aeeef078a711acdf8541ee27f3acfa9d72c7fc6c3b51cd36e60.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c1a465072726671d499ccf54ff11d324369c68ffe6b1da52f014cb74b441e0c6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b5a9a256079740e06428a594cae5d6fbfcc01ff718f5c229858b5acc4f126597.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8796dfa71b7599f7e5c2142c25d5d90c36d14c1f713629032cfddd55110e9e3d.jpg)  
Figure 23.2. Using the production rules

$$
\mathbb {R} \mapsto \mathbb {R} + \mathbb {R}
$$

$$
\mathbb {R} \mapsto x
$$

$$
\mathbb {R} \mapsto \ln (\mathbb {R})
$$

$$
\mathbb {R} \mapsto 2
$$

to generate  $x + \ln 2$ . Blue nodes are unexpanded types.

Consider a grammar that allows for the generation of simple English statements:

$$
\mathrm{S} \mapsto \mathbb {N} \mathrm{V}
$$

$$
\mathbb {V} \mapsto \mathbb {V} \mathbb {A}
$$

$$
\mathbb {A} \mapsto \text{rapidyl} | \text{effeciently}
$$

$$
\mathbb {N} \mapsto \text{Alice} | \text{Bob} | \text{Mykel} | \text{Tim}
$$

$$
\mathbb {V} \mapsto \text{runs} | \text{reads} | \text{writes}
$$

The types  $\mathbb{S}, \mathbb{N}, \mathbb{V}$ , and  $\mathbb{A}$  correspond to statements, nouns, verbs, and adverbs, respectively. An expression is generated by starting with the type  $\mathbb{S}$  and iteratively replacing types:

S

N V

Mykel V A

Mykel writes rapidly

Not all terminal symbol categories must be used. For instance, the statement "Alice runs" can also be generated.

Example 23.1. A grammar for producing simple English statements. Using | on the right-hand side of an expression is shorthand for or. Thus, the rule

$$
\mathbb {A} \mapsto \text{rapidly} | \text{effectintly}
$$

is equivalent to having two rules,  $\mathbb{A}\mapsto$  rapidly and  $\mathbb{A}\mapsto$  efficiently.

Expression optimization often constrains expressions to a maximum depth or penalizes expressions based on their depth or node count. Even if the grammar allows a finite number of expressions, the space is often too vast to search exhaustively. Hence, there is a need for algorithms that efficiently search the space of possible expressions for one that optimizes an objective function.

Consider a four-function calculator grammar that applies addition, subtraction, multiplication, and division to the ten digits:

$$
\begin{array}{l} \mathbb {R} \mapsto \mathbb {R} + \mathbb {R} \\ \mathbb {R} \mapsto \mathbb {R} - \mathbb {R} \\ \mathbb {R} \mapsto \mathbb {R} \times \mathbb {R} \\ \mathbb {R} \mapsto \mathbb {R} / \mathbb {R} \\ \mathbb {R} \mapsto 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\ \end{array}
$$

An infinite number of expressions can be generated because the nonterminal  $\mathbb{R}$  can always be expanded into one of the calculator operations.

Many expressions will produce the same value. Addition and multiplication operators are commutative, meaning that the order does not matter. For example,  $a + b$  is the same as  $b + a$ . These operations are also associative, meaning the order in which multiple operations of the same type occur do not matter. For example,  $a \times b \times c$  is the same as  $c \times b \times a$ . Other operations preserve values, like adding zero or multiplying by one.

Not all expressions under this grammar are mathematically valid. For example, division by zero is undefined. Removing zero as a terminal symbol is insufficient to prevent this error because zero can be constructed using other operations, such as  $1 - 1$ . Such exceptions are often handled by the objective function, which can catch exceptions and penalize them.

We can specify grammars in code as shown in example 23.3. Some basic operations on expression trees generated from a grammar are described in example 23.4. These operations are useful in the various algorithms described in this chapter.

Example 23.2. Some of the challenges associated with grammars, as illustrated with a four-function calculator grammar.

We may define a grammar using the grammar macro. The nonterminals are on the left of the equal sign, and the expressions with terminals and nonterminals are the on the right. The package includes some syntax to represent grammars more compactly.

```julia
using ExprRules   
grammar  $=$  @grammar begin   
 $\begin{array}{rl}{\mathsf{R}}&{=x}\end{array}$  #reference a variable   
 $\mathsf{R} = \mathsf{R}*\mathsf{A}$  # multiple children   
 $\mathsf{R} = \mathsf{f}(\mathsf{R})$  # call a function   
 $\mathsf{R} = \_ (\mathsf{randn}())$  # random variable generated on node creation   
 $\mathsf{R} = 1|2|3$  #equivalent to  $R = 1,R = 2$  ,and  $R = 3$ $\mathsf{R} = |(4:6)$  #equivalent to  $R = 4,R = 5$  ,and  $R = 6$ $A = 7$  # rules for different return types   
end
```

Example 23.3. Example of defining a grammar using the ExprRules.jl package.

Many of the expression optimization algorithms involve manipulating components of an expression tree in a way that preserves the way the types were expanded. The ExprRules.jl package provides support for this. A RuleNode object represents a node in an expression tree. It includes links to child nodes, so can be used to deduce the value of the subtree rooted at that node.

Calling rand with a specified starting type will generate a random expression represented by a RuleNode. Calling sample will select a random RuleNode from an existing RuleNode tree. Nodes are evaluated using Core.eval.

The method return_type returns the node's return type as a symbol, iterminal returns whether the symbol is terminal, child_types returns the list of nonterminal symbols associated with the node's production rule, and nchildren returns the number of children. These four methods each take as input the grammar and the node. The number of nodes in a RuleNode is obtained using length(node), and the depth is obtained using depth(node).

A NodeLoc type is used to refer to a node's location in the expression tree. Subtree manipulation often requires a NodeLoc, as shown below:

```txt
loc = sample(NodeLoc, node) # uniformly sample a node loc  
loc = sample(NodeLoc, node, :R, grammar) # sample a node loc of type R  
 subtree = get(node, loc) # get the node at loc
```

Example 23.4. Basic operations on expression trees generated from a grammar.

# 23.2 Genetic Programming

Genetic algorithms (chapter 9) use chromosomes that encode design points in a sequential format. Genetic programming $^{4}$  represents individuals using trees instead (figure 23.3), which are better at representing mathematical functions, programs, decision trees, and other hierarchical structures. This book focuses only on genetic operations that adhere to the constraints of the grammar. Sometimes genetic programming with this restriction is referred to as strongly typed genetic programming. $^{5}$

Similar to genetic algorithms, genetic programs are initialized randomly and support crossover and mutation. In tree crossover (figure 23.4), two parent trees are mixed to form a child tree. A random node is chosen in each parent, and the subtree at the chosen node in the first parent is replaced with the subtree at the chosen node of the second parent. Tree crossover works on parents with different sizes and shapes, allowing arbitrary trees to mix. In some cases one must ensure that replacement nodes have certain types, such as Boolean values input into the condition of an if statement. Tree crossover is implemented in algorithm 23.1.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/596f9e62b31d815da5ce6e7276c888253d7e275b9f2bad45718cf70b4dd3ff81.jpg)  
Parent A

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7dba34449b9df6ab8cb005acc99ff22bc37231ce9ffc9f0351cad87dad349ba3.jpg)  
Parent B  
Child

Tree crossover tends to produce trees with greater depth than the parent trees. Each generation tends to increase in complexity, which often results in overly complicated solutions and slower runtimes. We encourage parsimony, or simplicity, in the solution, by introducing a small bias in the objective function value based on a tree's depth or node count.

Applying tree mutation (figure 23.5) starts by choosing a random node in the tree. The subtree rooted at that node is deleted, and a new random subtree is generated to replace the old subtree. In contrast to mutation in binary chromosomes, tree mutation can typically occur at most once, often with a low probability around  $1\%$ . Tree mutation is implemented in algorithm 23.2.

4J.R.Koza, Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT Press, 1992.  
5D.J.Montana, "StronglyTyped Genetic Programming," Evolutionary Computation, vol. 3, no. 2, pp. 199-230, 1995.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/42f1639522695da8646559f55cacd1be44b9a6ad2ad3b098d588b6d924474e74.jpg)  
Figure 23.3. A tree representation of the Julia method:

Figure 23.4. Tree crossover is used to combine two parent trees to produce a child tree.

```julia
struct TreeCrossover <: CrossoverMethod
    grammar # rule set
    max_depth # maximum depth
end
function crossover(C::TreeCrossover, a, b)
    child = deepcopy(a)
    crosspoint = sample(b)
    typ = return_type(CGRAM, crosspoint.ind)
    d_subtree = depth(crosspoint)
    d_max = C.max_depth + 1 - d_subtree
    if d_max > 0 && contains_returntype(child, C.GRAM, typ, d_max)
        loc = sample(NodeLoc, child, typ, C.GRAM, d_max)
        insert!(child, loc, deepcopy(crosspoint))
    end
    return child
end
```

Algorithm 23.1. Tree crossover implemented for a and b of type RuleNode from ExprRules.jl. The TreeCrossover struct contains a rule set grammar and a maximum depth max_depth.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7f1b2b9726e27bfcb82a7f9b819006c97e221d2e7afb980898e83675599e8032.jpg)  
Before

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8b3df6fcb67c86213cbaf5485517818c3c54cac650cda06b1084fbed0757a3a1.jpg)  
After  
Figure 23.5. Tree mutation deletes a random subtree and generates a new one to replace it.

```julia
struct TreeMutation <- MutationMethod
    grammar # rule set
    p # mutation probability
end
function mutate(M::TreeMutation, a)
    child = deepcopy(a)
    if rand() < M.p
        loc = sample(NodeLoc, child)
        typ = return_type(M.+grammer, get(child, loc).ind)
        subtree = rand(RuleNode, M.+grammer, typ)
        insert!(child, loc, subtree)
    end
    return child
```

Algorithm 23.2. Tree mutation implemented for an individual a of type RuleNode from ExprRules.jl. The TreeMutation struct contains a rule set grammar and a mutation probability p.

Tree permutation (figure 23.6) is a second form of genetic mutation. The children of a randomly chosen node are randomly permuted. Tree permutation alone is typically not sufficient to introduce new genetic material and is often combined with tree mutation. Tree permutation is implemented in algorithm 23.3.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1d486daed030b8ea1a589ef453624a364641f267fc99fba3cc9f12014400e295.jpg)  
Before

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/1a2b7b131e2aef227775b03d894c22fbe0a34ee6bc391277151564cdc1f77038.jpg)  
After  
Figure 23.6. Tree permutation permutes the children of a randomly chosen node.

```julia
struct TreePermutation <- MutationMethod
    grammar # rule set
    p # mutation probability
end
function mutate(M::TreePermutation, a)
    child = deepcopy(a)
    if rand() < M.p
        node = sample(child)
        n = length(node.children)
        types = child_types(MGRAM, node)
        for i in 1 : n-1
            c = 1
            for k in i+1 : n
                if types[k] == types[i] && rand() < 1/(c++1)
                    node.children[i], node.children[k] =
                        node.children[k], node.children[i]
            end
        end
    return child
```

Algorithm 23.3. Tree permutation implemented for an individual a of type RuleNode from ExprRules.jl, where p is the mutation probability.

The implementation of genetic programming is otherwise identical to that of genetic algorithms. More care must typically be taken in implementing the crossover and mutation routines, particularly when determining what sorts of nodes can be generated and that only syntactically correct trees are produced. Genetic programming is used to generate an expression that approximates  $\pi$  in example 23.5.

Consider approximating  $\pi$  using only operations on a four-function calculator. We can solve this problem using genetic programming where nodes can be any of the elementary operations: add, subtract, multiply, and divide, and the digits  $1 - 9$ .

We use ExprRules.jl to specify our grammar:

```julia
grammar = @grammar begin  
R = | (1:9)  
R = R + R  
R = R - R  
R = R / R  
R = R * R  
end
```

We construct an objective function and penalize large trees:

```txt
function f(node) value  $=$  Core.eval(node, grammar) if isinf(value) || isnan(value) return Inf end  $\Delta =$  abs(value - π) return log(Δ) + length(node)/1e3   
end
```

We finally run our genetic program using a call to the population_method function from section 9.2:

```julia
M = GeneticAlgorithm(TruncationSelection(50), TreeCrossover(grammar, 10), TreeMutation(grammar, 0.25))  
population = [rand(RuleNode, grammar, :R) for i in 1:1000]  
population = population_method(M, f, population, k_max)  
best_tree = population[argmin(f.(population))]
```

The best performing tree is shown on the right. It evaluates to 3.141586, which matches  $\pi$  to four decimal places.

Example 23.5. Using genetic programming to estimate  $\pi$  using only digits and the four principle arithmetic operations.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3a3fb269cd0cf26eb4b9b0dbb823bb1fa451acde93bf9687abffc6a27b669f4f.jpg)

# 23.3 Grammatical Evolution

Grammatical evolution<sup>6</sup> operates on an integer array instead of a tree, allowing the same techniques employed in genetic algorithms to be applied. Unlike genetic algorithms, the chromosomes in grammatical evolution encode expressions based on a grammar. Grammatical evolution was inspired by genetic material, which is inherently serial like the chromosomes used in genetic algorithms.<sup>7</sup>

In grammatical evolution, designs are integer arrays much like the chromosomes used in genetic algorithms. Each integer is unbounded because indexing is performed using modular arithmetic. The integer array can be translated into an expression tree by parsing it from left to right. We begin with a starting symbol and a grammar. Suppose  $n$  rules in the grammar can be applied to the starting symbol. The  $j$ th rule is applied, where  $j = i \bmod_1 n$  and  $i$  is the first integer in the integer array.<sup>8</sup> We then consider the rules applicable to the resulting expression and use similar modular arithmetic based on the second integer in the array to select which rule to apply. This process is repeated until no rules can be applied and the phenotype is complete.<sup>9</sup> The decoding process is implemented in algorithm 23.4 and is worked through in example 23.6.

It is possible for the integer array to be too short, thereby causing the translation process to run past the length of the array. Rather than producing an invalid individual and penalizing it in the objective function, the process wraps around to the beginning of the array instead. This wrap-around effect means that the same decision can be read several times during the transcription process. Transcription can result in infinite loops, which can be prevented by a maximum depth.

Genetic operations work directly on the integer design array. We can adopt the operations used on real-valued chromosomes and apply them to the integer-valued chromosomes. The only change is that mutation must preserve real values. A mutation method for integer-valued chromosomes using zero-mean Gaussian perturbations is implemented in algorithm 23.5.

Grammatical evolution uses two additional genetic operators. The first, gene duplication, occurs naturally as an error in DNA replication and repair. Gene duplication can allow new genetic material to be generated and can store a second copy of a useful gene to reduce the chance of a lethal mutation removing the gene from the gene pool. Gene duplication chooses a random interval of genes in the chromosome to duplicate. A copy of the selected interval is appended to the back of the chromosome. Duplication is implemented in algorithm 23.6.

$^{6}$ C. Ryan, J.J. Collins, and M.O. Neill, "Grammatical Evolution: Evolving Programs for an Arbitrary Language," in European Conference on Genetic Programming, 1998.  
7 Our serial DNA is read and used to construct complicated protein structures. DNA is often referred to as the genotype—the object on which genetic operations are performed. The protein structure is the phenotype—the object encoded by the genotype whose performance is evaluated. The grammatical evolution literature often refers to the integer design vector as the genotype and the resulting expression as the phenotype.  
8 We use  $x \bmod_1 n$  to refer to the 1-index modulus:

$$
((x - 1) \mod n) + 1
$$

This type of modulus is useful with 1-based indexing. The corresponding Julia function is mod1.

9 No genetic information is read when there is only a single applicable rule.

```julia
struct DecodedExpression
    node   # node in expression tree
    n_rule_applied # number of rules applied
end
function _decode(x, grammar, typ, c_max, c)
    types = grammar[type]
    if length(type) > 1
        g = x[mod1(c += 1, length(x))]
        rule = types[mod1(g, length(type))]
    else
        rule = types[1]
    end
    node = RuleNode(rule)
    childtypes = child_types(grammar, node)
    if !isEmpty(childtypes) && c < c_max
        for ctyp in childtypes
            cnode, c = _decode(x, grammar, ctyp, c_max, c)
            push!(node.children, cnode)
        end
    end
    return (node, c)
end
function decode(x, grammar, sym, c_max=1000, c=0)
    node, c = _decode(x, grammar, sym, c_max, c)
DecodedExpression(node, c)
```

Algorithm 23.4. A method for decoding an integer design vector to produce an expression, where  $x$  is a vector of integers, grammar is a Grammar, and sym is the root symbol. The counter  $c$  is used during the recursion process and the parameter  $c_{\max}$  is an upper limit on the maximum number of rule applications, to prevent an infinite loop. The method returns a DecodedExpression, which contains the expression tree and the number of rules applied during the decoding process.

Consider a grammar for real-valued strings:

$$
\mathbb {R} \mapsto \mathbb {D} \mathbb {D}^{\prime} \mathbb {P} \mathbb {E}
$$

$$
\mathbb {D}^{\prime} \mapsto \mathbb {D} \left. \mathbb {D}^{\prime} \right| \epsilon
$$

$$
\mathbb {P} \mapsto . \mathbb {D} \mathbb {D}^{\prime} | \epsilon
$$

$$
\mathbb {E} \mapsto \operatorname{ESD} \mathbb {D}^{\prime} | \epsilon
$$

$$
\mathsf {S} \mapsto + | - | \epsilon
$$

$$
\mathbb {D} \mapsto 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
$$

where  $\mathbb{R}$  is a real value,  $\mathbb{D}$  is a terminal decimal,  $\mathbb{D}'$  is a nonterminal decimal,  $\mathbb{P}$  is the decimal part,  $\mathbb{E}$  is the exponent, and  $\mathbb{S}$  is the sign. Any  $\epsilon$  values produce empty strings.

Suppose our design is [205, 52, 4, 27, 10, 59, 6] and we have the starting symbol  $\mathbb{R}$ . There is only one applicable rule, so we do not use any genetic information and we replace  $\mathbb{R}$  with  $\mathbb{D}\mathbb{D}'\mathbb{P}\mathbb{E}$ .

Next we must replace  $\mathbb{D}$ . There are 10 options. We select  $205 \bmod_{1} 10 = 5$ , and thus obtain  $4\mathbb{D}'\mathbb{P}\mathbb{E}$

Next we replace  $\mathbb{D}'$ , which has two options. We select index  $52 \bmod_12 = 2$ , which corresponds to  $\epsilon$ .

Continuing in this manner we produce the string  $4E + 8$ .

The grammar can be implemented in ExprRules using:

```txt
grammar = @grammar begin  
R = D * De * P * E  
De = D * De | '''  
P = ".* * D * De | *"  
E = "E" * S * D * De | '''  
S = "+"| "-"|""  
D = "0" | "1" | "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9"  
end
```

and can be evaluated using:

```txt
$\mathbf{x} = [205, 52, 4, 27, 10, 59, 6]$   
str = Core.eval(decode(x, grammar, :R).node, grammar)
```

Example 23.6. The process by which an integer design vector in grammatical evolution is decoded into an expression.

Our implementation is depth-first. If 52 were instead 51, the rule  $\mathbb{D}' \mapsto \mathbb{D} \mathbb{D}'$  would be applied, followed by selecting a rule for the new  $\mathbb{D}$ , eventually resulting in  $43950.950E + 8$ .

```julia
struct AtomicIntegerMutation <- MutationMethod
     $\sigma$  # standard deviation
end
function mutate(M::IntegerGaussianMutation, child)
    return child + round.Int, rand(length(child)).*M. $\sigma$ )
```

```julia
struct GeneDuplication <- MutationMethod end
function mutate(M::GeneDuplication, child) 
    n = length(child)
    i, j = rand(1:n), rand(1:n)
    interval = min(i, j): max(i, j)
    return [child; deepcopy(child[interval])] end
```

The second genetic operation, pruning, tackles a problem encountered during crossover. As illustrated in figure 23.7, crossover will select a crossover point at random in each chromosome and construct a new chromosome using the left side of the first and the right side of the second chromosome. Unlike genetic algorithms, the trailing entries in chromosomes of grammatical evolution may not be used; during parsing, once the tree is complete, the remaining entries are ignored. The more unused entries, the more likely it is that the crossover point lies in the inactive region, thus not providing new beneficial material. An individual is pruned with a specified probability, and, if pruned, its chromosome is truncated such that only active genes are retained. Pruning is implemented in algorithm 23.7.

```julia
struct GenePruning <- MutationMethod
p # mutation probability
grammar # rule set
typ # type symbol
end
function mutate(M::GenePruning, child)
if rand() < M.p
    c = decode(child, M.grammar, M.typ).n_rule_applied
    if c < length(child)
        child = child[1:c]
    end
end
return child
```

Algorithm 23.5. The Gaussian mutation method modified to preserve integer values for integer-valued chromosomes. Each value is perturbed by a zero-mean Gaussian random value with standard deviation  $\sigma$  and then rounded to the nearest integer.

Algorithm 23.6. The gene duplication method used in grammatical evolution.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fbb4bb10aa2489a5ea55283495ea2379c4260def8eb0628746a71dfc28461af4.jpg)  
Figure 23.7. Crossover applied to chromosomes in grammatical evolution may not affect the active genes in the front of the chromosome. The child shown here inherits all of the active shaded genes from parent  $a$  so it will effectively act as an identical expression. Pruning was developed to overcome this issue.

Algorithm 23.7. The gene pruning method used in grammatical evolution.

Like genetic programming, grammatical evolution can use the genetic algorithm method. $^{10}$  Algorithm 23.8 applies multiple mutation methods in order to use pruning, duplication, and standard mutation approaches.

```julia
struct MultiMutate <- MutationMethod
Ms # mutation methods
end
function mutate(M::MultiMutate, child)
    for m in M.Ms
        child = mutate(m, child)
    end
    return child
end
```

Grammatical evolution suffers from two primary drawbacks. First, it is difficult to tell whether the chromosome is feasible without decoding it into an expression. Second, a small change in the chromosome may produce a large change in the corresponding expression.

# 23.4 Probabilistic Grammars

A probabilistic grammar $^{11}$  adds a weight to each rule in a grammar. When sampling from all applicable rules for a given node, we select a rule stochastically according to the relative weights. The probability of an expression is the product of the probabilities of sampling each rule. Algorithm 23.9 implements the probability calculation. Example 23.7 demonstrates sampling an expression from a probabilistic grammar and computes its likelihood.

```julia
struct ProbabilisticGrammar
    grammar # rule set
    ws # mapping of types to weights
end
function probability(probgram, node)
    typ = return_type(probgram.grammar, node)
    i = findfirst(isequal(node.ind), probgram.grammar[type])
    p = probgram.ws[type][i] / sum(probgram.ws[type])
    for c in node.children
        p *= probability(probgram, c)
    end
    return p
```

10 Genotype to phenotype mapping would occur in the objective function.

Algorithm 23.8. A method for applying all mutation methods stored in the vector Ms.

11 T. L. Booth and R. A. Thompson, "Applying Probability Measures to Abstract Languages," IEEE Transactions on Computers, vol. C-22, no. 5, pp. 442-450, 1973.

Algorithm 23.9. A method for computing the probability of an expression based on a probabilistic grammar, where program is a probabilistic grammar consisting of a grammar grammar and a mapping of types to weights for all applicable rules ws, and node is a RuleNode expression.

Optimization using a probabilistic grammar improves its weights with each iteration using elite samples from a population. At each iteration, a population of expressions is sampled and their objective function values are computed. Some number of the best expressions are considered the elite samples and can be used to update the weights. A new set of weights is generated for the probabilistic grammar, where the weight  $w_{i}^{\mathbb{T}}$  for the  $i$ th production rule applicable to return type  $\mathbb{T}$  is set to the number of times the production rule was used in generating the elite samples.[12] This update procedure is implemented in algorithm 23.10.

# 23.5 Probabilistic Prototype Trees

The probabilistic prototype tree $^{13}$  is a different approach that learns a distribution for every node in the expression tree. Each node in a probabilistic prototype tree contains a probability vector representing a categorical distribution over the grammar's production rules. The probability vectors are updated to reflect knowledge gained from successive generations of expressions. The maximum number of children for a node is the maximum number of nonterminals among rules in the grammar.

Probability vectors are randomly initialized when a node is created. Random probability vectors can be drawn from a Dirichlet distribution, which is a distribution over discrete distributions. $^{14}$  The original implementation initializes terminals to a scalar value of 0.6 and nonterminals to 0.4. In order to handle grammars, we maintain a probability vector for applicable rules to each parent type. Algorithm 23.11 defines a node type and implements this initialization method.

Expressions are sampled using the probability vectors in the probabilistic prototype tree. A rule in a node is drawn from the categorical distribution defined by the node's probability vector for the required return type, normalizing the associated probability vector values to obtain a valid probability distribution. The tree is traversed in depth-first order. This sampling procedure is implemented in algorithm 23.12 and visualized in figure 23.8.

Learning can use information either from an entire sampled population or from elite samples. Let the best expression in the current generation be  $x_{\mathrm{best}}$  with value  $y_{\mathrm{best}}$  and the best expression found so far be  $x_{\mathrm{elite}}$  with value  $y_{\mathrm{elite}}$ . The node probabilities are updated to increase the likelihood of generating  $x_{\mathrm{best}}$ .<sup>15</sup>

The probability of generating  $x_{\mathrm{best}}$  is the product of the probabilities of choosing each rule in  $x_{\mathrm{best}}$  when traversing through the probabilistic prototype tree. We

12 Probabilistic grammars can be extended to more complicated probability distributions that consider other factors, such as the depth in the expression or local dependencies among siblings in subtrees. One approach is to use Bayesian networks. P.K. Wong, L.Y. Lo, M.L. Wong, and K.S. Leung, "Grammar-Based Genetic Programming with Bayesian Network," in IEEE Congress on Evolutionary Computation (CEC), 2014.  
13 R. Salustowicz and J. Schmidhuber, "Probabilistic Incremental Program Evolution," Evolutionary Computation, vol. 5, no. 2, pp. 123-141, 1997.  
14 Named for the German mathematician Johann Peter Gustav Lejeune Dirichlet (1805-1859). D. Barber, Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.

15 The original probabilistic prototype tree implementation will periodically increase the likelihood of generating  $x_{\mathrm{elite}}$ .

Consider a probabilistic grammar for strings composed entirely of "a"s:

$$
\begin{array}{l} \mathbb {A} \mapsto \mathrm{a} \mathbb {A} \quad w_{1}^{\mathbb {A}} = 1 \\ \mapsto \mathrm{a} \mathbb {B} \mathrm{a} \mathbb {A} \quad w_{2}^{\mathbb {A}} = 3 \\ \mapsto \epsilon \quad w_{3}^{\mathbb {A}} = 2 \\ \mathbb {B} \mapsto \mathrm{aB} \quad w_{1}^{\mathbb {B}} = 4 \\ \mapsto \epsilon \quad w_{2}^{\mathrm{B}} = 1 \\ \end{array}
$$

where we have a set of weights  $\mathbf{w}$  for each parent type and  $\epsilon$  is an empty string.

Suppose we generate an expression starting with the type  $\mathbb{A}$ . The probability distribution over the three possible rules is:

$$
\begin{array}{l} \begin{array}{l l} P (\mathbb {A} \mapsto \mathsf {a A}) = 1 / (1 + 3 + 2) & = 1 / 6 \end{array} \\ P (\mathbb {A} \mapsto \mathsf {a B a A}) = 3 / (1 + 3 + 2) = 1 / 2 \\ P (\mathbb {A} \mapsto \epsilon) = 2 / (1 + 3 + 2) = 1 / 3 \\ \end{array}
$$

Suppose we sample the second rule and obtain a  $\mathbb{B}$  a A.

Next we sample a rule to apply to  $\mathbb{B}$ . The probability distribution over the two possible rules is:

$$
\begin{array}{l} \begin{array}{r l} P (\mathbb {B} \mapsto \mathrm{aB}) = 4 / (4 + 1) & = 4 / 5 \end{array} \\ P (\mathbb {B} \mapsto \epsilon) = 1 / (4 + 1) = 1 / 5 \\ \end{array}
$$

Suppose we sample the second rule and obtain a  $\epsilon$  a  $\mathbb{A}$

Next we sample a rule to apply to  $\mathbb{A}$ . Suppose we sample  $\mathbb{A} \mapsto \epsilon$  to obtain a  $\epsilon$  a  $\epsilon$ , which produces the "a" string "aa". The probability of the sequence of rules applied to produce "aa" under the probabilistic grammar is:

$$
P (\mathbb {A} \mapsto \mathsf {a B a A}) P (\mathbb {B} \mapsto \epsilon) P (\mathbb {A} \mapsto \epsilon) = \frac{1}{2} \cdot \frac{1}{5} \cdot \frac{1}{3} = \frac{1}{30}
$$

Note that this is not the same as the probability of obtaining "aa", as other sequences of production rules could also have produced it.

Example 23.7. Sampling an expression from a probabilistic grammar and computing the expression's likelihood.

```txt
function_update!(probgram,x)   
grammar  $=$  probgram.grammar   
typ  $=$  return_type(grammer，x)   
i  $=$  findfirst(isequal(x.ind)，grammar[type])   
probgram.ws[type][i] += 1   
for c in x.children   
_update!(probgram，c)   
end   
return probgram   
end   
function update!(probgram,Xs)   
for w in values(program.ws) fill!(w,0)   
end   
for x in Xs   
_update!(probgram，x)   
end   
return probgram   
end
```

Algorithm 23.10. A method for applying a learning update to a probabilistic grammar program based on an elite sample of expressions Xs.

```julia
struct PPTNode
    ps # mapping of symbol to rule probabilities
    children # child PPTNodes
end
function PPTNode(grammar;
    w-terminal = 0.6,
    w_nonterm = 1 - w.Terminal)
    ps = Dict(typ => normalize!([isterminal(grammar, i) ? w.Terminal : w_nonterm for i in grammar[type], 1) for typ in nonterminals(grammar))
PPTNode(ps, PPTNode])
end
function get_child(ppt::PPTNode, grammar, i)
    if i > length(ppt(children)
        push!(ppt的孩子, PPTNode(grammar))
    end
    return ppt.children[i]
end
```

Algorithm 23.11. A probabilistic prototype tree node type and associated initialization function where ps is a dictionary mapping a symbol corresponding to a return type to a probability vector over applicable rules, and children is a list of PPTNodes. The method get_child will automatically expand the tree when attempting to access a nonexistent child.

```julia
function rand(ppt, grammar, typ)  
rules = grammar[type]  
rule_index = sample/rules, Weights(ppt.ps[type])  
ctypes = child_types(grammar, rule_index)  
arr = Vector{RuleNode} (undef, length(ctypes))  
node = iseval(grammar, rule_index) ?  
RuleNode(rule_index, Core.eval(grammar, rule_index), arr) :  
RuleNode(rule_index, arr)  
for (i, typ) in enumerate(ctypes)  
node(children[i] = rand(get_child(ppt, grammar, i), grammar, typ)  
end  
return node  
end
```

Algorithm 23.12. A method for sampling an expression from a probabilistic prototype tree. The tree is expanded as needed.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/eed3a3f489c15ac7b92b2a7d393ef94eaaba17a207852a37703cf9c4362e88c6.jpg)  
Figure 23.8. A probabilistic prototype tree initially contains only the root node but expands as additional nodes are needed during expression generation.

compute a target probability for  $P(x_{\mathrm{best}})$ :

$$
P_{\text{target}} = P \left(x_{\text{best}}\right) + \left(1 - P \left(x_{\text{best}}\right)\right) \cdot \alpha \cdot \frac{\epsilon - y_{\text{elite}}}{\epsilon - y_{\text{best}}} \tag {23.1}
$$

where  $\alpha$  and  $\epsilon$  are positive constants. The fraction on the right-hand side produces larger steps toward expressions with better objective function values. The target probability can be calculated using algorithm 23.13.

```txt
function probability(ppt, grammar, expr)  
    typ = return_type(grammar, expr)  
    i = findfirst(isequal(expr.ind), grammar[type])  
    p = ppt.ps[type][i]  
    for (i,c) in enumerate(expr.children)  
        p *= probability(get_child(ppt, grammar, i), grammar, c)  
    end  
    return p  
end  
function p_target(ppt, grammar, x_best, y_best, y_/_elite,  $\alpha$ ,  $\epsilon$ )  
    p_best = probability(ppt, grammar, x_best)  
    return p_best + (1 - p_best)* $\alpha$ *( $\epsilon$ -y_/_elite)/( $\epsilon$ -y_best)  
end
```

The target probability is used to adjust the probability vectors in the probabilistic prototype tree. The probabilities associated with the chosen nodes are increased iteratively until the target probability is exceeded:

$$
P \left(x_{\text{best}}^{(i)}\right) \leftarrow P \left(x_{\text{best}}^{(i)}\right) + c \cdot \alpha \cdot \left(1 - P \left(x_{\text{best}}^{(i)}\right)\right) \tag {23.2}
$$

for all  $i$  where  $x_{\mathrm{best}}^{(i)}$  is the  $i$ th rule applied in expression  $x_{\mathrm{best}}$  and  $c$  is a scalar. $^{16}$

The adapted probability vectors are then renormalized to 1 by downscaling the values of all nonincreased vector components proportionally to their current value. The probability vector  $\mathbf{p}$ , where the  $i$ th component was increased, is adjusted according to:

$$
p_{j} \leftarrow p_{j} \frac{1 - p_{i}}{\| \mathbf {p} \| _{1} - p_{i}} \text{for} j \neq i \tag {23.3}
$$

The learning update is implemented in algorithm 23.14.

In addition to population-based learning, probabilistic prototype trees can also explore the design space via mutations. The tree is mutated to explore the region around  $\mathbf{x}_{\mathrm{best}}$ . Let  $\mathbf{p}$  be a probability vector in a node that was accessed when

Algorithm 23.13. Methods for computing the probability of an expression and the target probability, where ppt is the root node of the probabilistic prototype tree, grammar is the grammar, expr and x_best are RuleNode expressions, y_best and y_elite are scalar objective function values, and  $\alpha$  and  $\epsilon$  are scalar parameters.

16 The original paper recommends  $c = 0.1$

```matlab
function_update!(ppt,grammar,x,c,α) typ  $=$  return_type(grammar,x) i  $=$  findfirst(isequal(x.ind),grammar[typ]) p  $=$  ppt.ps[type] p[i]  $\text{一} =$  c\**(1-p[i]) psum  $=$  sum(p) for j in eachindex(p) ifj！=i p[j]\*=(1-p[i])/(psum-p[i]) end end for (pptchild,xchild) in zip(ppt.children,x.children) _update!(pptchild,grammar,xchild,c,α) end return ppt   
end function update!(ppt,grammar,x_best,y_best,y elitc,α,c,ε) p_targ  $=$  p_target(ppt,grammar,x_best,y_best,y elitc,α,ε) while probability(ppt,grammar,x_best)<p_targ _update!(ppt,grammar,x_best,c,α) end return ppt   
end
```

Algorithm 23.14. A method for applying a learning update to a probabilistic prototype tree with root ppt, grammar grammar, best expression x_best with objective function value y_best, elite objective function value y_elite, step factor  $\alpha$ , step factor multiplier c, and parameter  $\epsilon$ .

generating  $\mathbf{x}_{\mathrm{best}}$ . Each component in  $\mathbf{p}$  is mutated with a probability proportional to the problem size:

$$
\frac{p_{\text{mutation}}}{\# \mathbf {p} \sqrt{\# \mathbf {x}_{\text{best}}}} \tag {23.4}
$$

where  $p_{\text{mutation}}$  is a mutation parameter,  $\# \mathbf{p}$  is the number of components in  $\mathbf{p}$ , and  $\# \mathbf{x}_{\text{best}}$  is the number of rules applied in  $\mathbf{x}_{\text{best}}$ . A component  $i$ , selected for mutation, is adjusted according to:

$$
p_{i} \leftarrow p_{i} + \beta \cdot \left(1 - p_{i}\right) \tag {23.5}
$$

where  $\beta$  controls the amount of mutation. Small probabilities undergo larger mutations than do larger probabilities. All mutated probability vectors must be renormalized. Mutation is implemented in algorithm 23.15 and visualized in figure 23.9.

Finally, subtrees in the probabilistic prototype tree are pruned in order to remove stale portions of the tree. A child node is removed if its parent contains a probability component above a specified threshold such that, when chosen, causes the child to be irrelevant. This is always the case for a terminal and may

```txt
function mutate!(ppt, grammar, x_best, p MUTATION,  $\beta$  sqrtlen  $=$  sqrt(length(x_best)),   
） typ  $=$  return_type(grammar, x_best)  $\mathsf{p} =$  ppt.ps[typ] prob  $=$  p MUTATION/(length(p)*sqrtlen) for i in eachindex(p) if rand()  $<  <$  prob  $\mathsf{p}[\mathsf{i}]\mathsf{+} = \mathsf{\beta}*(1 - \mathsf{p}[\mathsf{i}])$  end   
end   
normalize!(p,1)   
for (pptchild,xchild) in zip(ppt.children, x_best.children) mutate!(pptchild, grammar, xchild, p MUTATION,  $\beta$  sqrtlen=sqrtlen)   
end   
return ppt   
end
```

Algorithm 23.15. A method for mutating a probabilistic prototype tree with root ppt, grammar grammar, best expression x_best, mutation parameter p_mutation, and mutation rate  $\beta$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f6d4bc972ea58de6f011ee44f5fcf64fd6d4b4a8fa8ce5ba2e4a847fb4aed8c4.jpg)  
Figure 23.9. Mutating a probability vector in a probabilistic prototype tree with  $\beta = 0.5$ . The mutated components were increased according to equation (23.5) and the resulting probability vector was renormalized. Smaller probabilities receive greater increases.

be the case for a nonterminal. Pruning is implemented in algorithm 23.16 and demonstrated in example 23.8.

```julia
function prune!(ppt, grammar; p_threshold=0.99)  
kmax, pmax = None, 0.0  
for (k, p) in ppt.ps  
pmax' = maximum(p)  
if pmax' > pmax  
kmax, pmax = k, pmax'  
end  
end  
if pmax > p_threshold  
i = argmax(ppt.ps[kmax])  
if isterminal(grammar, i)  
empty!(ppt.children[kmax])  
else  
max_arity_for_rule = maximum(nchildren(grammar, r) for r in grammar[kmax])  
while length(ppt.children) > max_arity_for_rule  
pop!(ppt.children)  
end  
end  
end  
for child in ppt.children  
prune!(child, grammar, p_threshold=p_threshold)  
end  
return ppt  
end
```

Algorithm 23.16. A method for pruning a probabilistic prototype tree with root ppt, grammar grammar, and pruning probability threshold p_treshold.

# 23.6 Summary

- Expression optimization allows for optimizing tree structures that, under a grammar, can express sophisticated programs, structures, and other designs lacking a fixed size.  
- Grammars define the rules used to construct expressions.  
- Genetic programming adapts genetic algorithms to perform mutation and crossover on expression trees.  
- Grammatical evolution operates on an integer array that can be decoded into an expression tree.

Consider a node with a probability vector over the rule set:

$$
\begin{array}{l} \mathbb {R} \mapsto \mathbb {R} + \mathbb {R} \\ \mathbb {R} \mapsto \ln (\mathbb {R}) \\ \mathbb {R} \mapsto 2 \mid x \\ \mathbb {R} \mapsto \mathbb {S} \\ \end{array}
$$

If the probability of selecting 2 or  $x$  grows large, then any children in the probabilistic prototype tree are unlikely to be needed and can be pruned. Similarly, if the probability of choosing  $S$  grows large, any children with return type  $\mathbb{R}$  are unneeded and can be pruned.

- Probabilistic grammars learn which rules are best to generate, and probabilistic prototype trees learn probabilities for every iteration of the expression rule generation process.

# 23.7 Exercises

Exercise 23.1. How many expression trees can be generated using the following grammar and the starting set  $\{\mathbb{R},\mathbb{I},\mathbb{F}\}$ ?

$$
\begin{array}{l} \mathbb {R} \mapsto \mathbb {I} \mid \mathbb {F} \\ \mathbb {I} \mapsto 1 \mid 2 \\ \mathbb {F} \mapsto \pi \\ \end{array}
$$

Solution: Six expression trees can be generated:

I

1

F

R

R

R

↓

↓

↓

↓

↓

↓

12

π

I

II

F

12

π

Exercise 23.2. The number of expression trees up to height  $h$  that can be generated under a grammar grows super-exponentially. As a reference, calculate the number of expressions of height  $h$  can be generated using the grammar: $^{17}$

$$
\mathbb {N} \mapsto \left\{\mathbb {N}, \mathbb {N} \right\} \mid \left\{\mathbb {N}, \right\} \mid \{, \mathbb {N} \} \mid \{\}
$$

Example 23.8. An example of when pruning for probabilistic prototype trees should be applied.

17 Let an empty expression have height 0, the expression  $\{\}$  have height 1, and so on.

Solution: Only one expression of height 0 exists and that is the empty expression. Let us denote this as  $a_0 = 1$ . Similarly, only one expressions of height 1 exist and that is the expression {}. Let us denote this as  $a_1 = 1$ . Three expressions exist for depth 2, 21 for depth 3, and so on.

Suppose we have constructed all expressions up to height  $h$ . All expressions of height  $h + 1$  can be constructed using a root node with left and right sub-expressions selected according to:

1. A left expression of height  $h$  and a right expression of height less than  $h$  
2. A right expression of height  $h$  and a left expression of height less than  $h$  
3. Left and right expressions of height  $h$

It follows that the number of expressions of height  $h + 1$  are:18

$$
a_{h + 1} = 2 a_{h} \left(a_{0} + \dots + a_{h - 1}\right) + a_{h}^{2}
$$

This corresponds to OEIS sequence Aoo1699.

Exercise 23.3. Define a grammar which can generate any nonnegative integer.

Solution: One can use the following grammar and the starting symbol  $\mathbb{I}$ :

$$
\mathbb {I} \mapsto \mathbb {D} + 10 \times \mathbb {I}
$$

$$
\mathbb {D} \mapsto 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
$$

Exercise 23.4. How do expression optimization methods handle divide-by-zero values or other exceptions encountered when generating random subtrees?

Solution: Constructing exception-free grammars can be challenging. Such issues can be avoided by catching the exceptions in the objective function and suitably penalizing them.

Exercise 23.5. Consider an arithmetic grammar such as:

$$
\mathbb {R} \mapsto x \mid y \mid z \mid \mathbb {R} + \mathbb {R} \mid \mathbb {R} - \mathbb {R} \mid \mathbb {R} \times \mathbb {R} \mid \mathbb {R} / \mathbb {R} \mid \ln \mathbb {R} \mid \sin \mathbb {R} \mid \cos \mathbb {R}
$$

Suppose the variables  $x, y$ , and  $z$  each have units, and the output is expected to be in particular units. How might such a grammar be modified to respect units?

Solution: There are many reasons why one must constrain the types of the variables manipulated during expression optimization. Many operators are valid only on certain inputs<sup>19</sup> and matrix multiplication requires that the dimensions of the inputs be compatible. Physical dimensionality of the variables is another concern. The grammar must reason about the units of the input values and of the valid operations that can be performed on them.

For instance,  $x \times y$ , with  $x$  having units  $\mathrm{kg}^a\mathrm{m}^b\mathrm{s}^c$ , and  $y$  having units  $\mathrm{kg}^d\mathrm{m}^e\mathrm{s}^f$ , will produce a value with units  $\mathrm{kg}^{a + d}\mathrm{m}^{b + e}\mathrm{s}^{c + f}$ . Taking the square root of  $x$  will produce a value with units  $\mathrm{kg}^{a / 2}\mathrm{m}^{b / 2}\mathrm{s}^{c / 2}$ . Furthermore, operations such as sin can be applied only to unitless inputs.

19 One typically does not take the square root of a negative number.

One approach for handling physical units is to associate an  $n$ -tuple with each node in the expression tree. The tuple records the exponent with respect to the allowable elementary units, which are specified by the user. If the elementary units involved are mass, length, and time, then each node would have a 3-tuple  $(a,b,c)$  to represent units  $\mathrm{kg}^a\mathrm{m}^b\mathrm{s}^c$ . The associated grammar must take these units into account when assigning production rules.[20]

Exercise 23.6. Consider the grammar

$$
\mathbb {S} \mapsto \mathbb {N P V P}
$$

$$
\mathbb {N P} \mapsto \mathbb {A D J N P} \mid \mathbb {A D J N}
$$

$$
\mathrm{VP} \mapsto \mathrm{VADV}
$$

$$
\mathbb {A D J} \mapsto a | \text{the} | \text{big} | \text{litle} | \text{blue} | \text{red}
$$

$$
\mathbb {N} \mapsto \text{mouse} | \text{cat} | \text{dog} | \text{pony}
$$

$$
\mathbb {V} \mapsto \operatorname{ran} | \operatorname{sat} | \operatorname{slept} | \operatorname{ate}
$$

$$
\mathbb {A D V} \mapsto \text{quietly} | \text{quickly} | \text{soundly} | \text{happliy}
$$

What is the phenotype corresponding to the genotype [2, 10, 19, 0, 6] and the starting symbol $?

Solution: The grammar can be encoded using ExprRules.jl using string composition:

```txt
grammar = @grammar begin  
S = NP * " " * VP  
NP = ADJ * " " * NP  
NP = ADJ * " " * N  
VP = V * " " * ADV  
ADJ = |([["a", "the", "big", "little", "blue", "red)])  
N = |([["mouse", "cat", "dog", "pony]])
V = |([["ran", "sat", "slept", "ate"]
ADV = |([["quietly", "quickly", "soundly", "happily)])  
end
```

We can use our decode method to obtain the solution.

Core.eval(decode([2,10,19,0,6], grammar, :S)[1], grammar)

This produces "little dog ate quickly".

Exercise 23.7. Use genetic programming to evolve the gear ratios for a clock. Assume all gears are restricted to have radii selected from  $\mathcal{R} = \{10,25,30,50,60,100\}$ . Each gear can either be attached to its parent's axle, thereby sharing the same rotation period, or be interlocked on its parent's rim, thereby having a rotation period depending on the parent's rotation period and on the gear ratio as shown in figure 23.10.

The clock can also contain hands, which are mounted on the axle of a parent gear. Assume the root gear turns with a period of  $t_{\mathrm{root}} = 0.1$  s and has a radius of 25. The objective is to produce a clock with a second, minute, and hour hand.

Score each individual according to:

20 For an overview, see A. Ratle and M. Sebag, "Genetic Programming and Domain Knowledge: Beyond the Limitations of Grammar-Guided Machine Discovery," in International Conference on Parallel Problem Solving from Nature, 2000.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/3d7012631f31fc7ff7ea4d0d5b8af6a604ab05df6636094d6fdd48e2e47318a7.jpg)  
Figure 23.10. The rotation period  $t_{c}$  of a child gear attached to the rim of a parent gear depends on the rotation period of the parent gear,  $t_{p}$  and the ratio of the gears' radii.

$$
\left(\underset{\text{hands}} {\operatorname{minimize}} \left(1 - t_{\text{hand}}\right) ^{2}\right) + \left(\underset{\text{hands}} {\operatorname{minimize}} \left(60 - t_{\text{hand}}\right) ^{2}\right) + \left(\underset{\text{hands}} {\operatorname{minimize}} \left(36 00 - t_{\text{hand}}\right) ^{2}\right) + \# \text{nodes} \cdot 10^{- 3}
$$

where  $t_{\text{hand}}$  is the rotation period of a particular hand in seconds and #nodes is the number of nodes in the expression tree. Ignore rotation direction.

Solution: We define a grammar for the clock problem. Let  $\mathbb{G}_r$  be the symbol for a gear of radius  $r$ , let  $\mathbb{A}$  be an axle, let  $\mathbb{R}$  be a rim, and let  $\mathbb{H}$  be a hand. Our grammar is:

$$
\mathbb {G}_{r} \mapsto \mathbb {R} \mathbb {A} \mid \epsilon
$$

$$
\mathbb {R} \mapsto \mathbb {R} \mathbb {R} | \mathbb {G}_{r} | \epsilon
$$

$$
\mathbb {A} \mapsto \mathbb {A} \mathbb {A} | \mathbb {G}_{r} | \mathrm{H} | \epsilon
$$

which allows each gear to have any number of rim and axle children. The expression  $\epsilon$  is an empty terminal.

A clock with a single second hand can be constructed according to:

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8dda80463cf831526ac312f833cb7d8638973fb1316ca96207203eb57c08bcab.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/91aa5b66a73668be4b05a826f807af93cdff54a2fd0ef99eb458d0829acdefc9.jpg)

Note that the grammar does not compute the rotation period of the gears. This is handled by the objective function. A recursive procedure can be written to return the rotation periods of all hands. The list of rotation periods is subsequently used to compute the objective function value.

Exercise 23.8. The four 4s puzzle $^{21}$  is a mathematical challenge in which we use four 4 digits and mathematical operations to generate expressions for each of the integers from 0 to 100. For example, the first two integers can be produced by  $4 + 4 - 4 - 4$  and  $44 / 44$ , respectively. Complete the four 4s puzzle.

21 W. W. R. Ball, Mathematical Recre  
tations and Essays. Macmillan, 1892.

Solution: Any of the methods covered in this chapter can be used to complete the four 4s puzzle. A simple approach is to use Monte Carlo sampling on a suitable grammar. Sampled expressions with exactly four 4s are evaluated and, if suitable, are recorded. This procedure is repeated until an expression has been found for each integer.

One such suitable grammar is:22

$$
\mathbb {R} \mapsto 4 \mid 44 \mid 44 4 \mid 44 44 \mid \mathbb {R} + \mathbb {R} \mid \mathbb {R} - \mathbb {R} \mid \mathbb {R} \times \mathbb {R} \mid \mathbb {R} / \mathbb {R} \mid
$$

$$
\mathbb {R}^{\mathbb {R}} \mid \lfloor \mathbb {R} \rfloor \mid \lceil \mathbb {R} \rceil \mid \sqrt{\mathbb {R}} \mid \mathbb {R}! \mid \Gamma (\mathbb {R})
$$

We round evaluated expressions to the nearest integer. An expression that is rounded up can be contained inside a ceiling operation, and an expression that is rounded down can be contained inside a floor operation, so all such expressions are valid.

Exercise 23.9. Consider the probabilistic grammar

$$
\mathbb {R} \mapsto \mathbb {R} + \mathbb {R} | \mathbb {R} \times \mathbb {R} | \mathbb {F} | \mathbb {I} w_{\mathbb {R}} = [ 1, 1, 5, 5 ]
$$

$$
\mathbb {F} \mapsto 1. 5 \mid \infty \quad w_{\mathbb {F}} = [ 4, 3 ]
$$

$$
\mathbb {I} \mapsto 1 \mid 2 \mid 3 \qquad \qquad w_{\mathbb {I}} = [ 1, 1, 1 ]
$$

What is the generation probability of the expression  $1.5 + 2$ ?

Solution: The expression is obtained by applying the production rules:

$$
\mathbb {R} \mapsto \mathbb {R} + \mathbb {R} \quad P = 1 / 12
$$

$$
\mathbb {R} \mapsto \mathbb {F} \quad P = 5 / 12
$$

$$
\mathbb {F} \mapsto 1. 5 \qquad P = 4 / 7
$$

$$
\mathbb {R} \mapsto \mathbb {I} \qquad \qquad P = 5 / 12
$$

$$
\mathbb {I} \mapsto 2 \qquad P = 1 / 3
$$

which has a probability of

$$
\frac{1}{12} \frac{5}{12} \frac{4}{7} \frac{5}{12} \frac{1}{3} = \frac{25}{90 72} \approx 0. 00 27 6
$$

Exercise 23.10. What is the probabilistic grammar from the previous question after clearing the counts and applying a learning update on  $1.5 + 2$ ?

Solution: The learning update clears all counts and then increments each production rule each time it is applied. The five applied rules are each incremented once, resulting in:

$$
\mathbb {R} \mapsto \mathbb {R} + \mathbb {R} \mid \mathbb {R} \times \mathbb {R} \mid \mathbb {F} \mid \mathbb {I} w_{\mathbb {R}} = [ 1, 0, 1, 1 ]
$$

$$
\mathbb {F} \mapsto 1. 5 \mid \infty \quad w_{\mathbb {F}} = [ 1, 0 ]
$$

$$
\mathbb {I} \mapsto 1 \mid 2 \mid 3 \quad w_{\mathbb {I}} = [ 0, 1, 0 ]
$$

22 The gamma function  $\Gamma (x)$  is an extension of the factorial function which accepts real and complexvalued inputs. For positive integers  $x$  it produces  $(x - 1)!$

# 24 Multidisciplinary Optimization

Multidisciplinary design optimization (MDO) involves solving optimization problems spanning across disciplines. Many real-world problems involve complicated interactions between several disciplines, and optimizing disciplines individually may not lead to an optimal solution. This chapter discusses a variety of techniques for taking advantage of the structure of MDO problems to reduce the effort required for finding good designs. $^{1}$

# 24.1 Disciplinary Analyses

There are many different disciplinary analyses that might factor into a design. For example, the design of a rocket might involve analysis from disciplines such as structures, aerodynamics, and controls. The different disciplines have their own analytical tools, such as finite element analysis. Often these disciplinary analyses tend to be quite sophisticated and computationally expensive. In addition, disciplinary analyses are often tightly coupled with each other, where one discipline may require the output of another's disciplinary analysis. Resolving these interdependencies can be nontrivial.

In an MDO setting, we still have a set of design variables as before, but we also keep track of the outputs, or response variables, of each disciplinary analysis.2 We write the response variables of the  $i$ th disciplinary analysis as  $\mathbf{y}^{(i)}$ . In general, the  $i$ th disciplinary analysis  $F_{i}$  can depend on the design variables or the response variables from any other discipline:

$$
\mathbf {y}^{(i)} \leftarrow F_{i} \left(\mathbf {x}, \mathbf {y}^{(1)}, \dots , \mathbf {y}^{(i - 1)}, \mathbf {y}^{(i + 1)}, \dots , \mathbf {y}^{(m)}\right) \tag {24.1}
$$

where  $m$  is the total number of disciplines. The inputs to a computational fluid dynamics analysis for an aircraft may include the deflections of the wing, which

1 An extensive survey is provided by J.R.R.A. Martins and A.B. Lambe, "Multidisciplinary Design Optimization: A Survey of Architectures," AIAA Journal, vol. 51, no. 9, pp. 2049-2075, 2013. Further discussion can be found in J. Sobieszczanski-Sobieski, A. Morris, and M. van Tooren, Multidisciplinary Design Optimization Supported by Knowledge Based Engineering. Wiley, 2015. See also: N.M. Alexandrov and M.Y. Hussaini, eds., Multidisciplinary Design Optimization: State of the Art. SIAM, 1997. J.R.R.A. Martins and A. Ning, Engineering Design Optimization. Cambridge University Press, 2022.

2 A disciplinary analysis can provide inputs for other disciplines, the objective function, or the constraints. In addition, it can also provide gradient information for the optimizer.

come from a structural analysis that requires the forces from computational fluid dynamics. An important part of formulating MDO problems is taking into account such dependencies between analyses.

In order to make reasoning about disciplinary analyses easier, we introduce the concept of an assignment. An assignment  $\mathcal{A}$  is a set of variable names and their corresponding values relevant to a multidisciplinary design optimization problem. To access a variable  $v$ , we write  $\mathcal{A}[v]$ .

A disciplinary analysis is a function that takes an assignment and uses the design point and response variables from other analyses to overwrite the response variable for its discipline:

$$
\mathcal {A}^{\prime} \leftarrow F_{i} (\mathcal {A}) \tag {24.2}
$$

where  $F_{i}(\mathcal{A})$  updates  $\mathbf{y}^{(i)}$  in  $\mathcal{A}$  to produce  $\mathcal{A}'$ .

Assignments can be represented in code using dictionaries. Each variable is assigned a name of type String. Variables are not restricted to floating-point numbers but can include other objects, such as vectors. Example 24.1 shows an implementation using a dictionary.

Consider an optimization with one design variable  $x$  and two disciplines. Suppose the first disciplinary analysis  $F_{1}$  computes a response variable  $y^{(1)} = f_{1}(x, y^{(2)})$  and the second disciplinary analysis  $F_{2}$  computes a response variable  $y^{(2)} = f_{2}(x, y^{(1)})$ .

This problem can be implemented as:

```julia
function F1(A)  
A["y1"] = f1(A["x"], A["y2"]);  
return A  
end  
function F2(A)  
A["y2"] = f2(A["x"], A["y1"]);  
return A  
end
```

The assignment may be initialized with guesses for  $y^{(1)}$  and  $y^{(2)}$ , and a known input for  $x$ . For example:

$$
A = \operatorname{Dict} \left(" x" \Rightarrow 1, " y 1" \Rightarrow 2, " y 2" \Rightarrow 3\right)
$$

3 A dictionary, also called an associative array, is a common data structure that allows indexing by keys rather than integers. See appendix A.1.8.

Example 24.1. Basic code syntax for the assignment-based representation of multidisciplinary design optimization problems.

# 24.2 Interdisciplinary Compatibility

Evaluating the objective function value and feasibility of a design point  $\mathbf{x}$  requires obtaining values for the response variables that satisfy interdisciplinary compatibility, which means that the response variables must be consistent with the disciplinary analyses. Interdisciplinary compatibility holds for a particular assignment if the assignment is unchanged under all disciplinary analyses:

$$
F_{i} (\mathcal {A}) = \mathcal {A} \text{forin1 :im} \tag {24.3}
$$

Running any analysis will produce the same values. Finding an assignment that satisfies interdisciplinary compatibility is called a multidisciplinary analysis.

System optimization for a single discipline requires an optimizer to select design variables and query the disciplinary analysis in order to evaluate the constraints and the objective function, as shown in figure 24.1. Single-discipline optimization does not require that we consider disciplinary coupling.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8df2bbbd0b2bcc070bf95688e5751436689b31812ec4cbf4072f55e0ea730c2b.jpg)  
Figure 24.1. Optimization diagram for a single discipline. Gradients may or may not be computed.

System optimization for multiple disciplines can introduce dependencies, in which case coupling becomes an issue. A diagram for two coupled disciplines is given in figure 24.2. Applying conventional optimization to this problem is less straightforward because interdisciplinary compatibility must be established.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/27cdd797e38b37e76de6859d1e8cfa83db1f5ed8bee5968ca83985f9fc39b8ef.jpg)  
Figure 24.2. Optimization diagram for a two-discipline analysis with interdisciplinary coupling.

If a multidisciplinary analysis does not have a dependency cycle, then solving for interdisciplinary compatibility is straightforward. We say discipline  $i$  depends on discipline  $j$  if  $i$  requires any of  $j$ 's outputs. This dependency relation can be used to form a dependency graph, where each node corresponds to a discipline and an edge  $j \rightarrow i$  is included if discipline  $i$  depends on  $j$ . Figure 24.3 shows examples of dependency graphs involving two disciplines with and without cycles.

4 A dependency cycle arises when disciplines depend on each other.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a70756f2b1fa6ce29162d7b21c662d6bf3d15045187a21d1e5c2daff66b0fafa.jpg)  
An acyclic dependency graph. An evaluation ordering can be specified such that the required inputs for each discipline are available from previously evaluated disciplines.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/7f1830a51c0cde1241b12ed9ee05eacb0a52c6bbe06c4c3d6a0c60df75514f91.jpg)  
A cyclic dependency graph. The structural analysis depends on the aerodynamics analysis and vice versa.  
Figure 24.3. Cyclic and acyclic dependency graphs.

If the dependency graph has no cycles, then there always exists an order of evaluation that, if followed, ensures that the necessary disciplinary analyses are evaluated before the disciplinary analyses that depend on them. Such an ordering is called a topological ordering and can be found using a topological sorting method such as Kahn's algorithm.<sup>5</sup> The reordering of analyses is illustrated in figure 24.4.

If the dependency graph has cycles, then no topological ordering exists. To address cycles, we can use the Gauss-Seidel method (algorithm 24.1), which attempts to resolve the multidisciplinary analysis by iterating until convergence. $^6$  The Gauss-Seidel algorithm is sensitive to the ordering of the disciplines as illustrated by example 24.2. A poor ordering can prevent or slow convergence. The best orderings are those with minimal feedback connections. $^7$

It can be advantageous to merge disciplines into a new disciplinary analysis—to group conceptually related analyses, simultaneously evaluate tightly coupled analyses, or more efficiently apply some of the architectures discussed in this chapter. Disciplinary analyses can be merged to form a new analysis whose response variables consist of the response variables of the merged disciplines. The form of the new analysis depends on the disciplinary interdependencies. If

5 A. B. Kahn, "Topological Sorting of Large Networks," Communications of the ACM, vol. 5, no. 11, pp. 558-562, 1962.  
6 The Gauss-Seidel algorithm can also be written to execute analyses in parallel.  
7 In some cases, disciplines can be separated into different clusters that are independent of each other. Each connected cluster can be solved using its own, smaller multidisciplinary analysis.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/5ab97f56547267aa8135434db815f1fb73b99fadd0a79b9fe314cdfb7a23a7e6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/4bc9c5458e7d40849495ac83f54a2a5ce36607dcc649e24500ed6543818f9fed.jpg)  
Figure 24.4. A topological sort can be used to reorder the disciplinary analyses to remove feedback connections.

```julia
function gauss_seidel!(Fs, A; k_max=100,  $\epsilon = 1\mathrm{e} - 4$    
k, converged  $= 0$  , false   
while !converged && k  $\leq$  k_max   
k += 1   
A_old = deepcopy(A)   
for F in Fs   
F(A)   
end   
converged = all(isapprox(A[v], A_old[v], rtol  $\vDash$ $\varepsilon$  ) for v in keys(A))   
end   
return (A, converged)   
end
```

Algorithm 24.1. The Gauss-Seidel algorithm for conducting a multidisciplinary analysis. Here, Fs is a vector of disciplinary analysis functions that take and modify an assignment, A. There are two optional arguments: the maximum number of iterations k_max and the relative error tolerance  $\epsilon$ . The method returns the modified assignment and whether it converged.

Consider a multidisciplinary design optimization problem with one design variable  $x$  and three disciplines, each with one response variable:

$$
y^{(1)} \gets F_{1} (x, y^{(2)}, y^{(3)}) = y^{(2)} - x
$$

$$
y^{(2)} \leftarrow F_{2} (x, y^{(1)}, y^{(3)}) = \sin (y^{(1)} + y^{(3)})
$$

$$
y^{(3)} \leftarrow F_{3} (x, y^{(1)}, y^{(2)}) = \cos (x + y^{(1)} + y^{(2)})
$$

The disciplinary analyses can be implemented as:

```lua
function F1(A)  
A["y1"] = A["y2"] - A["x"]  
return A  
end  
function F2(A)  
A["y2"] = sin(A["y1"] + A["y3"]),  
return A  
end  
function F3(A)  
A["y3"] = cos(A["x"]] + A["y2"]} + A["y1"]);  
return A  
end
```

Consider running a multidisciplinary analysis for  $x = 1$ , having initialized our assignment with all 1's:

$$
A = \operatorname{Dict} \left(" x" \Rightarrow 1, " y 1" \Rightarrow 1, " y 2" \Rightarrow 1, " y 3" \Rightarrow 1\right)
$$

Running the Gauss-Seidel algorithm with the ordering  $F_{1}, F_{2}, F_{3}$  converges, but running with  $F_{1}, F_{3}, F_{2}$  does not.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8ea80c3f4b9e14e7e3a5a6cfa79f3f6a8c8ef3c40d2bb3c0c21c6d1e52dcb1fe.jpg)

Example 24.2. An example that illustrates the importance of choosing an appropriate ordering when running a multidisciplinary analysis.

the merged disciplines are acyclic then an ordering exists in which the analyses can be serially executed. If the merged disciplines are cyclic, then the new analysis must internally run a multidisciplinary analysis to achieve compatibility.

# 24.3 Architectures

Multidisciplinary design optimization problems can be written:

$$
\underset{\mathbf {x}} {\text{minimize}} \quad f (\mathcal {A})
$$

$$
\text{subject} \quad \mathcal {A} \in \mathcal {X} \tag {24.4}
$$

$$
F_{i} (\mathcal {A}) = \mathcal {A} \text{foreachdiscipline} i \in 1: m
$$

where the objective function  $f$  and feasible set  $\mathcal{X}$  depend on both the design and response variables. The design variables in the assignment are specified by the optimizer. The condition  $F_{i}(\mathcal{A}) = \mathcal{A}$  ensures that the  $i$ th discipline is consistent with the values in  $\mathcal{A}$ . This last condition enforces interdisciplinary compatibility.

There are several challenges associated with optimizing multidisciplinary problems. The interdependence of disciplinary analyses causes the ordering of analyses to matter and often makes parallelization difficult or impossible. There is a trade-off between an optimizer that directly controls all variables and incorporating suboptimizers<sup>8</sup> that leverage discipline-specific expertise to optimize values locally. In addition, there is a trade-off between the expense of running disciplinary analyses and the expense of globally optimizing too many variables. Finally, every architecture must enforce interdisciplinary compatibility in the final solution.

The remainder of this chapter discusses a variety of different optimization architectures for addressing these challenges. These architectures are demonstrated using the hypothetical ride-sharing problem introduced in example 24.3.

# 24.4 Multidisciplinary Design Feasible

The multidisciplinary design feasible architecture structures the MDO problem such that standard optimization algorithms can be directly applied to optimize the design variables. A multidisciplinary design analysis is run for any given design point to obtain compatible response values.

A suboptimizer is an optimization routine called within another optimization routine.

Consider a ride-sharing company developing a self-driving fleet. This hypothetical company is simultaneously designing the vehicle, its sensor package, a routing strategy, and a pricing scheme. These portions of the design are referred to as  $\mathbf{v}$ ,  $\mathbf{s}$ ,  $\mathbf{r}$ , and  $\mathbf{p}$ , respectively, each of which contains numerous design variables. The vehicle, for instance, may include parameters governing the structural geometry, engine and drive train, battery capacity, and passenger capacity.

The objective of the ride-sharing company is to maximize profit. The profit depends on a large-scale simulation of the routing algorithm and passenger demand, which, in turn, depends on response variables from an autonomy analysis of the vehicle and its sensor package. Several analyses extract additional information. The performance of the routing algorithm depends on the demand generated by the pricing scheme and the demand generated by the pricing scheme depends on performance of the routing algorithm. The vehicle range and fuel efficiency depends on the weight, drag, and power consumption of the sensor package. The sensor package requires vehicle geometry and performance information to meet the necessary safety requirements. A dependency diagram is presented below.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/49763c55081810264118b01ed04b6e072319ea1689216b624c4b0da1fd922f5d.jpg)

Example 24.3. A ride-sharing problem used throughout this chapter to demonstrate optimization architectures.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/186fee23b9022a4e4bd2fe04a62e8d0f96568a21d58652b8fd84f0601ad8577c.jpg)  
Optimizer

minimize  $f\left(\mathbf{x}, \mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(m)}\right)$

subject to  $\left[\mathbf{x},\mathbf{y}^{(1)},\ldots ,\mathbf{y}^{(m)}\right]\in \mathcal{X}$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/11625be2ca9086d9d7fd569325ee2bd250e10d21607f66cc6d262182199dca15.jpg)

minimize  $f(\mathrm{MDA}(\mathbf{x}))$

subject to  $\operatorname{MDA}(\mathbf{x}) \in \mathcal{X}$

An architecture diagram is given in figure 24.5. It consists of two blocks, the optimizer and the multidisciplinary analysis. The optimizer is the method used for selecting design points with the goal of minimizing the objective function. The optimizer calls the multidisciplinary analysis block by passing it a design point  $x$  and receives a compatible assignment  $\mathcal{A}$ . If interdisciplinary compatibility is not possible, the multidisciplinary analysis block informs the optimizer and such design points are treated as infeasible. Figure 24.6 shows how an MDO problem can be transformed into a typical optimization problem using multidisciplinary design analyses.

The primary advantages of the multidisciplinary design feasible architecture are its conceptual simplicity and that it is guaranteed to maintain interdisciplinary compatibility at each step in the optimization. Its name reflects the fact that multidisciplinary design analyses are run at every design evaluation, ensuring that the system-level optimizer only considers feasible designs.

The primary disadvantage is that multidisciplinary design analyses are expensive to run, typically requiring several iterations over all analyses. Iterative Gauss-Seidel methods may be slow to converge or may not converge at all, depending on the initialization of the response variables and the ordering of the disciplinary analyses.

Lumping the analyses together makes it necessary for all local variables—typically only relevant to a particular discipline—to be considered by the analysis as a whole. Many practical problems have a very large number of local design variables, such as mesh control points in aerodynamics, element dimensions in structures, component placements in electrical engineering, and neural network

Figure 24.5. The multidisciplinary design feasible architecture. The optimizer chooses design points  $\mathbf{x}$ , and the multidisciplinary analysis computes a consistent assignment  $\mathcal{A}$ . The structure is similar to that of single-discipline optimization.

Figure 24.6. Formulating an MDO problem into a typical optimization problem using multidisciplinary design analyses, where  $\mathrm{MDA}(\mathbf{x})$  returns a multidisciplinary compatible assignment.

weights in machine learning. Multidisciplinary design feasible optimization requires that the system optimizer specify all of these values across all disciplines while satisfying all constraints.

The multidisciplinary design feasible architecture is applied to the ride-sharing problem in example 24.4.

The multidisciplinary design feasible architecture can be applied to the ride-sharing problem. The architectural diagram is shown below.

Optimizer

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/eec2344b9b7a1ea492ddae6b712d9970314920e6e672466ee52195edabcb2bf9.jpg)

Example 24.4. The multidisciplinary design feasible architecture applied to the ride-sharing problem. A multidisciplinary analysis over all response variables must be completed for every candidate design point. This tends to be very computationally intensive.

# 24.5 Sequential Optimization

The sequential optimization architecture (figure 24.7) is an architecture that can leverage discipline-specific tools and experience to optimize subproblems but can lead to suboptimal solutions. This architecture is included to demonstrate the limitations of a naive approach and to serve as a baseline against which other architectures can be compared.

A subproblem is an optimization procedure conducted at every iteration of an overarching optimization process. Sometimes design variables can be removed from the outer optimization procedure, the system-level optimizer, and can be more efficiently optimized in subproblems.

The design variables for the  $i$ th discipline can be partitioned according to  $\mathbf{x}^{(i)} = [\mathbf{x}_g^{(i)},\mathbf{x}_\ell^{(i)}]$ , where  $\mathbf{x}_g^{(i)}$  are global design variables shared with other disciplines and  $\mathbf{x}_{\ell}^{(i)}$  are local design variables used only by the associated disciplinary subproblem. The response variables can be similarly partitioned into the global response variables  $\mathbf{y}_g^{(i)}$  and the local response variables  $\mathbf{y}_{\ell}^{(i)}$ . Disciplinary autonomy is

The vehicle subproblem in the ride-sharing problem may include global design variables such as the vehicle capacity and range that affect other disciplines but may also include local design variables such as the seating configuration that do not impact other disciplines.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/bf83a1291cc31f5595ee98f7ec83567ef8b5dc1307ffc48f39d453208f013a53.jpg)  
Figure 24.7. The sequential optimization architecture. Each subproblem is represented by a blue block and optimizes a particular discipline on a local objective function.

achieved by optimizing the local variables in their own disciplinary optimizers. A local objective function  $f_{i}$  must be chosen such that optimizing it also benefits the global objective. A top-level optimizer is responsible for optimizing the global design variables  $\mathbf{x}_{g}$  with respect to the original objective function. An instantiation of  $\mathbf{x}_{g}$  is evaluated through sequential optimizations; each subproblem is optimized one after the other, passing its results to the next subproblem until all have been evaluated.

Sequential optimization takes advantage of the locality of disciplines; that many variables are unique to a particular discipline and do not need to be shared across discipline boundaries. Sequential optimization harnesses each discipline's proficiency at solving its discipline-specific problem. The subproblem optimizers have complete control over their discipline-specific design variables to meet local design objectives and constraints.

Except in special cases, sequential optimization does not lead to an optimal solution of the original problem for the same reason that Gauss-Seidel is not guaranteed to converge. The solution is sensitive to the local objective functions, and finding suitable local objective functions is often a challenge. Sequential optimization does not support parallel execution, and interdisciplinary compatibility is enforced through iteration and does not always converge.

Example 24.5 applies sequential optimization to the ride-sharing problem.

# 24.6 Individual Discipline Feasible

The individual discipline feasible (IDF) architecture removes the need to run expensive multidisciplinary design analyses and allows disciplinary analyses to be executed in parallel. It loses the guarantee that interdisciplinary compatibility is maintained throughout its execution, with eventual agreement enforced through equality constraints in the optimizer. Compatibility is not enforced in multidisciplinary analyses but rather by the optimizer itself.

IDF introduces coupling variables to the design space. For each discipline, an additional vector  $\mathbf{c}^{(i)}$  is added to the optimization problem to act as aliases for the response variables  $\mathbf{y}^{(i)}$ . The response variables are unknown until they are computed by their respective domain analyses; inclusion of the coupling variables allows the optimizer to provide these estimates to multiple disciplines simultaneously when running analyses in parallel. Equality between the cou

The sequential optimization architecture can optimize some variables locally. Figure 24.8 shows the result of applying sequential optimization to the ride-sharing problem.

The design variables for the vehicle, sensor system, routing algorithm, and pricing scheme are split into local discipline-specific variables and top-level global variables. For example, the vehicle subproblem can optimize local vehicle parameters  $\mathbf{v}_{\ell}$  such as drive train components, whereas parameters like vehicle capacity that are used by other analyses are controlled globally in  $\mathbf{v}_{g}$ .

The tight coupling between the vehicle and sensor systems is poorly handled by the sequential optimization architecture. While changes made by the vehicle subproblem are immediately addressed by the sensor subproblem, the effect of the sensor subproblem on the vehicle subproblem is not addressed until the next iteration.

Not all analyses require their own subproblems. The profit analysis is assumed not to have any local design variables and can thus be executed without needing a subproblem block.

Example 24.5. Sequential optimization for the ride-sharing problem.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/c527e794e1e12132531f69a8d994a1cd1f4199486f9f2e4e0e3eb27622121075.jpg)  
Figure 24.8. The sequential optimization architecture applied to the ride-sharing problem.

pling and response variables is typically reached through iteration. Equality is an optimization constraint,  $\mathbf{c}^{(i)} = \mathbf{y}^{(i)}$ , for each discipline.

Figure 24.9 shows the general IDF architecture. The system-level optimizer operates on the coupling variables and uses these to populate an assignment that is copied to the disciplinary analysis in each iteration:

$$
\mathcal {A} [ \mathbf {x}, \mathbf {y}^{(1)}, \dots , \mathbf {y}^{(m)} ] \leftarrow [ \mathbf {x}, \mathbf {c}^{(1)}, \dots , \mathbf {c}^{(m)} ] \tag {24.5}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/815e27648be4c72abc2f2464a05c8dd1a157c7feefd39623d4ad2f6358b9d009.jpg)  
Figure 24.9. The individual discipline feasible architecture allows disciplinary analyses to be run in parallel. This chapter assumes that disciplinary analyses mutate their inputs, so copies of the system level optimizer's assignment are passed to each disciplinary analysis.

Despite the added freedom to execute analyses in parallel, IDF suffers from the shortcoming that it cannot leverage domain-specific optimization procedures in the same way as sequential optimization as optimization is top-level only. Furthermore, the optimizer must satisfy additional equality constraints and has more variables to optimize. IDF can have difficulties with gradient-based optimization since the chosen search direction must take constraints into account as shown in figure 24.10. Changes in the design variables must not cause the coupling variables to become infeasible with respect to the disciplinary analyses. Evaluating the gradients of the objective and constraint function is very costly when the disciplinary analyses are expensive.

The individual discipline feasible architecture is applied to the ride-sharing problem in figure 24.11.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fbf78db4105ecc30a39340d1f7384e17241eba3a0514c4d74aed28abc38eb48f.jpg)  
Figure 24.10. The search direction for a point on a constraint boundary must lead into the feasible set.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8c9b9752b3dbb0e12c8cbf9424ca1373fa4a1cd79021c9987b6a4b3b82a8d656.jpg)  
Figure 24.11. The individual discipline feasible architecture applied to the ride-sharing problem. The individual design feasible architecture allows for parallel execution of analyses, but the system-level optimizer must optimize a large number of variables.

# 24.7 Collaborative Optimization

The collaborative optimization architecture (figure 24.12) breaks a problem into disciplinary subproblems that have full control over their local design variables and discipline-specific constraints. Subproblems can be solved using discipline-specific tools and can be optimized in parallel.

The  $i$ th subproblem has the form:

$$
\underset{\mathbf {x}^{(i)}} {\text{minimize}} f_{i} \left(\mathbf {x}^{(i)}, \mathbf {y}^{(i)}\right) \tag {24.6}
$$

$$
\text{subject} \quad [ \mathbf {x}^{(i)}, \mathbf {y}^{(i)} ] \in \mathcal {X}_{i}
$$

with  $\mathbf{x}^{(i)}$  containing a subset of the design variables  $\mathbf{x}$  and response variables  $\mathbf{y}^{(i)}$ . The constraint ensures that the solution satisfies discipline-specific constraints.

Interdisciplinary compatibility requires that the global variables  $\mathbf{x}_g^{(i)}$  and  $\mathbf{y}_g^{(i)}$  agree between all disciplines. We define a set of coupling variables  $\mathcal{A}_g$  that includes variables corresponding to all design and response variables that are global in at least one subproblem. Agreement is enforced by constraining each  $\mathbf{x}_g^{(i)}$  and  $\mathbf{y}_g^{(i)}$  to match its corresponding coupling variables:

$$
\mathbf {x}_{g}^{(i)} = \mathcal {A}_{g} \left[ \mathbf {x}_{g}^{(i)} \right] \quad \text{and} \quad \mathbf {y}_{g}^{(i)} = \mathcal {A}_{g} \left[ \mathbf {y}_{g}^{(i)} \right] \tag {24.7}
$$

where  $\mathcal{A}_g[\mathbf{x}_g^{(i)}]$  and  $\mathcal{A}_g[\mathbf{y}_g^{(i)}]$  are the coupling variables corresponding to the global design and response variables in the  $i$ th discipline. This constraint is enforced using the subproblem objective function:

$$
f_{i} = \left\| \mathbf {x}_{g}^{(i)} - \mathcal {A}_{g} \left[ \mathbf {x}_{g}^{(i)} \right] \right\| _{2}^{2} + \left\| \mathbf {y}_{g}^{(i)} - \mathcal {A}_{g} \left[ \mathbf {y}_{g}^{(i)} \right] \right\| _{2}^{2} \tag {24.8}
$$

Each subproblem thus seeks feasible solutions that minimally deviate from the coupling variables.

The subproblems are managed by a system-level optimizer that is responsible for optimizing the coupling variables  $\mathcal{A}_g$  to minimize the objective function. Evaluating an instance of the coupling variables requires running each disciplinary subproblem, typically in parallel.

Disciplinary subproblems may deviate from the coupling variables during the optimization process. This discrepancy occurs when two or more disciplines disagree on a variable or when subproblem constraints prevent matching the target values set by the system-level optimizer. The top-level constraint that  $f_{i} = 0$  for each discipline ensures that coupling is eventually attained.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8ddb6a4c5c9e4449410dd797a162a021961f8623f07faa3c8bb8089f0dac2f45.jpg)  
Figure 24.12. Design architecture for collaborative optimization.

The primary advantages of collaborative optimization stem from its ability to isolate some design variables into disciplinary subproblems. Collaborative optimization is readily applicable to real-world multidisciplinary problem solving, as each discipline is typically well segregated, and thus largely unaffected by small decisions made in other disciplines. The decentralized formulation allows traditional discipline optimization methods to be applied, allowing problem designers to leverage existing tools and methodologies.

Collaborative optimization requires optimizing over the coupling variables, which includes both design and response variables. Collaborative optimization does not perform well in problems with high coupling because the additional coupling variables can outweigh the benefits of local optimization.

Collaborative optimization is a distributed architecture that decomposes a single optimization problem into a smaller set of optimization problems that have the same solution when their solutions are combined. Distributed architectures have the advantage of reduced solving times, as subproblems can be optimized in parallel.

Collaborative optimization is applied to the ride-sharing problem in example 24.6.

The collaborative optimization architecture can be applied to the vehicle routing problem by producing six different disciplinary subproblems. Unfortunately, having six different subproblems requires any variables shared across disciplines to be optimized at the global level.

Figure 24.13 shows two disciplinary subproblems obtained by grouping the vehicle, sensor, and autonomy disciplines into a transport subproblem and the routing, demand, and profit disciplines into a network subproblem. The disciplines grouped into each subproblem are tightly coupled. Having only two subproblems significantly reduces the number of global variables considered by the system-level optimizer because presumably very few design variables are directly used by both the transport and network subproblems.

The subproblems are each multidisciplinary optimization problems, themselves amenable to optimization using the techniques covered in this chapter. We can, for example, use sequential optimization within the transport subproblem. We can also add another instance of collaborative optimization within the network subproblem.

Example 24.6. Applying collaborative optimization to the ride-sharing problem.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fd405815ee6707195e800537941d6d2a4636eb3224eab7a212a8901bf4a5f43e.jpg)  
Figure 24.13. The collaborative optimization architecture applied to the ride-sharing problem. The colored circles correspond to the disciplinary analyses contained within each subproblem.

# 24.8 Simultaneous Analysis and Design

The simultaneous analysis and design (SAND) architecture avoids the central challenge of coordinating between multiple disciplinary analyses by having the optimizer conduct the analyses. Instead of running an analysis  $F_{i}(\mathcal{A})$  to obtain a residual, SAND optimizes both the design and response variables subject to a constraint  $F_{i}(\mathcal{A}) = \mathcal{A}$ . The optimizer is responsible for simultaneously optimizing the design variables and finding the corresponding response variables.

Any disciplinary analysis can be transformed into residual form. The residual  $r_i(\mathcal{A})$  is used to indicate whether an assignment  $\mathcal{A}$  is compatible with the  $i$ th discipline. If  $F_i(\mathcal{A}) = \mathcal{A}$ , then  $r_i(\mathcal{A}) = 0$ ; otherwise,  $r_i(\mathcal{A}) \neq 0$ . We can obtain a residual form using the disciplinary analysis:

$$
r_{i} (\mathcal {A}) = \left\| F_{i} (\mathcal {A}) - \mathcal {A} [ \mathbf {y}^{(i)} ] \right\| \tag {24.9}
$$

though this is typically inefficient, as demonstrated in example 24.7.

Consider a disciplinary analysis that solves the equation  $\mathbf{A}\mathbf{y} = \mathbf{x}$ . The analysis is  $F(\mathbf{x}) = \mathbf{A}^{-1}\mathbf{x}$ , which requires an expensive matrix inversion. We can construct a residual form using equation (24.9):

$$
r_{1} (\mathbf {x}, \mathbf {y}) = \| F (\mathbf {x}) - \mathbf {y} \| = \left\| \mathbf {A}^{- 1} \mathbf {x} - \mathbf {y} \right\|
$$

Alternatively, we can use the original constraint to construct a more efficient residual form:

$$
r_{2} (\mathbf {x}, \mathbf {y}) = \| \mathbf {A} \mathbf {y} - \mathbf {x} \|
$$

The residual form of a discipline consists of the set of disciplinary equations that are solved by the disciplinary analysis. $^{10}$  It is often much easier to evaluate a residual than to run a disciplinary analysis. In SAND, figure 24.14, the analysis effort is the responsibility of the optimizer.

minimize  $f(\mathcal{A})$  subject to  $\mathcal{A} \in \mathcal{X}$ ,  $r_i(\mathcal{A}) = 0$  for each discipline

SAND can explore regions of the design space that are infeasible with respect to the residual equations, as shown in figure 24.15. Exploring infeasible regions

Example 24.7. Evaluating a disciplinary analysis in a residual is typically counter-productive. The analysis must typically perform additional work to solve the problem whereas a cleaner residual form can more efficiently verify whether the inputs are compatible.

10 In aerodynamics, these may include the Navier-Stokes equations. In structural engineering, these may include the elasticity equations. In electrical engineering, these may include the differential equations for current flow.

Figure 24.14. Simultaneous analysis and design places the entire burden on the optimizer. It uses disciplinary residuals rather than disciplinary analyses.

can allow us to traverse the design space more easily and find solutions in feasible regions disconnected from the feasible region of the starting design point. SAND suffers from having to simultaneously optimize a very large number of variables for which derivatives and other discipline-specific expertise are not available. Furthermore, SAND gains much of its value from residuals that can be computed more efficiently than can disciplinary analyses. Use of SAND in real-world applications is often limited by the inability to modify existing disciplinary analysis code to produce an efficient residual form.

SAND is applied to the ride-sharing problem in example 24.8.

Applying the simultaneous analysis and design architecture to the ride-sharing problem requires disciplinary residuals. These can potentially depend on all design and response variables. The architecture requires that the optimizer optimize all of the design variables and all of the response variables.

<table><tr><td>minimize
v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)</td><td>f(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p))</td></tr><tr><td>subject to</td><td>[v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)] ∈ X</td></tr><tr><td></td><td>rv(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)) = 0</td></tr><tr><td></td><td>rs(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)) = 0</td></tr><tr><td></td><td>ra(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)) = 0</td></tr><tr><td></td><td>r(r(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)) = 0</td></tr><tr><td></td><td>rd(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)) = 0</td></tr><tr><td></td><td>rp(v,s,r,p,y(v),y(s),y(a),y(r),y(d),y(p)) = 0</td></tr></table>

Figure 24.15. SAND can explore regions of the design space that are infeasible and potentially bridge the gap between feasible subsets.  
![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6439505b0b73606b0041c31d90dc6186aaf2fe0cddbc499bf7c3b7211cc284bd.jpg)  
Example 24.8. The simultaneous analysis and design architecture applied to the ride-sharing problem.

# 24.9 Summary

- Multidisciplinary design optimization requires reasoning about multiple disciplines and achieving agreement between coupled variables.  
- Disciplinary analyses can often be ordered to minimize dependency cycles.

- Multidisciplinary design problems can be structured in different architectures that take advantage of problem features to improve the optimization process.  
- The multidisciplinary design feasible architecture maintains feasibility and compatibility through the use of slow and potentially nonconvergent multidisciplinary design analyses.  
- Sequential optimization allows each discipline to optimize its discipline-specific variables but does not always yield optimal designs.  
- The individual discipline feasible architecture allows parallel execution of analyses at the expense of adding coupling variables to the global optimizer.  
- Collaborative optimization incorporates suboptimizers that can leverage domain specialization to optimize some variables locally.  
- The simultaneous analysis and design architecture replaces design analyses with residuals, allowing the optimizer to find compatible solutions but cannot directly use disciplinary solution techniques.

# 24.10 Exercises

Exercise 24.1. Provide an example of a practical engineering problem that is multidisciplinary.

Solution: Maximize the lift-to-drag ratio of an airfoil shape subject to a constraint on the structural stability of the airfoil.

Exercise 24.2. Provide an abstract example of a multidisciplinary problem where the order of the analyses is important.

Solution: Consider a problem where the disciplinary dependency graph is a (directed) tree: if the optimization starts from the root and proceeds by following the topological order, then convergence occurs after one traversal of the tree.

Exercise 24.3. What is one advantage of the individual discipline feasible architecture over the multidisciplinary design feasible and sequential optimization architectures?

Solution: It can execute disciplinary analyses in parallel.

Exercise 24.4. Consider applying multidisciplinary design analysis to minimizing the weight of a wing whose deformation and loading are computed by separate disciplines. We will use a simplified version of the problem, representing the wing as a horizontally mounted pendulum supported by a torsional spring.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/621403404923ec07bc8f32036ab99a9767307a57d2ff28dcc38b6717b5252890.jpg)

The objective is to minimize the spring stiffness,  $k$ , such that the pendulum's displacement does not exceed a target threshold. The pendulum length  $\ell$ , pendulum point mass  $m$ , and gravitational constant  $g$  are fixed.

We use two simplified analyses in place of more sophisticated analyses used to compute deformations and loadings of aircraft wings. Assuming the pendulum is rigid, the loading moment  $M$  is equal to  $mg\ell \cos(\theta)$ . The torsional spring resists deformation such that the pendulum's angular displacement  $\theta$  is  $M / k$ .

Formulate the spring-pendulum problem under the multidisciplinary design feasible architecture, and then solve it according to that architecture for:

$$
m = 1 \mathrm{kg}, \ell = 1 \mathrm{m}, g = 9. 81 \mathrm{m} / \mathrm{s}^{2}, \theta_{\max } = 10^{\circ}
$$

Solution: The spring-pendulum problem under the multidisciplinary design feasible architecture is:

$$
\underset{k} {\text{minimize}} \quad f (\mathrm{MDA} (k)) \quad \text{subjectto} \quad k > 0
$$

$$
\theta \leq \theta_{\max }
$$

$$
\mathrm{MDA} (k) \text{converged}
$$

where  $\mathrm{MDA}(k)$  performs a multidisciplinary analysis on the two disciplinary analyses: a loads analysis that computes  $\mathcal{A}[M] = mg\ell \cos(\mathcal{A}[\theta])$  and a displacement analysis that computes  $\mathcal{A}[\theta] = \mathcal{A}[M] / \mathcal{A}[k]$ . Whether the multidisciplinary design analysis converged is included as an additional response variable in order to enforce convergence.

Solving the optimization problem produces  $k \approx 55.353\mathrm{N}$ .

Exercise 24.5. Formulate the spring-pendulum problem under the individual design feasible architecture.

Solution: The spring-pendulum problem under the individual design feasible architecture is:

$$
\underset{k, \theta_{c}, M_{c}} {\text{minimize}} k
$$

$$
\text{subject} \quad k > 0
$$

$$
\theta_{c} = F_{\text{displacement}} \left(k, M_{c}\right)
$$

$$
M_{c} = F_{\text{loads}} \left(\theta_{c}\right)
$$

$$
\theta \leq \theta_{\max }
$$

where  $\theta_c$  and  $M_c$  are the additional coupling variables under control of the optimizer. The two disciplinary analyses can be executed in parallel.

Exercise 24.6. Formulate the spring-pendulum under the collaborative optimization architecture. Present the two disciplinary optimization problems and the system-level optimization problem.

Solution: The two disciplinary optimization problems for the spring-pendulum problem under the collaborative optimization architecture are:

$$
\underset{k, M} {\text{minimize}} J_{\text{displacement}} = (k_{g} - \mathcal {A} [ k_{g} ]) ^{2} + (M_{g} - \mathcal {A} [ M_{g} ]) ^{2} + (F_{\text{displacement}} (k_{g}, M_{g}) - \theta) ^{2}
$$

subject to  $\theta_{g}\leq \theta_{\mathrm{max}}$

$$
k > 0
$$

and

$$
\underset{\theta_{g}} {\text{minimize}} \quad J_{\text{loads}} = (\theta_{g} - \theta) ^{2} + (F_{\text{loads}} (\theta_{g}) - M) ^{2}
$$

where the subscript  $g$  indicates a global variable. The global variables are  $\mathcal{A}_g = \{k_g,\theta_g,M_g\}$

The system-level optimization problem is:

$$
\begin{array}{l} \underset{k_{g}, \theta_{g}, M_{g}} {\text{minimize}} k_{g} \\ \text{subject} \quad J_{\text{structures}} = 0 \\ J_{\mathrm{loads}} = 0 \\ \end{array}
$$

# APPENDICES

# A Julia

Julia is a scientific programming language that is free and open source. $^{1}$  It is a relatively new language that borrows inspiration from languages like Python, MATLAB, and R. It was selected for use in this book because it is sufficiently high level $^{2}$  so that the algorithms can be compactly expressed and readable while also being fast. This book is compatible with Julia version 1.10. This appendix introduces the concepts necessary for understanding the code included in the text, omitting many of the advanced features of the language.

# A.1 Types

Julia has a variety of basic types that can represent data given as truth values, numbers, strings, arrays, tuples, and dictionaries. Users can also define their own types. This section explains how to use some of the basic types and how to define new types.

# A.1.1 Booleans

The Boolean type in Julia, written as Bool, includes the values true and false. We can assign these values to variables. Variable names can be any string of characters, including Unicode, with a few restrictions.

```txt
$\alpha =$  true done  $=$  false
```

The variable name appears on the left side of the equal sign; the value that variable is to be assigned is on the right side.

<sup>1</sup>Julia may be obtained from http://julialang.org.

2 In contrast with languages like  $C++$ , Julia does not require programmers to worry about memory management and other lower-level details, yet it allows low-level control when needed.

We can make assignments in the Julia console. The console, or REPL (for read, eval, print, loop), will return a response to the expression being evaluated. The # symbol indicates that the rest of the line is a comment.

```julia
julia> x = true  
true  
julia> y = false; # semicolon suppresses the console output  
julia> typeof(x)  
Bool  
julia> x == y # test for equality  
false
```

The standard Boolean operators are supported:

```txt
julia> !x # not  
false  
julia> x && y # and  
false  
julia> x || y # or  
true
```

# A.1.2 Numbers

Julia supports integer and floating-point numbers, as shown here:

```txt
julia>typeof(42)  
Int64  
julia>typeof(42.0)  
Float64
```

Here, Int64 denotes a 64-bit integer, and Float64 denotes a 64-bit floating-point value. We can perform the standard mathematical operations:

```txt
julia> x = 44  
julia> y = 22  
julia> x + y  
6  
julia> x - y  
2  
julia> x * y  
8  
julia> x / y  
2.0
```

On 32-bit machines, an integer literal like 42 is interpreted as an Int32.

```txt
julia> x ^ y # exponentiation  
16  
julia> x % y # remainder from division  
0  
julia> div(x, y) # truncated division returns an integer  
2
```

Note that the result of  $x / y$  is a Float64, even when  $x$  and  $y$  are integers. We can also perform these operations at the same time as an assignment. For example,  $x += 1$  is shorthand for  $x = x + 1$ .

We can also make comparisons:

```txt
julia> 3 > 4  
false  
julia> 3 >= 4  
false  
julia> 3 ≥ 4 # unicode also works, use \ge[tab] in console  
false  
julia> 3 < 4  
true  
julia> 3 <= 4  
true  
julia> 3 ≤ 4 # unicode also works, use \le[tab] in console  
true  
julia> 3 == 4  
false  
julia> 3 < 4 < 5  
true
```

# A.1.3 Strings

A string is an array of characters. Strings are not used very much in this textbook except to report certain errors. An object of type String can be constructed using "characters. For example:

```txt
julia> x = "optimal"  
"optimal"  
julia> typeof(x)  
String
```

# A.1.4 Vectors

A vector is a one-dimensional array that stores a sequence of values. We can construct a vector using square brackets, separating elements by commas:

```txt
julia> x = []; # empty vector  
julia> x = trues(3); # Boolean vector containing three trues  
julia> x = ones(3); # vector of three ones  
julia> x = zeros(3); # vector of three zeros  
julia> x = rand(3); # vector of three random numbers between 0 and 1  
julia> x = [3, 1, 4]; # vector of integers  
julia> x = [3.1415, 1.618, 2.7182]; # vector of floats
```

An array comprehension can be used to create vectors:

```txt
julia> [sin(x) for x in 1:5]  
5-element Vector{Float64}:  
0.8414710.9092970.14112  
-0.756802  
-0.958924
```

We can inspect the type of a vector:

```julia
julia>typeof([3,1,4]) # 1-dimensional array of Int64s  
Vector{Int64} (alias for Array{Int64,1})  
julia>typeof([3.1415,1.618,2.7182]) # 1-dimensional array of Float64s  
Vector{Float64} (alias for Array{Float64,1})  
julia> Vector{Float64} # alias for a 1-dimensional array  
Vector{Float64} (alias for Array{Float64,1})
```

We index into vectors using square brackets:

```txt
julia> x[1] # first element is indexed by 13.1415  
julia> x[3] # third element  
2.7182  
julia> x[end] # use end to reference the end of the array  
2.7182  
julia> x[end-1] # this returns the second to last element  
1.618
```

We can pull out a range of elements from an array. Ranges are specified using a colon notation:

```julia
julia> x = [1, 2, 5, 3, 1]  
5-element Vector{Int64}:  
    12  
    53  
    1  
julia> x[1:3] # pull out the first three elements  
3-element Vector{Int64}:  
    12  
    5  
julia> x[1:2:end] # pull out every other element  
3-element Vector{Int64}:  
    15  
    1  
julia> x[end:-1:1] # pull out all the elements in reverse order  
5-element Vector{Int64}:  
    13  
    52  
    1
```

We can perform a variety of operations on arrays. The exclamation mark at the end of function names is used to indicate that the function mutates (i.e., changes) the input:

```julia
julia> length(x)  
5  
julia> [x, x] # concatenation  
2-element Vector{Vector{Int64}}:  
[1, 2, 5, 3, 1]  
[1, 2, 5, 3, 1]  
julia> push!(x, -1) # add an element to the end  
6-element Vector{Int64}:  
12  
53  
1  
-1  
julia> pop!(x) # remove an element from the end  
-1
```

```julia
julia> append!(x, [2, 3]) # append [2, 3] to the end of x  
7-element Vector{Int64}:  
12  
53  
12  
3  
julia> sort!(x) # sort the elements, altering the same vector  
7-element Vector{Int64}:  
11  
22  
33  
5  
julia> sort(x); # sort the elements as a new vector  
julia> x[1] = 2; print(x) # change the first element to 2  
[2, 1, 2, 2, 3, 3, 5]  
julia> x = [1, 2];  
julia> y = [3, 4];  
julia> x + y # add vectors  
2-element Vector{Int64}:  
46  
julia> 3x - [1, 2] # multiply by a scalar and subtract  
2-element Vector{Int64}:  
24  
julia> using LinearAlgebra  
julia> dot(x, y) # dot product available after using LinearAlgebra  
11  
julia> x·y # dot product using unicode character, use \cdot[tab] in console  
11  
julia> prod(y) # product of all the elements in y  
12
```

It is often useful to apply various functions elementwise to vectors. This is a form of broadcasting. With infix operators (e.g.,  $+$ ,  $*$ , and  $^ \land$ ), a dot is prefixed to indicate elementwise broadcasting. With functions like sqrt and sin, the dot is postfixed:

```txt
julia> x .\* y # elementwise multiplication   
2-element Vector{Int64}: 38   
julia> x .^ 2 # elementwise squaring   
2-element Vector{Int64}: 14   
julia> sin.(x) # elementwise application of sin   
2-element Vector{Float64}: 0.8414710.909297   
julia> sqrt.x # elementwise application of sqrt   
2-element Vector{Float64}: 1.01.41421   
julia> min.(x, a) # elementwise min between x and a scalar   
2-element Vector{ForwardDiff.Dual{Nothing, Int64, 1}}: Dual{Nothing}  $(1,0)$  Dual{Nothing}  $(2,0)$    
julia> x .= y # assign all elements of x to those in y   
2-element Vector{Int64}: 34
```

Arrays are assigned by reference; they are not copied when assigned to a new variable. To make a copy, we use the copy function.

```txt
julia>  $\mathbf{x} = [1,2,3]$    
julia> y=x;   
julia> y[1]  $= 4$    
julia> x[1] #x is also changed   
4   
julia> y  $=$  copy(x);   
julia> y[1]  $= 5$    
julia> x[1] #x is not changed   
4   
julia> x.  $=$  y# assign all elements of x to those in y   
3-element Vector{Int64}:   
52   
3
```

# A.1.5 Matrices

A matrix is a two-dimensional array. Like a vector, it is constructed using square brackets. We use spaces to delimit elements in the same row and semicolons to delimit rows. We can also index into the matrix and output submatrices using ranges. Some of the operations require the use of the built-in LinearAlgebra package:

```julia
julia> X = [12 3; 45 6; 78 9; 101112];  
julia> typeof(X) # a 2-dimensional array of Int64s  
Matrix{Int64} (alias for Array{Int64, 2})  
julia> X[2] # second element using column-major ordering  
4  
julia> X[3,2] # element in third row and second column  
8  
julia> X[1,:] # extract the first row  
3-element Vector{Int64}:  
12  
3  
julia> X(:,2] # extract the second column  
4-element Vector{Int64}:  
25  
811  
julia> X[:,1:2] # extract the first two columns  
4×2 Matrix{Int64}:  
12  
45  
78  
1011  
julia> X[1:2,1:2] # extract a 2x2 submatrix from the top left of x  
2×2 Matrix{Int64}:  
12  
45  
julia> Matrix{Float64} # alias for a 2-dimensional array  
Matrix{Float64} (alias for Array{Float64, 2})
```

We can also construct a variety of special matrices and use array comprehensions:

julia> Matrix(1.0I(3))

3×3 Matrix{Float64}:

3x3 identity matrix

Matrix operations include the following:  
```txt
1.00.00.00.01.00.00.00.01.0  
julia> Matrix(Diagonal([3, 2, 1])) # 3x3 diagonal matrix  
3x3 Matrix{Int64}:  
30 00 20  
00 1  
julia> zeros(3,2) # 3x2 matrix of zeros  
3x2 Matrix{Float64}:  
0.00.00.00.00.00.0  
julia> rand(3,2) # 3x2 random matrix  
3x2 Matrix{Float64}:  
0.06817450.08393410.875980.5844530.9494420.390314  
julia> [sin(x + y) for x in 1:3, y in 1:2] # array comprehension  
3x2 Matrix{Float64}:  
0.9092970.141120.14112 -0.756802  
-0.756802 -0.958924
```

```txt
julia> X' # complex conjugate transpose  
3×4 adjoint( ::Matrix{Int64} ) with eltype Int64:  
14 7102 58 113 69 12  
julia> 3X .+ 2 # multiplying by scalar and adding scalar  
4×3 Matrix{Int64}:  
58 11141720232629323538  
julia> X = [13; 31]; # create an invertible matrix  
julia> inv(X) # inversion  
2×2 Matrix{Float64}:  
-0.1250.3750.375 -0.125  
julia> pinv(X) # pseudoinverse  
2×2 Matrix{Float64}:  
-0.1250.375
```

```txt
0.375 -0.125  
julia> det(X) # determinant  
-8.0  
julia> [X X] # horizontal concatenation, same as hcat(X, X)  
2×4 Matrix{Int64}:  
13 13  
31 31  
julia> [X; X] # vertical concatenation, same as vcat(X, X)  
4×2 Matrix{Int64}:  
13  
31  
13  
31  
julia> sin.(X) # elementwise application of sin  
2×2 Matrix{Float64}:  
0.8414710.141120.141120.841471  
julia> map(sin, X) # elementwise application of sin  
2×2 Matrix{Float64}:  
0.8414710.141120.141120.841471  
julia> vec(X) # reshape an array as a vector  
4-element Vector{Int64}:  
13  
31
```

The backslash operator  $\backslash$  is used to efficiently solve linear systems  $\mathbf{A}\mathbf{x} = \mathbf{b}$  and associated least squares problems:

```txt
julia> A = [10; 1 -2]; # define a full-rank square matrix  
julia> b = [32, -4]; # define a vector  
julia> x = A\b # solve the linear system Ax = b  
2-element Vector{Float64}:  
32.018.0  
julia> A*x - b # verify the solution (should be close to zero)  
2-element Vector{Float64}:  
0.00.0  
julia> A = [12; 34; 56]; # define matrix with more rows than columns  
julia> b = [0, 8, 9]; # define a vector  
julia> x = A\b # solve the least squares problem min_x ||Ax - b||_22-element Vector{Float64}:
```

```txt
3.33333   
-1.08333   
julia> norm(A\*x - b) # compute the residual   
2.85774   
julia> A = [12 3; 45 6]; # define a matrix with more columns than rows   
julia> b = [7, 8]; # define a vector   
julia> x = A\b # solve the problem min_x ||x||_2 such that Ax = b   
3-element Vector{Float64}:   
-3.055560.1111113.27778   
julia> A\*x - b # verify the solution (should be close to zero)   
2-element Vector{Float64}:   
-1.77636e-151.77636e-15
```

# A.1.6 Tuples

A tuple is an ordered list of values, potentially of different types. They are constructed with parentheses. They are similar to vectors, but they cannot be mutated:

```julia
julia> x = () # the empty tuple  
()  
julia> isempty(x)  
true  
julia> x = (1,) # tuples of one element need the trailing comma  
(1,)  
julia> typeof(x)  
Tuple{Int64}  
julia> x = (1, 0, [1, 2], 2.5029, 4.6692) # third element is a vector  
(1, 0, [1, 2], 2.5029, 4.6692)  
julia> typeof(x)  
Tuple{Int64, Int64, Vector{Int64}, Float64, Float64}  
julia> x[2]  
0  
julia> x[end]  
4.6692  
julia> x[4:end]  
(2.5029, 4.6692)  
julia> length(x)  
5  
julia> x = (1, 2)  
(1, 2)  
julia> a, b = x;
```

```txt
julia> a  
1  
julia> b  
2
```

# A.1.7 Named Tuples

A named tuple is like a tuple, but each entry has its own name:

```julia
julia> x = (a=1, b=-Inf)  
(a = 1, b = -Inf)  
julia> x isa NamedTuple  
true  
julia> x.a  
1  
julia> a, b = x;  
julia> a  
1  
julia> (;:a⇒10)  
(a = 10,)  
julia> (;:a⇒10, :b⇒11)  
(a = 10, b = 11)  
julia> merge(x, (d=3, e=10)) # merge two named tuples  
(a = 1, b = -Inf, d = 3, e = 10)
```

# A.1.8 Dictionaries

A dictionary is a collection of key-value pairs. Key-value pairs are indicated with a double arrow operator  $=>$ . We can index into a dictionary using square brackets, just as with arrays and tuples:

```julia
julia> x = Dict(); # empty dictionary  
julia> x[3] = 4 # associate key 3 with value 44  
julia> x = Dict(3 => 4, 5 => 1) # create a dictionary with two key-value pairs  
Dict{Int64, Int64} with 2 entries:  
    5 => 13 => 4  
julia> x[5] # return the value associated with key 51  
julia> haskey(x, 3) # check whether dictionary has key 3  
true  
julia> haskey(x, 4) # check whether dictionary has key 4  
false
```

# A.1.9 Composite Types

A composite type is a collection of named fields. By default, an instance of a composite type is immutable (i.e., it cannot change). We use the struct keyword and then give the new type a name and list the names of the fields:

```txt
struct A a b end
```

Adding the keyword mutable makes it so that an instance can change:

```txt
mutable struct B a b end
```

Composite types are constructed using parentheses, between which we pass in values for each field:

```latex
$\mathbf{x} = \mathbf{A}(1.414, 1.732)$
```

We can extract the values of the fields using dot notation. For example, x.a returns the value of the field a in the composite type x.

The double-colon operator can be used to specify the type for any field:

```julia
struct A  
a::Int64  
b::Float64  
end
```

These type annotations require that we pass in an Int64 for the first field and a Float64 for the second field. For compactness, this book does not use type annotations, but it is at the expense of performance. Type annotations allow Julia to improve runtime performance because the compiler can optimize the underlying code for specific types.

# A.1.10 Abstract Types

So far we have discussed concrete types, which are types that we can construct. However, concrete types are only part of the type hierarchy. There are also abstract types, which are supertypes of concrete types and other abstract types.

We can explore the type hierarchy of the Float64 type shown in figure A.1 using the supertype and subtypes functions:

```julia
julia> supertype FLOAT64) AbstractFloat julia> supertype(AbstractFloat) Real julia> supertype(Real) Number julia> supertype(Number) Any julia> supertype(Any) # Any is at the top of the hierarchy Any julia> using InteractiveUtilities # required for using subtypes in scripts julia> subtypes(AbstractFloat) # different types of AbstractFloats 5-element Vector{Any}: BigFloat Core.BFloat16 Float16 Float32 Float64 julia> subtypes(Float64) # Float64 does not have any subtypes Type[]
```

We can define our own abstract types:

```julia
abstract type C end  
abstract type D <: C end # D is an abstract subtype of C  
struct E <: D # E is a composite type that is a subtype of D  
a  
end
```

# A.1.11 Parametric Types

Julia supports parametric types, which are types that take parameters. The parameters to a parametric type are given within braces and delimited by commas. We have already seen a parametric type with our dictionary example:

```txt
julia> x = Dict(3 => 1.4, 1 => 5.9)  
Dict{Int64, Float64} with 2 entries:  
3 => 1.41 => 5.9
```

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/b72b5943e2a949d9f93577f844205f0816215f7dc87e68db248f78c1be892097.jpg)  
Figure A.1. The type hierarchy for the Float64 type.

For dictionaries, the first parameter specifies the key type, and the second parameter specifies the value type. The example has Int64 keys and Float64 values, making the dictionary of type Dict{Int64,Float64}. Julia was able to infer these types based on the input, but we could have specified it explicitly:

```txt
julia> x = Dict[Int64,Float64]{3 => 1.4, 1 => 5.9};
```

While it is possible to define our own parametric types, we do not need to do so in this text.

# A.2 Functions

A function maps its arguments, given as a tuple, to a result that is returned.

# A.2.1 Named Functions

One way to define a named function is to use the function keyword, followed by the name of the function and a tuple of names of arguments:

```txt
function  $f(x,y)$  return  $\mathbf{x} + \mathbf{y}$  end
```

We can also define functions compactly using assignment form:

```julia
julia>  $f(x,y) = x + y;$  julia> f(3,0.1415) 3.1415
```

# A.2.2 Anonymous Functions

An anonymous function is not given a name, though it can be assigned to a named variable. One way to define an anonymous function is to use the arrow operator:

```julia
julia> h = x → x^2 + 1 # assign anonymous function with input x to a variable h
#1 (generic function with 1 method)
julia> h(3)
10
julia> g(f, a, b) = [f(a), f(b)]; # applies function f to a and b and returns array
julia> g(h, 5, 10)
2-element Vector[Int64]:
26
```

```txt
101  
julia> g(x→sin(x)+1, 10, 20)  
2-element Vector{Float64}:  
0.45597888911063021.9129452507276277
```

# A.2.3 Optional Arguments

We can assign a default value to an argument, making the specification of that argument optional:

```julia
julia>  $f(x = 10) = x^{\wedge}2$  julia> f()   
100   
julia> f(3)   
9   
julia>  $f(x,y,z = 1) = x*y + z;$  julia> f(1,2,3)   
5   
julia> f(1,2)   
3
```

# A.2.4 Keyword Arguments

Functions may use keyword arguments, which are arguments that are named when the function is called. Keyword arguments are given after all the positional arguments. A semicolon is placed before any keywords, separating them from the other arguments:

```julia
julia> f(  $\text{;}x = 0) = x + 1$    
julia> f()   
1   
julia> f(x = 10)   
11   
julia> f(x, y = 10; z = 2) = (x + y)*z; julia> f(1)   
22   
julia> f(2, z = 3)   
36   
julia> f(2, 3)   
10   
julia> f(2, 3, z = 1)   
5
```

# A.2.5 Dispatch

The types of the arguments passed to a function can be specified using the double colon operator. If multiple methods of the same function are provided, Julia will execute the appropriate method. The mechanism for choosing which method to execute is called dispatch:

```txt
julia> f(x::Int64) = x + 10;  
julia> f(x::Float64) = x + 3.1415;  
julia> f(1)  
11  
julia> f(1.0)  
4.141500000000001  
julia> f(1.3)  
4.4415000000000004
```

The method with a type signature that best matches the types of the arguments given will be used:

```julia
julia>  $f(x) = 5$    
julia>  $f(x:::Float64) = 3.1415;$  julia> f([3,2，1])   
5   
julia> f(0.00787499699)   
3.1415
```

# A.2.6 Splitting

It is often useful to splat the elements of a vector or a tuple into the arguments to a function using the ... operator:

```matlab
julia>  $f(x,y,z) = x + y - z$   
julia> a = [3, 1, 2];  
julia> f(a...)  
2  
julia> b = (2, 2, 0);  
julia> f(b...)  
4  
julia> c = ([0,0],[1,1]);  
julia> f([2,2], c...)  
2-element Vector{Int64}:  
11
```

# A.3 Control Flow

We can control the flow of our programs using conditional evaluation and loops. This section provides some of the syntax used in the book.

# A.3.1 Conditional Evaluation

Conditional evaluation will check the value of a Boolean expression and then evaluate the appropriate block of code. One of the most common ways to do this is with an if statement:

```txt
if  $x <   y$  # run this if  $\mathbf{x} <   \mathbf{y}$    
elseif  $x > y$  # run this if  $x > y$    
else # run this if  $x = = y$    
end
```

We can also use the ternary operator with its question mark and colon syntax. It checks the Boolean expression before the question mark. If the expression evaluates to true, then it returns what comes before the colon; otherwise, it returns what comes after the colon:

```txt
julia>  $f(x) = x > 0$  ? x : 0;  
julia> f(-10)  
0  
julia> f(10)  
10
```

# A.3.2 Loops

A loop allows for repeated evaluation of expressions. One type of loop is the while loop, which repeatedly evaluates a block of expressions until the specified condition after the while keyword is met. The following example sums the values in the array X:

```latex
$\begin{array}{rl} & {\mathrm{X} = [1,2,3,4,6,8,11,13,16,18]}\\ & {\mathrm{s} = 0}\\ & {\mathrm{while}!isempty(X)}\\ & {\mathrm{s} += \mathrm{pop}!(X)}\\ & {\mathrm{end}} \end{array}$
```

Another type of loop is the for loop, which uses the for keyword. The following example will also sum over the values in the array  $x$  but will not modify  $x$ :

```matlab
X = [1, 2, 3, 4, 6, 8, 11, 13, 16, 18]  
s = 0  
for y in X  
    s += y  
end
```

The in keyword can be replaced by  $=$  or  $\in$ . The following code block is equivalent:

```julia
X = [1, 2, 3, 4, 6, 8, 11, 13, 16, 18]  
s = 0  
for i = 1: length(X)  
    s += X[i]  
end
```

# A.3.3 Iterator

We can iterate over collections in contexts such as for loops and array comprehensions. To demonstrate various iterators, we will use the collect function, which returns an array of all items generated by an iterator:

```julia
julia> X = ["feed", "sing", "ignore"];  
julia> collect enumerated(X)) # return the count and the element  
3-element Vector{Tuple[Int64, String]};  
(1, "feed")  
(2, "sing")  
(3, "ignore")  
julia> collect(eachindex(X)) # equivalent to 1:length(X)  
3-element Vector[Int64];  
12  
3  
julia> Y = [-5, -0.5, 0];  
julia> collect(zip(X, Y)) # iterate over multiple iterators simultaneously  
3-element Vector{Tuple<String, Float64]};  
("feed", -5.0)  
("sing", -0.5)  
("ignore", 0.0)  
julia> import IterTools: subsets  
julia> collect(subsets(X)) # iterate over all subsets  
8-element Vector{Vector<String]};  
[]
```

```txt
["feed"]  
["sing"]  
["feed", "sing"]  
["ignore"]  
["feed", "ignore"]  
["sing", "ignore"]  
["feed", "sing", "ignore"]  
julia> collect(eachindex(X)) # iterate over indices into a collection  
3-element Vector{Int64}:  
12  
3  
julia> Z = [12; 34; 56];  
julia> import Base(.)terators: product  
julia> collect/product(X,Y)) # iterate over Cartesian product of multiple iterators  
3x3 Matrix{Tuple:String, Float64}]:  
("feed", -5.0) ("feed", -0.5) ("feed", 0.0) ("sing", -5.0) ("sing", -0.5) ("sing", 0.0) ("ignore", -5.0) ("ignore", -0.5) ("ignore", 0.0)
```

# A.4 Packages

A package is a collection of Julia code and possibly other external libraries that can be imported to provide additional functionality. This section briefly reviews a few of the key packages that we build upon in this book. To add a registered package like Distributions.jl, we can run

using Pkg

Pkg.add("Distributions")

To update packages, we use

Pkg.update()

To use a package, we use the keyword using as follows:

using Distributions

Several code blocks in this text specify a package import with using. Some code blocks make use of functions that are not explicitly imported. For instance, the var function is provided by Statistics.jl, and the golden ratio  $\varphi$  is defined in Base.MathConstants.jl. Other excluded packages are InteractiveUtilities.jl, LinearAlgebra.jl, QuadGK.jl, Random.jl, and StatsBase.jl.

# B Test Functions

Researchers in optimization use several test functions to evaluate optimization algorithms. This section covers several test functions used throughout this book.

# B.1 Ackley's Function

Ackley's function (figure B.1) is used to test a method's susceptibility to getting stuck in local minima. It is comprised of two primary components—a sinusoidal component that produces a multitude of local minima and an exponential bell curve centered at the origin, which establishes the function's global minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/dfe9d3ca25d969953d62cb184ba058db9dea1ce00b64edef58c2a0a63f12aae1.jpg)  
Figure B.1. The two-dimensional version of Ackley's function. The global minimum is at the origin.

Ackley's function is defined for any number of dimensions  $d$ :

$$
f (\mathbf {x}) = - a \exp \left(- b \sqrt{\frac{1}{d} \sum_{i = 1}^{d} x_{i}^{2}}\right) - \exp \left(\frac{1}{d} \sum_{i = 1}^{d} \cos \left(c x_{i}\right)\right) + a + \exp (1) \tag {B.1}
$$

with a global minimum at the origin with an optimal value of zero. Typically,  $a = 20$ ,  $b = 0.2$ , and  $c = 2\pi$ . Ackley's function is implemented in algorithm B.1.

```julia
function ackley(x, a=20, b=0.2, c=2π)  
d = length(x)  
return -a*exp(-b*sqrt(sum(x.^2)/d)) - exp(sum(cos(c*xi) for xi in x)/d) + a + exp(1)  
end
```

Algorithm B.1. Ackley's function with  $d$ -dimensional input vector  $\mathbf{x}$  and three optional parameters.

# B.2 Booth's Function

Booth's function (figure B.2) is a two-dimensional quadratic function.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a600108e51249ebb7b9496118a7847100097c952d7398d0dc111b4c6b72de3c6.jpg)  
Figure B.2. Booth's function with a global minimum at [1,3].

Its equation is given by

$$
f (\mathbf {x}) = (x_{1} + 2 x_{2} - 7) ^{2} + (2 x_{1} + x_{2} - 5) ^{2} \tag {B.2}
$$

with a global minimum at  $[1,3]$  with an optimal value of zero. It is implemented in algorithm B.2.

$$
\operatorname{booth} (x) = (x [ 1 ] + 2 x [ 2 ] - 7) ^{\wedge} 2 + (2 x [ 1 ] + x [ 2 ] - 5) ^{\wedge} 2
$$

Algorithm B.2. Booth's function with two-dimensional input vector  $\mathbf{x}$ .

# B.3 Branin Function

The Branin function (figure B.3) is a two-dimensional function,

$$
f (\mathbf {x}) = a \left(x_{2} - b x_{1}^{2} + c x_{1} - r\right) ^{2} + s (1 - t) \cos \left(x_{1}\right) + s \tag {B.3}
$$

with recommended values  $a = 1, b = 5.1 / (4\pi^2), c = 5 / \pi, r = 6, s = 10,$  and  $t = 1 / (8\pi)$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f44dbd860a68b264cbc127bdb8e629ebd07939fc21cf260930c85781834e0c7b.jpg)  
Figure B.3. The Branin function, with four global minima.

It has no local minima aside from global minima with  $x_{1} = \pi + 2\pi m$  for integral  $m$ . Four of these minima are:

$$
\left\{\left[ \begin{array}{l} - \pi \\ 12. 27 5 \end{array} \right], \left[ \begin{array}{l} \pi \\ 2. 27 5 \end{array} \right], \left[ \begin{array}{l} 3 \pi \\ 2. 47 5 \end{array} \right], \left[ \begin{array}{l} 5 \pi \\ 12. 87 5 \end{array} \right] \right\} \tag {B.4}
$$

with  $f(\mathbf{x}^{*})\approx 0.397887$  . It is implemented in algorithm B.3.

```matlab
function branin(x; a=1, b=5.1/(4π^2), c=5/π, r=6, s=10, t=1/(8π))  
return a*(x[2]-b*x[1]^2+c*x[1]-r)^2 + s*(1-t)*cos(x[1]) + s  
end
```

Algorithm B.3. The Branin function with two-dimensional input vector  $\mathbf{x}$  and six optional parameters.

# B.4 Flower Function

The flower function (figure B.4) is a two-dimensional test function whose contour function has flower-like petals originating from the origin.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/162ca4bb321065d76768f6f14c206e88c2af3e1633f52224e80c5b088802863b.jpg)  
Figure B.4. The flower function.

The equation is

$$
f (\mathbf {x}) = a \| \mathbf {x} \| + b \sin \left(c \tan^{- 1} \left(x_{2}, x_{1}\right)\right) \tag {B.5}
$$

with its parameters typically set to  $a = 1, b = 1$ , and  $c = 4$ .

The flower function is minimized near the origin but does not have a global minimum due to atan being undefined at  $[0, 0]$ . It is implemented in algorithm B.4.

```julia
function flower(x; a=1, b=1, c=4)  
return a*norm(x) + b*sin(c*atan(x[2], x[1]))  
end
```

Algorithm B.4. The flower function with two-dimensional input vector  $\mathbf{x}$  and three optional parameters.

# B.5 Michalewicz Function

The Michalewicz function (figure B.5) is a  $d$ -dimensional optimization function with several steep valleys.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/fb04af8112a72eeb0e2704bf016e0f43fad2c6fe1e0b8552546af75b33ab400d.jpg)  
Figure B.5. The Michalewicz function.

Its equation is

$$
f (\mathbf {x}) = - \sum_{i = 1}^{d} \sin \left(x_{i}\right) \sin^{2 m} \left(\frac{i x_{i}^{2}}{\pi}\right) \tag {B.6}
$$

where the parameter  $m$ , typically 10, controls the steepness. The global minimum depends on the number of dimensions. In two dimensions the minimum is at approximately [2.20, 1.57] with  $f(\mathbf{x}^{*}) = -1.8011$ . It is implemented in algorithm B.5.

```matlab
function michalewicz(x; m=10)  
return -sum(sin(v)*sin(i*v^2/π)^(2m) for (i,v) in enumerate(x))  
end
```

Algorithm B.5. The Michalewicz function with input vector  $x$  and optional steepness parameter  $m$ .

# B.6 Rosenbrock's Banana Function

The Rosenbrock function (figure B.6), also called Rosenbrock's valley or Rosenbrock's banana function, is a well-known unconstrained test function developed by Rosenbrock in 1960. It has a global minimum inside a long, curved valley. Most optimization algorithms have no problem finding the valley but have difficulties traversing along the valley to the global minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a1ca43ea600401056247c3f261eb3becb734b2ee85e2afc83df98bbbab293e64.jpg)

The Rosenbrock function is

$$
f (\mathbf {x}) = (a - x_{1}) ^{2} + b (x_{2} - x_{1}^{2}) ^{2} \qquad \mathrm{(B .7)}
$$

with a global minimum at  $[a, a^2]$  at which  $f(\mathbf{x}^*) = 0$ . This text uses  $a = 1$  and  $b = 5$ .

1 H. H. Rosenbrock, "An Automatic Method for Finding the Greatest or Least Value of a Function," The Computer Journal, vol. 3, no. 3, pp. 175-184, 1960.

Figure B.6. The Rosenbrock function with  $a = 1$  and  $b = 5$ . The global minimum is at [1,1].

$$
\operatorname{rosenbrock} (x; a = 1, b = 5) = (a - x [ 1 ]) ^{\wedge} 2 + b * (x [ 2 ] - x [ 1 ] ^{\wedge} 2) ^{\wedge} 2
$$

The Rosenbrock function can be extended to multiple dimensions. One common extension $^2$  for  $n$  dimensions is:

$$
f (\mathbf {x}) = \sum_{i = 1}^{n - 1} (a - x_{i}) ^{2} + b \left(x_{i + 1} - x_{i}^{2}\right) ^{2} \tag {B.8}
$$

The Rosenbrock function is implemented in algorithm B.6. The multidimensional Rosenbrock function is implemented in algorithm B.7.

```matlab
function extended_rosenbrock(x; a=1, b=5)  
return sum((a .- x[1:end-1]).^2 .+ b*(x[2:end] .- x[1:end-1].^2).^2)  
end
```

Algorithm B.6. The Rosenbrock function with two-dimensional input vector  $\mathbf{x}$  and two optional parameters.

2 D. E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, 1989.

Algorithm B.7. A multidimensional extension to the Rosenbrock function with an  $m$ -dimensional input vector  $\mathbf{x}$  for  $m > 1$  and two optional parameters.

# B.7 Wheeler's Ridge

Wheeler's ridge (figure B.7) is a two-dimensional function with a single global minimum in a deep curved peak. The function has two ridges, one along the positive and one along the negative first coordinate axis. A gradient descent method will diverge along the negative axis ridge. The function is very flat away from the optimum and the ridge.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/37258cb1d2c20572929b92293d2f2f4cf436878a7a037451e4c32bde4e2371ad.jpg)

The function is given by

$$
f (\mathbf {x}) = - \exp \left(- \left(x_{1} x_{2} - a\right) ^{2} - \left(x_{2} - a\right) ^{2}\right) \tag {B.9}
$$

Figure B.7. Wheeler's ridge showing the two ridges and the peak containing the global minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/047bbd9238cfde3bdf62acacb00861958f0bc1da768461f7398bdaa56eda1e83.jpg)  
Figure B.8. A contour plot of the minimal region of Wheeler's ridge.

with  $a$  typically equal to 1.5, for which the global optimum of  $-1$  is at  $[1,3/2]$ .

$$
w h e e l e r (x, a = 1. 5) = - \exp (- (x [ 1 ] * x [ 2 ] - a) ^{\wedge} 2 - (x [ 2 ] - a) ^{\wedge} 2)
$$

Wheeler's ridge has a smooth contour plot (figure B.8) when evaluated over  $x_{1} \in [0,3]$  and  $x_{2} \in [0,3]$ . It is implemented in algorithm B.8.

# B.8 Circle Function

The circle function (algorithm B.9) is a simple multiobjective test function given by

$$
\mathbf {f} (\mathbf {x}) = \left[ \begin{array}{l} 1 - r \cos (\theta) \\ 1 - r \sin (\theta) \end{array} \right] \tag {B.10}
$$

where  $\theta = x_{1}$  and  $r$  is obtained by passing  $x_{2}$  through

$$
r = \frac{1}{2} + \frac{1}{2} \left(\frac{2 x_{2}}{1 + x_{2}^{2}}\right) \tag {B.11}
$$

The Pareto frontier has  $r = 1$  and mod  $(\theta, 2\pi) \in [0, \pi/2]$  or  $r = -1$  and mod  $(\theta, 2\pi) \in [\pi, 3\pi/2]$ .

```matlab
function circle(x)  
     $\theta = x[1]$ $r = 0.5 + 0.5*(2x[2] / (1 + x[2]^2))$ $y1 = 1 - r*\cos(\theta)$ $y2 = 1 - r*\sin(\theta)$   
    return [y1, y2]  
end
```

Algorithm B.8. Wheeler's ridge, which takes in a two-dimensional design point  $x$  and an optional scalar parameter  $a$ .

Algorithm B.9. The circle function, which takes in a two-dimensional design point  $x$  and produces a two-dimensional objective value.

# C Mathematical Concepts

This appendix covers mathematical concepts used in the derivation and analysis of optimization methods. These concepts are used throughout this book.

# C.1 Asymptotic Notation

Asymptotic notation is often used to characterize the growth of a function. This notation is sometimes called big-Oh notation, since the letter  $O$  is used because the growth rate of a function is often called its order. This notation can be used to describe the error associated with a numerical method or the time or space complexity of an algorithm. This notation provides an upper bound on a function as its argument approaches a certain value.

Mathematically, if  $f(x) = O(g(x))$  as  $x \to a$  then the absolute value of  $f(x)$  is bounded by the absolute value of  $g(x)$  times some positive and finite  $c$  for values of  $x$  sufficiently close to  $a$ :

$$
| f (x) | \leq c | g (x) | \quad \text{for} x \rightarrow a \tag {C.1}
$$

Writing  $f(x) = O(g(x))$  is a common abuse of the equal sign. For example,  $x^{2} = O(x^{2})$  and  $2x^{2} = O(x^{2})$ , but, of course,  $x^{2} \neq 2x^{2}$ . In some mathematical texts,  $O(g(x))$  represents the set of all functions that do not grow faster than  $g(x)$ . One might write, for example,  $5x^{2} \in O(x^{2})$ . An example of asymptotic notation is given in example C.1.

If  $f(x)$  is a linear combination<sup>1</sup> of terms, then  $O(f)$  corresponds to the order of the fastest growing term. Example C.2 compares the orders of several terms.

1 A linear combination is a weighted sum of terms. If the terms are in a vector  $\mathbf{x}$ , then the linear combination is  $w_{1}x_{1} + w_{2}x_{2} + \dots = \mathbf{w}^{\top}\mathbf{x}$ .

Consider  $f(x) = 10^{6}e^{x}$  as  $x \to \infty$ . We want to find a  $c$  and a  $g$  such that  $|f(x)| \leq c|g(x)|$ . By choosing  $c = 10^{6}$  and  $g(x) = e^{x}$ , we can satisfy this inequality as  $x \to \infty$ . Thus,  $f = O(e^{x})$  as  $x \to \infty$ .

Example C.1. Asymptotic notation for a constant times a function.

Consider  $f(x) = \cos (x) + x + 10x^{3 / 2} + 3x^2$ . Here,  $f$  is a linear combination of terms. The terms  $\cos (x), x, x^{3 / 2}$ , and  $x^2$  grow at different rates as  $x$  approaches infinity. Below, we plot  $f(x)$  along with  $c|g(x)|$  for different functions  $g(x)$  with  $c$  chosen such that  $c|g(x)|$  exceeds  $|f(x)|$  when  $x = 2$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/eae38402acf171c40a05976a48ef4c25d2050903823c57d50b40e51594412a3f.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/15c65a42a59eb2d0d1837375aca2792b02caa5eb77fd0cb88c77944a7590f8bc.jpg)

$$
- - f (x) - - 16 x^{3 / 2} - - 12 x^{2} - - 6 x^{3}
$$

There is no constant  $c$  such that  $f(x)$  is always less than  $c|x^{3/2}|$  for sufficiently large values of  $x$ . The same is true for  $\cos(x)$  and  $x$ . We do find that there are values for  $c$  such that  $c|x^2|$  and  $c|x^3|$  are always greater than  $f(x)$  for sufficiently large  $x$ . Hence,  $f(x) = O(x^3)$ , and in general  $f(x) = O(x^m)$  for  $m \geq 2$ , along with other function classes like  $f(x) = e^x$ . We typically discuss the order that provides the tightest upper bound. Thus,  $f = O(x^2)$  as  $x \to \infty$ .

Example C.2. An illustration of finding the order of a linear combination of terms.

# C.2 Taylor Expansion

The Taylor expansion, also called the Taylor series, of a function is critical to understanding many of the optimization methods covered in this book, so we derive it here.

From the first fundamental theorem of calculus,2 we know that

$$
f (x + h) = f (x) + \int_{0}^{h} f^{\prime} (x + a) d a \tag {C.2}
$$

Nesting this definition produces the Taylor expansion of  $f$  about  $x$ :

$$
\begin{array}{l} f (x + h) = f (x) + \int_{0}^{h} \left(f^{\prime} (x) + \int_{0}^{a} f^{\prime \prime} (x + b) d b\right) d a (C.3) \\ = f (x) + f^{\prime} (x) h + \int_{0}^{h} \int_{0}^{a} f^{\prime \prime} (x + b) d b d a (C.4) \\ = f (x) + f^{\prime} (x) h + \int_{0}^{h} \int_{0}^{a} \left(f^{\prime \prime} (x) + \int_{0}^{b} f^{\prime \prime \prime} (x + c) d c\right) d b d a \quad (\mathrm{C}. 5) \\ = f (x) + f^{\prime} (x) h + \frac{f^{\prime \prime} (x)}{2 !} h^{2} + \int_{0}^{h} \int_{0}^{a} \int_{0}^{b} f^{\prime \prime \prime} (x + c) d c d b d a (C.6) \\ \end{array}
$$

$$
\vdots \tag {C.7}
$$

$$
\begin{array}{l} = f (x) + \frac{f^{\prime} (x)}{1 !} h + \frac{f^{\prime \prime} (x)}{2 !} h^{2} + \frac{f^{\prime \prime \prime} (x)}{3 !} h^{3} + \dots (C.8) \\ = \sum_{n = 0}^{\infty} \frac{f^{(n)} (x)}{n !} h^{n} (C.9) \\ \end{array}
$$

In some contexts, it may be more convenient to write the Taylor expansion of  $f(x)$  about a point  $x_0$  such that it is a function solely of  $x$  instead of  $f(x + h)$ :

$$
f (x) = \sum_{n = 0}^{\infty} \frac{f^{(n)} \left(x_{0}\right)}{n !} \left(x - x_{0}\right) ^{n} \tag {C.10}
$$

The Taylor expansion represents a function as an infinite sum of polynomial terms based on repeated derivatives at a single point. Any analytic function can be represented by its Taylor expansion within a local neighborhood.

A function can be locally approximated by using the first few terms of the Taylor expansion. Figure C.1 shows increasingly better approximations for  $\cos(x)$  about  $x = 1$ . Including more terms increases the accuracy of the local approximation, but error still accumulates as one moves away from the expansion point.

2 The first fundamental theorem of calculus relates a function to the integral of its derivative:

$$
f (b) - f (a) = \int_{a}^{b} f^{\prime} (x) d x
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/2fab448e7dd6d07bc97a4265ef7f4c11dcbfabd037f5c833f5b22aa7b4d2e2d0.jpg)  
Figure C.1. Successive approximations of  $\cos (x)$  about 1 based on the first  $n$  terms of the Taylor expansion.

A linear Taylor approximation uses the first two terms of the Taylor expansion:

$$
f (x) \approx f \left(x_{0}\right) + f^{\prime} \left(x_{0}\right) \left(x - x_{0}\right) \tag {C.11}
$$

A quadratic Taylor approximation uses the first three terms:

$$
f (x) \approx f \left(x_{0}\right) + f^{\prime} \left(x_{0}\right) \left(x - x_{0}\right) + \frac{1}{2} f^{\prime \prime} \left(x_{0}\right) \left(x - x_{0}\right) ^{2} \tag {C.12}
$$

and so on.

In multiple dimensions, the Taylor expansion about  $\mathbf{x}_0$  generalizes to

$$
f (\mathbf {x}) = f \left(\mathbf {x}_{0}\right) + \nabla f \left(\mathbf {x}_{0}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}_{0}\right) + \frac{1}{2} \left(\mathbf {x} - \mathbf {x}_{0}\right) ^{\top} \nabla^{2} f \left(\mathbf {x}_{0}\right) \left(\mathbf {x} - \mathbf {x}_{0}\right) + \dots \tag {C.13}
$$

The first two terms form the tangent plane at  $\mathbf{x}_0$ . The third term incorporates local curvature. This text will use only the first three terms shown here.

# C.3 Convexity

A convex combination of two vectors  $\mathbf{x}$  and  $\mathbf{y}$  is the result of

$$
\alpha \mathbf {x} + (1 - \alpha) \mathbf {y} \tag {C.14}
$$

for some  $\alpha \in [0,1]$ . Convex combinations can be made from  $m$  vectors,

$$
w_{1} \mathbf {v}^{(1)} + w_{2} \mathbf {v}^{(2)} + \dots + w_{m} \mathbf {v}^{(m)} \tag {C.15}
$$

with nonnegative weights  $\mathbf{w}$  that sum to one.

A convex set is a set for which a line drawn between any two points in the set is entirely within the set. Mathematically, a set  $S$  is convex if we have

$$
\alpha \mathbf {x} + (1 - \alpha) \mathbf {y} \in \mathcal {S} \tag {C.16}
$$

for all  $\mathbf{x}, \mathbf{y}$  in  $S$  and for all  $\alpha$  in  $[0,1]$ . A convex and a nonconvex set are shown in figure C.2.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/6b6e40407ab3dc0f8a75892bf11a40692d7846a1ec1ddd3d5ae857e8fe892e70.jpg)  
A convex set

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/e415c4ff001b89e47191468bac62602d98b8d9c06aa1dc3102a846b42d46cc0c.jpg)  
Not a convex set  
Figure C.2. Convex and nonconvex sets.

A convex function is a bowl-shaped function whose domain is a convex set. By bowl-shaped, we mean it is a function such that any line drawn between two points in its domain does not lie below the function. A function  $f$  is convex over a convex set  $S$  if, for all  $\mathbf{x}, \mathbf{y}$  in  $S$  and for all  $\alpha$  in  $[0,1]$ ,

$$
f (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) \leq \alpha f (\mathbf {x}) + (1 - \alpha) f (\mathbf {y}) \tag {C.17}
$$

Convex and concave regions of a function are shown in figure C.3.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/a4c3ed2ec2b33ce878c998291aaa8ba54a76beac009a458a85da0f59b56c431b.jpg)  
Figure C.3. Convex and nonconvex portions of a function.

A function  $f$  is strictly convex over a convex set  $S$  if, for all  $\mathbf{x}, \mathbf{y}$  in  $S$  and  $\alpha$  in (0,1),

$$
f (\alpha \mathbf {x} + (1 - \alpha) \mathbf {y}) <   \alpha f (\mathbf {x}) + (1 - \alpha) f (\mathbf {y}) \tag {C.18}
$$

Strictly convex functions have at most one minimum, whereas a convex function can have flat regions. Examples of strict and nonstrict convexity are shown in figure C.4.

3 Optimization of convex functions is the subject of the textbook by S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/347af4efe7a798f07fd0a9c7b6627dee1f22a3fa31558952e0ffa6516c2b3da2.jpg)  
Strictly convex function with one global minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/8b9382765e9f8777f141c089389d55f4d25d3720f3b2a8015898194d47255b40.jpg)  
Convex function without a unique global minimum.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/03ef5ccba2047d1a1343925d48bf94f742de0fb39be822026004b309d2363eab.jpg)  
Strictly convex function without a global minimum.

A function  $f$  is concave if  $-f$  is convex. Furthermore,  $f$  is strictly concave if  $-f$  is strictly convex.

Not all convex functions are unimodal (defined in section 3.1) and not all unimodal functions are convex, as shown in figure C.5.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/165540a0a5247101a821e6d1c7c3179325ff8e3125e7f657a23dd59d9a086c79.jpg)  
Unimodal and convex

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/9b9c18bed18dd8c0e481b70d9c0fb728c24d3b063f54fc8ca539da9d3cebbd02.jpg)  
Unimodal but nonconvex

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/488ec36517a157bd1dbcad39c234f78f910ae3a27ca34e917dd95334b57875c2.jpg)  
Figure C.4. Not all convex functions have single global minima.  
Convex but nonunimodal  
Figure C.5. Convexity and unimodality are not the same thing.

# C.4 Norms

A norm is a function that assigns a length to a vector. To compute the distance between two vectors, we evaluate the norm of the difference between those two vectors. For example, the distance between points  $\mathbf{a}$  and  $\mathbf{b}$  using the Euclidean norm is

$$
\left\| \mathbf {a} - \mathbf {b} \right\| _{2} = \sqrt{\left(a_{1} - b_{1}\right) ^{2} + \left(a_{2} - b_{2}\right) ^{2} + \cdots + \left(a_{n} - b_{n}\right) ^{2}} \tag {C.19}
$$

A function  $f$  is a norm  $\mathrm{if}^4$

1.  $f(\mathbf{x}) = 0$  if and only if  $\mathbf{a}$  is the zero vector.  
2.  $f(a\mathbf{x}) = |a|f(\mathbf{x})$  , such that lengths scale.  
3.  $f(\mathbf{a} + \mathbf{b})\leq f(\mathbf{a}) + f(\mathbf{b})$  , also known as the triangle inequality.

The  $L_{p}$  norms are a commonly used set of norms parameterized by a scalar  $p \geq 1$ . The Euclidean norm in equation (C.19) is the  $L_{2}$  norm. Several  $L_{p}$  norms are shown in table C.1.

The  $L_{p}$  norms are defined according to:

$$
\left\| \mathbf {x} \right\| _{p} = \lim_{\rho \rightarrow p} \left(| x_{1} | ^{\rho} + | x_{2} | ^{\rho} + \dots + | x_{n} | ^{\rho}\right) ^{\frac{1}{\rho}} \tag {C.20}
$$

where the limit is necessary for defining the infinity norm,  $L_{\infty}$ .

# C.5 Matrix Calculus

This section derives two common gradients:  $\nabla_{\mathbf{x}}\mathbf{b}^{\top}\mathbf{x}$  and  $\nabla_{\mathbf{x}}\mathbf{x}^{\top}\mathbf{A}\mathbf{x}$ .

To obtain  $\nabla_{\mathbf{x}}\mathbf{b}^{\top}\mathbf{x}$ , we first expand the dot product:

$$
\mathbf {b}^{\top} \mathbf {x} = \left[ b_{1} x_{1} + b_{2} x_{2} + \dots + b_{n} x_{n} \right] \tag {C.21}
$$

The partial derivative with respect to a single coordinate is:

$$
\frac{\partial}{\partial x_{i}} \mathbf {b}^{\top} \mathbf {x} = b_{i} \tag {C.22}
$$

Thus, the gradient is:

$$
\nabla_{\mathbf {x}} \mathbf {b}^{\top} \mathbf {x} = \nabla_{\mathbf {x}} \mathbf {x}^{\top} \mathbf {b} = \mathbf {b} \tag {C.23}
$$

4 Some properties that follow from these axioms include:

$$
f (- \mathbf {x}) = f (\mathbf {x})
$$

$$
f (\mathbf {x}) \geq 0
$$

5 The  $L_{\infty}$  norm is also referred to as the max norm, Chebyshev distance, or chessboard distance. The latter name comes from the minimum number of moves a chess king needs to move between two chess squares.

$$
L_{1} \colon \| \mathbf {x} \| _{1} = | x_{1} | + | x_{2} | + \dots + | x_{n} |
$$

$$
L_{2} \colon \| \mathbf {x} \| _{2} = \sqrt{x_{1}^{2} + x_{2}^{2} + \cdots + x_{n}^{2}}
$$

$$
L_{\infty}: \| \mathbf {x} \| _{\infty} = \max \left(\left| x_{1} \right|, \left| x_{2} \right|, \dots , \left| x_{n} \right|\right)
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/f5e942c2a4662983ff4339061243e7bfcdc7039fb652e248cd4ee431f8a88a4c.jpg)  
Table C.1. Common  $L_{p}$  norms. The illustrations show the shape of the norm contours in two dimensions. All points on the contour are equidistant from the origin under that norm.

To obtain  $\nabla_{\mathbf{x}}\mathbf{x}^{\top}\mathbf{A}\mathbf{x}$  for a square matrix  $\mathbf{A}$ , we first expand  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x}$ :

$$
\begin{array}{l} \mathbf {x}^{\top} \mathbf {A} \mathbf {x} = \left[ \begin{array}{l} x_{1} \\ x_{2} \\ \dots \\ x_{n} \end{array} \right] ^{\top} \left[ \begin{array}{c c c c} a_{11} & a_{12} & \dots & a_{1 n} \\ a_{21} & a_{22} & \dots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \dots & a_{n n} \end{array} \right] \left[ \begin{array}{l} x_{1} \\ x_{2} \\ \dots \\ x_{n} \end{array} \right] (C.24) \\ = \left[ \begin{array}{c} x_{1} \\ x_{2} \\ \dots \\ x_{n} \end{array} \right] ^{\top} \left[ \begin{array}{c} x_{1} a_{11} + x_{2} a_{12} + \dots + x_{n} a_{1 n} \\ x_{1} a_{21} + x_{2} a_{22} + \dots + x_{n} a_{2 n} \\ \vdots \\ x_{1} a_{n 1} + x_{2} a_{n 2} + \dots + x_{n} a_{n n} \end{array} \right] (C.25) \\ x_{1}^{2} a_{11} + x_{1} x_{2} a_{12} + \dots + x_{1} x_{n} a_{1 n} + \\ = \begin{array}{c} x_{1} x_{2} a_{21} + x_{2}^{2} a_{22} + \dots + x_{2} x_{n} a_{2 n} + \\ \vdots \end{array} (C.26) \\ x_{1} x_{n} a_{n 1} + x_{2} x_{n} a_{n 2} + \dots + x_{n}^{2} a_{n n} \\ \end{array}
$$

The partial derivative with respect to the  $i$ th component is

$$
\frac{\partial}{\partial x_{i}} \mathbf {x}^{\top} \mathbf {A} \mathbf {x} = \sum_{j = 1}^{n} x_{j} \left(a_{i j} + a_{j i}\right) \tag {C.27}
$$

The gradient is thus:

$$
\begin{array}{l} \nabla_{\mathbf {x}} \mathbf {x}^{\top} \mathbf {A} \mathbf {x} = \left[ \begin{array}{c} \sum_{j = 1}^{n} x_{j} \left(a_{1 j} + a_{j 1}\right) \\ \sum_{j = 1}^{n} x_{j} \left(a_{2 j} + a_{j 2}\right) \\ \vdots \\ \sum_{j = 1}^{n} x_{j} \left(a_{n j} + a_{j n}\right) \end{array} \right] (C.28) \\ = \left[ \begin{array}{c c c c} a_{11} + a_{11} & a_{12} + a_{21} & \dots & a_{1 n} + a_{n 1} \\ a_{21} + a_{12} & a_{22} + a_{22} & \dots & a_{2 n} + a_{n 2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} + a_{1 n} & a_{n 2} + a_{2 n} & \dots & a_{n n} + a_{n n} \end{array} \right] \left[ \begin{array}{l} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{array} \right] (C.29) \\ = \left(\mathbf {A} + \mathbf {A}^{\top}\right) \mathbf {x} (C.30) \\ \end{array}
$$

# C.6 Positive Definiteness

The positive definiteness of matrices arises in linear algebra and optimization for a variety of reasons. A symmetric matrix  $\mathbf{A}$  is positive definite if  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x}$  is positive for all points other than the origin:  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x} > 0$  for all  $\mathbf{x} \neq \mathbf{0}$ . A symmetric matrix  $\mathbf{A}$  is positive semidefinite if  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x}$  is always nonnegative:  $\mathbf{x}^{\top}\mathbf{A}\mathbf{x} \geq 0$  for all  $\mathbf{x}$ . If all eigenvalues of  $\mathbf{A}$  are strictly positive, then  $\mathbf{A}$  is positive definite and automatically also positive semidefinite. If some eigenvalues are zero but still nonnegative, it is positive semidefinite but not positive definite.

Positive definiteness is useful in optimization because it guarantees that a function has a unique global minimum under certain conditions. For example, if the matrix  $\mathbf{A}$  is positive definite in the function  $f(\mathbf{x}) = \mathbf{x}^{\top}\mathbf{A}\mathbf{x}$ , then  $f$  has a unique global minimum. If  $f$  is instead simply a twice-differentiable function, then the second-order Taylor approximation of  $f$  at  $\mathbf{x}_0$  is

$$
f (\mathbf {x}) \approx f \left(\mathbf {x}_{0}\right) + \nabla f \left(\mathbf {x}_{0}\right) ^{\top} \left(\mathbf {x} - \mathbf {x}_{0}\right) + \frac{1}{2} \left(\mathbf {x} - \mathbf {x}_{0}\right) ^{\top} \mathbf {H}_{0} \left(\mathbf {x} - \mathbf {x}_{0}\right) \tag {C.31}
$$

where  $\mathbf{H}_0$  is the Hessian of  $f$  evaluated at  $\mathbf{x}_0$ . Knowing that  $(\mathbf{x} - \mathbf{x}_0)^\top \mathbf{H}_0(\mathbf{x} - \mathbf{x}_0)$  has a unique global minimum is sufficient to determine whether the quadratic approximation has a unique global minimum.

# C.7 Matrix Decompositions

A matrix decomposition or matrix factorization is the decomposition of a matrix into a product of matrices. The component matrices making up this product often have known useful properties. There are many types of matrix decompositions. This section reviews a few used in this textbook assuming an  $m \times n$  matrix  $\mathbf{A}$  with real components.<sup>7</sup>

# C.7.1 Cholesky Decomposition

A Cholesky decomposition of  $\mathbf{A}$  satisfies

$$
\mathbf {A} = \mathbf {L L}^{\top} \tag {C.32}
$$

where  $\mathbf{L}$  is a lower triangular matrix with positive diagonal entries. This decomposition is unique when  $\mathbf{A}$  is positive definite.

The component  $f(\mathbf{x}_0)$  merely shifts the function vertically. The component  $\nabla f(\mathbf{x}_0)^\top (\mathbf{x} - \mathbf{x}_0)$  is a linear term which is dominated by the quadratic term.

7 Many of these decompositions extend to complex matrices as well. A review of matrix factorizations is provided by T. Lyche, Numerical Linear Algebra and Matrix Factorizations. Springer, 2020.

Solving a system of linear equations  $\mathbf{A}\mathbf{x} = \mathbf{b}$  for a positive definite  $\mathbf{A}$  typically takes  $O(n^{3})$  operations. If the Cholesky decomposition is known, then solving  $\mathbf{L}\mathbf{L}^{\top}\mathbf{x} = \mathbf{b}$  can be done by first solving  $\mathbf{L}\mathbf{y} = \mathbf{b}$  with forward substitution in  $O(n^{2})$  and then solving  $\mathbf{L}^{\top}\mathbf{x} = \mathbf{y}$  with back substitution in  $O(n^{2})$  time. While computing the Cholesky decomposition itself generally takes  $O(n^{3})$ , it can be computed once for  $\mathbf{A}$  and then reused to solve  $\mathbf{A}\mathbf{x} = \mathbf{b}$  for multiple values of  $\mathbf{b}$ .

In Julia, the Cholesky decomposition is provided by the cholesky method in LinearAlgebra.jl:

```txt
julia> A = [21.0 -5.012.0; -5.013.0 -6.0; 12.0 -6.09.0];  
julia> d = cholesky(A);  
julia> d.L  
3×3 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:  
4.58258 · ·  
-1.091093.4365 · ·  
2.61861 -0.9145521.143  
julia> d.L*d.L'  
3×3 Matrix{Float64}:  
21.0 -5.012.0  
-5.013.0 -6.012.0 -6.09.0
```

The cholesky method also provides an upper triangular matrix  $U$  such that  $\mathbf{A} = \mathbf{U}^{\top}\mathbf{U}$ :

```txt
julia> d.U  
3×3 LinearAlgebra. UpperTriangular{Float64, Matrix{Float64}}: 4.58258 -1.091092.61861  
• 3.4365 -0.914552  
• 1.143  
julia> d.U'*d.U  
3×3 Matrix{Float64}: 21.0 -5.012.0  
-5.013.0 -6.012.0 -6.09.0
```

# C.7.2 LDL Decomposition

An LDL decomposition modifies the Cholesky decomposition to incorporate an additional diagonal matrix  $\mathbf{D}$  such that:

$$
\mathbf {A} = \mathbf {L D L}^{\top} \tag {C.33}
$$

and  $\mathbf{L}$  is a lower triangular matrix whose diagonal entries are all 1. This decomposition is sometimes called the square-root-free Cholesky decomposition because it avoids computing square roots.

We can recover a Cholesky decomposition from this decomposition through

$$
\mathbf {A} = \mathbf {L D L}^{\top} = \mathbf {L D}^{1 / 2} \left(\mathbf {D}^{1 / 2}\right) ^{\top} \mathbf {L}^{\top} = \left(\mathbf {L D}^{1 / 2}\right) \left(\mathbf {L D}^{1 / 2}\right) ^{\top} \tag {C.34}
$$

In Julia, the LDL decomposition is provided by the ldt method in LinearAlgebra.jl. The implementation requires that A be tridiagonal.

# C.7.3 LQ Decomposition

An  $LQ$  decomposition of  $\mathbf{A}$  satisfies

$$
\mathbf {A} = \mathbf {L Q} \tag {C.35}
$$

for an  $m \times n$  lower triangular  $\mathbf{L}$  and an  $n \times n$  orthogonal matrix  $\mathbf{Q}$ .

An orthogonal matrix, or orthonormal matrix, satisfies  $\mathbf{Q}^{\top}\mathbf{Q} = \mathbf{Q}\mathbf{Q}^{\top} = \mathbf{I}$ . As such, the transpose of an orthogonal matrix is also its inverse,  $\mathbf{Q}^{\top} = \mathbf{Q}^{-1}$ . Multiplication by an Orthogonal matrix also preserves the dot product:

$$
\mathbf {u} \cdot \mathbf {v} = \mathbf {u}^{\top} \mathbf {v} = \mathbf {u}^{\top} \mathbf {Q}^{\top} \mathbf {Q} \mathbf {v} = (\mathbf {Q} \mathbf {u}) ^{\top} (\mathbf {Q} \mathbf {v}) \tag {C.36}
$$

In Julia, the LQ decomposition is provided by the lq method in LinearAlgebra.jl:

```txt
julia> d = lq(A)  
LinearAlgebra.LQ{Float64, Matrix{Float64}, Vector{Float64}}  
L factor:  
3×3 Matrix{Float64}:  
-24.69820.00.09.79829 -11.57560.0  
-15.79063.220471.13328  
Q factor: 3×3 LinearAlgebra.LQPackedQ{Float64, Matrix{Float64}, Vector{Float64}}  
julia> d.L * d.Q  
3×3 Matrix{Float64}:  
21.0 -5.012.0  
-5.013.0 -6.012.0 -6.09.0
```

# C.7.4 Singular Value Decomposition

A singular value decomposition of an  $m \times n$  matrix  $\mathbf{A}$  satisfies

$$
\mathbf {A} = \mathbf {U S V}^{\top} \tag {C.37}
$$

where  $\mathbf{U}$  is an  $m \times m$  orthogonal matrix,  $\mathbf{S}$  is an  $m \times n$  nonnegative diagonal matrix, and  $\mathbf{V}$  is an  $n \times n$  orthogonal matrix. Singular value decomposition is useful in a wide range of applications, including least squares fitting, computing the pseudoinverse, identifying the rank of a matrix, and dimensionality reduction.

The columns of  $\mathbf{U}$  are the eigenvectors of  $\mathbf{A}\mathbf{A}^{\top}$ , whereas the columns of  $\mathbf{V}$  are the eigenvectors of  $\mathbf{A}^{\top}\mathbf{A}$ . The diagonal entries of  $\mathbf{S}$ , called the singular values of  $\mathbf{A}$ , are the square roots of the eigenvalues of  $\mathbf{A}\mathbf{A}^{\top}$  and  $\mathbf{A}^{\top}\mathbf{A}$ . These singular values are typically arranged in decreasing order. If  $\mathbf{A}$  has rank  $k$ , then there will be  $k$  singular values.

In Julia, the singular value decomposition is provided by the svd $^8$  method in LinearAlgebra.jl:

```txt
julia> d = svd(A)  
LinearAlgebra.SVD{Float64, Float64, Matrix{Float64}, Vector{Float64}}  
U factor:  
3×3 Matrix{Float64}:  
-0.771231 -0.445746 -0.4544380.376402 -0.895060.239144  
-0.5133460.01338330.858077  
singular values:  
3-element Vector{Float64}:  
31.427710.59970.972612  
Vt factor:  
3×3 Matrix{Float64}:  
-0.7712310.376402 -0.513346  
-0.445746 -0.895060.0133833  
-0.4544380.2391440.858077  
julia> d.U * Diagonal(d.S) * d.Vt  
3×3 Matrix{Float64}:  
21.0 -5.012.0  
-5.013.0 -6.012.0 -6.09.0
```

The singular value decomposition is an example of a complete orthogonal decomposition,

$$
\mathbf {A} = \mathbf {U} \left[ \begin{array}{l l} \mathbf {T} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} \end{array} \right] \mathbf {V}^{\top} \tag {C.38}
$$

where  $\mathbf{U}$  is an  $m \times m$  orthogonal matrix,  $\mathbf{T}$  is an  $m \times n$  nonnegative diagonal matrix, and  $\mathbf{V}$  is an  $n \times n$  orthogonal matrix.

8 By default, svd produces a 'thin' decomposition only containing the  $k$  singular values for a rank  $k$  matrix  $\mathbf{A}$ . In this case,  $\mathbf{U}$  is  $n \times k$  and  $\mathbf{V}$  is  $k \times n$ . We can call svd(A, full=true) to obtain a full singular value decomposition.

# C.8 Gaussian Distribution

The probability density function for a univariate Gaussian, also called the normal distribution, is:

$$
\mathcal {N} (x \mid \mu , \nu) = \frac{1}{\sqrt{2 \pi \nu}} e^{- \frac{(x - \mu) ^{2}}{2 \nu}} \tag {C.39}
$$

where  $\mu$  is the mean and  $\nu$  is the variance. The standard deviation is  $\sigma = \sqrt{\nu}$ . This distribution is plotted in figure C.6.

The cumulative distribution function of a distribution maps  $x$  to the probability that drawing a value from that distribution will produce a value less than or equal to  $x$ . For a univariate Gaussian, the cumulative distribution function is given by

$$
\Phi (x) \equiv \frac{1}{2} + \frac{1}{2} \operatorname{erf} \left(\frac{x - \mu}{\sigma \sqrt{2}}\right) \tag {C.40}
$$

where erf is the error function:

$$
\operatorname{erf} (x) \equiv \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{- \tau^{2}} d \tau \tag {C.41}
$$

# C.9 Gaussian Quadrature

Gaussian quadrature is a technique for approximating integrals using a weighted sum of function evaluations. $^{10}$  The general form of the approximation is

$$
\int_{a}^{b} p (x) f (x) d x \approx \sum_{i = 1}^{m} w_{i} f \left(x_{i}\right) \tag {C.42}
$$

where  $p(x)$  is a known nonnegative weight function<sup>11</sup> over the finite or infinite interval  $[a, b]$ .

An  $m$ -point quadrature rule is a unique choice of points  $x_{i} \in (a,b)$  and weights  $w_{i} > 0$  for  $i \in \{1,\dots,m\}$  that define a Gaussian quadrature approximation such that any polynomial of degree  $2m - 1$  or less is integrated exactly over  $[a,b]$  with the given weight function.

9 The multivariate Gaussian is discussed in chapter 8 and chapter 18. The univariate Gaussian is used throughout.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-10/31599c98-0508-4426-95fa-1cc50cbc03bb/61614047c24b087afa19f88a237e496eb2654ff534bad832fcb30602b6399cef.jpg)  
Figure C.6. A univariate Gaussian distribution,  $\mathcal{N}(\mu ,\nu)$

10 For a detailed overview of Gaussian quadrature, see J. Stoer and R. Bulirsch, Introduction to Numerical Analysis, 3rd ed. Springer, 2002.

The weight function is often a probability density function in practice.

Given a domain and a weight function, we can compute a class of orthogonal polynomials. We will use  $b_{i}(x)$  to denote an orthogonal polynomial<sup>12</sup> of degree  $i$ . Any polynomial of degree  $m$  can be represented as a linear combination of the orthogonal polynomials up to degree  $m$ . We form a quadrature rule by selecting  $m$  points  $x_{i}$  to be the zeros of the orthogonal polynomial  $p_{m}$  and obtain the weights by solving the system of equations:

$$
\sum_{i = 1}^{m} b_{k} \left(x_{i}\right) w_{i} = \left\{ \begin{array}{l l} \int_{a}^{b} p (x) b_{0} (x) ^{2} d x & \text{for} k = 0 \\ 0 & \text{for} k = 1, \dots , m - 1 \end{array} \right. \tag {C.43}
$$

12 Orthogonal polynomials are covered in chapter 21.

Consider the Legendre polynomials for integration over  $[-1, 1]$  with the weight function  $p(x) = 1$ . Suppose our function of interest is well approximated by a fifth degree polynomial. We construct a 3-point quadrature rule, which produces exact results for polynomials up to degree 5.

The Legendre polynomial of degree 3 is  $\mathrm{Le}_3(x) = \frac{5}{2} x^3 - \frac{3}{2} x$ , which has roots at  $x_1 = -\sqrt{3/5}$ ,  $x_2 = 0$ , and  $x_3 = \sqrt{3/5}$ . The Legendre polynomials of lesser degree are  $\mathrm{Le}_0(x) = 1$ ,  $\mathrm{Le}_1(x) = x$ , and  $\mathrm{Le}_2(x) = \frac{3}{2} x^2 - \frac{1}{2}$ . The weights are obtained by solving the system of equations:

$$
\left[ \begin{array}{c c c} \mathrm{Le}_{0} (- \sqrt{3 / 5}) & \mathrm{Le}_{0} (0) & \mathrm{Le}_{0} (\sqrt{3 / 5}) \\ \mathrm{Le}_{1} (- \sqrt{3 / 5}) & \mathrm{Le}_{1} (0) & \mathrm{Le}_{1} (\sqrt{3 / 5}) \\ \mathrm{Le}_{2} (- \sqrt{3 / 5}) & \mathrm{Le}_{2} (0) & \mathrm{Le}_{2} (\sqrt{3 / 5}) \end{array} \right] \left[ \begin{array}{c} w_{1} \\ w_{2} \\ w_{3} \end{array} \right] = \left[ \begin{array}{c} \int_{- 1}^{1} \mathrm{Le}_{0} (x) ^{2}   d x \\ 0 \\ 0 \end{array} \right]
$$

$$
\left[ \begin{array}{c c c} 1 & 1 & 1 \\ - \sqrt{3 / 5} & 0 & \sqrt{3 / 5} \\ 4 / 10 & - 1 / 2 & 4 / 10 \end{array} \right] \left[ \begin{array}{c} w_{1} \\ w_{2} \\ w_{3} \end{array} \right] = \left[ \begin{array}{c} 2 \\ 0 \\ 0 \end{array} \right]
$$

which yields  $w_{1} = w_{3} = 5 / 9$  and  $w_{2} = 8 / 9$ .

Consider integrating the 5th degree polynomial  $f(x) = x^{5} - 2x^{4} + 3x^{3} + 5x^{2} - x + 4$ . The exact value is  $\int_{-1}^{1} p(x)f(x) \, dx = 158 / 15 \approx 10.533$ . The quadrature rule produces the same value:

$$
\sum_{i = 1}^{3} w_{i} f (x_{i}) = \frac{5}{9} f \left(- \sqrt{\frac{3}{5}}\right) + \frac{8}{9} f (0) + \frac{5}{9} f \left(\sqrt{\frac{3}{5}}\right) \approx 10. 53 3.
$$

Example C.3. Obtaining a 3-term quadrature rule for exactly integrating polynomials up to degree 5.

Gauss solved equation (C.43) for the interval  $[-1, 1]$  and the weighting function  $p(x) = 1$ . The orthogonal polynomials for this case are the Legendre polynomials. Algorithm C.1 implements Gaussian quadrature for Legendre polynomials and example C.3 works out a quadrature rule for integration over  $[-1, 1]$ .

We can transform any integral over the bounded interval  $[a, b]$  to an integral over  $[-1, 1]$  using the transformation

$$
\int_{a}^{b} f (x) d x = \frac{b - a}{2} \int_{- 1}^{1} f \left(\frac{b - a}{2} x + \frac{a + b}{2}\right) d x \tag {C.44}
$$

Quadrature rules can thus be precalculated for the Legendre polynomials and then applied to integration over any finite interval. [13] Example C.4 applies such a transformation and algorithm C.2 implements integral transformations in a Gaussian quadrature method for finite domains.

Consider integrating  $f(x) = x^{5} - 2x^{4} + 3x^{3} + 5x^{2} - x + 4$  over  $[-3,5]$ . We can transform this into an integration over  $[-1,1]$  using equation (C.44):

$$
\int_{- 3}^{5} f (x) d x = \frac{5 + 3}{2} \int_{- 1}^{1} f \left(\frac{5 + 3}{2} x + \frac{5 - 3}{2}\right) d x = 4 \int_{- 1}^{1} f (4 x + 1) d x
$$

We use the 3-term quadrature rule obtained in example C.3 to integrate  $g(x) = 4f(4x + 1) = 4096x^5 + 3072x^4 + 1280y^3 + 768y^2 + 240y + 40$  over  $[-1,1]$ :

$$
\int_{- 1}^{1} p (x) g (x) d x = \frac{5}{9} g (- \sqrt{3 / 5}) + \frac{8}{9} g (0) + \frac{5}{9} g (\sqrt{3 / 5}) = 18 20. 8
$$

13 Similar techniques can be applied to integration over infinite intervals, such as  $[0,\infty)$  using the Laguerre polynomials and  $(-\infty ,\infty)$  using the Hermite polynomials.

Example C.4. Integrals over finite regions can be transformed into integrals over  $[-1,1]$  and solved with quadrature rules for Legendre polynomials.

```julia
struct Quadrule
ws # weights
xs # nodes
end
function quadrule_Xegendre(m)
bs = [legendre(i) for i in 1 : m+1]
xs = roots(bs[end])
A = [bs[k](xs[i]) for k in 1 : m, i in 1 : m]
b = zeros(m)
b[1] = 2
ws = A\b
return Quadrule(ws, xs)
end
```

Algorithm C.1. A method for constructing m-point Legendre quadrature rules over  $[-1,1]$ . The resulting type contains both the nodes xs and the weights ws.

```txt
quadint(f, quadrule) = sum(w*f(x) for (w,x) in zip(quadrule.ws, quadrule.xls))  
function quadint(f, quadrule, a, b)  
 $\alpha = \frac{(b - a)}{2}$ $\beta = \frac{(a + b)}{2}$ $g = x \rightarrow \alpha * f(\alpha * x + \beta)$   
return quadint(g, quadrule)  
end
```

Algorithm C.2. The function quadint for integrating a univariate function  $f$  with a given quadrature rule quadrule over the finite domain  $[a, b]$ .

# References

1. A. Ahmadi-Javid, "Entropic Value-At-Risk: A New Coherent Risk Measure," Journal of Optimization Theory and Applications, vol. 155, no. 3, pp. 1105-1123, 2011 (cit. on p. 430).  
2. N. M. Alexandrov and M. Y. Hussaini, eds., Multidisciplinary Design Optimization: State of the Art. SIAM, 1997 (cit. on p. 509).  
3. S. Amari, "Natural Gradient Works Efficiently in Learning," Neural Computation, vol. 10, no. 2, pp. 251-276, 1998 (cit. on p. 93).  
4. Aristotle, Metaphysics, trans. by W. D. Ross. 350 BCE, Book I, Part 5 (cit. on p. 2).  
5. L. Armijo, "Minimization of Functions Having Lipschitz Continuous First Partial Derivatives," Pacific Journal of Mathematics, vol. 16, no. 1, pp. 1-3, 1966 (cit. on p. 65).  
6. J. Arora, Introduction to Optimum Design, 4th ed. Academic Press, 2016 (cit. on p. 4).  
7. R.K. Arora, Optimization: Algorithms and Applications. Chapman and Hall/CRC, 2015 (cit. on p. 6).  
8. T.W. Athan and P.Y. Papalambros, "A Note on Weighted Criteria Methods for Compromise Solutions in Multi-Objective Optimization," Engineering Optimization, vol. 27, no. 2, pp. 155-176, 1996 (cit. on p. 327).  
9. C. Audet and J. E. Dennis Jr., "Mesh Adaptive Direct Search Algorithms for Constrained Optimization," SIAM Journal on Optimization, vol. 17, no. 1, pp. 188-217, 2006 (cit. on pp. 113, 132).  
10. D. A. Bader, W. E. Hart, and C. A. Phillips, "Parallel Algorithm Design for Branch and Bound," in Tutorials on Emerging Methodologies and Applications in Operations Research, H. J. Greenberg, ed., Kluwer Academic Press, 2004 (cit. on p. 465).  
11. W.W.R. Ball, Mathematical Recreations and Essays. Macmillan, 1892 (cit. on p. 507).  
12. D. Barber, Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012 (cit. on p. 496).

13. A.G. Baydin, R. Cornish, D.M. Rubio, M. Schmidt, and F. Wood, "Online Learning Rate Adaptation with Hypergradient Descent," in International Conference on Learning Representations (ICLR), 2018 (cit. on p. 84).  
14. A. D. Belegundu and T. R. Chandrupatla, Optimization Concepts and Applications in Engineering, 2nd ed. Cambridge University Press, 2011 (cit. on pp. 6, 306).  
15. R. Bellman, "On the Theory of Dynamic Programming," Proceedings of the National Academy of Sciences of the United States of America, vol. 38, no. 8, pp. 716-719, 1952 (cit. on p. 3).  
16. R. Bellman, Eye of the Hurricane: An Autobiography. World Scientific, 1984 (cit. on p. 468).  
17. H. Benaroya and S. M. Han, Probability Models in Engineering and Science. Taylor & Francis, 2005 (cit. on p. 438).  
18. F. Berkenkamp, A. P. Schoellig, and A. Krause, "Safe Controller Optimization for Quadrotors with Gaussian Processes," in IEEE International Conference on Robotics and Automation (ICRA), 2016 (cit. on p. 411).  
19. D. P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods. Athena Scientific, 1996 (cit. on p. 191).  
20. M. Besançon, T. Papamarkou, D. Anthoff, A. Arslan, S. Byrne, D. Lin, and J. Pearson, "Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem," Journal of Statistical Software, vol. 98, no. 16, pp. 1-30, 2021 (cit. on p. 158).  
21. H.-G. Beyer and B. Sendhoff, "Robust Optimization—A Comprehensive Survey," Computer Methods in Applied Mechanics and Engineering, vol. 196, no. 33, pp. 3190-3218, 2007 (cit. on p. 421).  
22. C.M. Bishop and H. Bishop, Deep Learning: Foundations and Concepts. Springer, 2024 (cit. on p. 4).  
23. T. L. Booth and R. A. Thompson, "Applying Probability Measures to Abstract Languages," IEEE Transactions on Computers, vol. C-22, no. 5, pp. 442-450, 1973 (cit. on p. 495).  
24. C. Boutilier, R. Patrascu, P. Poupart, and D. Schuurmans, "Constraint-Based Optimization and Utility Elicitation Using the Minimax Decision Criterion," Artificial Intelligence, vol. 170, no. 8-9, pp. 686-713, 2006 (cit. on p. 337).  
25. G.E. P. Box, W.G. Hunter, and J.S. Hunter, Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building, 2nd ed. Wiley, 2005 (cit. on pp. 343, 421).  
26. S. Boyd, S.-J. Kim, L. Vandenberghe, and A. Hassibi, "A Tutorial on Geometric Programming," Optimization and Engineering, vol. 8, pp. 67-127, 2007 (cit. on p. 239).