---
title: A Brief Introduction to Stochastic Calculus
parent_directory: I. Foundations/Mathematics
formatted: 2025-12-21 10:50:00 AM
formatter_model: "kimi-k2-turbo"
cli-tool: "claude-code"
primary_tags:
  - stochastic calculus
  - ito lemma
  - brownian motion
secondary_tags:
  - quadratic variation
  - stochastic integrals
  - martingales
  - stochastic differential equations
  - black-scholes formula
  - mathematical finance
cssclasses: academia
---

# A Brief Introduction to Stochastic Calculus

These notes provide a very brief introduction to stochastic calculus, the branch of mathematics that is most identified with financial engineering and mathematical finance. We will ignore most of the technical details and take an "engineering" approach to the subject. We will only introduce the concepts that are necessary for deriving the Black-Scholes formula later in the course. These concepts include quadratic variation, stochastic integrals and stochastic differential equations. We will of course also introduce Itô's Lemma, probably the most important result in stochastic calculus.

# 1 Martingales, Brownian Motion and Quadratic Variation

We make the following assumptions throughout.

- There is a probability triple  $(\Omega, \mathcal{F}, P)$  where

-  $P$  is the "true" or physical probability measure  
-  $\Omega$  is the universe of possible outcomes. We use  $\omega \in \Omega$  to represent a generic outcome, typically a sample path of a stochastic process.  
- the set  $\mathcal{F}$  represents the set of possible events where an event is a subset of  $\Omega$ .

- There is also a filtration,  $\{\mathcal{F}_t\}_{t \geq 0}$ , that models the evolution of information through time. So for example, if it is known by time  $t$  whether or not an event,  $E$ , has occurred, then we have  $E \in \mathcal{F}_t$ . If we are working with a finite horizon,  $[0, T]$ , then we can take  $\mathcal{F} = \mathcal{F}_T$ .  
- We also say that a stochastic process,  $X_{t}$ , is  $\mathcal{F}_t$ -adapted if the value of  $X_{t}$  is known at time  $t$  when the information represented by  $\mathcal{F}_t$  is known. All the processes we consider will be  $\mathcal{F}_t$ -adapted so we will not bother to state this in the sequel.  
- In the continuous-time models that we will study, it will be understood that the filtration  $\{\mathcal{F}_t\}_{t \geq 0}$  will be the filtration generated by the stochastic processes (usually a Brownian motion,  $W_t$ ) that are specified in the model description.

## 1.1 Martingales and Brownian Motion

Definition 1 A stochastic process,  $\{W_t:0\leq t\leq \infty \}$ , is a standard Brownian motion if

1.  $W_{0} = 0$  
2. It has continuous sample paths  
3. It has independent, stationary increments.  
4.  $W_{t}\sim \mathcal{N}(0,t)$

Definition 2 An  $n$ -dimensional process,  $W_{t} = (W_{t}^{(1)},\dots ,W_{t}^{(n)})$ , is a standard  $n$ -dimensional Brownian motion if each  $W_{t}^{(i)}$  is a standard Brownian motion and the  $W_{t}^{(i)}$ 's are independent of each other.

Definition 3 A stochastic process,  $\{X_{t}:0\leq t\leq \infty \}$ , is a martingale with respect to the filtration,  $\mathcal{F}_t$ , and probability measure,  $P$ , if

1.  $\operatorname{E}^P [|X_t|] < \infty$  for all  $t \geq 0$  
2.  $\operatorname{E}^P[X_{t+s}|\mathcal{F}_t] = X_t$  for all  $t, s \geq 0$ .

**Example 1 (Brownian martingales)**

Let  $W_{t}$  be a Brownian motion. Then  $W_{t}$ ,  $W_{t}^{2} - t$  and  $\exp \left( \theta W_{t} - \theta^{2} t / 2 \right)$  are all martingales.

The latter martingale is an example of an exponential martingale. Exponential martingales are of particular significance since they are positive and may be used to define new probability measures.

**Exercise 1 (Conditional expectations as martingales)** Let  $Z$  be a random variable and set  $X_{t} \coloneqq \operatorname{E}[Z|\mathcal{F}_{t}]$ . Show that  $X_{t}$  is a martingale.

## 1.2 Quadratic Variation

Consider a partition of the time interval,  $[0,T]$  given by

$$
0 = t_{0} < t_{1} < t_{2} < \dots < t_{n} = T.
$$

Let  $X_{t}$  be a Brownian motion and consider the sum of squared changes

$$
Q_{n} (T) := \sum_{i = 1}^{n} \left[ \Delta X_{t_{i}} \right]^{2} \tag{1}
$$

where  $\Delta X_{t_i} := X_{t_i} - X_{t_{i-1}}$ .

Definition 4 (Quadratic Variation) The quadratic variation of a stochastic process,  $X_{t}$ , is equal to the limit of  $Q_{n}(T)$  as  $\Delta t \coloneqq \max_{i}(t_{i} - t_{i - 1}) \to 0$ .

Theorem 1 The quadratic variation of a Brownian motion is equal to  $T$  with probability 1.

The functions with which you are normally familiar, e.g. continuous differentiable functions, have quadratic variation equal to zero. Note that any continuous stochastic process or function<sup>1</sup> that has non-zero quadratic variation must have infinite total variation where the total variation of a process,  $X_{t}$ , on  $[0,T]$  is defined as

$$
\text{TotalVariation} := \lim_{\Delta t \rightarrow 0} \sum_{i = 1}^{n} | X_{t_{k}} - X_{t_{k - 1}} |.
$$

This follows by observing that

$$
\sum_{i = 1}^{n} \left(X_{t_{k}} - X_{t_{k - 1}}\right)^{2} \leq \left(\sum_{i = 1}^{n} \left| X_{t_{k}} - X_{t_{k - 1}} \right|\right) \max_{1 \leq k \leq n} \left| X_{t_{k}} - X_{t_{k - 1}} \right|. \tag{2}
$$

If we now let  $n \to \infty$  in (2) then the continuity of  $X_{t}$  implies the impossibility of the process having finite total variation and non-zero quadratic variation. Theorem 1 therefore implies that the total variation of a Brownian motion is infinite. We have the following important result which proves very useful if we need to price options when there are multiple underlying Brownian motions, as is the case with quanto options for example.

Theorem 2 (Levy's Theorem) A continuous martingale is a Brownian motion if and only if its quadratic variation over each interval  $[0, t]$  is equal to  $t$ .

# 2 Stochastic Integrals

We now discuss the concept of a stochastic integral, ignoring the various technical conditions that are required to make our definitions rigorous. In this section, we write  $X_{t}(\omega)$  instead of the usual  $X_{t}$  to emphasize that the quantities in question are stochastic.

Definition 5 A stopping time of the filtration  $\mathcal{F}_t$  is a random time,  $\tau$ , such that the event  $\{\tau \leq t\} \in \mathcal{F}_t$  for all  $t > 0$ .

In non-mathematical terms, we see that a stopping time is a random time whose value is part of the information accumulated by that time.

Definition 6 We say a process,  $h_t(\omega)$ , is elementary if it is piece-wise constant so that there exists a sequence of stopping times  $0 = t_0 < t_1 < \ldots < t_n = T$  and a set of  $\mathcal{F}_{t_i}$ -measurable functions,  $e_i(\omega)$ , such that

$$
h_{t} (\omega) = \sum_{i} e_{i} (\omega) I_{[ t_{i}, t_{i + 1})} (t)
$$

where  $I_{[t_i,t_{i + 1})}(t) = 1$  if  $t\in [t_i,t_{i + 1})$  and 0 otherwise.

Definition 7 The stochastic integral of an elementary function,  $h_t(\omega)$ , with respect to a Brownian motion,  $W_t$ , is defined as

$$
\int_{0}^{T} h_{t} (\omega) d W_{t} (\omega) := \sum_{i = 0}^{n - 1} e_{i} (\omega) \left(W_{t_{i + 1}} (\omega) - W_{t_{i}} (\omega)\right). \tag{3}
$$

Note that our definition of an elementary function assumes that the function,  $h_t(\omega)$ , is evaluated at the left-hand point of the interval in which  $t$  falls. This is a key component in the definition of the stochastic integral: without it the results below would no longer hold. Moreover, defining the stochastic integral in this way makes the resulting theory suitable for financial applications. In particular, if we interpret  $h_t(\omega)$  as a trading strategy and the stochastic integral as the gains or losses from this trading strategy, then evaluating  $h_t(\omega)$  at the left-hand point is equivalent to imposing the non-anticipativity of the trading strategy, a property that we always wish to impose.

For a more general process,  $X_{t}(\omega)$ , we have

$$
\int_{0}^{T} X_{t} (\omega) d W_{t} (\omega) := \lim_{n \to \infty} \int_{0}^{T} X_{t}^{(n)} (\omega) d W_{t} (\omega)
$$

where  $X_{t}^{(n)}$  is a sequence of elementary processes that converges (in an appropriate manner) to  $X_{t}$ .

**Example 2** We want to compute  $\int_0^T W_t dW_t$ . Towards this end, let  $0 = t_0^n < t_1^n < t_2^n < \ldots < t_n^n = T$  be a partition of  $[0,T]$  and define

$$
X_{t}^{n} := \sum_{i = 0}^{n - 1} W_{t_{i}^{n}} I_{[ t_{i}^{n}, t_{i + 1}^{n})} (t)
$$

where  $I_{[t_i^n, t_{i+1}^n)}(t) = 1$  if  $t \in [t_i^n, t_{i+1}^n)$  and is 0 otherwise. Then  $X_t^n$  is an adapted elementary process and, by continuity of Brownian motion, satisfies  $\lim_{n \to \infty} X_t^n = W_t$  almost surely as  $\max_i |t_{i+1}^n - t_i^n| \to 0$ . The

stochastic integral of  $X_{t}^{n}$  is given by

$$
\begin{array}{l} \int_{0}^{T} X_{t}^{n} d W_{t} = \sum_{i = 0}^{n - 1} W_{t_{i}^{n}} \left(W_{t_{i + 1}^{n}} - W_{t_{i}^{n}}\right) \\ = \frac{1}{2} \sum_{i = 0}^{n - 1} \left(W_{t_{i + 1}^{n}}^{2} - W_{t_{i}^{n}}^{2} - \left(W_{t_{i + 1}^{n}} - W_{t_{i}^{n}}\right)^{2}\right) \\ = \frac{1}{2} W_{T}^{2} - \frac{1}{2} W_{0}^{2} - \frac{1}{2} \sum_{i = 0}^{n - 1} \left(W_{t_{i + 1}^{n}} - W_{t_{i}^{n}}\right)^{2}. \tag{4} \\ \end{array}
$$

By Theorem 1 the sum on the right-hand-side of (4) converges in probability to  $T$  as  $n \to \infty$ . And since  $W_0 = 0$  we obtain

$$
\int_{0}^{T} W_{t} d W_{t} = \lim_{n \to \infty} \int_{0}^{T} X_{t}^{n} d W_{t} = \frac{1}{2} W_{T}^{2} - \frac{1}{2} T.
$$

Note that we will generally evaluate stochastic integrals using Ito's Lemma (to be discussed later) without having to take limits of elementary processes as we did in Example 2.

Definition 8 We define the space  $L^2[0, T]$  to be the space of processes,  $X_t(\omega)$ , such that

$$
\operatorname{E} \left[ \int_{0}^{T} X_{t} (\omega)^{2} d t \right] < \infty .
$$

Theorem 3 (Itô's Isometry) For any  $X_{t}(\omega)\in L^{2}[0,T]$  we have

$$
\mathrm{E} \left[ \left(\int_{0}^{T} X_{t} (\omega) d W_{t} (\omega)\right)^{2} \right] = \mathrm{E} \left[ \int_{0}^{T} X_{t} (\omega)^{2} d t \right].
$$

Proof: (For the case where  $X_{t}$  is an elementary process)

Let  $X_{t} = \sum_{i}e_{i}(\omega)I_{[t_{i},t_{i + 1})}(t)$  be an elementary process where the  $e_i(\omega)'s$  and  $t_i$ 's are as defined in Definition 6. We therefore have  $\int_0^T X_t(\omega)dW_t(\omega)\coloneqq \sum_{i = 0}^{n - 1}e_i(\omega)\left(W_{t_{i + 1}}(\omega) - W_{t_i}(\omega)\right)$ . We then have

$$
\begin{array}{l} \mathrm{E} \left[ \left(\int_{0}^{T} X_{t} (\omega) d W_{t} (\omega)\right)^{2} \right] = \mathrm{E} \left[ \left(\sum_{i = 0}^{n - 1} e_{i} (\omega) \left(W_{t_{i + 1}} (\omega) - W_{t_{i}} (\omega)\right)\right)^{2} \right] \\ = \sum_{i = 0}^{n - 1} \mathrm{E} \left[ e_{i}^{2} (\omega) \left(W_{t_{i + 1}} (\omega) - W_{t_{i}} (\omega)\right)^{2} \right] \\ + 2 \sum_{0 \leq i < j \leq n - 1}^{n - 1} \operatorname{E} \left[ e_{i} e_{j} (\omega) \left(W_{t_{i + 1}} (\omega) - W_{t_{i}} (\omega)\right) \left(W_{t_{j + 1}} (\omega) - W_{t_{j}} (\omega)\right) \right] \\ = \sum_{i = 0}^{n - 1} \mathrm{E} \left[ e_{i}^{2} (\omega) \underbrace{\mathrm{E}_{t_{i}} \left[ \left(W_{t_{i + 1}} (\omega) - W_{t_{i}} (\omega)\right)^{2} \right]}_{= t_{i + 1} - t_{i}} \right] \\ + 2 \sum_{0 \leq i < j \leq n - 1}^{n - 1} \mathrm{E} \left[ e_{i} e_{j} (\omega) \left(W_{t_{i + 1}} (\omega) - W_{t_{i}} (\omega)\right) \underbrace{\mathrm{E}_{t_{j}} \left[ \left(W_{t_{j + 1}} (\omega) - W_{t_{j}} (\omega)\right) \right]}_{= 0} \right] \\ \end{array}
$$

$$
\begin{array}{l} = \mathrm{E} \left[ \sum_{i = 0}^{n - 1} e_{i}^{2} (\omega) (t_{i + 1} - t_{i}) \right] \\ = \mathrm{E} \left[ \int_{0}^{T} X_{t} (\omega)^{2} d t \right] \\ \end{array}
$$

which is what we had to show.

Theorem 4 (Martingale Property of Stochastic Integrals) The stochastic integral,  $Y_{t} \coloneqq \int_{0}^{t} X_{s}(\omega) dW_{s}(\omega)$ , is a martingale for any  $X_{t}(\omega) \in L^{2}[0,T]$ .

**Exercise 2** Check that  $\int_0^t X_s(\omega)dW_t(\omega)$  is indeed a martingale when  $X_{t}$  is an elementary process. (Hint: Follow the steps we took in our proof of Theorem 3.)

## 2.1 Stochastic Differential Equations

Definition 9 An  $n$ -dimensional Itô process,  $X_{t}$ , is a process that can be represented as

$$
X_{t} = X_{0} + \int_{0}^{t} a_{s} d s + \int_{0}^{t} b_{s} d W_{s} \tag{5}
$$

where  $W$  is an  $m$ -dimensional standard Brownian motion, and  $a$  and  $b$  are  $n$ -dimensional and  $n \times m$ -dimensional  $\mathcal{F}_t$ -adapted processes, respectively<sup>4</sup>.

We often use the notation

$$
d X_{t} = a_{t} d t + b_{t} d W_{t}
$$

as shorthand for (5). An  $n$ -dimensional stochastic differential equation (SDE) has the form

$$
d X_{t} = a \left(X_{t}, t\right) d t + b \left(X_{t}, t\right) d W_{t}; \quad X_{0} = x \tag{6}
$$

where as before,  $W_{t}$  is an  $m$ -dimensional standard Brownian motion, and  $a$  and  $b$  are  $n$ -dimensional and  $n \times m$ -dimensional adapted processes, respectively. Once again, (6) is shorthand for

$$
X_{t} = x + \int_{0}^{t} a \left(X_{s}, s\right) d t + \int_{0}^{t} b \left(X_{s}, t\right) d W_{s}. \tag{7}
$$

While we do not discuss the issue here, various conditions exist to guarantee existence and uniqueness of solutions to (7). A useful tool for solving SDE's is Ito's Lemma which we now discuss.

# 3 Itô's Lemma

Itô's Lemma is the most important result in stochastic calculus, the "sine qua non" of the field. We first state and give an outline proof of a basic form of the result.

**Theorem 5 (Itô's Lemma for 1-dimensional Brownian Motion)**

Let  $W_{t}$  be a Brownian motion on  $[0,T]$  and suppose  $f(x)$  is a twice continuously differentiable function on  $\mathbf{R}$ . Then for any  $t \leq T$  we have

$$
f \left(W_{t}\right) = f (0) + \frac{1}{2} \int_{0}^{t} f^{\prime \prime} \left(W_{s}\right) d s + \int_{0}^{t} f^{\prime} \left(W_{s}\right) d W_{s}. \tag{8}
$$

Proof: (Sketch) Let  $0 = t_0 < t_1 < t_2 < \ldots < t_n = t$  be a partition of  $[0, t]$ . Clearly

$$
f \left(W_{t}\right) = f (0) + \sum_{i = 0}^{n - 1} \left(f \left(W_{t_{i + 1}}\right) - f \left(W_{t_{i}}\right)\right). \tag{9}
$$

Taylor's Theorem implies

$$
f \left(W_{t_{i + 1}}\right) - f \left(W_{t_{i}}\right) = f^{\prime} \left(W_{t_{i}}\right) \left(W_{t_{i + 1}} - W_{t_{i}}\right) + \frac{1}{2} f^{\prime \prime} \left(\theta_{i}\right) \left(W_{t_{i + 1}} - W_{t_{i}}\right)^{2} \tag{10}
$$

for some  $\theta_{i}\in (W_{t_{i}},W_{t_{i + 1}})$ . Substituting (10) into (9) we obtain

$$
f \left(W_{t}\right) = f (0) + \sum_{i = 0}^{n - 1} f^{\prime} \left(W_{t_{i}}\right) \left(W_{t_{i + 1}} - W_{t_{i}}\right) + \frac{1}{2} \sum_{i = 0}^{n - 1} f^{\prime \prime} \left(\theta_{i}\right) \left(W_{t_{i + 1}} - W_{t_{i}}\right)^{2}. \tag{11}
$$

If we let  $\delta \coloneqq \max_{i}|t_{i + 1} - t_i|\to 0$  then it can be shown that the terms on the right-hand-side of (11) converge to the corresponding terms on the right-hand-side of (8) as desired. (This should not be surprising as we know the quadratic variation of Brownian motion on  $[0,t]$  is equal to  $t$ .)

A more general version of Itô's Lemma can be stated for Itô processes.

**Theorem 6 (Itô's Lemma for 1-dimensional Itô process)**

Let  $X_{t}$  be a 1-dimensional Itô process satisfying the SDE

$$
d X_{t} = \mu_{t} d t + \sigma_{t} d W_{t}.
$$

If  $f(t,x):[0,\infty)\times R\to R$  is a  $C^{1,2}$  function and  $Z_{t}\coloneqq f(t,X_{t})$  then

$$
\begin{array}{l} d Z_{t} = \frac{\partial f}{\partial t} (t, X_{t}) d t + \frac{\partial f}{\partial x} (t, X_{t}) d X_{t} + \frac{1}{2} \frac{\partial^{2} f}{\partial x^{2}} (t, X_{t}) (d X_{t})^{2} \\ = \left(\frac{\partial f}{\partial t} (t, X_{t}) + \frac{\partial f}{\partial x} (t, X_{t}) \mu_{t} + \frac{1}{2} \frac{\partial^{2} f}{\partial x^{2}} (t, X_{t}) \sigma_{t}^{2}\right) d t + \frac{\partial f}{\partial x} (t, X_{t}) \sigma_{t} d W_{t}. \\ \end{array}
$$

## 3.1 The "Box" Calculus

In the statement of Ito's Lemma, we implicitly assumed that  $(dX_{t})^{2} = \sigma_{t}^{2}dt$ . The "box calculus" is a series of simple rules for calculating such quantities. In particular, we use the rules

$$
d t \times d t = d t \times d W_{t} = 0 \quad \text{and}
$$

$$
d W_{t} \times d W_{t} = d t
$$

when determining quantities such as  $(dX_{t})^{2}$  in the statement of Itô's Lemma above. Note that these rules are consistent with Theorem 1. When we have two correlated Brownian motions,  $W_{t}^{(1)}$  and  $W_{t}^{(2)}$ , with correlation coefficient,  $\rho_{t}$ , then we easily obtain that  $dW_{t}^{(1)} \times dW_{t}^{(2)} = \rho_{t} dt$ . We use the box calculus for computing the quadratic variation of Itô processes.

**Exercise 3** Let  $W_{t}^{(1)}$  and  $W_{t}^{(2)}$  be two independent Brownian motions. Use Levy's Theorem to show that

$$
W_{t} := \rho W_{t}^{(1)} + \sqrt{1 - \rho^{2}} W_{t}^{(2)}
$$

is also a Brownian motion for a given constant,  $\rho$ .

## 3.2 Some Examples

**Example 3** Suppose a stock price,  $S_{t}$ , satisfies the SDE

$$
d S_{t} = \mu_{t} S_{t} d t + \sigma_{t} S_{t} d W_{t}.
$$

Then we can use the substitution,  $Y_{t} = \log (S_{t})$  and Ito's Lemma applied to the function  $f(x) := \log (x)$  to obtain

$$
S_{t} = S_{0} \exp \left(\int_{0}^{t} \left(\mu_{s} - \sigma_{s}^{2} / 2\right) d s + \int_{0}^{t} \sigma_{s} d W_{s}\right). \tag{12}
$$

Note that  $S_{t}$  does not appear on the right-hand-side of (12) so that we have indeed solved the SDE. When  $\mu_{s} = \mu$  and  $\sigma_{s} = \sigma$  are constants we obtain

$$
S_{t} = S_{0} \exp \left(\left(\mu - \sigma^{2} / 2\right) t + \sigma d W_{t}\right) \tag{13}
$$

so that  $\log (S_t)\sim \mathsf{N}\left((\mu -\sigma^2 /2)t,\sigma^2 t\right)$

**Example 4 (Ornstein-Uhlenbeck Process)**

Let  $S_{t}$  be a security price and suppose  $X_{t} = \log (S_{t})$  satisfies the SDE

$$
d X_{t} = \left[ - \gamma \left(X_{t} - \mu t\right) + \mu \right] d t + \sigma d W_{t}.
$$

Then we can apply Itô's Lemma to  $Y_{t} \coloneqq \exp (\gamma t)X_{t}$  to obtain

$$
\begin{array}{l} d Y_{t} = \exp (\gamma t) d X_{t} + X_{t} d (\exp (\gamma t)) \\ = \exp (\gamma t) \left[ \left[ - \gamma \left(X_{t} - \mu t\right) + \mu \right] d t + \sigma d W_{t}\right] + X_{t} \gamma \exp (\gamma t) d t \\ = \exp (\gamma t) ([ \gamma \mu t + \mu ] d t + \sigma d W_{t}) \\ \end{array}
$$

so that

$$
Y_{t} = Y_{0} + \mu \int_{0}^{t} e^{\gamma s} (\gamma s + 1) d s + \sigma \int_{0}^{t} e^{\gamma s} d W_{s} \tag{14}
$$

or alternatively (after simplifying the Riemann integral in (14))

$$
X_{t} = X_{0} e^{- \gamma t} + \mu t + \sigma e^{- \gamma t} \int_{0}^{t} e^{\gamma s} d W_{s}. \tag{15}
$$

Once again, note that  $X_{t}$  does not appear on the right-hand-side of (15) so that we have indeed solved the SDE. We also obtain  $\operatorname{E}[X_t] = X_0 e^{-\gamma t} + \mu t$  and

$$
\begin{array}{l} \operatorname{Var} \left(X_{t}\right) = \operatorname{Var} \left(\sigma e^{- \gamma t} \int_{0}^{t} e^{\gamma s} d W_{s}\right) = \sigma^{2} e^{- 2 \gamma t} \operatorname{E} \left[ \left(\int_{0}^{t} e^{\gamma s} d W_{s}\right)^{2} \right] \\ = \sigma^{2} e^{- 2 \gamma t} \int_{0}^{t} e^{2 \gamma s} d s \quad (\text{byIto'sIsometry}) \\ = \frac{\sigma^{2}}{2 \gamma} \left(1 - e^{- 2 \gamma t}\right). \\ \end{array}
$$

These moments should be compared with the corresponding moments for  $\log (S_t)$  in the previous example.