
# Nonlinearity and Nonlinear Econometric Models in Finance

RUEY S. TSAY, PhD

H.G.B. Alexander Professor of Econometrics and Statistics,

University of Chicago Booth School of Business

Abstract: Many financial and economic data exhibit nonlinear characteristics. Prices of commodities such as crude oil often rise quickly but decline slowly. The monthly U.S. unemployment rate exhibits sharp increases followed by slow decreases. To model these characteristics in a satisfactory manner, one must employ nonlinear econometric models or use nonparametric statistical methods. For most applications, it suffices to employ simple nonlinear models. For example, the quarterly growth rate of the U.S. gross domestic product can be adequately described by the Markov switching or threshold autoregressive models. These models typically classify the state of the U.S. economy into two categories corresponding roughly to expansion and contraction.

In this entry, we study nonlinearity in financial data, discuss various nonlinear models available in the literature, and demonstrate application of nonlinear models in finance with real examples. The models discussed include bilinear models, threshold autoregressive models, smooth threshold autoregressive models, Markov switching models, and nonlinear additive autoregressive models. We also consider nonparametric methods and neural networks, and apply nonparametric methods to estimate interest models. To detect nonlinearity in financial data, we introduce various nonlinearity tests available in the literature and apply the tests to some financial series. Finally, we analyze the monthly U.S. unemployment rate and compare out-of-sample prediction of nonlinear models with linear ones via several criteria.

# STUDY OF NONLINEARITY IN ECONOMETRICS AND STATISTICS

Assume, for simplicity, a univariate time series  $x_{t}$  is observed at equally spaced time points. We denote the observations by  $\{x_{t}|t = 1,\dots ,T\}$  where  $T$  is the sample size. A purely stochastic time series  $x_{t}$  is said to be linear if it can be written as

$$ x_{t} = \mu + \sum_{i = 0}^{\infty} \psi_{i} a_{t - i} \tag {1}
$$ where  $\mu$  is a constant,  $\psi_{i}$  are real numbers with  $\psi_0 = 1$ , and  $\{a_t\}$  is a sequence of independent and identically distributed (IID) random variables with a well-defined distribution function. We assume that the distribution of  $a_{t}$  is


continuous and  $E(a_{t}) = 0$ . In many cases, we further assume that  $\mathrm{Var}(a_t) = \sigma_a^2$  or, even stronger, that  $a_{t}$  is Gaussian. If  $\sigma_{a}^{2}\sum_{i = 1}^{\infty}\psi_{i}^{2} < \infty$ , then  $X_{t}$  is weakly stationary (i.e., the first two moments of  $x_{t}$  are time-invariant). The well-known autoregressive moving-average (ARMA) process of Box et al. (2008) is linear because it has an moving-average (MA) representation in equation (1). Any stochastic process that does not satisfy the condition of equation (1) is said to be nonlinear. The prior definition of nonlinearity is for purely stochastic time series. One may extend the definition by allowing the mean of  $x_{t}$  to be a linear function of some exogenous variables, including the time index and some periodic functions. But such a mean function can be handled easily by using a regression model with time series errors discussed in Tsay (2010, Chapter 2), and we shall not consider the extension here. Mathematically, a purely stochastic time series model for  $x_{t}$  is a function of an IID sequence consisting of the current and past shocks—that is,

$$ x_{t} = f \left(a_{t}, a_{t - i}, \dots\right) \tag {2}
$$

The linear model in equation (1) says that  $f(.)$  is a linear function of its arguments. Any nonlinearity in  $f(.)$  results in a nonlinear model. The general nonlinear model in equation (2) is too vague to be useful in practice. Further assumptions are needed to make the model applicable.

To put nonlinear models available in the literature in a proper perspective, we write the model of  $x_{t}$  in terms of its conditional moments. Let  $F_{t - 1}$  be the  $\sigma$ -field generated by available information at time  $t - 1$  (inclusive). Typically,  $F_{t - 1}$  denotes the collection of linear combinations of elements in  $\{x_{t - 1}, x_{t - 2}, \ldots\}$  and  $\{a_{t - 1}, a_{t - 2}, \ldots\}$ . The conditional mean and variance of  $x_{t}$  given  $F_{t - 1}$  are

$$
\mu_{t} = E \left(x_{t} \mid F_{t - i}\right) \equiv g \left(F_{t - i}\right) \tag {3}
$$

$$
\sigma_{t}^{2} = \operatorname{Va r} \left(x_{t} \mid F_{t - 1}\right) \equiv h \left(F_{t - 1}\right)
$$ where  $g(.)$  and  $h(.)$  are well-defined functions with  $h(.) > 0$ . Thus, we restrict the model to


$$ x_{t} = g \left(F_{t - 1}\right) + \sqrt{h \left(F_{t - 1}\right)} e_{t}
$$ where  $e_{t} = a_{t} / \sigma_{t}$  is a standardized shock (or innovation). For the linear series  $x_{t}$  in equation (1),  $g(.)$  is a linear function of elements of  $F_{t - 1}$  and  $h(.) = \sigma_a^2$ . The development of nonlinear models involves making extensions of the two equations in equation (3). If  $g(.)$  is nonlinear,  $x_{t}$  is said to be nonlinear in mean. If  $h(.)$  is time-variant, then  $x_{t}$  is nonlinear in variance. The conditional heteroscedastic models, for example, the GARCH model of Bollerslev (1986), are nonlinear in variance because their conditional variances  $\sigma_t^2$  evolve over time. Based on the well-known Wold decomposition, a weakly stationary and purely stochastic time series can be expressed as a linear function of uncorrelated shocks. For stationary volatility series, these shocks are uncorrelated, but dependent. The models discussed in this entry represent another extension to nonlinearity derived from modifying the conditional mean equation in equation (3).


Many nonlinear time series models have been proposed in the statistical literature, such as the bilinear models of Granger and Andersen (1978), the threshold autoregressive (TAR) model of Tong (1978), the state-dependent model of Priestley (1980), and the Markov switching model of Hamilton (1989). The basic idea underlying these nonlinear models is to let the conditional mean  $\mu_t$  evolve over time according to some simple parametric nonlinear function. Recently, a number of nonlinear models have been proposed by making use of advances in computing facilities and computational methods. Examples of such extensions include the nonlinear state-space modeling of Carlin, Polson, and Stoffer (1992), the functional-coefficient autoregressive model of Chen and Tsay (1993a), the nonlinear additive autoregressive model of Chen and Tsay (1993b), the multivariate adaptive regression spline of Lewis and Stevens (1991), and the generalized autoregressive score (GAS) model of Creal et al. (2010). The basic idea of these extensions is either using simulation methods to describe the evolution of the conditional distribution of  $x_{t}$  or using data-driven methods to explore the nonlinear characteristics of a series. Finally, nonparametric and semiparametric methods such as kernel regression and artificial neural networks have also been applied to explore the nonlinearity in a time series. We discuss some nonlinear models in this entry that are applicable to financial time series. The discussion includes some nonparametric and semiparametric methods.


Apart from the development of various nonlinear models, there is substantial interest in studying test statistics that can discriminate linear series from nonlinear ones. Both parametric and nonparametric tests are available. Most parametric tests employ either the Lagrange multiplier or likelihood ratio statistics. Nonparametric tests depend on either higher order spectra of  $x_{t}$  or the concept of dimension correlation developed for chaotic time series. We review some nonlinearity tests, discuss modeling and forecasting of nonlinear models, and provide an application of nonlinear models.

# NONLINEAR MODELS

Most nonlinear models developed in the statistical literature focus on the conditional mean equation in equation (3); see Priestley (1988) and Tong (1990) for summaries of nonlinear models. Our goal here is to introduce some nonlinear models that are useful in finance.

# Bilinear Model

The linear model in equation (1) is simply the first-order Taylor series expansion of the  $f(.)$  function in equation (2). As such, a natural extension to nonlinearity is to employ the second-order terms in the expansion to improve the approximation. This is the basic idea of bilinear models, which can be defined as

$$
\begin{array}{l} x_{t} = c + \sum_{\substack {i = 1 \\ m - s}}^{p} \phi_{i} x_{t - i} - \sum_{j = 1}^{q} \theta_{j} a_{t - j} \tag{4} \\ + \sum_{i = 1}^{m} \sum_{j = 1}^{s} \beta_{i j} x_{t - i} a_{t - j} + a_{t} \\ \end{array}
$$ where  $p, q, m$ , and  $s$  are nonnegative integers. This model was introduced by Granger and Andersen (1978) and has been widely investigated. Subba Rao and Gabr (1984) discuss some properties and applications of the model, and Liu and Brockwell (1988) study general bilinear models. Properties of bilinear models such as stationarity conditions are often derived by (a) putting the model in a state-space form and (b) using the state transition equation to express the state as a product of past innovations and random coefficient vectors. A special generalization of the bilinear model in equation (4) has conditional heteroscedasticity. For example, consider the model


$$ x_{t} = \mu + \sum_{i = 1}^{s} \beta_{i} a_{t - i} a_{t} + a_{t} \tag {5}
$$ where  $\{a_t\}$  is a white noise series. The first two conditional moments of  $x_{t}$  are


$$
\begin{array}{l} E \left(x_{t} \mid F_{t - 1}\right) = \mu \\ \mathrm{Va r} (x_{t} | F_{t - 1}) = \left(1 + \sum_{i = 1}^{s} \beta_{i} a_{t - i}\right)^{2} \sigma_{a}^{2} \\ \end{array}
$$ which confirm that the model has time-varying volatility.


Example 1. Consider the monthly simple returns of the CRSP equal-weighted index from January 1926 to December 2008 for 996 observations. Denote the series by  $R_{t}$ . The sample partial autocorrelation function (PACF) of  $R_{t}$  shows significant serial correlations at lags 1 and 3 so that an AR(3) model is used for the mean equation. The squared series of the AR(3) residuals suggests that the conditional heteroscedasticity might depend on lags 1, 3 and 8 of the residuals. Therefore, we employ the special bilinear model

$$
\begin{array}{l} R_{t} = \mu + \phi_{1} R_{t - 1} + \phi_{3} R_{t - 3} \\ + (1 + \beta_{1} a_{t - 1} + \beta_{3} a_{t - 3}) a_{t} \\ \end{array}
$$ for the series, where  $a_{t} = \beta_{0}\epsilon_{t}$  with  $\epsilon_t$  being an IID series with mean zero and variance 1. Note that lag 8 is omitted for simplicity. Assuming that the conditional distribution of  $a_{t}$  is normal, we use the conditional maximum


![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/8a30e9c46513b088396b0bf4f40743a9fbf3aeb8bf4d772458dcfdbe243293d9.jpg) likelihood method and obtain the fitted model
Figure 1 Time Plot of a Simulated 2-Regime TAR(1) Series


$$
\begin{array}{l} R_{t} = 0. 0 1 1 4 + 0. 1 6 7 R_{t - 1} - 0. 0 9 5 R_{t - 3} \\ + 0. 0 7 1 (1 + 0. 3 7 7 a_{t - 1} - 0. 6 4 6 a_{t - 3}) \epsilon_{t} \\ \end{array}
$$ where the standard errors of the parameters are, in the order of appearance, 0.0023, 0.032, 0.027, 0.002, 0.147, and 0.136, respectively. All estimates are significantly different from zero at the  $5\%$  level. Define


$$
\hat {\epsilon}_{t} = \frac{R_{t} - 0 . 0 1 1 4 - 0 . 1 6 7 R_{t - 1} + 0 . 0 9 5 R_{t - 3}}{0 . 0 7 1 (1 + 0 . 3 7 7 \hat {a}_{t - 1} - 0 . 6 4 6 \hat {a}_{t - 3})}
$$ where  $\hat{\epsilon}_t = 0$  for  $t\leq 3$  as the standardized residual series of the model. The sample autocorrelation function (ACF) of  $\hat{\epsilon}_t$  shows no significant serial correlations, but the series is not independent because the squared series  $\hat{\epsilon}_t^2$  has significant serial correlations. The validity of model (6) deserves further investigation. For comparison, we also consider an AR(3)-ARCH(3) model for the series and obtain


$$
\begin{array}{l} R_{t} = 0. 0 1 3 + 0. 2 2 3 R_{t - i} + 0. 0 0 6 R_{t - 2} \\ - 0. 0 1 3 R_{t - 3} + a_{t} \\ \end{array}
$$

$$
\begin{array}{l} \sigma_{t}^{2} = 0. 0 0 2 + 0. 1 8 5 a_{t - 1}^{2} + 0. 3 0 1 a_{t - 2}^{2} \tag {7} \\ + 0. 1 9 7 a_{t - 3}^{2} \\ \end{array}
$$ where all estimates but the coefficients of  $R_{t-2}$  and  $R_{t-3}$  are highly significant. The standardized residual series of the model shows no serial correlations, but the squared residuals show  $Q(10) = 19.78$  with a  $p$ -value of 0.031. Models (6) and (7) appear to be similar, but the latter seems


to fit the data better. Further study shows that an AR(1)-GARCH(1,1) model fits the data well.

# Threshold Autoregressive (TAR) Model

This model is motivated by several nonlinear characteristics commonly observed in practice such as asymmetry in declining and rising patterns of a process. It uses piecewise linear models to obtain a better approximation of the conditional mean equation. However, in contrast to the traditional piecewise linear model that allows for model changes to occur in the "time" space, the TAR model uses threshold space to improve linear approximation. Let us start with a simple 2-regime AR(1) model

$$ x_{t} = \left\{ \begin{array}{l l} - 1. 5 x_{t - 1} + a_{t} & \text{if } x_{t - 1} <   0 \\ 0. 5 x_{t - 1} + a_{t} & \text{if } x_{t - 1} \geq 0 \end{array} \right. \tag {8}
$$ where the  $a_{t}$  are IID  $N(0,1)$ . Here the threshold variable is  $x_{t - 1}$  and the threshold is 0.


Figure 1 shows the time plot of a simulated series of  $x_{t}$  with 200 observations. A horizontal line of zero is added to the plot, which illustrates several characteristics of TAR models. First, despite the coefficient -1.5 in the first regime, the process  $x_{t}$  is geometrically ergodic and stationary. In fact, the necessary and sufficient condition for model (8) to be geometrically ergodic is  $\phi_1^{(1)} < 1$ ,  $\phi_1^{(2)} < 1$  and  $\phi_1^{(1)}\phi_1^{(2)} < 1$ , where

$\phi_1^{(i)}$  is the AR coefficient of regime  $i$ ; see Petrucelli and Woolford (1984) and Chen and Tsay (1991).

Ergodicity is an important concept in time series analysis. For example, the statistical theory showing that the sample mean  $\bar{x} = (\sum_{t=1}^{T} x_{t}) / T$  of  $x_{t}$  converges to the mean of  $x_{t}$  is referred to as the ergodic theorem, which can be regarded as the counterpart of the central limit theory for the IID case. Second, the series exhibits an asymmetric increasing and decreasing pattern. If  $x_{t-1}$  is negative, then  $x_{t}$  tends to switch to a positive value due to the negative and explosive coefficient  $-1.5$ . Yet when  $x_{t-1}$  is positive, it tends to take multiple time periods for  $x_{t}$  to reduce to a negative value. Consequently, the time plot of  $x_{t}$  shows that regime 2 has more observations than regime 1, and the series contains large upward jumps when it becomes negative. The series is therefore not time-reversible. Third, the model contains no constant terms, but  $E(x_{t})$  is not zero. The sample mean of the particular realization is 0.61 with a standard deviation of 0.07. In general,  $E(x_{t})$  is a weighted average of the conditional means of the two regimes, which are nonzero. The weight for each regime is simply the probability that  $x_{t}$  is in that regime under its stationary distribution. It is also clear from the discussion that, for a TAR model to have zero mean, nonzero constant terms in some of the regimes are needed. This is very different from a stationary linear model for which a nonzero constant implies that the mean of  $x_{t}$  is not zero.

A time series  $x_{t}$  is said to follow a  $k$ -regime self-exciting TAR (SETAR) model with threshold variable  $x_{t - d}$  if it satisfies

$$
\begin{array}{l} x_{t} = \phi_{0}^{(j)} + \phi_{0}^{(j)} x_{t - 1} - \dots \phi_{p}^{(j)} x_{t - p} + a_{t}^{(j)} \tag {9} \\ i f \quad \gamma_{j - 1} \leq x_{t - d} <   \gamma_{j} \\ \end{array}
$$ where  $k$  and  $d$  are positive integers,  $j = 1,\dots ,$ $k,\gamma_{i}$  are real numbers such that  $-\infty = \gamma_0 < \gamma_1 < \dots < \gamma_{k - 1} < \gamma k = \infty$  the superscript  $(j)$  is used to signify the regime, and  $\{a_t^{(j)}\}$  are IID sequences with mean O and variance  $\sigma_j^2$  and are mutually independent for different  $j.$  The parameter  $d$  is referred to as the delay pa


rameter and  $\gamma_{j}$  are the thresholds. Here it is understood that the AR models are different for different regimes; otherwise, the number of regimes can be reduced. Equation (9) says that a SETAR model is a piecewise linear AR model in the threshold space. It is similar in spirit to the usual piecewise linear models in regression analysis, where model changes occur in the order in which observations are taken. The SETAR model is nonlinear provided that  $k > 1$ .

Properties of general SETAR models are hard to obtain, but some of them can be found in Tong (1990), Chan (1993), Chan and Tsay (1998), and the references therein. In recent years, there is increasing interest in TAR models and their applications; see, for instance, Hansen (1997), Tsay (1998), and Montgomery et al. (1998). Tsay (1989) proposed a testing and modeling procedure for univariate SETAR models. The model in equation (9) can be generalized by using a threshold variable  $z_{t}$  that is measurable with respect to  $F_{t-1}$  (i.e., a function of elements of  $F_{t-1}$ ). The main requirements are that  $z_{t}$  is stationary with a continuous distribution function over a compact subset of the real line and that  $z_{t-d}$  is known at time  $t$ . Such a generalized model is referred to as an open-loop TAR model.

Example 2. To demonstrate the application of TAR models, consider the U.S. monthly civilian unemployment rate, seasonally adjusted and measured in percentage, from January 1948 to March 2009 for 735 observations. The data are obtained from the Bureau of Labor Statistics, Department of Labor, and are shown in Figure 2. The plot shows two main characteristics of the data. First, there appears to be a slow but upward trend in the overall unemployment rate. Second, the unemployment rate tends to increase rapidly and decrease slowly. Thus, the series is not time-reversible and may not be unit-root stationary, either.

Because the sample autocorrelation function decays slowly, we employ the first differenced series  $y_{t} = (1 - B)u_{t}$  in the analysis, where  $u_{t}$  is the monthly unemployment rate. Using univariate ARIMA models, we obtain the model

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/663918f07f4d85169282458d5364eda8bd1401442e22c37a2368bde6bfd74017.jpg)
Figure 2 Time Plot of Monthly U.S. Civilian Unemployment Rate, Seasonally Adjusted, from January 1948 to March 2009


$$
\begin{array}{l} (1 - 1. 1 3 B + 0. 2 7 B^{2}) (1 - 0. 5 1 B^{1 2}) y t \\ = (1 - 1. 1 2 B + 0. 4 4 B^{2}) \left(1 - 0. 8 2 B^{1 2}\right) a_{t} \tag {10} \\ \end{array}
$$ where  $\hat{\sigma}_a = 0.187$  and all estimates but the AR(2) coefficient are statistically significant at the  $5\%$  level. The t-ratio of the estimate of AR(2) coefficient is  $-1.66$ . The residuals of model (10) give  $Q(12) = 12.3$  and  $Q(24) = 25.5$ , respectively. The corresponding  $p$ -values are 0.056 and 0.11, respectively, based on  $\chi^2$  distributions with 6 and 18 degrees of freedom. Thus, the fitted model adequately describes the serial dependence of the data. Note that the seasonal AR and MA coefficients are highly significant with standard error 0.049 and 0.035, respectively, even though the data were seasonally adjusted. The adequacy of seasonal adjustment deserves further study. Using model (10), we obtain the 1-step ahead forecast of 8.8 for the April 2009 unemployment rate, which is close to the actual data of 8.9.


To model nonlinearity in the data, we employ TAR models and obtain the model

$$ y_{t} = \left\{ \begin{array}{c c} 0. 0 8 3 y_{t - 2} + 0. 1 5 8 y_{t - 3} + 0. 1 1 8 y_{t - 4} \\ - 0. 1 8 0 y_{t - 1 2} + a_{1 t} & \text{if } y_{t - i} \leq 0. 1 \\ 0. 4 2 1 y_{t - 2} + 0. 2 3 9 y_{t - 3} - 0. 1 2 7 y_{t - 1 2} \\ + a_{2 t} & \text{if } y_{t - i} > 0. 1 \end{array} \right. \tag {11}
$$ where the standard errors of  $a_{it}$  are 0.180 and 0.217, respectively, the standard errors of the AR parameters in regime 1 are 0.046, 0.043, 0.042, and 0.037 whereas those of the AR parameters in regime 2 are 0.054, 0.057, and 0.075, respectively. The number of data points in regimes 1 and 2 are 460 and 262, respectively. The standardized residuals of model (11) only shows some minor serial correlation at lag 12. Based on the fitted TAR model, the dynamic dependence in the data appears to be stronger when the change in monthly unemployment rate is greater than  $0.1\%$ . This is understandable because a substantial increase in the unemployment rate is indicative of weakening in the U.S. economy, and policy makers might be more inclined to take action to help the economy, which in turn may affect the dynamics of the unemployment rate series. Consequently, model (11) is capable of describing the time-varying dynamics of the U.S. unemployment rate.


The MA representation of model (10) is

$$
\begin{array}{l} \psi (B) \approx 1 + 0. 0 1 B + 0. 1 8 B^{2} + 0. 2 0 B^{3} \\ + 0. 1 8 B^{4} + 0. 1 5 B^{5} + \dots . \\ \end{array}
$$

It is then not surprising to see that no  $y_{t - 1}$  term appears in model (11).

Threshold models can be used in finance to handle the leverage effect, that is, volatility responds differently to prior positive and negative returns. The models can also be used to study arbitrage trading in index futures and cash prices. See Tsay (2010, chap. 8) for discussions and demonstration. Here we focus on volatility modeling and introduce an alternative approach to parameterization of threshold GARCH (TGARCH) models. In some applications, this new general TGARCH model fares better than the model of Glosten et al. (1993).

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/5b659cc55ae052642cf80a905959fa3af1ebc9a813653a1fb6536a38fbcf341f.jpg)
Figure 3 Time Plot of the Daily Log Returns, in Percentages, for IBM Stock from January 2, 2001 to December 31, 2009


Example 3. Consider the daily log returns, in percentages and including dividends, of IBM stock from January 2, 2001 to December 31, 2009 for 2,263 observations. Figure 3 shows the time plot of the series. The volatility seems to be larger at the beginning and end of the data span. If GARCH models are entertained, we obtain the following GARCH(1,1) model for the series:

$$ r_{t} = 0. 0 5 8 + a_{t}, \quad a_{t} = \sigma_{t} \epsilon_{t} \tag {12}
$$

$$
\sigma_{t}^{2} = 0. 0 4 1 + 0. 0 9 3 a_{t - 1}^{2} + 0. 8 9 4 \sigma_{t - 1}^{2}
$$ where  $r_t$  is the log return,  $\{e_t\}$  is a Gaussian white noise sequence with mean zero and variance 1.0, the standard error of the constant term in the mean equation is 0.026, and those of the volatility equation are 0.012, 0.020, and 0.021, respectively. All estimates are statistically significant at the 5\% level. The Ljung-Box statistics of the standardized


residuals,  $\hat{\epsilon}_{t} = \hat{a}_{\mathrm{t}} / \hat{\sigma}_{\mathrm{t}}$  give  $Q(10) = 10.08(0.43)$  and  $Q(20) = 23.24(0.28)$ , where the number in parentheses denotes  $p$ -value obtained using the asymptotic  $X_{m}^{2}$  distribution. For the squared standardized residuals, we obtain  $Q(10) = 7.38(0.69)$  and  $Q(20) = 15.43(0.75)$ . The model is adequate in modeling the serial dependence and conditional heteroscedasticity of the data. But the unconditional mean for  $r_t$  of model (12) is 0.058, which is substantially larger than the sample mean 0.024, indicating that the model might be misspecified.

Next, we employ the TGARCH model of Glosten et al. (1993) and obtain

$$
\begin{array}{l} r_{t} = 0. 0 1 5 + a_{t}, \quad a_{t} = \sigma_{t} \epsilon_{t} \\ \sigma_{t}^{2} = 0. 0 3 2 + 0. 0 3 3 a_{t - 1}^{2} + 0. 0 9 1 N_{t - 1} a_{t - 1}^{2} \\ + 0. 9 1 1 \sigma_{t - 1}^{2} \tag {13} \\ \end{array}
$$ where  $N_{t - 1}$  is the indicator for negative  $a_{t - 1}$  such that  $N_{t - 1} = 1$  if  $a_{t - 1} < 0$  and  $= 0$  otherwise, the standard error of the parameter in the mean equation is 0.026, and those of the volatility equation are 0.005, 0.005, 0.006, and 0.008, respectively. All estimates except the constant term of the mean equation are highly significant. Let  $\tilde{a}_t$  be the standardized residuals of model (13). We have  $Q(10) = 9.81(0.46)$  and  $Q(20) = 22.17(0.33)$  for the  $\{\tilde{a}_t\}$  series and


$Q(10) = 22.12(0.01)$  and  $Q(20) = 31.15(0.05)$  for  $\{\tilde{a}_t^2\}$ . The model fails to describe the conditional heteroscedasticity of the data at the  $5\%$  level.

The idea of TAR models can be used to refine the prior TGARCH model by allowing for increased flexibility in modeling the asymmetric response in volatility. More specifically, we consider a TAR-GARCH(1,1) model for the series and use the constrained optimization method L-BFGS-B to perform estimation. The resulting model is

$$ r_{t} = 0. 0 2 3 + a_{t}, a t = \sigma_{t} \epsilon_{t}
$$

$$
\begin{array}{l} \sigma_{t}^{2} = 0. 0 8 6 + 0. 0 4 4 a_{t - 1}^{2} + 0. 8 1 5 \sigma_{t - 1}^{2} \\ + (- 0. 1 1 4 + 0. 0 5 2 a_{t - 1}^{2} + 0. 2 1 4 \sigma_{t - 1}^{2}) N_{t - 1} \tag {14} \\ \end{array}
$$ where all estimates are significant at the  $5\%$  level and  $N_{t - 1}$  is defined in equation (13). The estimate  $-0.114$  is only marginally significant because its standard error is 0.055. The coefficient of  $\sigma_{t - 1}^2$  is greater than 1 when  $a_{t - 1} < 0$ , but it is not significantly different from 1 in view of its standard error.


Let  $\hat{a}_{t}$  be the standardized residuals of model (14). We obtain  $Q(10) = 9.10(0.52)$  and  $Q(20) = 21.82(0.35)$  for  $\{\hat{a}_t\}$  and  $Q(10) = 19.80(0.03)$  and  $Q(20) = 27.41(0.12)$  for  $\{\hat{a}_t^2\}$ . Thus, model (14) is adequate in modeling the serial correlation and conditional heteroscedasticity of the daily log returns of IBM stock considered. The unconditional mean return of model (14) is 0.023, which is much closer to the sample mean 0.024 than those implied by models (12) and (13). Comparing the fitted TAR-GARCH and TGARCH models, we see that the asymmetric behavior in daily IBM stock volatility is much stronger than what is allowed in a TGARCH model. Specifically, the coefficient of  $\sigma_{t - 1}^2$  also depends on the sign of  $a_{t - 1}$ .

# Smooth Transition AR (STAR) Model

A criticism of the SETAR model is that its conditional mean equation is not continuous. The thresholds  $\{\gamma_j\}$  are the discontinuity points of the conditional mean function  $\mu_t$ . In response to this criticism, smooth TAR models have been proposed; see Chan and Tong (1986) and Teräsvirta (1994) and the references therein. A time series  $x_{t}$  follows a 2-regime  $\mathrm{STAR}(p)$  model if it satisfies


$$
\begin{array}{l} x_{t} = c_{0} + \sum_{i = 1}^{p} \phi_{0, i} x_{t - i} + F \left(\frac{x_{t - d} - \Delta}{s}\right) \tag {15} \\ \times \left(c_{1} + \sum_{i = 1}^{p} \phi_{1, i} x_{t - i}\right) + a_{t} \\ \end{array}
$$ where  $d$  is the delay parameter,  $\Delta$  and  $s$  are parameters representing the location and scale of model transition, and  $F(.)$  is a smooth transition function. In practice,  $F(.)$  often assumes one of three forms—namely, logistic, exponential, or a cumulative distribution function. From equation (15) and with  $0\leq F(.)\leq 1$ , the conditional mean of a STAR model is a weighted linear combination between the following two equations:


$$
\mu_{1 t} = c_{0} + \sum_{i = 1}^{p} \phi_{0, i} x_{t - i}
$$

$$
\mu_{2 t} = \left(c_{0} + c_{1}\right) + \sum_{i = 1}^{p} \left(\phi_{0, i} + \phi_{1, i}\right) x_{t - i}
$$

The weights are determined in a continuous manner by  $F((x_{t - d} - \Delta) / s)$ . The prior two equations also determine properties of a STAR model. For instance, a prerequisite for the stationarity of a STAR model is that all zeros of both AR polynomials are outside the unit circle. An advantage of the STAR model over the TAR model is that the conditional mean function is differentiable. However, experience shows that the transition parameters  $\Delta$  and  $s$  of a STAR model are hard to estimate. In particular, most empirical studies show that standard errors of the estimates of  $\Delta$  and  $s$  are often quite large, resulting in  $t$ -ratios about 1.0; see Teräsvirta (1994). This uncertainty leads to various complications in interpreting an estimated STAR model.

Example 4. To illustrate the application of STAR models in financial time series analysis, we consider the monthly simple stock returns for Minnesota Mining and Manufacturing (3M) Company from February 1946 to December 2008. If ARCH models are entertained, we obtain the following ARCH(2) model


$$
R_{t} = 0. 0 1 3 + a_{t}, \quad a_{t} = \sigma_{t} \epsilon_{t} \tag {16}
$$

$$
\sigma_{t}^{2} = 0. 0 0 3 + 0. 0 8 8 a_{t - 1}^{2} + 0. 1 0 9 a_{t - 2}^{2}
$$ where standard errors of the estimates are 0.002, 0.0003, 0.047, and 0.050, respectively. As discussed before, such an ARCH model fails to show the asymmetric responses of stock volatility to positive and negative prior shocks. The STAR model provides a simple alternative that may overcome this difficulty Applying STAR models to the monthly returns of 3M stock, we obtain the model


$$
\begin{array}{l} R_{t} = 0. 0 1 5 + a t, \quad a_{t} = \sigma_{t} \epsilon_{t} \\ \sigma_{t}^{2} = \left(0. 0 0 3 + 0. 2 0 5 a_{t - 1}^{2} + 0. 0 9 2 a_{t - 2}^{2}\right) \tag {17} \\ + \frac{0 . 0 0 1 - 0 . 2 3 9 a_{t - 1}^{2}}{1 + \exp (- 1 0 0 0 a_{t - 1})} \\ \end{array}
$$ where the standard error of the constant term in the mean equation is 0.002 and the standard errors of the estimates in the volatility equation are 0.0002, 0.074, 0.043, 0.0004, and 0.080, respectively. The scale parameter 1000 of the logistic transition function is fixed a priori to simplify the estimation. This STAR model provides some support for asymmetric responses to positive and negative prior shocks. For a large negative  $a_{t-1}$ , the volatility model approaches the ARCH(2) model


$$
\sigma_{t}^{2} = 0. 0 0 3 + 0. 2 0 5 a_{t - 1}^{2} + 0. 0 9 2 a_{t - 2}^{2}
$$

Yet for a large positive  $a_{t - 1}$ , the volatility process behaves like the ARCH(2) model

$$
\sigma_{t}^{2} = 0. 0 0 4 - 0. 0 3 4 a_{t - 1}^{2} + 0. 0 9 2 a_{t - 2}^{2}
$$

The negative coefficient of  $a_{t-1}^2$  in the prior model is counterintuitive, but the magnitude is small. As a matter of fact, for a large positive shock  $a_{t-1}$ , the ARCH effects appear to be weak even though the parameter estimates remain statistically significant.

# Markov Switching Model

The idea of using probability switching in nonlinear time series analysis is discussed in Tong (1983). Using a similar idea, but emphasizing aperiodic transition between various states of an economy, Hamilton (1989) considers the Markov switching autoregressive (MSA) model. Here the transition is driven by a hidden two-state Markov chain. A time series  $x_{t}$  follows an MSA model if it satisfies

$$
\left\{ \begin{array}{l l} c_{1} + \sum_{i = 1}^{p} \phi_{1, i} x_{t - i} + a_{1 t} & \text{if } s_{t} = 1 \\ c_{2} + \sum_{i = 1}^{p} \phi_{2, i} x_{t - i} + a_{2 t} & \text{if } s_{t} = 2 \end{array} \right. \tag {18}
$$ where  $s_t$  assumes values in  $\{1,2\}$  and is a first-order Markov chain with transition probabilities


$$
P (s_{t} = 2 | s_{t - i} = 1) = w_{1}, \quad P (s_{t} = 1 | s_{t - 1} = 2) = w_{2}
$$

The innovative series  $\{a_{1t}\}$  and  $\{a_{2t}\}$  are sequences of IID random variables with mean zero and finite variance and are independent of one another. A small  $w_{i}$  means that the model tends to stay longer in state  $i$ . In fact,  $1 / w_{i}$  is the expected duration of the process to stay in state  $i$ . From the definition, an MSA model uses a hidden Markov chain to govern the transition from one conditional mean function to another. This is different from that of a SETAR model for which the transition is determined by a particular lagged variable. Consequently, a SETAR model uses a deterministic scheme to govern the model transition whereas an MSA model uses a stochastic scheme.

In practice, the stochastic nature of the states implies that one is never certain about which state  $x_{t}$  belongs to in an MSA model. When the sample size is large, one can use some filtering techniques to draw inference on the state of  $x_{t}$ . Yet as long as  $x_{t - d}$  is observed, the regime of  $x_{t}$  is known in a SETAR model. This difference has important practical implications in forecasting. For instance, forecasts of an MSA model are always a linear combination of forecasts produced by submodels of individual states. But those of a SETAR model only come from a single regime provided that  $x_{t - d}$  is observed.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/8fcc9e469067451c6da2f573cb63bf572ec8a1240e3c9a268a3f8dd5608d3301.jpg)
Figure 4 Time Plot of the Growth Rate of the U.S. Quarterly Real GNP from 1947.II to 1991.I Note: The data are seasonally adjusted and in percentages.

Forecasts of a SETAR model also become a linear combination of those produced by models of individual regimes when the forecast horizon exceeds the delay  $d$ . It is much harder to estimate an MSA model than other models because the states are not directly observable. Hamilton (1990) uses the EM algorithm, which is a statistical method iterating between taking expectation and maximization. McCulloch and Tsay (1994) consider a Markov chain Monte Carlo (MCMC) method to estimate general MSA models. For applications of MCMC methods in finance, see Tsay (2010, Chapter 12).

McCulloch and Tsay (1993) generalize the MSA model in equation (18) by letting the transition probabilities  $w_{1}$  and  $w_{2}$  be logistic, or probit, functions of some explanatory variables available at time  $t - 1$ . Chen, McCulloch, and Tsay (1997) use the idea of Markov switching as a tool to perform model comparison and selection between nonnested nonlinear time series models (e.g., comparing bilinear and SETAR models). Each competing model is represented by a state. This approach to select a model is a generalization of the odds ratio commonly used in Bayesian analysis. Finally, the MSA model can easily be generalized to the case of more than two states. The computational intensity involved increases rapidly, however. For more discussions of Markov switching models in econometrics, see Hamilton (1994, Chapter 22).


Example 5. Consider the growth rate, in percentages, of the U.S. quarterly real gross national product (GNP) from the second quarter of 1947 to the first quarter of 1991. The data are seasonally adjusted and shown in Figure 4, where a horizontal line of zero growth is also given. It is reassuring to see that a majority of the growth rates are positive. This series has been widely used in nonlinear analysis of economic time series. Tiao and Tsay (1994) and Potter (1995) use TAR models, whereas Hamilton (1989) and McCulloch and Tsay (1994) employ Markov switching models.

Employing the MSA model in equation (18) with  $p = 4$  and using a Markov chain Monte Carlo method, McCulloch and Tsay (1994) obtain the estimates shown in Table 1. The results have several interesting findings. First, the mean growth rate of the marginal model for state 1 is  $0.909 / (1 - 0.265 - 0.029 + 0.126 + 0.11) = 0.965$  and that of state 2 is  $-0.42 / (1 - 0.216 - 0.628 + 0.073 + 0.097) = -1.288$ . Thus, state 1 corresponds to quarters with positive growth, or expansion periods, whereas state 2

Table 1 Estimation Results of a Markov Switching Model with  $p = 4$  for the Growth Rate of U.S. Quarterly Real GNP, Seasonally Adjusted

<table><tr><td colspan="8">State 1</td></tr><tr><td>Parameter</td><td>ci</td><td>φ1</td><td>φ2</td><td>φ3</td><td>φ4</td><td>σi</td><td>wi</td></tr><tr><td>Estimate</td><td>0.909</td><td>0.265</td><td>0.029</td><td>-0.126</td><td>-0.110</td><td>0.816</td><td>0.118</td></tr><tr><td>Std. Error</td><td>0.202</td><td>0.113</td><td>0.126</td><td>0.103</td><td>0.109</td><td>0.125</td><td>0.053</td></tr><tr><td colspan="8">State 2</td></tr><tr><td>Estimate</td><td>-0.420</td><td>0.216</td><td>0.628</td><td>-0.073</td><td>-0.097</td><td>1.017</td><td>0.286</td></tr><tr><td>Std. Error</td><td>0.324</td><td>0.347</td><td>0.377</td><td>0.364</td><td>0.404</td><td>0.293</td><td>0.064</td></tr></table> consists of quarters with negative growth, or a contraction period. Second, the relatively large posterior standard deviations of the parameters in state 2 reflect that there are few observations in that state. This is expected as Figure 4 shows few quarters with negative growth. Third, the transition probabilities appear to be different for different states. The estimates indicate that it is more likely for the U.S. GNP to get out of a contraction period than to jump into one  $-0.286$  versus 0.118. Fourth, treating  $1 / w_{i}$  as the expected duration for the process to stay in state  $i$ , we see that the expected durations for a contraction period and an expansion period are approximately 3.69 and 11.31 quarters. Thus, on average, a contraction in the U.S. economy lasts about a year, whereas an expansion can last for 3 years. Finally, the estimated AR coefficients of  $\mathrm{x}_{t-2}$  differ substantially between the two states, indicating that the dynamics of the U.S. economy are different between expansion and contraction periods.

Note: The estimates and their standard errors are posterior means and standard errors of a Gibbs sampling with 5000 iterations.


# Nonparametric Methods

In some financial applications, we may not have sufficient knowledge to prespecify the nonlinear structure between two variables  $Y$  and  $X$ . In other applications, we may wish to take advantage of the advances in computing facilities and computational methods to explore the functional relationship between  $Y$  and  $X$ . These considerations lead to the use of nonparametric methods and techniques. Nonparametric methods, however, are not without cost. They are highly data dependent and can easily result in overfitting. Our goal here is to introduce some nonparametric methods for financial applications and some nonlinear models that make use of nonparametric methods and techniques. The nonparametric methods discussed include kernel regression, local least squares estimation, and neural network.


The essence of nonparametric methods is smoothing. Consider two financial variables  $Y$  and  $X$ , which are related by

$$
Y_{t} = m \left(X_{t}\right) + a_{t} \tag {19}
$$ where  $m(.)$  is an arbitrary, smooth, but unknown function and  $\{a_t\}$  is a white noise sequence. We wish to estimate the nonlinear function  $m(.)$  from the data. For simplicity, consider the problem of estimating  $m(.)$  at a particular date for which  $X = x$ . That is, we are interested in estimating  $m(x)$ . Suppose that at  $X = x$  we have repeated independent observations  $y_1,\ldots ,y_T$ . Then the data become


$$ y_{t} = m (x) + a_{t}, \quad t = 1, \ldots , T
$$

Taking the average of the data, we have

$$
\frac{\sum_{t = 1}^{T} y_{t}}{T} = m (x) + \frac{\sum_{t = 1}^{T} a_{t}}{T}
$$

By the law of large numbers, the average of the shocks converges to zero as  $T$  increases. Therefore, the average  $\hat{y} = (\sum_{T=1}^{T} y_t) / T$  is a consistent estimate of  $m(x)$ . That the average  $\bar{y}$  provides a consistent estimate of  $m(x)$  or, alternatively, that the average of shocks converges to zero shows the power of smoothing.


In financial time series, we do not have repeated observations available at  $X = x$ . What we observed are  $\{(y_{t},x_{t})\}$  for  $t = 1,\dots ,T$ . But if the function  $m(.)$  is sufficiently smooth, then the value of  $Y_{t}$  for which  $X_{t}\approx x$  continues to provide accurate approximation of  $m(x)$ . The value of  $Y_{t}$  for which  $X_{t}$  is far away from  $x$  provides less accurate approximation for  $m(x)$ . As a compromise, one can use a weighted average of  $y_{t}$  instead of the simple average to estimate  $m(x)$ . The weight should be larger for those  $Y_{t}$  with  $X_{t}$  close to  $x$  and smaller for those  $Y_{t}$  with  $X_{t}$  far away from  $x$ . Mathematically, the estimate of  $m(x)$  for a given  $x$  can be written as

$$
\hat {m} (x) = \frac{1}{T} \sum_{\mathrm{t} = 1}^{\mathrm{T}} w_{t} (x) y_{t} \tag {20}
$$ where the weights  $w_{t}(x)$  are larger for those  $y_{t}$  with  $x_{t}$  close to  $x$  and smaller for those  $y_{t}$  with  $x_{t}$  far away from  $x$ . In equation (20), we assume that the weights sum to  $T$ . One can treat  $1 / T$  as part of the weights and make the weights sum to one.


From equation (20), the estimate  $\hat{m}(x)$  is simply a local weighted average with weights determined by two factors. The first factor is the distance measure (i.e., the distance between  $x_{t}$  and  $x$ ). The second factor is the assignment of weight for a given distance. Different ways to determine the distance between  $x_{t}$  and  $x$  and to assign the weight using the distance give rise to different nonparametric methods. In what follows, we discuss the commonly used kernel regression and local linear regression methods.

# Kernel Regression

Kernel regression is perhaps the most commonly used nonparametric method in smoothing. The weights here are determined by a kernel, which is typically a probability density function, is denoted by  $K(x)$ , and satisfies


$$
K (x) \geq 0, \quad \int K (z) d z = 1
$$

However, to increase the flexibility in distance measure, one often rescales the kernel using a variable  $h > 0$ , which is referred to as the bandwidth. The rescaled kernel becomes

$$
K_{h} (x) = \frac{1}{h} K (x / h), \quad \int K_{h} (z) d z = 1 \tag {21}
$$

The weight function can now be defined as

$$ w_{t} (x) = \frac{K_{h} \left(x - x_{t}\right)}{\sum_{t = 1}^{T} K_{h} \left(x - x_{t}\right)} \tag {22}
$$ where the denominator is a normalization constant that makes the smoother adaptive to the local intensity of the  $X$  variable and ensures the weights sum to one. Plugging equation (22) into the smoothing formula (20), we have the well-known Nadaraya-Watson kernel estimator


$$
\hat {m} (x) = \sum_{t = 1}^{T} w_{t} (x) y_{t} = \frac{\sum_{t = 1}^{T} K_{h} \left(x - x_{t}\right) y_{t}}{\sum_{t = 1}^{T} K_{h} \left(x - x_{t}\right)} \tag {23}
$$ see Nadaraya (1964) and Watson (1964). In practice, many choices are available for the kernel  $K(x)$ . However, theoretical and practical considerations lead to a few choices, including the Gaussian kernel


$$
K_{h} (x) = \frac{1}{h \sqrt{2 \pi}} \exp \left(- \frac{x^{2}}{2 h^{2}}\right)
$$ and the Epanechnikov kernel (Epanechnikov, 1969)


$$
K_{h} (x) = \frac{0 . 7 5}{h} \left(1 - \frac{x^{2}}{h^{2}}\right) I \left(\left| \frac{x}{h} \right| \leq 1\right)
$$ where  $1(A)$  is an indicator such that  $1(A) = 1$  if  $A$  holds and  $1(A) = 0$  otherwise. Figure 5 shows the Gaussian and Epanechnikov kernels for  $h = 1$ .


To gain insight into the bandwidth  $h$ , we evaluate the Nadaraya-Watson estimator with the Epanechnikov kernel at the observed values  $\{x_{t}\}$  and consider two extremes. First, if  $h \to 0$ , then

$$
\hat {m} \left(x_{t}\right)\rightarrow \frac{K_{h} (0) y_{t}}{K_{h} (0)} = y_{t}
$$ indicating that small bandwidths reproduce the data. Second, if  $h \to \infty$ , then

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/a70dec348cc4e428e10cc4fb4c9764245a7df8d80505f6254fea6f32e498a11c.jpg)
Figure 5 Standard Normal Kernel (Solid Line) and Epanechnikov Kernel (Dashed Line) with Bandwidth  $h = 1$


$$
\hat {m} (x_{t}) \rightarrow \frac{\sum_{t = 1}^{T} K_{h} (0) y_{t}}{\sum_{t = 1}^{T} K_{h} (0)} = \frac{1}{T} \sum_{t = 1}^{T} y_{t} = \bar {y}
$$ suggesting that large bandwidths lead to an oversmoothed curve—the sample mean. In general, the bandwidth function  $h$  acts as follows. If  $h$  is very small, then the weights focus on a few observations that are in the neighborhood around each  $x_{t}$ . If  $h$  is very large, then the weights will spread over a larger neighborhood of  $x_{t}$ . Consequently, the choice of  $h$  plays an important role in kernel regression. This is the well-known problem of bandwidth selection in kernel regression.


# Bandwidth Selection

There are several approaches for bandwidth selection; see Hardle (1990) and Fan and Yao (2003). The first approach is the plug-in method, which is based on the asymptotic expansion of the mean integrated squared error (MISE) for kernel smoothers

$$
\mathrm{MI SE} = E \int_{- \infty}^{\infty} [ \hat {m} (x) - m (x) ]^{2} d x
$$ where  $m(.)$  is the true function. The quantity  $E[\hat{m} (x) - m(x)]^2$  of the MISE is a pointwise measure of the mean squared error (MSE) of  $\hat{m} (x)$  evaluated at  $x$ .


Under some regularity conditions, one can derive the optimal bandwidth that minimizes the MISE. The optimal bandwidth typically depends on several unknown quantities that must be estimated from the data with some preliminary smoothing. Several iterations are often needed to obtain a reasonable estimate of the optimal bandwidth. In practice, the choice of preliminary smoothing can become a problem. Fan and Yao (2003) give a normal reference bandwidth selector as

$$
\hat {h}_{\mathrm{op t}} \left\{ \begin{array}{l l} 1. 0 6 s T^{- 1 / 5} & \text{fo rt he Ga us si an ke rn el} \\ 2. 3 4 s T^{- 1 / 5} & \text{fo rt he Ep an ec hn ik ov ke rn el} \end{array} \right.
$$ where  $s$  is the sample standard error of the independent variable, which is assumed to be stationary.


The second approach to bandwidth selection is the leave-one-out cross-validation. First, one observation  $(x_{j},y_{j})$  is left out. The remaining  $T - 1$  data points are used to obtain the following smoother at  $x_{j}$ :

$$
\hat {m}_{h, j} (x_{j}) = \frac{1}{T - 1} \sum_{t \neq j} w_{t} (x_{j}) y_{t}
$$ which is an estimate of  $y_{j}$ , where the weights  $w_{t}(x_{j})$  sum to  $T - 1$ . Second, perform step-1 for  $j = 1,\dots ,T$  and define the function


$$
C V (h) = \frac{1}{T} \sum_{j = 1}^{T} [ y_{j} - \hat {m}_{h, j} (x_{j}) ]^{2} W (x_{j})
$$ where  $w(.)$  is a nonnegative weight function satisfying  $\sum_{j=1}^{n} W(x_j) = T$ , that can be used to down-weight the boundary points if necessary. Decreasing the weights assigned to data points close to the boundary is needed because those points often have fewer neighboring observations. The function  $CV(h)$  is called the cross-validation function because it validates the ability of the smoother to predict  $\{y_t\} T_{t=1}$ . One chooses the bandwidth  $h$  that minimizes the  $CV(.)$  function.


# Local Linear Regression Method

Assume that the second derivative of  $m(.)$  in model (19) exists and is continuous at  $x$ , where  $x$  is a given point in the support of  $m(.)$ . Denote the data available by  $\{(y_t, x_t)\}_{t=1}^T$ . The local linear regression method to nonparametric regression is to find  $a$  and  $b$  that minimize

$$
L (a, b) = \sum_{t = 1}^{T} \left[ y_{t} - a - b \left(x - x_{t}\right) \right]^{2} K_{h} \left(x - x_{t}\right) \tag {24}
$$ where  $K_{h}(.)$  is a kernel function defined in equation (21) and  $h$  is a bandwidth. Denote the resulting value of  $a$  by  $\hat{a}$ . The estimate of  $m(x)$  is then defined as  $\hat{a}$ . In practice,  $x$  assumes an observed value of the independent variable. The estimate  $\hat{b}$  can be used as an estimate of the first derivative of  $m(.)$  evaluated at  $x$ .


Under the least squares theory, equation (24) is a weighted least squares problem and one can derive a closed-form solution for  $a$ . Specifically, taking the partial derivatives of  $L(a,b)$  with respect to both  $a$  and  $b$  and equating the derivatives to zero, we have a system of two equations with two unknowns:

$$
\begin{array}{l} \sum_{t = 1}^{T} K_{h} \left(x - x_{t}\right) y_{t} = a \sum_{t = 1}^{T} K_{h} \left(x - x_{t}\right) \\ + b \sum_{t = 1}^{T} (x - x_{t}) K_{h} (x - x_{t}) \\ \end{array}
$$

$$
\begin{array}{l} \sum_{t = 1}^{T} y_{t} \left(x - x_{t}\right) K_{h} \left(x - x_{t}\right) = a \sum_{t = 1}^{T} \left(x - x_{t}\right) K_{h} \left(x - x_{t}\right) \\ + b \sum_{t = 1}^{T} (x - x_{t})^{2} K_{h} (x - x_{t}) \\ \end{array}
$$

Define

$$ s_{T, \ell} = \sum_{t = 1}^{T} K_{h} (x - x_{t}) (x - x_{t})^{\ell}, \quad \ell = 0, 1, 2.
$$

The prior system of equations becomes

$$
\begin{array}{l} \left[ \begin{array}{c c} s_{T, 0} & s_{T, 1} \\ s_{T, 1} & s_{T, 2} \end{array} \right] \left[ \begin{array}{c} a \\ b \end{array} \right] \\ = \left[ \begin{array}{c} \sum_{t = 1}^{T} K_{h} (x - x_{t}) y_{t} \\ \sum_{t = 1}^{T} (x - x_{t}) K_{h} (x - x_{t}) y_{t} \end{array} \right] \\ \end{array}
$$

Consequently, we have

$$
\begin{array}{l} \hat {a} = \\ \frac{s_{T , 2} \sum_{t = 1}^{T} K_{h} (x - x_{t}) y_{t} - s_{T , 1} \sum_{t = 1}^{T} (x - x_{t}) K_{h} (x - x_{t}) y_{t}}{s_{T , 0} s_{T , 2} - s_{T , 1}^{2}} \\ \end{array}
$$

The numerator and denominator of the prior fraction can be further simplified as

$$
\begin{array}{l} s_{T, 2} = \sum_{t = 1}^{T} K_{h} (x - x_{t}) y_{t} \\ - s_{T, 1} \sum_{t = 1}^{T} (x - x_{t}) K_{h} (x - x_{t}) y_{t} \\ = \sum_{t = 1}^{T} \left[ K_{h} \left(x - x_{t}\right) \left(s_{T, 2} - \left(x - x_{t}\right) s_{T, 1}\right) \right] y t. \\ \end{array}
$$

$$
\begin{array}{l} s_{T, 0} s_{T, 2} - s_{T, 1}^{2} = \sum_{t = 1}^{T} K_{h} (x - x_{t}) s_{T, 2} \\ - \sum_{t = 1}^{T} (x - x_{t}) K_{h} (x - x_{t}) s_{T, 1} \\ = \sum_{t = 1}^{T} K_{h} (x - x_{t}) [ (s_{T, 2} - (x - x_{t}) s_{T, 1}) ] \\ \end{array}
$$

In summary, we have

$$
\hat {a} = \frac{\sum_{t = 1}^{T} w_{t} y_{t}}{\sum_{t = 1}^{T} w_{t}} \tag {25}
$$ where  $w_{t}$  is defined as


$$ w_{t} = K_{h} (x - x_{t}) \left[ s_{T, 2} - (x - x_{t}) s_{T, 1} \right]
$$

In practice, to avoid possible zero in the denominator, we use  $\hat{m}(x)$  next to estimate  $m(x)$ :

$$
\hat {m} (x) = \frac{\sum_{t = 1}^{T} w_{t} y_{t}}{\sum_{t = 1}^{T} w_{t} + 1 / T^{2}} \tag {26}
$$

Notice that a nice feature of equation (26) is that the weight  $w_{t}$  satisfies

$$
\sum_{t = 1}^{T} (x - x_{t}) w_{t} = 0
$$

Also, if one assumes that  $m(.)$  of equation (19) has the first derivative and finds the minimizer of

$$
\sum_{t = 1}^{T} (y_{t} - a)^{2} K_{h} (x - x_{t})
$$ then the resulting estimator is the Nadaraya-Watson estimator mentioned earlier. In general, if one assumes that  $m(x)$  has a bounded kth derivative, then one can replace the linear polynomial in equation (24) by a  $(k - 1)$ -order polynomial. We refer to the estimator in equation (26) as the local linear regression smoother. Fan (1993) shows that, under some regularity conditions, the local linear regression estimator has some important sampling properties. The selection of bandwidth can be carried out via the same methods as before.


# Financial Time Series Application

In time series analysis, the explanatory variables are often the lagged values of the series. Consider the simple case of a single explanatory variable. Here model (19) becomes

$$ x_{t} = m (x_{t - i}) + a_{t}
$$ and the kernel regression and local linear regression method discussed before are directly applicable. When multiple explanatory variables exist, some modifications are needed to implement the nonparametric methods. For the kernel regression, one can use a multivariate kernel such as a multivariate normal density function with a prespecified covariance matrix:


$$
K_{h} (x) = \frac{1}{(h \sqrt{2 \pi})^{p} | \boldsymbol {\Sigma} |^{1 / 2}} \exp \left(- \frac{1}{2 h^{2}} x^{\prime} \Sigma^{- 1} x\right)
$$ where  $p$  is the number of explanatory variables and  $\Sigma$  is a prespecified positive-definite matrix. Alternatively, one can use the product of


univariate kernel functions as a multivariate kernel—for example,

$$
K_{h} (x) = \prod_{i = 1}^{p} \frac{0 . 7 5}{h_{i}} \left(1 - \frac{x_{i}^{2}}{h_{i}^{2}}\right) I \left(| \frac{x_{i}}{h_{i}} | <   1\right)
$$

This latter approach is simple, but it overlooks the relationship between the explanatory variables.

Example 6. To illustrate the application of nonparametric methods in finance, consider the weekly 3-month Treasury bill secondary market rate from 1970 to 1997 for 1,461 observations. The data are obtained from the Federal Reserve Bank of St. Louis and are shown in Figure 6. This series has been used in the literature as an example of estimating stochastic diffusion equations using discretely observed data. Here we consider a simple model

$$ y_{t} = \mu (x_{t - 1}) d t + \sigma (x_{t - 1}) d w_{t}
$$ where  $x_{t}$  is the 3-month Treasury bill rate,  $y_{t} = x_{t} - x_{t-1}$ ,  $w_{t}$  is a standard Brownian motion, and  $\mu(.)$  and  $\sigma(.)$  are smooth functions of  $x_{t-1}$ , and apply the local smoothing function lowess of R or S-Plus to obtain nonparametric estimates of  $\mu(.)$  and  $\sigma(.)$ ; see Cleveland (1979). For simplicity, we use  $|y_{t}|$  as a proxy of the volatility of  $x_{t}$ .


For the simple model considered,  $\mu (x_{t - 1})$  is the conditional mean of  $y_{t}$  given  $x_{t - 1}$ , that is,  $\mu (x_{t - 1}) = E(y_t|x_{t - 1})$ . Figure 7(a) shows the scatterplot of  $y(t)$  versus  $x_{t - 1}$ . The plot also contains the local smooth estimate of  $\mu (x_{t - 1})$  obtained by the method of lowess in the statistical package R. The estimate is essentially zero. However, to better understand the estimate, Figure 7(b) shows the estimate  $\hat{\mu} (x_{t - 1})$  on a finer scale. It is interesting to see that  $\hat{\mu} (x_{t - 1})$  is positive when  $x_{t - 1}$  is small, but becomes negative when  $x_{t - 1}$  is large. This is in agreement with the common sense that when the interest rate is high, it is expected to come down, and when the rate is low, it is expected to increase. Figure 7(c) shows the scatterplot of  $|y(t)|$  versus  $x_{t - 1}$  and the estimate of  $\hat{\sigma} (x_{t - 1})$  via lowess. The plot confirms that the higher the interest rate, the larger the volatility. Figure 7(d) shows the estimate  $\hat{\sigma} (x_{t - 1})$  on a finer scale. Clearly the volatility is an increasing function of  $x_{t - 1}$  and the slope seems to accelerate when  $x_{t - 1}$  is approaching  $10\%$ . This exam

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/e8a5cee56e27dbfaaa48b640060965a0f076e1c5166e2c57333510aca138fbc5.jpg)
Figure 6 Time Plot of U.S. Weekly 3-Month Treasury Bill Rate in the Secondday Market from 1970 to 1997


ple demonstrates that simple non-parametric methods can be helpful in understanding the dynamic structure of a financial time series.

The following nonlinear models are derived with the help of nonparametric methods.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/8d604f23ec5a64a6708f95309021e8d96d28a549ebbbacbecb780f7a59cb2386.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/d1fbaf83a920f469bf92afa14b0d2f796791ff93f279088a95fc2f4d3bd038e6.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/4d698d53ef12377224c35d6ee5588f688ad9132ea19deea574da16005b537b2c.jpg)
Figure 7 Estimation of Conditional Mean and Volatility of Weekly 3-Month Treasury Bill Rate via a Local Smoothing Method: (a)  $y_{t}$  versus  $x_{t-1}$ , where  $y_{t} = x_{t} - x_{t-1}$  and  $x_{t}$  is the interest rate; (b) estimate of  $\mu(x_{t-1})$ ; (c)  $|y_{t}|$  versus  $x_{t-1}$ ; and (d) estimate of  $\sigma(x_{t-1})$

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/4675091e34172f8731ef146c26a1c1b8b86c16e359afd1e94475c7aa731e89d9.jpg)

# Functional Coefficient AR Model

Recent advances in nonparametric techniques enable researchers to relax parametric constraints in proposing nonlinear models. In some cases, nonparametric methods are used in a preliminary study to help select a parametric nonlinear model. This is the approach taken by Chen and Tsay (1993a) in proposing the functional-coefficient autoregressive (FAR) model that can be written as

$$ x_{t} = f_{1} \left(X_{t - 1}\right) x_{t - 1} + \dots + f_{p} \left(X_{t - 1}\right) x_{t - p} + a_{t} \tag {27}
$$ where  $X_{t - 1} = (x_{t - 1},\dots ,X_{t - k})'$  is a vector of lagged values of  $x_{t}$ . If necessary  $x_{t - 1}$  may also include other explanatory variables available at time  $t - 1$ . The functions  $f_{i}(.)$  of equation (27) are assumed to be continuous, even twice differentiable, almost surely with respect to their arguments. Most of the nonlinear models discussed before are special cases of the FAR model. In application, one can use nonparametric methods such as kernel regression or local linear regression to estimate the functional coefficients  $f_{i}(.)$ , especially when the dimension of  $X_{t - 1}$  is low (e.g.,  $X_{t - 1}$  is a scalar). Recently, Cai, Fan, and Yao (2000) applied the local linear regression method to estimate  $f_{i}(.)$  and showed that substantial improvements in 1-step ahead forecasts can be achieved by using FAR models.


# Nonlinear Additive AR Model

A major difficulty in applying nonparametric methods to nonlinear time series analysis is the "curse of dimensionality." Consider a general nonlinear  $\mathrm{AR}(p)$  process  $x_{t} = f(x_{t - 1},\ldots ,$ $x_{t - p}) + a_t$  . A direct application of nonparametric methods to estimate  $f(.)$  would require  $p$  dimensional smoothing, which is hard to do when  $p$  is large, especially if the number of data points is not large. A simple, yet effective way to overcome this difficulty is to entertain an additive model that only requires lower dimensional smoothing. A time series  $x_{t}$  follows a nonlinear additive AR (NAAR) model if


$$ x_{t} = f_{0} (t) + \sum_{i = 1}^{p} f_{i} \left(x_{t - i}\right) + a_{t} \tag {28}
$$ where the  $f_{i}(.)$  are continuous functions almost surely. Because each function  $f_{i}(.)$  has a single argument, it can be estimated nonparametrically using one-dimensional smoothing techniques and hence avoids the curse of dimensionality. In application, an iterative estimation method that estimates  $f_{i}(.)$  nonparametrically conditioned on estimates of  $f_{j}(.)$  for all  $j \neq i$  is used to estimate a NAAR model; see Chen and Tsay (1993b) for further details and examples of NAAR models.


The additivity assumption is rather restrictive and needs to be examined carefully in application. Chen, Liu, and Tsay (1995) consider test statistics for checking the additivity assumption.

# Nonlinear State-Space Model

Making using of recent advances in MCMC methods (Gelfand and Smith, 1990), Carlin, Polson, and Stoffer (1992) propose a Monte Carlo approach for nonlinear state-space modeling. The model considered is

$$
S_{t} = f_{t} \left(S_{t - 1}\right) + u_{t}, \quad x_{t} = g_{t} \left(S_{t}\right) + v_{t} \tag {29}
$$ where  $S_{t}$  is the state vector,  $f_{t}(.)$  and  $g_{t}(.)$  are known functions depending on some unknown parameters,  $\{u_t\}$  is a sequence of IID multivariate random vectors with zero mean and nonnegative definite covariance matrix  $\Sigma u,\{v_t\}$  is a sequence of IID random variables with mean zero and variance  $\sigma_v^2$  ,and  $\{u_t\}$  is independent of  $\{v_t\}$


Monte Carlo techniques are employed to handle the nonlinear evolution of the state transition equation because the whole conditional distribution function of  $S_{t}$  given  $S_{t - 1}$  is needed for a nonlinear system. Other numerical smoothing methods for nonlinear time series analysis have been considered by Kitagawa

(1998) and the references therein. MCMC methods (or computing-intensive numerical methods) are powerful tools for nonlinear time series analysis. Their potential has not been fully explored. However, the assumption of knowing  $f_{t}(.)$  and  $g_{t}(.)$  in model (29) may hinder practical use of the proposed method. A possible solution to overcome this limitation is to use nonparametric methods such as the analyses considered in FAR and NAAR models to specify  $f_{t}(.)$  and  $g_{t}(.)$  before using nonlinear state-space models.

# Neural Networks

A popular topic in modern data analysis is neural network, which can be classified as a semiparametric method. The literature on neural network is enormous, and its application spreads over many scientific areas with varying degrees of success; see Ripley (1993, Sections 2 and 10). Cheng and Titterington (1994) provide information on neural networks from a statistical viewpoint. In this subsection, we focus solely on the feed-forward neural networks in which inputs are connected to one or more neurons, or nodes, in the input layer, and these nodes are connected forward to further layers until they reach the output layer. Figure 8 shows an example of a simple feed-forward network for univariate time series analysis with one hidden layer. The input layer has two nodes, and the hidden layer has three. The input nodes are connected forward to each and every node in the hidden layer, and these hidden nodes are connected to the single node in the output layer. We call the network a 2-3-1 feedforward network. More complicated neural networks, including those with feedback connections, have been proposed in the literature, but the feed-forward networks are most relevant to our study.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/bee295b80f74f71556e34d2ba67aa3f68ba3927b49ad10af164362ef20e7872c.jpg)
Figure 8 A Feed-Forward Neural Network with One Hidden Layer for Univariate Time Series Analysis


# Feed-Forward Neural Networks

A neural network processes information from one layer to the next by an "activation function." Consider a feed-forward network with one hidden layer. The  $j$ th node in the hidden layer is defined as

$$ h_{j} = f_{j} \left(\alpha_{0 j} + \sum_{i \rightarrow j} w_{i j} x_{i}\right) \tag {30}
$$ where  $x_{i}$  is the value of the ith input node,  $f_{j}(.)$  is an activation function typically taken to be the logistic function


$$ f_{j} (z) = \frac{\exp (z)}{1 + \exp (z)}
$$

$\alpha_{0j}$  is called the bias, the summation  $i \rightarrow j$  means summing over all input nodes feeding to  $j$ , and  $w_{ij}$  are the weights. For illustration, the  $j$ th node of the hidden layer of the 2-3-1 feed-forward network in Figure 8 is

$$ h_{j} = \frac{\exp \left(\alpha_{0 j} + w_{1 j} x_{1} + w_{2 j} x_{2}\right)}{1 + \exp \left(\alpha_{0 j} + w_{1 j} x_{1} + w_{2 j} x_{2}\right)}, \quad j = 1, 2, 3. \tag {31}
$$

For the output layer, the node is defined as

$$ o = f_{o} \left(\alpha_{0 o} + \sum_{j \rightarrow o} w_{j o} h_{j}\right) \tag {32}
$$ where the activation function  $f_{\mathrm{o}}(.)$  is either linear or a Heaviside function. If  $f_{\mathrm{o}}(.)$  is linear, then


$$ o = \alpha_{0 o} + \sum_{j = 1}^{k} w_{j o} h_{j}
$$ where  $k$  is the number of nodes in the hidden layer. By a Heaviside function, we mean  $f_{0}(z) = 1$  if  $z > 0$  and  $f_{0}(z) = 0$  otherwise. A neuron with a Heaviside function is called a threshold neuron, with "1" denoting that the neuron fires its message. For example, the output of the


2-3-1 network in Figure 8 is

$$ o = \alpha_{0 o} + w_{1 o} h_{1} + w_{2 o} h_{2} + w_{3 o} h_{3}
$$ if the activation function is linear; it is


$$ o = \left\{ \begin{array}{l l} 1 & \text{if } \alpha_{0 o} + w_{1 o} h_{1} + w_{2 o} h_{2} + w_{3 o} h_{3} > 0 \\ 0 & \text{if } \alpha_{0 o} + w_{1 o} h_{1} + w_{2 o} h_{2} + w_{3 o} h_{3} \leq 0 \end{array} \right.
$$ if  $f_{\mathrm{o}}(.)$  is a Heaviside function.


Combining the layers, the output of a feedforward neural network can be written as

$$ o = f_{o} \left[ \alpha_{0 o} + \sum_{j \rightarrow o} w_{j o} f_{j} \left(\alpha_{0 j} + \sum_{i \rightarrow j} w_{i j} x_{i}\right)\right] \tag {33}
$$

If one also allows for direct connections from the input layer to the output layer, then the network becomes

$$
\begin{array}{l} o = f_{o} \left[ \alpha_{0 o} + \sum_{i \rightarrow o} \alpha_{i o} x_{i} + \sum_{j \rightarrow o} w_{j o} f_{j} \right. \tag {34} \\ \left. \times \left(\alpha_{0 j} + \sum_{i \rightarrow j} w_{i j} x_{i}\right)\right] \\ \end{array}
$$ where the first summation is summing over the input nodes. When the activation function of the output layer is linear, the direct connections from the input nodes to the output node represent a linear function between the inputs and output. Consequently, in this particular case model (34) is a generalization of linear models. For the 2-3-1 network in Figure 8, if the output activation function is linear, then equation (33) becomes


$$ o = \alpha_{0 o} + \sum_{j = 1}^{3} w_{j o} h_{j}
$$ where  $h_j$  is given in equation (31). The network thus has 13 parameters. If equation (34) is used, then the network becomes


$$ o = \alpha_{0 \mathrm{o}} + \sum_{i = 1}^{2} \alpha_{i o} x_{i} + \sum_{j = 1}^{3} w_{j o} h_{j}
$$ where again  $h_j$  is given in equation (31). The number of parameters of the network increases to 15.


We refer to the function in equation (33) or (34) as a semiparametric function because its func tional form is known, but the number of nodes and their biases and weights are unknown. The direct connections from the input layer to the output layer in equation (34) mean that the network can skip the hidden layer. We refer to such a network as a skip-layer feed-forward network.


Feed-forward networks are known as multilayer percutrons in the neural network literature. They can approximate any continuous function uniformly on compact sets by increasing the number of nodes in the hidden layer; see Hornik, Stinchcombe, and White (1989), Hornik (1993), and Chen and Chen (1995). This property of neural networks is the universal approximation property of the multilayer percutrons. In short, feed-forward neural networks with a hidden layer can be seen as a way to parameterize a general continuous nonlinear function.

# Training and Forecasting

Application of neural networks involves two steps. The first step is to train the network (i.e., to build a network, including determining the number of nodes and estimating their biases and weights). The second step is inference, especially forecasting. The data are often divided into two nonoverlapping subsamples in the training stage. The first subsample is used to estimate the parameters of a given feed-forward neural network. The network so built is then used in the second subsample to perform forecasting and compute its forecasting accuracy. By comparing the forecasting performance, one selects the network that outperforms the others as the "best" network for making inference. This is the idea of cross-validation widely used in statistical model selection. Other model selection methods are also available.

In a time series application, let  $\{(r_t,x_t)|t = 1,\dots,T\}$  be the available data for network training, where  $x_{t}$  denotes the vector of inputs and  $r_t$  is the series of interest (e.g., log returns of an asset). For a given network, let  $o_t$  be the output of the network with input  $x_{t}$ ; see equation (34). Training a neural network amounts to choosing its biases and weights to minimize some fitting criterion—for example, the least squares


$$
S^{2} = \sum_{t = 1}^{T} (r_{t} - o_{t})^{2}
$$

This is a nonlinear estimation problem that can be solved by several iterative methods. To ensure the smoothness of the fitted function, some additional constraints can be added to the prior minimization problem. In the neural network literature, the back propagation (BP) learning algorithm is a popular method for network training. The BP method, introduced by Bryson and Ho (1969), works backward starting with the output layer and uses a gradient rule to modify the biases and weights iteratively. (Appendix 2A of Ripley, 1993, provides a derivation of back propagation.) Once a feed-forward neural network is built, it can be used to compute forecasts in the forecasting subsample.

Example 7. To illustrate applications of the neural network in finance, we consider the monthly log returns, in percentages and including dividends, for IBM stock from January 1926 to December 1999. We divide the data into two subsamples. The first subsample consisting of returns from January 1926 to December 1997 for 864 observations is used for modeling. Using model (34) with three inputs and two nodes in the hidden layer, we obtain a 3-2-1 network for the series. The three inputs are  $r_{t - 1}$ ,  $r_{t - 2}$ , and  $r_{t - 3}$  and the biases and weights are given next:

$$
\begin{array}{l} \hat {r}_{t} = 3. 2 2 - 1. 8 1 f_{1} \left(r_{t - 1}\right) - 2. 2 8 f_{2} \left(r_{t - 1}\right) \tag {35} \\ - 0. 0 9 r_{t - 1} - 0. 0 5 r_{t - 2} - 0. 1 2 r_{t - 3} \\ \end{array}
$$ where  $r_{t - 1} = (r_{t - 1},r_{t - 2},r_{t - 3})$  and the two logistic functions are


$$
\begin{array}{l} f_{1} \left(\boldsymbol {r}_{t - 1}\right) = \\ \frac{\exp \left(- 8 . 3 4 - 1 8 . 9 7 r_{t - 1} + 2 . 1 7 r_{t - 2} - 1 9 . 1 7 r_{t - 3}\right)}{1 + \exp \left(- 8 . 3 4 - 1 8 . 9 7 r_{t - 1} + 2 . 1 7 r_{t - 2} - 1 9 . 1 7 r_{t - 3}\right)} \\ f_{2} \left(\boldsymbol {r}_{t - 1}\right) = \\ \end{array}
$$

$$
\frac{\exp (3 9 . 2 5 - 2 2 . 1 7 r_{t - 1} - 1 7 . 3 4 r_{t - 2} - 5 . 9 8 r_{t - 3})}{1 + \exp (3 9 . 2 5 - 2 2 . 1 7 r_{t - 1} - 1 7 . 3 4 r_{t - 2} - 5 . 9 8 r_{t - 3})}
$$

The standard error of the residuals for the prior model is 6.56. For comparison, we also built an AR model for the data and obtained

$$ r_{t} = 1. 1 0 1 + 0. 0 7 7 r_{t - 1} + a_{t}, \quad \sigma_{a} = 6. 6 1 \tag {36}
$$

The residual standard error is slightly greater than that of the feed-forward model in equation (35).

# Forecast Comparison

The monthly returns of IBM stock in 1998 and 1999 form the second subsample and are used to evaluate the out-of-sample forecasting performance of neural networks. As a benchmark for comparison, we use the sample mean of  $r_t$  in the first subsample as the 1-step ahead forecast for all the monthly returns in the second subsample. This corresponds to assuming that the log monthly price of IBM stock follows a random walk with drift. The mean squared forecast error (MSFE) of this benchmark model is 91.85. For the AR(1) model in equation (36), the MSFE of 1-step ahead forecasts is 91.70. Thus, the AR(1) model slightly outperforms the benchmark. For the 3-2-1 feed-forward network in equation (35), the MSFE is 91.74, which is essentially the same as that of the AR(1) model.

Example 8. Nice features of the feed-forward network include its flexibility and wide applicability. For illustration, we use the network with a Heaviside activation function for the output layer to forecast the direction of price movement for IBM stock considered in Example 7. Define a direction variable as

$$ d_{t} = \left\{ \begin{array}{l l} 1 & \text{if } r_{t} \geq 0 \\ 0 & \text{if } r_{t} <   0 \end{array} \right.
$$

We use eight input nodes consisting of the first four lagged values of both  $r_t$  and  $d_t$  and four nodes in the hidden layer to build an 8-4-1 feed-forward network for  $d_t$  in the first subsample. The resulting network is then used to compute the 1-step ahead probability of an "upward movement" (i.e., a positive return) for the following month in the second subsample.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/29f97e6570ec0e856c7e62a0fb4ce4f713945bfbc7b81361f12554e33304a45d.jpg)
Figure 9 One-Step Ahead Probability Forecasts for a Positive Monthly Return for IBM Stock Using an 8-4-1 Feed-Forward Neural Network Note: The forecasting period is from January 1998 to December 1999.

Figure 9 shows a typical output of probability forecasts and the actual directions in the second subsample with the latter denoted by circles. A horizontal line of 0.5 is added to the plot. If we take a rigid approach by letting  $\hat{d}_t = 1$  if the probability forecast is greater than or equal to 0.5 and  $\hat{d}_t = 0$  otherwise, then the neural network has a successful rate of 0.58. The success rate of the network varies substantially from one estimation to another, and the network uses 49 parameters.

To gain more insight, we did a simulation study of running the 8-4-1 feed-forward network 500 times and computed the number of errors in predicting the upward and downward movement using the same method as before. The mean and median of errors over the 500 runs are 11.28 and 11, respectively, whereas the maximum and minimum number of errors are 18 and 4. For comparison, we also did a simulation with 500 runs using a random walk with drift—that is,

$$
\hat {d}_{t} = \left\{ \begin{array}{l l} 1 & \text{if } \hat {r}_{t} = 1. 1 9 + \epsilon_{t} \geq 0, \\ 0 & \text{ot he rw is e} \end{array} \right.
$$ where 1.19 is the average monthly log return for IBM stock from January 1926 to December


1997 and  $\{\epsilon_t\}$  is a sequence of IID  $N(0,1)$  random variables. The mean and median of the number of forecast errors become 10.53 and 11, whereas the maximum and minimum number of errors are 17 and 5, respectively. Figure 10 shows the histograms of the number of forecast errors for the two simulations. The results show that the 8-4-1 feed-forward neural network does not outperform the simple model that assumes a random walk with drift for the monthly log price of IBM stock.

# NONLINEARITY TESTS

In this section, we discuss some nonlinearity tests available in the literature that have decent power against the nonlinear models considered earlier in this entry. The tests discussed include both parametric and nonparametric statistics. The Ljung-Box statistics of squared residuals, the bispectral test, and the Brock, Dechert, and Scheinkman (BDS) test are nonparametric methods. The RESET test (Ramsey, 1969), the  $F$  tests of Tsay (1986, 1989), and other Lagrange multiplier and likelihood ratio tests depend on specific parametric functions. Because nonlinearity may occur in many ways, there exists no single test that dominates the others in detecting nonlinearity.

Figure 10 Histograms of the Number of Forecasting Errors for the Directional Movements of Monthly Log Returns of IBM Stock
![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/3867809ec894a923972b2d526d9ea4d025e5da066acb000836b2c70e13518f3b.jpg)
Note: The forecasting period is from January 1998 to December 1999.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/74b4d444ae3b5d0483261941b7bc56245baf0aac13202796e9844ab750159885.jpg)


# Nonparametric Tests

Under the null hypothesis of linearity, residuals of a properly specified linear model should be independent. Any violation of independence in the residuals indicates inadequacy of the entertained model, including the linearity assumption. This is the basic idea behind various nonlinearity tests. In particular, some of the nonlinearity tests are designed to check for possible violation in quadratic forms of the underlying time series.

# Q-Statistic of Squared Residuals

McLeod and Li (1983) apply the Ljung-Box statistics to the squared residuals of an ARMA  $(p,q)$  model to check for model inadequacy. The test statistic is

$$
Q (m) = T (T + 2) \sum_{i = 1}^{m} \frac{\hat {\rho}_{i}^{2} (a_{i}^{2})}{T - i}
$$ where  $T$  is the sample size,  $\mathfrak{m}$  is a properly chosen number of autocorrelations used in the test,


$a_{t}$  denotes the residual series, and  $\hat{\rho}_i(a_t^2)$  is the lag-  $i$  ACF of  $a_{t}^{2}$ . If the entertained linear model is adequate,  $Q(m)$  is asymptotically a chi-squared random variable with  $m - p - q$  degrees of freedom. The prior  $Q$ -statistic is useful in detecting conditional heteroscedasticity of  $a_{t}$  and is asymptotically equivalent to the Lagrange multiplier test statistic of Engle (1982) for ARCH models. The null hypothesis of the statistics is  $H_{o}:\beta_{1} = \dots = \beta_{m} = 0$ , where  $\beta_{i}$  is the coefficient of  $a_{t - i}^{2}$  in the linear regression

$$ a_{t}^{2} = \beta_{0} + \beta_{1} a_{t - 1}^{2} + \dots + \beta_{m} a_{t - m}^{2} + e_{t}
$$ for  $t = m + 1,\ldots ,T$ . Because the statistic is computed from residuals (not directly from the observed returns), the number of degrees of freedom is  $m - p - q$ .


# Bispectral Test

This test can be used to test for linearity and Gaussianity. It depends on the result that a properly normalized bispectrum of a linear time series is constant over all frequencies and that the constant is zero under normality. The bispectrum of a time series is the Fourier transform of its third-order moments. For a stationary time series  $x_{t}$  in equation (1), the third-order moment is defined as


$$ c (u, v) = g \sum_{k = - \infty}^{\infty} \psi_{k} \psi_{k + u} \psi_{k + v} \tag {37}
$$ where  $u$  and  $v$  are integers,  $g = E(a_t^3)$ ,  $\psi_0 = 1$ , and  $\psi_k = 0$  for  $k < 0$ . Taking Fourier transforms of equation (37), we have


$$ b_{3} \left(w_{1}, w_{2}\right) = \frac{g}{4 \pi^{2}} \Gamma \left[ - \left(w_{1} + w_{2}\right) \right] \Gamma \left(w_{1}\right) \Gamma \left(w_{2}\right) \tag {38}
$$ where  $\Gamma (w) = \sum_{u = 0}^{\infty}\psi_{u}\exp (-iwu)$  with  $i = \sqrt{-1}$ , and  $w_{i}$  are frequencies. Yet the spectral density function of  $x_{t}$  is given by


$$ p (w) = \frac{\sigma_{a}^{2}}{2 \pi} | \Gamma (w) |^{2}
$$ where  $w$  denotes the frequency. Consequently, the function


$$
\begin{array}{l} b \left(w_{1}, w_{2}\right) = \frac{\left| b_{3} \left(w_{1} , w_{2}\right) \right|^{2}}{p \left(w_{1}\right) p \left(w_{2}\right) p \left(w_{1} + w_{2}\right)} \tag {39} \\ = \text{co ns ta nt} (w_{1}, w_{2}) \\ \end{array}
$$

The bispectrum test makes use of the property in equation (39). Basically, it estimates the function  $b(w_{1},w_{2})$  in equation (39) over a suitably chosen grid of points and applies a test statistic similar to Hotelling's  $T^2$  statistic to check the constancy of  $b(w_{1},w_{2})$ . For a linear Gaussian series,  $E(a_t^3) = g = 0$  so that the bispectrum is zero for all frequencies  $(w_{1},w_{2})$ . For further details of the bispectral test, see Priestley (1988), Subba Rao and Gabr (1984), and Hinich (1982). Limited experience shows that the test has decent power when the sample size is large.

# BDS Statistic

Brock, Dechert, and Scheinkman (1987) propose a test statistic, commonly referred to as the BDS test, to detect the IID assumption of a time series. The statistic is, therefore, different from other test statistics discussed because the latter mainly focus on either the second- or third-order properties of  $x_{t}$ . The basic idea of the BDS test is to make use of a "correlation integral" popular in chaotic time series analysis. Given a  $k$ -dimensional time series  $X_{t}$  and observations  $\{X_{t}\}_{t = 1}^{T_{k}}$ , define the correlation integral as


$$
C_{k} (\delta) = \lim_{T_{k} \rightarrow \infty} \frac{2}{T_{k} \left(T_{k} - 1\right)} \sum_{i <   j} I_{\delta} \left(X_{i}, X_{j}\right) \tag {40}
$$ where  $I_{\delta}(u,v)$  is an indicator variable that equals one if  $\| u - v\| < \delta$  and zero otherwise, where  $\| .\|$  is the supnorm. The correlation integral measures the fraction of data pairs of  $\{X_t\}$  that are within a distance of  $\delta$  from each other.


Consider next a time series  $x_{t}$ . Construct  $k$ -dimensional vectors  $X_{t}^{k} = (x_{t}, x_{t + 1}, \dots, x_{t + k - 1})'$ , which are called  $k$ -histories. The idea of the BDS test is as follows. Treat a  $k$ -history as a point in the  $k$ -dimensional space. If  $\{x_{t}\}_{t = 1}^{T}$  are indeed IID random variables, then the  $k$ -histories  $\{X_{t}\}_{t = 1}^{T_{k}}$  should show no pattern in the  $k$ -dimensional space. Consequently, the correlation integrals should satisfy the relation  $C_k(\delta) = [C_1(\delta)]^k$ . Any departure from the prior relation suggests that  $x_{t}$  are not IID. As a simple but informative example, consider a sequence of IID random variables from the uniform distribution over  $[0, 1]$ . Let  $[a, b]$  be a subinterval of  $[0, 1]$  and consider the "2-history"  $(x_{t}, x_{t + 1})$ , which represents a point in the two-dimensional space. Under the IID assumption, the expected number of 2-histories in the subspace  $[a, b] \times [a, b]$  should equal the square of the expected number of  $x_{t}$  in  $[a, b]$ .

This idea can be formally examined by using sample counterparts of correlation integrals. Define

$$
C_{\ell} (\delta , T) = \frac{2}{T_{k} (T_{k} - 1)} \sum_{i <   j} I_{\delta} \left(X_{i}^{*}, X_{j}^{*}\right), \quad \ell = 1, k
$$ where  $T_{\ell} = T - \ell +1$  and  $X_{i}^{*} = x_{i}$  if  $\ell = 1$  and  $X_{i}^{*} = X_{i}^{k}$  if  $\ell = k$ . Under the null hypothesis that  $\{x_{t}\}$  are IID with a nondegenerated distribution function  $F(.)$ , Brock, Dechert, and Scheinkman (1987) show that


$$
\begin{array}{l l} C_{k} (\delta , T) \to [ C_{1} (\delta) ]^{k} & \text{wi th pr ob ab il it y1}, \\ & \text{as} \quad T \to \infty \end{array}
$$ for any fixed  $k$  and  $\delta$ . Furthermore, the statistic  $\sqrt{T}\{C_k(\delta, T) - [C_1(\delta, T)]^k\}$  is asymptotically distributed as normal with mean zero and variance


$$
\begin{array}{l} \sigma_{k}^{2} (\delta) = \\ 4 \left(N^{k} + 2 \sum_{j = 1}^{k - 1} N^{k - j} C^{2 j} + (k - 1)^{2} C^{2 k} - k^{2} N C^{2 k - 2}\right) \\ \end{array}
$$ where  $C = \int [F(z + \delta) - F(z - \delta)]dF(z)$  and  $N = \int [F(z + \delta) - F(z - \delta)]^2 dF(z)$ . Note that  $C_1\{\delta, T\}$  is a consistent estimate of  $C$ , and  $N$  can be consistently estimated by


$$
\begin{array}{l} N (\delta , T) = \frac{6}{T_{k} (T_{k} - 1) (T_{k} - 2)} \\ \sum_{t <   s <   u} I_{\delta} \left(x_{t}, x_{s}\right) I_{\delta} \left(x_{s}, x_{u}\right) \\ \end{array}
$$

The BDS test statistic is then defined as

$$
D_{k} (\delta , T) = \sqrt{T} \left\{C_{k} (\delta , T) - \left[ C_{1} (\delta , T) \right]^{k} \right\} / \sigma_{k} (\delta , T) \tag {41}
$$ where  $\sigma_{k}(\delta, T)$  is obtained from  $\sigma_{k}(\delta)$  when  $C$  and  $N$  are replaced by  $C_{1}(\delta, T)$  and  $N(\delta, T)$ , respectively. This test statistic has a standard normal limiting distribution. For further discussion and examples of applying the BDS test, see Hsieh (1989) and Brock, Hsieh, and LeBaron (1991). In application, one should remove linear dependence, if any, from the data before applying the BDS test. The test may be sensitive to the choices of  $\delta$  and  $k$ , especially when  $k$  is large.


# Parametric Tests

Turning to parametric tests, we consider the RESET test of Ramsey (1969) and its generalizations. We also discuss some test statistics for detecting threshold nonlinearity.

# The RESET Test

Ramsey (1969) proposes a specification test for linear least squares regression analysis. The test is referred to as a RESET test and is readily applicable to linear AR models. Consider the linear  $\mathrm{AR}(p)$  model

$$ x_{t} = \boldsymbol {X}_{t - 1}^{\prime} \phi + a_{t} \tag {42}
$$ where  $X_{t - 1} = (1,x_{t - 1},\dots ,x_{t - p})'$  and  $\pmb {\phi} = (\phi_0,$ $\phi_1,\ldots ,\phi_p)'$ . The first step of the RESET test is to obtain the least squares estimate  $\hat{\pmb{\phi}}$  of equation (42) and compute the fit  $\hat{x}_t = X'_t\hat{\pmb{\phi}}$  the residual  $\hat{a}_t = x_t - \hat{x}_t$  and the sum of squared residuals  $SSR_0 = \sum_{t = p + 1}^T\hat{a}_t^2$  where  $T$  is the sample size. In the second step, consider the linear regression


$$
\hat {a}_{t} = \boldsymbol {X}_{t - 1}^{\prime} \boldsymbol {\alpha}_{1} + \boldsymbol {M}_{t - 1}^{\prime} \boldsymbol {\alpha}_{2} + v_{t} \tag {43}
$$ where  $M_{t - 1} = (\hat{x}_t^2,\dots ,\hat{x}_t^{s + 1})'$  for some  $s\geq 1$  , and compute the least squares residuals


$$
\hat {v}_{t} = \hat {a}_{t} - X_{t - 1}^{\prime} \hat {\alpha}_{1} - M_{t - 1}^{\prime} \hat {\alpha}_{2}
$$ and the sum of squared residuals  $SSR_{1} = \sum_{t=p+1}^{T} \hat{v}_{t}^{2}$  of the regression. The basic idea of the RESET test is that if the linear AR $(p)$  model in equation (42) is adequate, then  $\alpha_{1}$  and  $\alpha_{2}$  of equation (43) should be zero. This can be tested by the usual  $F$  statistic of equation (43) given by


$$
F = \frac{\left(S S R_{0} - S S R_{1}\right) / g}{S S R_{1} / (T - p - g)} \quad \text{wi th} \quad g = s + p + 1 \tag {44}
$$ which, under the linearity and normality assumption, has an  $F$  distribution with degrees of freedom  $g$  and  $T - p - g$ .


Because  $\hat{x}_t^k$  for  $k = 2,\ldots ,s + 1$  tend to be highly correlated with  $X_{t - 1}$  and among themselves, principal components of  $M_{t - 1}$  that are not colinear with  $X_{t - 1}$  are often used in fitting equation (43).

Keenan (1985) proposes a nonlinearity test for time series that uses  $\hat{x}_t^2$  only and modifies the second step of the RESET test to avoid multicollinearity between  $\hat{x}_t^2$  and  $X_{t - 1}$ . Specifically the linear regression (43) is divided into two steps. In step 2(a), one removes linear dependence of  $\hat{x}_t^2$  on  $X_{t - 1}$  by fitting the regression

$$
\hat {x}_{t}^{2} = \boldsymbol {X}_{t - 1}^{\prime} \beta + u_{t}
$$ and obtaining the residual  $\hat{u}_t = \hat{x}_t^2 - X_{t-1}\hat{\beta}$ . In step 2(b), consider the linear regression


$$
\hat {a}_{t} = \hat {u}_{t} \alpha + v_{t}
$$ and obtain the sum of squared residuals  $SS R_{1} = \sum_{t=p+1}^{T} (\hat{a}_{t} - \hat{u}_{t} \hat{\alpha})^{2} = \sum_{t=p+1}^{T} \hat{v}_{t}^{2}$  to test the null hypothesis  $\alpha = 0$ .


# The F Test

To improve the power of Keenan's test and the RESET test, Tsay (1986) uses a different choice of the regressor  $M_{t-1}$ . Specifically, he suggests using  $M_{t-1} = \text{vech}(X_{t-1}X_{t-1}')$ , where  $\text{vech}(A)$  denotes the half-stacking vector of the matrix  $A$  using elements on and below the diagonal only. For example, if  $p = 2$ , then  $M_{t-1} = (x_{t-1}^2, x_{t-1}x_{t-2}, x_{t-2}^2)'$ . The dimension of  $M_{t-1}$  is  $p(p + 1)/2$  for an AR( $p$ ) model. In practice, the test is simply the usual partial  $F$  statistic for testing  $\alpha = 0$  in the linear least squares regression

$$ x_{t} = X_{t - 1}^{\prime} \phi + M_{t - 1}^{\prime} \pmb {\alpha} + e_{t}
$$ where  $e_t$  denotes the error term. Under the assumption that  $x_{t}$  is a linear  $\operatorname{AR}(p)$  process, the partial  $F$  statistic follows an  $F$  distribution with degrees of freedom  $g$  and  $T - p - g - 1$ , where  $g = p(p + 1) / 2$ . We refer to this  $F$  test as the Ori-F test. Luukkonen, Saikkonen, and Teräsvirta (1988) further extend the test by augmenting  $M_{t - 1}$  with cubic terms  $x_{t - i}^{3}$  for  $i = 1,\dots ,p$ .


# Threshold Test

When the alternative model under study is a SETAR model, one can derive specific test statistics to increase the power of the test. One of the specific tests is the likelihood ratio statistic. This test, however, encounters the difficulty of undefined parameters under the null hypothesis of linearity because the threshold is undefined for a linear AR process. Another specific test seeks to transform testing threshold nonlinearity into detecting model changes. It is then interesting to discuss the differences between these two specific tests for threshold nonlinearity.

To simplify the discussion, let us consider the simple case that the alternative model is a 2-regime SETAR model with threshold variable  $x_{t - d}$ . The null hypothesis  $H_{o}$ :  $x_{t}$  follows the lin ear AR  $(p)$  model


$$ x_{t} = \phi_{0} + \sum_{i = 1}^{p} \phi_{i} x_{t - i} + a_{t} \tag {45}
$$ whereas the alternative hypothesis  $H_{a}:x_{t}$  follows the SETAR model


$$ x_{t} = \left\{ \begin{array}{l l} \phi_{0}^{(1)} + \sum_{i = 1}^{p} \phi_{i}^{(1)} x_{t - i} + a_{1 t} & \text{if } x_{t - d} <   r_{1} \\ \phi_{0}^{(2)} + \sum_{i = 1}^{p} \phi_{i}^{(2)} x_{t - i} + a_{2 t} & \text{if } x_{t - d} \geq r_{1} \end{array} \right. \tag {46}
$$ where  $r_1$  is the threshold. For a given realization  $\{x_t\}_{t=1}^T$  and assuming normality let  $l_0(\hat{\phi}, \hat{\sigma}_a^2)$  be the log likelihood function evaluated at the maximum likelihood estimates of  $\phi = (\phi_0, \dots, \phi_p)'$  and  $\sigma_a^2$ . This is easy to compute. The likelihood function under the alternative is also easy to compute if the threshold  $r_1$  is given. Let  $l_1(r_1; \hat{\phi}_1, \hat{\sigma}_1^2; \hat{\phi}_2, \hat{\sigma}_2^2)$  be the log likelihood function evaluated at the maximum likelihood estimates of  $\phi_i = (\phi_0^{(i)}, \dots, \phi_p^{(i)})'$  and  $\sigma_i^2$  conditioned on knowing the threshold  $r_1$ . The log likelihood ratio  $l(r_1)$  defined as


$$ l (r_{1}) = l_{1} (r_{1}; \hat {\phi}_{1}, \hat {\sigma}_{1}^{2}; \hat {\phi}_{2}, \hat {\phi}_{2}^{2}) - l_{0} (\hat {\phi}, \hat {\phi}_{a}^{2})
$$ is then a function of the threshold  $r_1$ , which is unknown. Yet under the null hypothesis, there is no threshold and  $r_1$  is not defined. The parameter  $r_1$  is referred to as a nuisance parameter under the null hypothesis. Consequently, the asymptotic distribution of the likelihood ratio is very different from that of the conventional likelihood ratio statistics. (See Chan, 1991, for further details and critical values of the test.) A common approach is to use  $l_{\mathrm{max}} = \sup_{v < r_1 < u} l(r_1)$  as the test statistic, where  $v$  and  $u$  are prespecified lower and upper bounds of the threshold. Davis (1987) and Andrews and Ploberger (1994) provide further discussion on hypothesis testing involving nuisance parameters under the null hypothesis. Simulation is often used to obtain empirical critical values of the test statistic  $l_{\mathrm{max}}$ , which depends on the choices of  $v$  and  $u$ . The average of  $l(r_1)$  over  $r_1 \in [v, u]$  is also considered by Andrews and Ploberger as a test statistic.


Tsay (1989) makes use of arranged autoregression and recursive estimation to derive an alternative test for threshold nonlinearity. The arranged autoregression seeks to transfer the SETAR model under the alternative hypothesis  $H_{a}$  into a model change problem with the threshold  $r_1$  serving as the change point. To see this, the SETAR model in equation (46) says that  $x_{t}$  follows essentially two linear models depending on whether  $x_{t - d} < r_1$  or  $x_{t - d} \geq r_1$ . For a realization  $\{x_{t}\}_{t = 1}^{T}$ ,  $x_{t - d}$  can assume values  $\{x_{1},\ldots ,X_{T - d}\}$ . Let  $x_{(1)} \leq x_{(2)} \leq \dots \leq x_{(T - d)}$  be the ordered statistics of  $\{x_{t}\}_{t = 1}^{T - d}$  (i.e., arranging the observations in increasing order). The SETAR model can then be written as

$$
\begin{array}{l} x_{(j) + d} = \beta_{0} + \sum_{i = 1}^{p} \beta_{i} x_{(j) + d - i} + a_{(j) + d}, \tag {47} \\ j = 1, \ldots , T - d \\ \end{array}
$$ where  $\beta_{i} = \phi_{i}^{(1)}$  if  $x_{(j)} < r_1$  and  $\beta_{i} = \phi_{i}^{(2)}$  if  $x_{(j)}\geq$ $r_1$  . Consequently, the threshold  $r_1$  is a change point for the linear regression in equation (47), and we refer to equation (47) as an arranged autoregression (in increasing order of the threshold  $x_{t - d}$  ). Note that the arranged autoregression in (47) does not alter the dynamic dependence of  $x_{t}$  on  $x_{t - i}$  for  $i = 1,\dots ,p$  because  $x_{(j) + d}$  still depends on  $x_{(j) + d - i}$  for  $i = 1,\ldots ,p$  . What is done is simply to present the SETAR model in the threshold space instead of in the time space. That is, the equation with a smaller  $x_{t - d}$  appears before that with a larger  $x_{t - d}$  . The threshold test of Tsay (1989) is obtained as follows.


- Step 1. Fit equation (47) using  $j = 1, \dots, m$ , where  $m$  is a prespecified positive integer (e.g., 30). Denote the least squares estimates of  $\beta_{i}$  by  $\hat{\beta}_{i,m}$ , where  $m$  denotes the number of data points used in estimation.
- Step 2. Compute the predictive residual

$$
\hat {a}_{(m + 1) + d} = x_{(m + 1) + d} - \hat {\beta}_{0, m} - \sum_{i = 1}^{p} \hat {\beta}_{i, m} x_{(m + 1) + d - i}
$$ and its standard error. Let  $\hat{e}_{(m + 1) + d}$  be the standardized predictive residual.


- Step 3. Use the recursive least squares method to update the least squares estimates to  $\hat{\beta}_{i,m+1}$  by incorporating the new data point  $x_{(m+1)+d}$ .
- Step 4. Repeat steps 2 and 3 until all data points are processed.
- Step 5. Consider the linear regression of the standardized predictive residual

$$
\begin{array}{l} \hat {e}_{(m + j) + d} = \alpha_{0} + \sum_{i = 1}^{p} \alpha_{i} x_{(m + j) + d - i} + v_{t}. \tag {48} \\ j = 1, \dots , T - d - m \\ \end{array}
$$ and compute the usual  $F$  statistic for testing  $\alpha_{i} = 0$  in equation (48) for  $i = 0,\ldots ,p$ . Under the null hypothesis that  $x_{t}$  follows a linear  $\operatorname{AR}(p)$  model, the  $F$  ratio has a limiting  $F$  distribution with degrees of freedom  $p + 1$  and  $T - d - m - p$ .


We refer to the earlier  $F$  test as a TAR-F test. The idea behind the test is that under the null hypothesis there is no model change in the arranged autoregression in equation (47) so that the standardized predictive residuals should be close to IID with mean zero and variance 1. In this case, they should have no correlations with the regressors  $x_{(m + j) + d - i}$ . For further details including formulas for a recursive least squares method and some simulation study on performance of the TAR-F test, see Tsay (1989). The TAR-F test avoids the problem of nuisance parameters encountered by the likelihood ratio test. It does not require knowing the threshold  $r_1$ . It simply tests that the predictive residuals have no correlations with regressors if the null hypothesis holds. Therefore, the test does not depend on knowing the number of regimes in the alternative model. Yet the TAR-F test is not as powerful as the likelihood ratio test if the true model is indeed a 2-regime SETAR model with a known innovational distribution.

# Applications

In this subsection, we apply some of the nonlinearity tests discussed previously to five time series. For a real financial time series, an AR model is used to remove any serial correlation

Table 2 Nonlinearity Tests for Simulated Series and Some Log Stock Returns

<table><tr><td></td><td>Q</td><td>Q</td><td></td><td colspan="3">BDS (δ = 1.5σa)</td></tr><tr><td>Data</td><td>(5)</td><td>(10)</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>N(0,1)</td><td>3.2</td><td>6.5</td><td>-0.32</td><td>-0.14</td><td>-0.15</td><td>-0.33</td></tr><tr><td>t6</td><td>0.9</td><td>1.7</td><td>-0.87</td><td>-1.18</td><td>-1.56</td><td>-1.71</td></tr><tr><td>ln(ew)</td><td>2.9</td><td>4.9</td><td>9.94</td><td>11.72</td><td>12.83</td><td>13.65</td></tr><tr><td>ln(vw)</td><td>1.0</td><td>9.8</td><td>8.61</td><td>9.88</td><td>10.70</td><td>11.29</td></tr><tr><td>ln.ibm)</td><td>0.6</td><td>7.1</td><td>4.96</td><td>6.09</td><td>6.68</td><td>6.82</td></tr><tr><td></td><td></td><td>d = 1</td><td></td><td colspan="3">BDS(δ = σa)</td></tr><tr><td>Data</td><td>Ori-F</td><td>TAR-F</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>N(0,1)</td><td>1.13</td><td>0.87</td><td>-0.77</td><td>-0.71</td><td>-1.04</td><td>-1.27</td></tr><tr><td>t6</td><td>0.69</td><td>0.81</td><td>-0.35</td><td>-0.76</td><td>-1.25</td><td>-1.49</td></tr><tr><td>ln(ew)</td><td>5.05</td><td>6.77</td><td>10.01</td><td>11.85</td><td>13.14</td><td>14.45</td></tr><tr><td>ln(vw)</td><td>4.95</td><td>6.85</td><td>7.01</td><td>7.83</td><td>8.64</td><td>9.53</td></tr><tr><td>ln.ibm)</td><td>1.32</td><td>1.51</td><td>3.82</td><td>4.70</td><td>5.45</td><td>5.72</td></tr></table> in the data, and the tests apply to the residual series of the model. The five series employed are as follows:

Note: The sample size of simulated series is 500 and that of stock returns is 864. The BDS test uses  $k = 2, \ldots, 5$ .


1.  $r_{1t}$ : A simulated series of IID  $N(0,1)$  with 500 observations.
2.  $r_{2t}$ : A simulated series of IID Student- $t$  distribution with 6 degrees of freedom. The sample size is 500.
3.  $a_{3t}$ : The residual series of monthly log returns of CRSP equal-weighted index from 1926 to 1997 with 864 observations. The linear AR model used is

$$
\begin{array}{l} (1 - 0. 1 8 0 B + 0. 0 9 9 B^{3} - 0. 1 0 5 B^{9}) r_{3 t} \\ = 0. 0 0 8 6 + a_{3 t} \\ \end{array}
$$

4.  $a_{4t}$ : The residual series of monthly log returns of CRSP value-weighted index from 1926 to 1997 with 864 observations. The linear AR model used is

$$
\begin{array}{l} (1 - 0. 0 9 8 B + 0. 1 1 1 B^{3} - 0. 0 8 8 B^{5}) r_{4 t} \\ = 0. 0 0 7 8 + a_{4 t} \\ \end{array}
$$

5.  $a_{5t}$ : The residual series of monthly log returns of IBM stock from 1926 to 1997 with 864 observations. The linear AR model used is

$$
(1 - 0. 0 7 7 B) r_{5 t} = 0. 0 1 1 + a_{5 t}
$$

Table 2 shows the results of the nonlinearity test. For the simulated series and IBM returns, the  $F$  tests are based on an AR(6) model. For the index returns, the AR order is the same as the model given earlier. For the BDS test, we chose  $\delta = \hat{\sigma}_a$  and  $\delta = 1.5\hat{\sigma}_a$  with  $k = 2,\dots,5$ . Also given in the table are the Ljung-Box statistics that confirm no serial correlation in the residual series before applying nonlinearity tests. Compared with their asymptotic critical values, the BDS test and  $F$  tests are insignificant at the 5\% level for the simulated series. However, the BDS tests are highly significant for the real financial time series. The  $F$  tests also show significant results for the index returns, but they fail to suggest nonlinearity in the IBM log returns. In summary, the tests confirm that the simulated series are linear and suggest that the stock returns are nonlinear.

# 1 MODELING

Nonlinear time series modeling necessarily involves subjective judgment. However, there are some general guidelines to follow. It starts with building an adequate linear model on which nonlinearity tests are based. For financial time series, the Ljung-Box statistics and Engle's test are commonly used to detect conditional heteroscedasticity. For general series, other tests discussed in the previous section apply. If nonlinearity is statistically significant, then one chooses a class of nonlinear models to entertain. The selection here may depend on the experience of the analyst and the substantive matter of the problem under study.


For volatility models, the order of an ARCH process can often be determined by checking the partial autocorrelation function of the squared series. For GARCH and exponential GARCH models, only lower orders such as (1,1), (1,2), and (2,1) are considered in most applications. Higher order models are hard to estimate and understand. For TAR models, one may use the procedures given in Tong (1990) and Tsay (1989, 1998) to build an adequate model. When the sample size is sufficiently large, one may apply nonparametric techniques to explore the nonlinear feature of the data and choose a proper nonlinear model accordingly; see Chen and Tsay (1993a) and Cai, Fan, and Yao (2000). The MARS procedure of Lewis and Stevens (1991) can also be used to explore the dynamic structure of the data.

Finally, information criteria such as the Akaike information criterion (Akaike, 1974) and the generalized odd ratios in Chen, McCulloch, and Tsay (1997) can be used to discriminate between competing nonlinear models. The chosen model should be carefully checked before it is used for prediction.

# FORECASTING

Unlike the linear model, there exist no closed-form formulas to compute forecasts of most nonlinear models when the forecast horizon is greater than 1. We use parametric bootstraps to compute nonlinear forecasts. It is understood that the model used in forecasting has been rigorously checked and is judged to be adequate for the series under study. By a model, we mean the dynamic structure and innovational distri butions. In some cases, we may treat the estimated parameters as given.


# Parametric Bootstrap

Let  $T$  be the forecast origin and  $\ell$  be the forecast horizon  $(\ell >0)$ . That is, we are at time index  $T$  and interested in forecasting  $x_{T + \ell}$ . The parametric bootstrap considered computes realizations  $x_{T + 1},X_{T + \ell}$  sequentially by (a) drawing a new innovation from the specified innovative distribution of the model, and (b) computing  $x_{T + 1}$  using the model, data, and previous forecasts  $x_{T + 1},\ldots ,x_{T + i - 1}$ . This results in a realization for  $x_{T + \ell}$ . The procedure is repeated  $M$  times to obtain  $M$  realizations of  $x_{T + \ell}$  denoted by  $\{x_{T + \ell}^{(j)}\}_{j = 1}^{M}$ . The point forecast of  $X_{T + \ell}$  is then the sample average of  $x_{T + \ell}^{(j)}$ . Let the forecast be  $x_{T}(\ell)$ . We used  $M = 3000$  in some applications and the results seem fine. The realizations  $\{x_{T + \ell}^{(j)}\}_{j = 1}^{M}$  can also be used to obtain an empirical distribution of  $x_{T + \ell}$ . We make use of this empirical distribution later to evaluate forecasting performance.

# Forecasting Evaluation

There are many ways to evaluate the forecasting performance of a model, ranging from directional measures to magnitude measures to distributional measures. A directional measure considers the future direction (up or down) implied by the model. Predicting that tomorrow's S&P 500 index will go up or down is an example of directional forecasts that are of practical interest. Predicting the year-end value of the daily S&P 500 index belongs to the case of magnitude measure. Finally, assessing the likelihood that the daily S&P 500 index will go up  $10\%$  or more between now and the year end requires knowing the future conditional probability distribution of the index. Evaluating the accuracy of such an assessment needs a distributional measure.

In practice, the available data set is divided into two subsamples. The first subsample of the data is used to build a nonlinear model, and the second subsample is used to evaluate the forecasting performance of the model. We refer to the two subsamples of data as estimation and forecasting subsamples. In some studies, a rolling forecasting procedure is used in which a new data point is moved from the forecasting subsample into the estimation subsample as the forecast origin advances. In what follows, we briefly discuss some measures of forecasting performance that are commonly used in the literature. Keep in mind, however, that there exists no widely accepted single measure to compare models. A utility function based on the objective of the forecast might be needed to better understand the comparison.


# Directional Measure

A typical measure here is to use a  $2 \times 2$  contingency table that summarizes the number of "hits" and "misses" of the model in predicting ups and downs of  $x_{T + \ell}$  in the forecasting subsample. Specifically, the contingency table is given as

<table><tr><td>Actual</td><td colspan="3">Predicted</td></tr><tr><td></td><td>up</td><td>down</td><td></td></tr><tr><td>up</td><td>m11</td><td>m12</td><td>m10</td></tr><tr><td>down</td><td>m21</td><td>m22</td><td>m20</td></tr><tr><td></td><td>m01</td><td>m02</td><td>m</td></tr></table> where  $m$  is the total number of  $\ell$ -step ahead forecasts in the forecasting subsample,  $m_{11}$  is the number of "hits" in predicting upward movements,  $m_{21}$  is the number of "misses" in predicting downward movements of the market, and so on. Larger values in  $m_{11}$  and  $m_{22}$  indicate better forecasts. The test statistic


$$
X^{2} = \sum_{i = 1}^{2} \sum_{j = 1}^{2} \frac{\left(m_{i j} - m_{i 0} m_{0 j} / m\right)^{2}}{m_{i 0} m_{0 j} / m}
$$ can then be used to evaluate the performance of the model. A large  $\chi^2$  signifies that the model outperforms the chance of random choice. Under some mild conditions,  $\chi^2$  has an asymptotic chi-squared distribution with 1 degree of free


dom. For further discussion of this measure, see Dahl and Hylleberg (1999).

For illustration of the directional measure, consider the 1-step ahead probability forecasts of the 8-4-1 feed-forward neural network shown in Figure 9. The  $2 \times 2$  table of "hits" and "misses" of the network is

<table><tr><td>Actual</td><td colspan="3">Predicted</td></tr><tr><td></td><td>up</td><td>down</td><td></td></tr><tr><td>up</td><td>12</td><td>2</td><td>14</td></tr><tr><td>down</td><td>8</td><td>2</td><td>10</td></tr><tr><td></td><td>20</td><td>4</td><td>24</td></tr></table>

The table shows that the network predicts the upward movement well, but fares poorly in forecasting the downward movement of the stock. The chi-squared statistic of the table is 0.137 with 77-value 0.71. Consequently, the network does not significantly outperform a random-walk model with equal probabilities for "upward" and "downward" movements.

# Magnitude Measure

Three statistics are commonly used to measure performance of point forecasts. They are the mean squared error (MSE), mean absolute deviation (MAD), and mean absolute percentage error (MAPE). For  $\ell$ -step ahead forecasts, these measures are defined as

$$
M S E (\ell) = \frac{1}{m} \sum_{j = 0}^{m - 1} \left[ x_{T + \ell + j} - x_{T + j} (\ell) \right]^{2} \tag {49}
$$

$$
M A D (\ell) = \frac{1}{m} \sum_{j = 0}^{m - 1} | x_{T + \ell + j} - x_{T + j} (\ell) | \tag {50}
$$

$$
M A P E (\ell) = \frac{1}{m} \sum_{j = 0}^{m - 1} \left| \frac{x_{T + j} (\ell)}{x_{T + j + \ell}} - 1 \right| \tag {51}
$$ where  $m$  is the number of  $\ell$ -step ahead forecasts available in the forecasting subsample.


In application, one often chooses one of the above three measures, and the model with the smallest magnitude on that measure is regarded as the best  $\ell$ -step ahead forecasting model. It is possible that different  $\ell$  may result in selecting different models. The measures also have other limitations in model comparison; see, for instance, Clements and Hendry (1993).

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/4dcd9ad535e22dc13e5e71cfe2581767c7f56f0c69e209ae33fc35f3c291b187.jpg)
Figure 11 Time Plot of the U.S. Quarterly Unemployment Rate, Seasonally Adjusted, from 1948 to 1993


# Distributional Measure

Practitioners recently began to assess forecasting performance of a model using its predictive distributions. Strictly speaking, a predictive distribution incorporates parameter uncertainty in forecasts. We call it conditional predictive distribution if the parameters are treated as fixed. The empirical distribution of  $x_{T + \ell}$  obtained by the parametric bootstrap is a conditional predictive distribution. This empirical distribution is often used to compute a distributional measure. Let  $u_{T}(\ell)$  be the percentile of the observed  $x_{T + \ell}$  in the prior empirical distribution. We then have a set of  $m$  percentiles  $\{u_{T + j}(\ell)\}_{j = 0}^{m - 1}$ , where again  $m$  is the number of  $\ell$ -step ahead forecasts in the forecasting subsample. If the model entertained is adequate,  $\{u_{T + j}(\ell)\}$  should be a random sample from the uniform distribution on [0, 1]. For a sufficiently large  $m$ , one can compute the Kolmogorov-Smirnov statistic of  $\{u_{T + j}(\ell)\}$  with respect to uniform [0, 1]. The statistic can be used for both model checking and forecasting comparison.

# 2 APPLICATION

In this section, we illustrate nonlinear time series models by analyzing the quarterly U.S. civilian unemployment rate, seasonally adjusted, from 1948 to 1993. This series was analyzed in detail by Montgomery, Zarnowitz, Tsay, and Tiao (1998). We repeat some of the analyses here using nonlinear models. Figure 11 shows the time plot of the data. Well-known characteristics of the series include that (a) it tends to move countercyclically with U.S. business cycles, and (b) the rate rises quickly but decays slowly. The latter characteristic suggests that the dynamic structure of the series is nonlinear.

Denote the series by  $x_{t}$  and let  $\Delta x_{t} = x_{t} - x_{t - 1}$  be the change in unemployment rate. The linear model

$$
\begin{array}{l} (1 - 0. 3 1 B^{4}) (1 - 0. 6 5 B) \Delta x_{t} = (1 - 0. 7 8 B^{4}) a_{t}, \\ \hat {\sigma}_{a}^{2} = 0. 0 9 0 \tag {52} \\ \end{array}
$$ was built by Montgomery et al. (1998), where the standard errors of the three coefficients are 0.11, 0.06, and 0.07, respectively. This is a seasonal model even though the data were seasonally adjusted. It indicates that the seasonal adjustment procedure used did not successfully remove the seasonality. This model is used as a benchmark model for forecasting comparison.


Table 3 Nonlinearity Test for Changes in the U.S. Quarterly Unemployment Rate: 1948.II-1993.IV

<table><tr><td>Type</td><td>Ori-F</td><td>LST</td><td>TAR(1)</td><td>TAR(2)</td><td>TAR(3)</td><td>TAR(4)</td></tr><tr><td>Test</td><td>2.80</td><td>2.83</td><td>2.41</td><td>2.16</td><td>2.84</td><td>2.98</td></tr><tr><td>p value</td><td>.0007</td><td>.0002</td><td>.0298</td><td>.0500</td><td>.0121</td><td>.0088</td></tr></table>

Note: An AR(5) model was used in the tests, where LST denotes the test of Luukkonen et al. (1988) and TAR(d) means threshold test with delay  $d$ .

To test for nonlinearity, we apply some of the nonlinearity tests discussed earlier in this entry with an AR(5) model for the differenced series  $\Delta x_{t}$ . The results are given in Table 3. All of the tests reject the linearity assumption. In fact, the linearity assumption is rejected for all  $\operatorname{AR}(p)$  models we applied, where  $p = 2,\ldots ,10$ .

Using a modeling procedure similar to that of Tsay (1989), Montgomery et al. (1998) build the following TAR model for the  $\Delta x_{t}$  series:

$$
\Delta x_{t} = \left\{ \begin{array}{c} 0. 0 1 + 0. 7 3 \Delta x_{t - 1} + 0. 1 0 \Delta x_{t - 2} + a_{1 t} \\ \text{if } \Delta x_{t - 2} \leq 0. 1, \\ 0. 1 8 + 0. 8 0 \Delta x_{t - 1} - 0. 5 6 \Delta x_{t - 2} + a_{2 t} \\ \text{ot he rw is e} \end{array} \right. \tag {53}
$$

The sample variances of  $a_{1t}$  and  $a_{2t}$  are 0.76 and 0.165, respectively, the standard errors of the three coefficients of regime 1 are 0.03, 0.10, and 0.12, respectively, and those of regime 2 are 0.09, 0.1, and 0.16. This model says that the change in the U.S. quarterly unemployment rate,  $\Delta x_{t}$ , behaves like a piecewise linear model in the reference space of  $x_{t-2} - x_{t-3}$  with threshold 0.1. Intuitively, the model implies that the dynamics of unemployment act differently depending on the recent change in the unemployment rate. In the first regime, the unemployment rate has had either a decrease or a minor increase. Here the economy should be stable, and essentially the change in the rate follows a simple AR(1) model because the lag-2 coefficient is insignificant. In the second regime, there is a substantial jump in the unemployment rate (0.1 or larger). This typically corresponds to the contraction phase in the business cycle. It is also the period during which government interventions and industrial restructuring are likely to occur. Here  $\Delta x_{t}$  fol lows an AR(2) model with a positive constant, indicating an upward trend in  $x_{t}$ . The AR(2) polynomial contains two complex characteristic roots, which indicate possible cyclical behavior in  $\Delta x_{t}$ . Consequently, the chance of having a turning point in  $x_{t}$  increases, suggesting that the period of large increases in  $x_{t}$  should be short. This implies that the contraction phases in the U.S. economy tend to be shorter than the expansion phases.


Applying a Markov chain Monte Carlo method, Montgomery et al. (1998) obtain the following Markov switching model for  $\Delta x_{t}$ :

$$
\Delta x_{t} = \left\{ \begin{array}{c} - 0. 0 7 + 0. 3 8 \Delta x_{t - 1} - 0. 0 5 \Delta x_{t - 2} + \epsilon_{1 t} \\ \text{if } s_{t} = 1 \\ 0. 1 6 + 0. 8 6 \Delta x_{t - 1} - 0. 3 8 \Delta x_{t - 2} + \epsilon_{2 t} \\ \text{if } s_{t} = 2 \end{array} \right. \tag {54}
$$

The conditional means of  $\Delta x_{t}$  are  $-0.10$  for  $s_t = 1$  and 0.31 for  $s_t = 2$ . Thus, the first state represents the expansionary periods in the economy, and the second state represents the contractions. The sample variances of  $\epsilon_{1t}$  and  $\epsilon_{2t}$  are 0.031 and 0.192, respectively. The standard errors of the three parameters in state  $s_t = 1$  are 0.03, 0.14, and 0.11, and those of state  $s_t = 2$  are 0.04, 0.13, and 0.14, respectively. The state transition probabilities are  $P(s_t = 2|s_{t - 1} = 1) = 0.084(0.060)$  and  $P(s_{t} = 1|s_{t - 1} = 2) = 0.126(0.053)$ , where the number in parentheses is the corresponding standard error. This model implies that in the second state the unemployment rate  $x_{t}$  has an upward trend with an AR(2) polynomial possessing complex characteristic roots. This feature of the model is similar to the second regime of the TAR model in equation (53). In the first state, the unemployment rate  $x_{t}$  has a slightly decreasing trend with a much weaker autoregressive structure.


# Forecasting Performance

A rolling procedure was used by Montgomery et al. (1998) to forecast the unemployment rate  $x_{t}$ . The procedure works as follows:

1. Begin with forecast origin  $T = 83$ , corresponding to 1968.II, which was used in the literature to monitor the performance of various econometric models in forecasting unemployment rate. Estimate the linear, TAR, and MSA models using the data from 1948.I to the forecast origin (inclusive).
2. Perform 1-quarter to 5-quarter ahead forecasts and compute the forecast errors of each model. Forecasts of nonlinear models used are computed by using the parametric bootstrap method explained earlier in this entry.
3. Advance the forecast origin by 1 and repeat the estimation and forecasting processes until all data are employed.
4. Use MSE and mean forecast error to compare performance of the models.

Table 4 shows the relative MSE of forecasts and mean forecast errors for the linear model in equation (52), the TAR model in equation (53), and the MSA model in equation (54), using the linear model as a benchmark. The comparisons are based on overall performance as well as the status of the U.S. economy at the forecast origin. From the table, we make the following observations:

1. For the overall comparison, the TAR model and the linear model are very close in MSE, but the TAR model has smaller biases. Yet the MSA model has the highest MSE and smallest biases.
2. For forecast origins in economic contractions, the TAR model shows improvements over the linear model both in MSE and bias. The MSA model also shows some improvement over the linear model, but the improvement is not as large as that of the TAR model.

Table 4 Out-of-Sample Forecast Comparison Among Linear, TAR, and MSA Models for the U.S. Quarterly Unemployment Rate

<table><tr><td>(A)</td><td colspan="5">Relative MSE of Forecast</td></tr><tr><td>Model</td><td>1-step</td><td>2-step</td><td>3-step</td><td>4-step</td><td>5-step</td></tr><tr><td colspan="6">(a) Overall Comparison</td></tr><tr><td>Linear</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td>TAR</td><td>1.00</td><td>1.04</td><td>0.99</td><td>0.98</td><td>1.03</td></tr><tr><td>MSA</td><td>1.19</td><td>1.39</td><td>1.40</td><td>1.45</td><td>1.61</td></tr><tr><td>MSE</td><td>0.08</td><td>0.31</td><td>0.67</td><td>1.13</td><td>1.54</td></tr><tr><td colspan="6">(b) Forecast Origins in Economic Contractions</td></tr><tr><td>Linear</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td>TAR</td><td>0.85</td><td>0.91</td><td>0.83</td><td>0.72</td><td>0.72</td></tr><tr><td>MSA</td><td>0.97</td><td>1.03</td><td>0.96</td><td>0.86</td><td>1.02</td></tr><tr><td>MSE</td><td>0.22</td><td>0.97</td><td>2.14</td><td>3.38</td><td>3.46</td></tr><tr><td colspan="6">(c) Forecast Origins in Economic Expansions</td></tr><tr><td>Linear</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td>TAR</td><td>1.06</td><td>1.13</td><td>1.10</td><td>1.15</td><td>1.17</td></tr><tr><td>MSA</td><td>1.31</td><td>1.64</td><td>1.73</td><td>1.84</td><td>1.87</td></tr><tr><td>MSE</td><td>0.06</td><td>0.21</td><td>0.45</td><td>0.78</td><td>1.24</td></tr><tr><td rowspan="2">(B) Model</td><td colspan="5">Mean of Forecast Errors</td></tr><tr><td>1-step</td><td>2-step</td><td>3-step</td><td>4-step</td><td>5-step</td></tr><tr><td colspan="6">(a) Overall Comparison</td></tr><tr><td>Linear</td><td>0.03</td><td>0.09</td><td>0.17</td><td>0.25</td><td>0.33</td></tr><tr><td>TAR</td><td>-0.10</td><td>-0.02</td><td>-0.03</td><td>-0.03</td><td>-0.01</td></tr><tr><td>MSA</td><td>0.00</td><td>-0.02</td><td>-0.04</td><td>-0.07</td><td>-0.12</td></tr><tr><td colspan="6">(b) Forecast Origins in Economic Contractions</td></tr><tr><td>Linear</td><td>0.31</td><td>0.68</td><td>1.08</td><td>1.41</td><td>1.38</td></tr><tr><td>TAR</td><td>0.24</td><td>0.56</td><td>0.87</td><td>1.01</td><td>0.86</td></tr><tr><td>MSA</td><td>0.20</td><td>0.41</td><td>0.57</td><td>0.52</td><td>0.14</td></tr><tr><td colspan="6">(c) Forecast Origins in Economic Expansions</td></tr><tr><td>Linear</td><td>-0.01</td><td>0.00</td><td>0.03</td><td>0.08</td><td>0.17</td></tr><tr><td>TAR</td><td>-0.05</td><td>-0.11</td><td>-0.17</td><td>-0.19</td><td>-0.14</td></tr><tr><td>MSA</td><td>-0.03</td><td>-0.08</td><td>-0.13</td><td>-0.17</td><td>-0.16</td></tr></table>

Note: The starting forecast origin is 1968.II, where the row marked by "MSE" shows the MSE of the benchmark linear model.

3. For forecast origins in economic expansions, the linear model outperforms both nonlinear models.

The results suggest that the contributions of nonlinear models over linear ones in forecasting the U.S. quarterly unemployment rate are mainly in the periods when the U.S. economy is in contraction. This is not surprising because, as mentioned before, it is during the economic contractions that government interventions and industrial restructuring are most likely to occur. These external events could introduce nonlinearity in the U.S. unemployment rate. Intuitively, such improvements are important because it is during the contractions that people pay more attention to economic forecasts.


# KEY POINTS

- Nonlinearity exists in many financial data, including log returns of widely used market indexes such as CRSP equal- and value-weight indexes.
- Nonlinearity also appears in asset volatility. Indeed, simple threshold models such as the threshold GARCH model can be used to better describe the behavior of asset volatility. The model has been used to model the leverage effect between return and volatility.
- Simple nonparametric methods such as the local linear regression method can be used to provide a deeper understanding of interest rate dynamics.
- The unemployment rate example shows that, even though nonlinear models may not outperform linear ones in all forecast origins, they can provide more accurate forecasts when the U.S. economy is under contraction. This is useful because people in general pay more attention to forecasts during economic recession.
- Among the nonlinear models, the Markov switching model has the smallest bias in out-of-sample prediction. The model, however, has a larger mean square of forecast errors than the threshold autoregressive model. This behavior is consistent with the structure of the model because the true states of the economy are never certain under the switching model.

# REFERENCES

Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control AC-19: 716-723.
Andrews, D. W. K., and Ploberger, W. (1994). Optimal tests when a nuisance parameter is present only under the alternative. Econometrica 62: 1383-1414.
Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. (2008). Time Series Analysis: Forecasting and Control, 4th ed. Hoboken, NJ: John Wiley.
Brock, W., Dechert, W. D., and Scheinkman, J. (1987). A test for independence based on the correlation dimension. Working paper, Department of Economics, University of Wisconsin, Madison.
Brock, W., Hsieh, D. A., and LeBaron, B. (1991). Nonlinear Dynamics, Chaos and Instability: Statistical Theory and Economic Evidence. Cambridge, MA: MIT Press.
Bryson, A. E., and Ho, Y. C. (1969). Applied Optimal Control. New York: Blaisdell.
Cai, Z., Fan, J., and Yao, Q. (2000). Functional-coefficient regression models for nonlinear time series. Journal of the American Statistical Association 95: 941-956.
Carlin, B. P., Polson, N. G., and Stoffer, D. S. (1992). A Monte Carlo approach to nonnormal and nonlinear state space modeling. Journal of the American Statistical Association 87: 493-500.
Chan, K. S. (1991). Percentage points of likelihood ratio tests for threshold autoregression. Journal of the Royal Statistical Society Series B 53: 691-696.
Chan, K. S. (1993). Consistency and limiting distribution of the least squares estimator of a threshold autoregressive model. The Annals of Statistics 21: 520-533.
Chan, K. S., and Tong H. (1986). On estimating thresholds in autoregressive models. Journal of Time Series Analysis 7: 179-190.
Chan, K. S., and Tsay, R. S. (1998). Limiting properties of the conditional least squares estimator of a continuous TAR model. Biometrika 85: 413-426.
Chen, C., McCulloch, R. E., and Tsay, R. S. (1997). A unified approach to estimating and modeling univariate linear and nonlinear time series. Statistica Sinica 7: 451-472.
Chen, R., and Tsay, R. S. (1991). On the ergodicity of TAR(1) processes. Annals of Applied Probability 1: 613-634.

