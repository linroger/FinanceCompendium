
# Important Functions and Their Features

MARKUS HÖCHSTÖTTER, PhD

Assistant Professor, University of Karlsruhe

SVETLOZAR T. RACHEV, PhD, Dr Sci

Frey Family Foundation Chair Professor, Department of Applied Mathematics and Statistics,

Stony Brook University, and Chief Scientist, FinAnalytica

FRANK J. FABOZZI, PhD, CFA, CPA

Professor of Finance, EDHEC Business School

Abstract: Probability theory can be understood as a particular field in mathematics. Hence, it is only to be expected that it relies intensely on theory from analysis and algebra. For example, the fact that the cumulative probability over all values a random variable can assume has to be equal to one is not always feasible to check for without a profound knowledge of mathematics. Continuous probability distributions involve a good deal of analysis and the more sophisticated a distribution is, the more mathematics is necessary to handle it.

In this entry, we review the functions that are used in financial modeling: continuous functions, the indicator function, the derivative of a function, monotonic functions, and the integral. Moreover, as special functions, we get to know the factorial, the gamma, beta, and Bessel functions as well as the characteristic function of random variables. (For a more detailed discussion of these functions, see Khuri [2003], MacCluer [2009], and Richardson [2008].)

# CONTINUOUS FUNCTION

In this section, we introduce general continuous functions.

# General Idea

Let  $f(x)$  be a continuous function for some real-valued variable  $x$ . The general idea behind continuity is that the graph of  $f(x)$  does not exhibit gaps. In other words,  $f(x)$  can be thought of as being seamless. We illustrate this in Figure 1. For increasing  $x$ , from  $x = 0$  to  $x = 2$ , we can move along the graph of  $f(x)$  without ever having to jump. In the figure, the graph is generated by the two functions  $f(x) = x^2$  for  $x \in [0,1)$ , and  $f(x) = \ln (x) + 1$  for  $x \in [1,2)$ .


Note that the function  $f(x) = \ln (x)$  is the natural logarithm. It is the inverse function to the exponential function  $g(x) = e^{x}$  where  $e = 2.7183$  is the Euler constant. The inverse has the effect that  $f(g(x)) = \ln(e^{x}) = x$ , that is, in and  $e$  cancel each other out.


A function  $f(x)$  is discontinuous if we have to jump when we move along the graph of the function. For example, consider the graph in Figure 2. Approaching  $x = 1$  from the left, we have to jump from  $f(x) = 1$  to  $f(1) = 0$ . Thus, the function  $f$  is discontinuous at  $x = 1$ . Here,  $f$  is given by  $f(x) = x^2$  for  $x \in [0,1)$ , and  $f(x) = \ln (x)$  for  $x \in [1,2)$ .

Figure 2 Discontinuous Function  $f(x)$
![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/edaeb26878ccd547947913fa9759cadaf3ac8494a3e09a44690754a06d21f8d5.jpg)
Note: For  $x \in [0,1)$ ,  $f(x) = x^2$  and for  $x \in [1,2)$ ,  $f(x) = \ln (x)$ .


# Formal Derivation

For a formal treatment of continuity, we first concentrate on the behavior of  $f$  at a particular value  $x^{*}$ .

We say that a function  $f(x)$  is continuous at  $x^{*}$  if, for any positive distance  $\delta$ , we obtain a related distance  $\varepsilon(\delta)$  such that

$$
\begin{array}{l} f (x ^ {*}) - \delta \leq f (x) \leq f (x ^ {*}) + \delta , \quad \text {f o r a l l} \\ x \in (x ^ {*} - \varepsilon (\delta), x ^ {*} + \varepsilon (\delta)) \\ \end{array}
$$

What does that mean? We use Figure 3 to illustrate. (The function is  $f(x) = \sin (x)$  with  $x^{*} = 0.2$ .) At  $x^{*}$ , we have the value  $f(x^{*})$ . Now, we select a neighborhood around  $f(x^{*})$  of some arbitrary distance  $\delta$  as indicated by the dashed horizontal lines through  $f(x^{*}) - \delta$  and  $f(x^{*}) + \delta$ , respectively. From the intersections of these horizontal lines and the function graph (solid line), we extend two vertical dash-dotted lines down to the  $x$ -axis so that we obtain the two values  $x^{L}$  and  $x^{U}$ , respectively. Now, we measure the distance between  $x^{L}$  and  $x^{*}$  and also the distance between  $x^{U}$  and  $x^{*}$ . The smaller of the two yields the distance  $\varepsilon(\delta)$ . With this distance  $\varepsilon(\delta)$  on the  $x$ -axis, we obtain the environment  $(x^{*} - \varepsilon(\delta), x^{*} + \varepsilon(\delta))$  about  $x^{*}$ . (Note that  $x^{L} = x^{*} - \varepsilon_{\delta}$ , since the distance between  $x^{L}$  and  $x^{*}$  is the shorter one.) The environment is indicated by the dashed lines extending vertically above  $x^{*} - \varepsilon(\delta)$  and  $x^{*} + \varepsilon(\delta)$ , respectively. We require that all  $x$  that lie in  $(x^{*} - \varepsilon(\delta), x^{*} + \varepsilon(\delta))$  yield values  $f(x)$  inside of the environment  $[f(x^{*}) - \delta, f(x^{*}) + \delta]$ . We can see by Figure 3 that this is satisfied.


Let us repeat this procedure for a smaller distance  $\delta$ . We obtain new environments  $[f(x^{*}) - \delta, f(x^{*}) + \delta]$  and  $(x^{*} - \varepsilon(\delta), x^{*} + \varepsilon(\delta))$ . If, for all  $x$  in  $(x^{*} - \varepsilon(\delta), x^{*} + \varepsilon(\delta))$ , the  $f(x)$  are inside of  $[f(x^{*}) - \delta, f(x^{*}) + \delta]$ , again, then we can take an even smaller  $\delta$ . We continue this for successively smaller values of  $\delta$  just short of becoming 0 or until the condition on the  $f(x)$  is no longer satisfied. As we can easily see in Figure 3, we could go on forever and the condition on the  $f(x)$ would always be satisfied. Hence, the graph of  $f$  is seamless or continuous at  $x$ .

Figure 3 Continuity Criterion
![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/9e427783ad549c3e02cbf729b7110e9a5bba388757bb5892bb6a00f6de0b8192.jpg)
Note: Function  $f = \sin (x)$ , for  $-1 \leq x \leq 1$ .


Finally, we say that the function  $f$  is continuous if it is continuous at all  $x$  for which  $f$  is defined, that is, in the domain of  $f$ . Note that only the domain of  $f$  is of interest. For example, the square root function  $f(x) = \sqrt{x}$  is only defined for  $x \geq 0$ . Thus, we do not care about whether  $f$  is continuous for any  $x$  other than  $x \geq 0$ .

# INDICATOR FUNCTION

The indicator function acts like a switch. Often, it is denoted by  $1_{\mathrm{A}}(X)$  where  $A$  is the event of interest and  $X$  is a random variable. So,  $1_{\mathrm{A}}(X)$  is 1 if the event  $A$  is true, that is, if  $X$  assumes a value in  $A$ . Otherwise,  $1_{\mathrm{A}}(X)$  is 0. Formally, this is expressed as

$$
1 _ {A} (X) = \left\{ \begin{array}{l l} 1 & X \in A \\ 0 & \text {o t h e r w i s e} \end{array} \right.
$$

Usually, indicator functions are applied if we are interested in whether a certain event has occurred or not. For example, in a simple way, the value  $V$  of a company may be described by a real numbered random variable  $X$  on  $\Omega = R$  with a particular probability distribution  $P$ . Now, the value  $V$  of the company may be equal to  $X$  as long as  $X$  is greater than 0. In the case where  $X$  assumes a negative value or 0, then  $V$  is automatically 0, that is, the company is bankrupt. So, the event of interest is  $A = [0, \infty)$ , that is, we want to know whether  $X$  is still positive. Using the indicator function this can be expressed as


$$
1 _ {[ 0, \infty)} (X) = \left\{ \begin{array}{l l} 1 & X \in [ 0, \infty) \\ 0 & \mathrm {o t h e r w i s e} \end{array} \right.
$$

Finally, the company value can be given as

$$
V = 1 _ {[ 0, \infty)} (X) \cdot X = \left\{ \begin{array}{l} X X \in [ 0, \infty) \\ 0 \text {o t h e r w i s e} \end{array} \right.
$$

The company value  $V$  as a function is depicted in Figure 4. We can clearly detect the kink at  $x = 0$  where the indicator function becomes 1 and, hence,  $V = X$ .

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/ad5e44d90ba7e9fdf041d86aa0ab022c5ee9ff779c796919a220e5290cedc185.jpg)
Figure 4 The Company Value V as a Function of the Random Variable X Using the Indicator Function  $1_{[0,\infty)}(X)\cdot X$

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/da5ae9fbb79a6dce7e72204d846310c2359359a9792710c544ad1b101f720255.jpg)
Figure 5 Function  $f$  (solid) with Derivatives  $f'(x)$  at  $x$ , for  $0 < x < 0.5$  (dashed),  $x = 1$  (dash-dotted), and  $x = 1.571$  (dotted)

# DERIVATIVES

Suppose we have some continuous function  $f$  with the graph given by the solid line in Figure 5. We now might be interested in the growth rate of  $f$  at some position  $x$ . That is, we might want to know by how much  $f$  increases or decreases when we move from some  $x$  by a step of a given size, say  $\Delta x$ , to the right. This difference in  $f$  we denote by  $\Delta f$ . This  $\Delta$  symbol is called delta.


Let us next have a look at the graphs given by the solid lines in Figure 6. These represent the graphs of  $f$  and  $g$ . The important difference between  $f$  and  $g$  is that, while  $g$  is linear,  $f$  is not, as can be seen by  $f$ 's curvature.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/765360da2c3451abbe66a1440bdd620f980c670dda8ca87279d601db28570c08.jpg)
Figure 6 Functions  $f$  and  $g$  with Slopes Measured at the Points  $(x^{*},f(x^{*}))$  and  $(x^{+},g(x^{+}))$  Indicated by the  $\bullet$  Symbol


We begin the analysis of the graphs' slopes with function  $g$  on the top right of the figure. Let us focus on the point  $(x^{+}, g(x^{+}))$  given by the solid circle at the lower end of graph  $g$ . Now, when we move to the right by  $\Delta x_{4}$  along the horizontal dashed line, the corresponding increase in  $g$  is given by  $\Delta y_{4}$ , as indicated by the vertical dashed line. If, on the other hand, we moved to the right by the longer distance,  $\Delta x_{5}$ , the according increment of  $g$  would be given by  $\Delta y_{5}$ . (This vertical increment  $\Delta y_{5}$  is also indicated by a vertical dashed line.) Since  $g$  is linear, it has constant slope everywhere and, hence, also at the point  $(x^{+}, f(x^{+}))$ . We denote that slope by  $s_{4}$ . This implies that the ratios representing the relative increments (i.e., the slopes) have to be equal. That is,

$$ s _ {4} = \frac {\Delta y _ {4}}{\Delta x _ {4}} = \frac {\Delta y _ {5}}{\Delta x _ {5}}
$$

Next, we focus on the graph of  $f$  on the lower left of Figure 6. Suppose we measured the slope of  $f$  at the point  $(x^{*},f(x^{*}))$ . If we extended a step along the dashed line to the right by  $\Delta x_{1}$ , the corresponding increment in  $f$  would be  $\Delta y_{1}$  as indicated by the leftmost vertical dashed line. If we moved, instead, by the longer  $\Delta x_{2}$  to the right, the corresponding increment in  $f$  would be  $\Delta y_{2}$ . And a horizontal increment of  $\Delta x_{3}$  would result in an increase of  $f$  by  $\Delta y_{3}$ .

In contrast to the graph of  $g$ , the graph of  $f$  does not exhibit the property of a constant increment  $\Delta y$  in  $f$  per unit step  $\Delta x$  to the right. That is, there is no constant slope of  $f$ , which results in the fact that the three ratios of the relative increase of  $f$  are different. To be precise, we have

$$
\frac {\Delta y _ {1}}{\Delta x _ {1}} > \frac {\Delta y _ {2}}{\Delta x _ {2}} > \frac {\Delta y _ {3}}{\Delta x _ {3}}
$$ as can be seen in Figure 6. So, the shorter our step  $\Delta x$  to the right, the steeper the slopes of the thin solid lines through  $(x^{*},f(x^{*}))$  and the corresponding points on the curve,  $(x^{*} + \Delta x_{1}$ $f(x^{*} + \Delta x_{1}))$ $(x^{*} + \Delta x_{2},f(x^{*} + \Delta x_{2}))$  ,and  $(x^{*} + \Delta x_{2},$


$f(x^{*} + \Delta x_{2}))$  , respectively. That means that, the smaller the increment  $\Delta x$  , the higher the relative increment  $\Delta y$  of  $f$  . So, finally, if we moved only a minuscule step to the right from  $(x^{*},f(x^{*}))$  , we would obtain the steepest thin line and, consequently, the highest relative increase in  $f$  given by

$$
\frac {\Delta y}{\Delta x} \tag {1}
$$

By letting  $\Delta x$  approach 0, we obtain the marginal increment, in case the limit of (1) exists (i.e., if the ratio has a finite limit). Formally,

$$
\frac {\Delta y}{\Delta x} \xrightarrow {\Delta x \to 0} s (x) \quad \text {w i t h} \quad - \infty <   s (x) <   \infty
$$

This marginal increment  $s(x)$  is different, at any point on the graph of  $f$ , while we have seen that it is constant for all points on the graph of  $g$ .

# Construction of the Derivative

The limit analysis of marginal increments now brings us to the notion of a derivative that we discuss next. Earlier we introduced the limit growth rate of some continuous function at some point  $(x_0, f(x_0))$ . To represent the slope of the line through  $(x_0, f(x_0))$  and  $(x_0 + \Delta x, f(x_0 + \Delta x))$ , we define the difference quotient

$$
\frac {f \left(x _ {0} + \Delta x\right) - f \left(x _ {0}\right)}{\Delta x} \tag {2}
$$

If we let  $\Delta x\to 0$  , we obtain the limit of the difference quotient (2). If this limit is not finite, then we say that it does not exist. Suppose we were not only interested in the behavior of  $f$  when moving  $\Delta x$  to the right but also wanted to analyze the reaction by  $f$  to a step  $\Delta x$  to the left. We would then obtain two limits of (2). The first with  $\Delta x^{+} > 0$  (i.e., a step to the right) would be the upper limit  $L^U$

$$
\frac {f (x _ {0} + \Delta x ^ {+}) - f (x _ {0})}{\Delta x ^ {+}} \xrightarrow {\Delta x ^ {+} \rightarrow 0} L ^ {U}
$$ and the second with  $\Delta x^{-} < 0$  (i.e., a step to the left), would be the lower limit  $L^L$


$$
\frac {f \left(x _ {0} + \Delta x ^ {-}\right) - f \left(x _ {0}\right)}{\left| \Delta x ^ {-} \right|} \xrightarrow {\Delta x ^ {-} \rightarrow 0} L ^ {U}
$$

If  $L^U$  and  $L^L$  are equal,  $L^U = L^L = L$ , then  $f$  is said to be differentiable at  $x_0$ . The limit  $L$  is the derivative of  $f$ . We commonly write the derivative in the fashion

$$ f ^ {\prime} \left(x _ {0}\right) = \left. \frac {d f (x)}{d x} \right| _ {x = x _ {0}} = \left. \frac {d y}{d x} \right| _ {x = x _ {0}} \tag {3}
$$

On the right side of (3), we have replaced  $f(x)$  by the variable  $y$  as we will often do, for convenience. If the derivative (3) exists for all  $x$ , then  $f$  is said to be differentiable.

Let us now return to Figure 5. Recall that the graph of the continuous function  $f$  is given by the solid line. We start at  $x = -1$ . Since  $f$  is not continuous at  $x = -1$ , we omit this end point (1,1) from our analysis. For  $-1 < x < 0$ , we have that  $f$  is constant with slope  $s = -1$ . Consequently, the derivative  $f'(x) = -1$ , for these  $x$ .

At  $x = 0$ , we observe that  $f$  is linear to the left with  $f'(x) = -1$  and that it is also linear to the right, however, with  $f'(x) = 1$ , for  $0 < x < 0.5$ . So, at  $x = 0, L^U = 1$  while  $L^L = -1$ . Since here  $L^U \neq L^L$ , the derivative of  $f$  does not exist at  $x = 0$ .

For  $0 < x < 0.5$ , we have the constant derivative  $f'(x) = 1$ . The corresponding slope of 1 through  $(0,0)$  and  $(0.5,0.5)$  is indicated by the dashed line. At  $x = 0.5$ , the left side limit  $L^L = 1$  while the right side limit  $L^U = 0.8776$ . (This value of  $\cos(0.5) = 0.8776$  is a result from calculus.) Hence, the two limits are not equal and, consequently,  $f$  is not differentiable at  $x = 0.5$ .

Without formal proof, we state that  $f$  is differentiable for all  $0.5 < x < 2$ . For example, at  $x = 1$ ,  $L^L = L^U = 0.5403$  and, thus, the derivative  $f'(1) = 0.5403$ . The dash-dotted line indicating this derivative is called the tangent of  $f$  at  $x = 1$ . In Figure 5, the arrow indexed  $f'(1)$  points at this tangent. As another example, we select  $x = 1.571$  where  $f$  assumes its maximum value. Here, the derivative  $f'(1.571) = 0$  and, hence, the tangent at  $x = 1.571$  is flat as indicated by the horizontal dotted line. In Figure 5, the arrow indexed  $f'$  (1.571) points at this tangent.


# MONOTONIC FUNCTION

Suppose we have some function  $f(x)$  for real-valued  $x$ . For example, the graph of  $f$  may look like that in Figure 7. We see that on the interval [0,1], the graph is increasing from  $f(0) = 0$  to  $f(1) = 1$ . For  $1 \leq x \leq 2$ , the graph remains at the level  $f(1) = 1$  like a platform. And, finally, between  $x = 2$  and  $x = 3$ , the graph is increasing, again, from  $f(2) = 1$  to  $f(3) = 2$ .

In contrast, we may have another function,  $g(x)$ . Its graph is given by Figure 8. It looks somewhat similar to the graph in Figure 7, however, without the platform. The graph of  $g$  never remains at a level, but increases constantly. Even for the smallest increments from one value of  $x$ , say  $x_{1}$ , to the next higher, say  $x_{2}$ , there is always an upward slope in the graph.

Both functions,  $f$  and  $g$ , never decrease. The distinction is that  $f$  is monotonically increasing since the graph can remain at some level, while  $g$  is strictly monotonic increasing since its graph never remains at any level. If we can differentiate  $f$  and  $g$ , we can express this in terms of the derivatives of  $f$  and  $g$ . Let  $f'$  be the derivative of  $f$  and  $g'$  the derivative of  $g$ . Then, we have the following definitions of continuity for continuous functions with existing derivatives:

Monotonically increasing functions: A continuous function  $f$  with derivative  $f'$  is monotonically increasing if its derivative  $f' \geq 0$ .

Strictly monotonic increasing functions: A continuous function  $g$  with derivative  $g'$  is strictly monotonic increasing if its derivative  $g' > 0$ .

Analogously, a function  $f(x)$  is monotonically decreasing if it behaves in the opposite manner. That is,  $f$  never increases when moving from some  $x$  to any higher value  $x_{1} > x$ . When  $f$  is continuous with derivative  $f'$ , then we say that  $f$  is monotonically decreasing if  $f'(x) \leq 0$  and that it is strictly monotonic increasing if  $f'(x) < 0$  for all  $x$ . For these two cases, illustrations are given by mirroring the graphs in Figures 7 and 8 against their vertical axes, respectively.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/8a81a679d0d3dbd0db5b0f9c9c95cf6efa76232a905cc950a4e3941890ca4b19.jpg)
Figure 7 Monotonically Increasing Function  $f$


# INTEGRAL

Here we derive the concept of integration necessary to understand the probability density and continuous distribution function. The integral of some function over some set of values represents the area between the function values and the horizontal axis. To sketch the idea, we start with an intuitive graphical illustration.


We begin by analyzing the area  $A$  between the graph (solid line) of the function  $f(t)$  and the horizontal axis between  $t = 0$  and  $t = T$  in Figure 9. Looking at the graph, it appears quite complicated to compute this area  $A$  in comparison to, for example, the area of a rectangle where we would only need to know its width and length. However, we can approximate this area by rectangles as will be done next.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/e066ad04270e0384849c44b21932e3e4db9aeab9088e31bd9eaedd0a87138049.jpg)
Figure 8 Strictly Monotonic Increasing Function  $g$

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/fc67d65b323b0ef047786f1247f672e5b960bda64316ec6f108051b64ada5279.jpg)
Figure 9 Approximation of the Area  $A$  between Graph of  $f(t)$  and the Horizontal Axis, for  $0 \leq t \leq T$


# Approximation of the Area through Rectangles

Let's approximate the area  $A$  under the function graph in Figure 9 as follows. As a first step, we dissect the interval between 0 and  $T$  into  $n$  equidistant intervals of length  $\Delta t = t_{i + 1} - t_i$  for  $i = 0,1,\ldots ,n - 1$ . For each such interval, we consider the function value  $f(t_{i + 1})$  at the rightmost point,  $t_{i + 1}$ . To obtain an estimate of the area under the graph for the respective interval, we multiply the value  $f(t_{i + 1})$  at  $t_{i + 1}$  by the interval width  $\Delta t$  yielding  $A$  ( $t_{i + 1}) = \Delta t\cdot f$  ( $t_{i + 1}$ ), which equals the area of the rectangle above interval  $i + 1$  as displayed in Figure 9. Finally, we add up the areas  $A(t_1),A(t_2),\dots,A(T)$  of all rectangles resulting in the desired estimate of the area  $A$

$$
\sum_ {i = 0} ^ {n - 1} A \left(t _ {i + 1}\right) = \sum_ {i = 0} ^ {n - 1} \Delta t \cdot f \left(t _ {i + 1}\right) \tag {4}
$$

We repeat the just described procedure for decreasing interval widths  $\Delta t$ .

# Integral as the Limiting Area

To derive the perfect approximation of the area under the curve in Figure 9, we let the interval width  $\Delta t$  gradually vanish until it almost equals 0, proceeding as before. We denote this infinitesimally small width by the step rate  $dt$ . Now, the difference between the function values at either end, that is,  $f(t_{i})$  and  $f(t_{i + 1})$ , of the interval  $i + 1$  will be nearly indistinguishable since  $t_i$  and  $t_{i + 1}$  almost coincide. Hence, the corresponding rectangle with area  $A(t_{i + 1})$  will turn into a dash with infinitesimally small base  $dt$ .

Summation as in equation (4) of the areas of the dashes becomes infeasible. For this purpose, the integral has been introduced as the limit of (4) as  $\Delta t\to 0$  .(Conditions under which these limits exist are omitted here.) It is denoted by

$$
\int_ {0} ^ {T} f (t) d t \tag {5}
$$ where the limits 0 and  $T$  indicate which interval the integration is performed on. In our case, the integration variable is  $t$  while the function  $f(t)$  is called the integrand. In words, equation (5) is the integral of the function  $f(t)$  over  $t$  from 0 to  $T$ . It is immaterial how we denote the integration variable. The same result as in equation (5)


would result if we wrote

$$
\int_ {0} ^ {T} f (y) d y
$$ instead. The important factors are the integrand and the integral limits.


Note that instead of using the function values of the right boundaries of the intervals  $f(t_{i+1})$  in equation (4), referred to as the right-point rule, we might as well have taken the function values of the left boundaries  $f(t_i)$ , referred to as the left-point rule, which would have led to the same integral. Moreover, we might have taken the function  $f(0.5 \cdot (t_{i+1} + t_i))$  values evaluated at the mid-points of the intervals and still obtained the same interval. This latter procedure is called the mid-point rule.

If we keep 0 as the lower limit of the integral in equation (5) and vary  $T$ , then equation (5) becomes a function of the variable  $T$ . We may denote this function by

$$
F (T) = \int_ {0} ^ {T} f (t) d t \tag {6}
$$

# Relationship Between Integral and Derivative

In equation (6) the relationship between  $f(t)$  and  $F(T)$  is as follows. Suppose we compute the derivative of  $F(T)$  with respect to  $T$  and assume that  $F(T)$  is differentiable, for  $T > 0$ . The result is

$$
F ^ {\prime} (T) = \frac {d F (T)}{d T} = f (T) \tag {7}
$$

Hence, from equation (7) we see that the marginal increment of the integral at any point (i.e., its derivative) is exactly equal to the integrand evaluated at the according value. This need not generally be true. But in most cases, particularly in financial modeling, this statement is valid.

The implication of this discussion for probability theory is as follows. Let  $P$  be a continuous probability measure with probability distribu tion function  $F$  and (probability) density function  $f$ . There is the unique link between  $f$  and  $P$  given through


$$
P (X \leq x) = F (x) = \int_ {- \infty} ^ {\infty} f (x) d x \tag {8}
$$

Formally, the integration of  $f$  over  $x$  is always from  $-\infty$  to  $\infty$ , even if the support is not on the entire real line. This is no problem, however, since the density is zero outside the support and, hence, integration over those parts yields 0 contribution to the integral. For example, suppose that some density function were

$$ f (x) = \left\{ \begin{array}{c c} h (x), & x \geq 0 \\ 0 & x <   0 \end{array} \right. \tag {9}
$$ where  $h(x)$  is just some function such that  $f$  satisfies the requirements for a density function. That is, the support is only on the positive part of the real line. Substituting the function from equation (9) into equation (8) yields the equality


$$
\int_ {- \infty} ^ {\infty} f (x) d x = \int_ {0} ^ {\infty} f (x) d x = \int_ {0} ^ {\infty} h (x) d x \tag {10}
$$

# SOME FUNCTIONS

Here we introduce some functions needed in probability theory to describe probability distributions of random variables: factorials, gamma function, beta function, Bessel function of the third kind, and characteristic function. While the first four are functions of very special shape, the characteristic function is of a more general structure. It is the function characterizing the probability distribution of some random variable and, hence, is of unique form for each random variable.

# Factorial

Let  $k \in N$  (i.e.,  $k = 1, 2, \ldots$ ). Then the factorial of this natural number  $k$ , denoted by the symbol!, is given by

$$ k! = k \cdot (k - 1) \cdot (k - 2) \dots 1 \tag {11}
$$

A factorial is the product of this number and all natural numbers smaller than  $k$  including 1. By definition, the factorial of zero is one (i.e.,  $0! \equiv 1$ ). For example, the factorial of 3 is  $3! = 3 \cdot 2 \cdot 1 = 6$ .

# Gamma Function

The gamma function for nonnegative values  $x$  is defined by

$$
\Gamma (x) = \int_ {0} ^ {\infty} e ^ {- t} t ^ {x - 1} d t, \quad x \geq 0 \tag {12}
$$

The gamma function has the following properties. If the  $x$  correspond with a natural number  $n \in \mathbb{N}$  (i.e.,  $n = 1,2,\ldots$ ), then we have that equation (12) equals the factorial given by equation (11) of  $n - 1$ . Formally, this is

$$
\Gamma (n) = (n - 1)! = (n - 1) \cdot (n - 2) \dots \dots 1
$$

Furthermore, for any  $x \geq 0$ , it holds that  $\Gamma(x + 1) = x\Gamma(x)$ .

In Figure 10, we have displayed part of the gamma function for  $x$  values between 0.1 and 5. Note that, for either  $x \to 0$  or  $x \to \infty$ ,  $\Gamma(x)$  goes to infinity.

# Beta Function

The beta function with parameters  $c$  and  $d$  is defined as

$$
\begin{array}{l} B (c, d) = \int_ {0} ^ {1} u ^ {c - 1} (1 - u) ^ {d - 1} d u \\ = \frac {\Gamma (c) \Gamma (d)}{\Gamma (c + d)} \\ \end{array}
$$ where  $\Gamma$  is the gamma function from equation (12).


# Bessel Function of the Third Kind

The Bessel function of the third kind is defined as

$$
K _ {1} (x) = \frac {1}{2} \int_ {0} ^ {\infty} \exp \left\{- \frac {x}{2} \left(y + \frac {1}{y}\right) \right\} d y
$$

This function is often a component of other, more complex functions such as the density function of the NIG distribution.

# Characteristic Function

Before advancing to introduce the characteristic function, we briefly explain complex numbers.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/23ef5502a54f7f4c14ca8cac104abbfb7b6f44273eae80d02e4d1922a985e498.jpg)
Figure 10 Gamma Function  $\Gamma (x)$

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/76a29b67-ac4d-47f0-86d4-1e10a3a8dda0/efdb03dc71173532942cb0e5d18cd412a4a0c992892af6f90fbf84bb41e64238.jpg)
Figure 11 Graphical Representation of the Complex Number  $z = 0.8 + 0.9i$

Suppose we were to take the square root of the number  $-1$ , that is,  $\sqrt{-1}$ . So far, our calculus has no solution for this since the square root of negative numbers has not yet been introduced. However, by introducing the imaginary number  $i$ , which is defined as

$$ i = \sqrt {- 1}
$$ we can solve square roots of any real number. Now, we can represent any number as the combination of a real  $(Re)$  part  $a$  plus some units  $b$  of  $i$ , which we refer to as the imaginary  $(Im)$  part. Then, any number  $z$  will look like


$$ z = a + i \cdot b \tag {13}
$$

The number given by equation (13) is a complex number. The set of complex numbers is symbolized by  $C$ . This set contains the real numbers that are those complex numbers with  $b = 0$ . Graphically, we can represent the complex numbers on a two-dimensional space as given in Figure 11.

Now, we can introduce the characteristic function as some function  $\phi$  mapping real numbers into the complex numbers. Formally, we write this as  $\phi: \mathbf{R} \to \mathbf{C}$ . Suppose we have some ran- dom variable  $X$  with density function  $f$ . The characteristic function is then defined as


$$
\phi (t) = \int_ {- \infty} ^ {\infty} e ^ {i t x} f (x) d x \tag {14}
$$ which transforms the density  $f$  into some complex number at any real position  $t$ . Equation (14) is commonly referred to as the Fourier transformation of the density.


The relationship between the characteristic function  $\varphi$  and the density function  $f$  of some random variable is unique. So, when we state either one, the probability distribution of the corresponding random variable is unmistakably determined.

# KEY POINTS

- Continuous functions are an integral component of mathematical analysis. They are useful whenever jumps in the function values are undesirable. This is often the case when financial asset returns are modeled; that is, one assumes that, in particular logarithmic returns, they may assume any value on the real line such that the related probability distribution is continuous with continuous probability density.


- The indicator function is defined as a function yielding one for certain specified argument values and zero in any other case. It is helpful in expressing so-called exclusive either-or behavior of random variables (i.e., when random variables can only assume exactly one of two values). For example, when one models call option prices where, at maturity, the value of the option is equal to either zero or the difference between the market value of the underlying and the strike price, one resorts to the indicator function.
- The derivative of some function expresses the function's rate of growth at some point for infinitesimally small increments. In words, it expresses by how much the function changes if one takes a very small step. In probability theory, a derivative is used in the context of a continuous probability distribution to express by how much the distribution function increases at a certain value (i.e., the marginal rate of probability at a certain value).
- The integral is the continuous analogue of the sum of discrete values. In probability theory, the probability of individual outcomes is always zero when the distribution is continu ous. In order to express the probability of at most a certain value, we cannot sum the individual probabilities of all values less than or equal to the critical value. Instead, at each value, we have the density function which we integrate up to the critical value, yielding the requested probability.

- The characteristic function is the unique representation of a probability distribution. For certain distributions, the probability density function or the distribution function are unknown. Instead, it is necessary to resort to the characteristic function. Technically, the characteristic function is a function involving complex numbers (i.e., numbers including the square root of minus one) to express the behavior of some function at certain frequencies. It is closely linked to the Fourier transform used in engineering.

# REFERENCES

Khuri, A. (2003). Advanced Calculus with Applications in Statistics, 2nd ed. Hoboken, NJ: John Wiley & Sons.
MacCluer, B. D. (2009). Elementary Functional Analysis. New York: Springer.
Richardson, L. F. (2008). Advanced Calculus: An Introduction to Linear Analysis. Hoboken, NJ: John Wiley & Sons.
