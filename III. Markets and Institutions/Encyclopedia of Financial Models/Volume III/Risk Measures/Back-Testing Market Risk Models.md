
# Back-Testing Market Risk Models

KEVIN DOWD, PhD

Partner, Cobden Partners, London

Abstract: Back-testing is the quantitative evaluation of a model, and back-testing a risk or probability density forecasting model involves a comparison of the model's density forecasts against subsequently realized outcomes of the random variable whose density is forecast. One purpose of back-testing is to determine whether the forecasts are sufficiently close to realized outcomes to enable us to conclude that the forecasts are statistically compatible with those outcomes. Back-tests conducted for this purpose involve statistical hypothesis tests to determine if a model's forecasts are acceptable. Hypothesis tests can be applied to observations involving a loss that exceeds the value-at-risk at a given confidence interval, or they can be applied to forecasts of VaRs at multiple confidence intervals. A second purpose of back-testing is to assist risk managers to diagnose problems with their risk models and so help improve them. A third purpose of back-testing is to rank the performance of a set of alternative risk models to determine which model gives the "best" density forecast evaluation performance.

To back-test a model is to evaluate it in quantitative terms, and back-testing a risk (or probability density forecasting) model involves a comparison of the model's density forecasts against subsequently realized outcomes of the underlying random variable whose density is forecast. The importance of back-testing is self-evident: If risk managers are to have confidence in their risk models, then those models need to be properly back-tested and to have performed well under those back-tests.

Back-tests can be used for three complementary purposes. The first is to assess whether a model's density forecasts are statistically compatible with the realized values of the underlying random variable. The second purpose is diagnostic: to generate feedback about the model's potential weaknesses to assist the model builder and help him/her to "correct" the model. The third purpose is to rank alternative models. A good risk model should fare well by all three criteria: It should pass its statistical tests, should not generate any worrying diagnostics, and should rank well in comparison to alternative models.


The archetypal market risk model is a model that forecasts the value at risk (VaR) of a portfolio over one or more confidence levels, for a specified horizon. We will assume for the most part that the horizon is a trading day.

To back-test such a model, we need a dataset that consists of the model's forecasts, on the one hand, and the daily profits or losses (P/L) generated by the portfolio, on the other. The first task in back-testing is therefore to assemble such a dataset. For most market risk managers, the forecasts themselves should be readily available. However, obtaining suitable profit and loss data is a more difficult problem than it might initially appear to be. The reason is that we do not need data on the profits or losses actually generated by a portfolio, but data on the profits or losses attributable to the market risks taken: We want P/L data that reflect underlying market volatility rather than accounting prudence. We also need to clean our P/L data to get rid of components that are not directly related to current or recent market risk-taking. Such components include fee income, hidden and unrealized P/L, earnings attributable to nonmarket risks, such as yields on corporate bonds, and the impact of intraday trading on P/L.


Having obtained our dataset, the next stage is to carry out a preliminary data analysis. We should plot a back-testing chart—a plot of the realized P/L over time with the VaR forecasts superimposed on it—and look for any odd or outstanding features. It is also good practice to supplement back-testing charts with P/L histograms, which sometimes give a clearer indication of the empirical P/L distribution, and quantile-quantile (QQ) charts, which plot the quantiles of an empirical P/L distribution against those of a forecasted P/L distribution. It is also a good idea to examine summary P/L statistics, including the obvious statistics of mean, variance, skewness, kurtosis, range, and so on and the number and size of extreme observations. A preliminary data analysis can be very helpful in enabling practitioners to get to know their data and get a feel for any problems they might encounter.

# STATISTICAL BACK-TESTING

The first type of back-tests are statistical tests based on a hypothesis-testing paradigm. We first specify the null hypothesis that we wish to test—typically the null hypothesis is that the model is adequate—and select an alternative hypothesis to be accepted if the null is rejected. We then select a significance level and estimate the probability associated with the null hypothesis being "true." We would accept the null hypothesis if the estimated value of this probability, the estimated prob-value, exceeds the chosen significance level, and we would reject it otherwise. The higher the significance level, the more likely we are to accept the null hypothesis, and the less likely we are to incorrectly reject a true model (that is, to make a Type I error). Unfortunately, it also means that we are more likely to incorrectly accept a false model (that is, to make a Type II error). Any test therefore involves a trade-off between these two types of possible error. Ideally, we should select a significance level that takes account of the likelihoods and costs of these errors and strikes an appropriate balance between them. However, in practice, it is common to select some arbitrary significance level such as  $5\%$  and apply that level in all our tests. A significance level of this magnitude gives the model a certain benefit of the doubt, and implies that we would reject the model only if the evidence against it is reasonably strong.


# EXCEEDANCE-BASED STATISTICAL APPROACHES

Suppose that we have a sample of  $n$  daily VaR forecasts  $\mathrm{VaR}_t$  and a corresponding sample of  $n$  realized loss outcomes  $L_{t}$ , where  $t$  goes from 1 to  $n$ .  $L_{t}$  is denominated in units in which realized losses are positive and realized profits are negative.

Some common approaches to back-testing involve exceedance observations, where an exceedance observation (also called a tail loss) is a loss that exceeds the VaR. These exceedance observations  $h_t$  are obtained by putting our sample observations through the following transformation:

$$ h _ {t} = \left\{ \begin{array}{l} 1 \\ 0 \end{array} \right\} \quad \text {i f} \quad \left\{ \begin{array}{l} L _ {t} > V a R _ {t} \\ L _ {t} \leq V a R _ {t} \end{array} \right. \tag {1}
$$

This transformation gives a unit value to all observations where there is a loss exceeding VaR and a zero value to all other observations.

# Binomial (Kupiec) Approach

We can now apply the basic frequency (or binomial) test suggested by Kupiec (1995): We test whether the observed frequency of exceedances is consistent with the frequency predicted by the model. In particular, under the null hypothesis that the model is "good," the number of exceedances  $x$  follows a binomial distribution with probability  $p$ , where  $p$  is the tail probability or 1 minus the confidence level. The probability of  $x$  exceedances given  $n$  observations is therefore:

$$
\operatorname {P r o b} (x \mid n, p) = \binom {n} {x} p ^ {x} (1 - p) ^ {n - x} \tag {2}
$$

Equation (2) also tells us that the only information required to implement a binomial test is information about the values of  $n, p,$  and  $x$ . This probability is then calculated using a suitable calculation engine (e.g., using the "binomdist" function in Excel).

To illustrate, suppose  $n = 1,000$  and we take the confidence level  $\alpha$  to be 0.95. Our model therefore predicts that  $p = 1 - \alpha = 0.05$  and the null hypothesis is  $H_0$ :  $p = 0.05$ . We then expect  $np = 50$  exceedances under the null. Now suppose that the number of exceedances,  $x$ , is 60. This corresponds to an empirical frequency,  $\hat{p}$ , equal to 0.060. Since  $\hat{p}$ , exceeds 0.05; we might specify a one-sided alternative hypothesis  $H_1$ :  $p > 0.05$ . The prob-value of the test is the probability under the null that  $x \geq 60$ . This is most easily calculated as  $1 - \operatorname*{Pr}[x \leq 59]$ , which equals 0.0867 given the values of  $n$  and  $p$ . At a conventional significance level such as  $5\%$ , we would then "pass" the model as acceptable. It is also clear that as  $x$  gets larger and moves further away from its predicted value of 50, then the probability of observing  $x$  exceedances will fall. Values of  $x$  with prob-values lower than our significance level would lead to rejections of the null hypothesis and a "fail" result for the model. In fact, if we work with a  $5\%$  significance level, it is straightforward to show that we would accept the null if  $x \leq 62$  and reject it if  $x \geq 63$ .


We can also apply binomial tests using a two-sided alternative hypothesis  $H_{1}$ :  $p \neq 0.05$ . We could do so by estimating a confidence interval for the number of exceedances and checking whether  $x$  lies within this interval. For example, if we want to test using a  $5\%$  significance level, we would estimate a  $95\%$  confidence interval for  $x$ , the bounds of which would delineate the lower and upper  $5\%$  tails of  $x$ 's density function. With  $n = 1,000$  and  $p = 0.05$ , the  $95\%$  confidence interval for  $x$  is [36, 66]. We would then accept the null if  $x$  falls within this range and otherwise reject it.

# A Normal Approximation

Testing can be simplified further if we work with a normal approximation to the binomial. Provided  $n$  is sufficiently large—and  $n$  would be sufficiently large with the sample sizes that risk managers typically work with—then the distribution of  $x$  is approximately normal with mean  $np$  and variance  $np(1 - p)$ . This implies, in turn, that the variable  $z = (x - np) / \sqrt{np(1 - p)}$  is distributed as standard normal, and we can test whether the observed value of  $z$  is compatible with this distribution. For instance, if we wished to carry out a two-sided test, we know that the 95\% confidence interval for a standard normal is  $[-1.96, +1.96]$ , so we would accept the null if (and only if)  $z$  falls in this range.

# Tests of Independence

Besides predicting that  $x$  should be binomial or approximately normal with large samples, the null hypothesis of model adequacy often leads to the prediction that  $x$  should be independent. "Independence" means that there should be no temporal pattern in the  $x$  series that is, the probability of the next observation being an exceedance should be independent of whether any previous observation was an exceedance or not. Where this prediction arises, it is important that it be tested too: A bad model might pass the earlier tests, but still be inadequate because it produces predictable exceedances or clusters of exceedances that ought not to arise. Evidence of exceedance clustering would suggest that the model is misspecified, even if the model has the correct exceedance frequency.


One of the simplest independence tests is a runs test, in which we test whether the number of runs in a time series is consistent with what we would expect under independence. We can apply a runs test to any data that are time-ordered and expressed in binary form, as is the case with observations in our  $x$  series that either take the value 0 or the value 1. A run is then a sequence of consecutive identical numbers, and the number of runs  $R$  is equal to the number of sign changes plus 1. If  $u$  is the number of observations taking one value and  $v$  the number taking the other value, then under the independence null the mean and variance of the number of runs are, respectively:

$$
\mu_ {R} = 1 + \frac {2 u v}{u + v} \tag {3}
$$

$$
\sigma_ {R} ^ {2} = \frac {2 u v (2 u v - u - v)}{(u + v) ^ {2} (u + v - 1)} \tag {4}
$$

If the total number of observations is large, then  $R$  is approximately normal and  $z = (R - \mu_R) / \sigma_R$  approximately standard normal, and we can test accordingly.

A more sophisticated version of the same idea is suggested by Engle and Manganelli (2004): They propose estimating a binary regression model—that is, they regress  $h_t$  against possible explanatory variables, such as lagged returns or lagged squared returns—and then test for the joint insignificance of the explanatory variables. A binary regression approach is more powerful than a basic runs test because it can take account of the impact of other possible variables, which a runs test does not.

# Conditional Testing (Christoffersen) Approach

We can also carry out tests of the distribution and independence of  $x$  within the same testing framework, and this takes us to the conditional back-testing approach of Christoffersen (1998). His idea is to separate out the particular predictions being tested and then test each prediction separately. We begin by rephrasing the earlier frequency or unconditional coverage test in likelihood ratio (LR) form.

Given that the observed frequency of exceedances is  $x / n$ , then under the hypothesis/ prediction of correct unconditional coverage, the test statistic

$$
\begin{array}{l} L R _ {u c} = - 2 \ln [ (1 - p) ^ {n - x} p ^ {x} ] \tag {5} \\ + 2 \ln [ (1 - x / n) ^ {n - x} (x / n) ^ {x} ] \\ \end{array}
$$ is distributed as a  $\chi^2 (1)$ , a chi-squared with 1 degree of freedom. As we can see from equation (5), this boils down to a test of whether the empirical frequency  $x / n$  is "close" to the predicted frequency  $p$ .


Turning to the independence prediction, let  $n_{ij}$  be the number of days that state  $j$  occurred after state  $i$  occurred the previous day, where the states refer to the occurrence or not of an exceedance, and let  $\pi_{ij}$  be the probability of state  $j$  in any given day, given that the previous day's state was  $i$ . Under the hypothesis of independence, the test statistic

$$
\begin{array}{l} L R _ {i n d} = - 2 \ln \left[ (1 - \hat {\pi} _ {2}) ^ {n _ {0 0} + n _ {1 1}} \hat {\pi} _ {2} ^ {n _ {0 1} + n _ {1 1}} \right] \\ + 2 \ln \left[ (1 - \hat {\pi} _ {0 1}) ^ {n _ {0 0}} \hat {\pi} _ {0 1} ^ {n _ {0 1}} (1 - \hat {\pi} _ {1 1}) ^ {n _ {1 0}} \pi_ {1 1} ^ {n _ {1 1}} \right] \tag {6} \\ \end{array}
$$ is also distributed as a  $\chi^2 (1)$ , and note that we can recover estimates of the probabilities from


$$
\begin{array}{l} \hat {\pi} _ {0 1} = \frac {n _ {0 1}}{n _ {0 0} + n _ {0 1}} \\ \hat {\pi} _ {1 1} = \frac {n _ {1 1}}{n _ {1 0} + n _ {1 1}} \tag {7} \\ \hat {\pi} _ {2} = \frac {n _ {0 1} + n _ {1 1}}{n _ {0 0} + n _ {1 0} + n _ {0 1} + n _ {1 1}} \\ \end{array}
$$

It follows that under the combined hypothesis of correct coverage and independence the test statistic

$$
L R _ {c c} = L R _ {u c} + L R _ {i n d} \tag {8}
$$ is distributed as  $\chi^2 (2)$ . The Christoffersen approach enables us to test both coverage and independence hypotheses at the same time. Moreover, if the model fails such a test, this approach enables us to test each hypothesis separately, and so establish whether the model fails because of incorrect coverage or because of lack of independence.


# Strengths and Limitations of Exceedance-Based Approaches

These exceedance tests have the advantages that they have a simple intuition, are easy to apply, and do not require a great deal of information. However, they often lack power (that is, the ability to identify bad models) except with very large sample sizes, because they throw potentially valuable information away: Focusing on tests of exceedances over VaR at a given confidence level is equivalent to throwing away information about the model's forecasts of VaRs at other confidence levels, and this discarded information often includes useful information about the sizes of tail losses predicted by a risk model (or information about VaRs at higher confidence levels). This can mean that a "bad" risk model will pass an exceedance-based test if it generates an acceptably accurate frequency of exceedances, even if its forecasts of losses larger than VaR are very poor.

# STATISTICAL

# BACK-TESTING OF VaRs AT MULTIPLE CONFIDENCE LEVELS

This line of reasoning suggests that we should consider back-testing the performance of a model's VaR forecasts over multiple confidence levels. Indeed, pushed to the limit, it suggests that we consider back-testing a model's VaR forecasts over all confidence levels at the same time. We would proceed by applying the following transformation:


$$ p _ {t} = F _ {t} \left(X _ {t}\right) \tag {9}
$$ where  $F_{t}(.)$  is the (typically time-dependent) probability-integral transformation (PIT) that maps the realized one-day loss or profit,  $X_{t}$ , to its cumulative density value, where the forecast is made the previous day. So, for example, if our model specifies that losses are standard normal, then a value  $X_{t} = 1.645$  would give us  $p_{t} = F_{t}(1.645) = 0.95$ , and so forth.


We can now deduce that  $p_t$  is stationary and distributed as standard uniform under the hypothesis that the VaR model is adequate.  $p_t$  is also independent because consecutive values of  $p_t$  have no common factors. Hence  $p_t$  is predicted to be independent and identically distributed (IID)  $U(0,1)$  under the null hypothesis.

As an aside, it is worth noting at this point that the independence assumption does not arise in cases where we have a multi-step-ahead as opposed to a one-step-ahead VaR model: An example of the latter is a VaR model that produces daily VaR forecasts over a daily forecast horizon; an example of the former is a VaR model that produces daily VaR forecasts over a multiday horizon. The forecast horizon is equal to one day in the one case, and equal to more than one day in the other. The  $p_t$  are predicted to be independent for one-day-ahead VaR forecasts because consecutive observations are not affected by common shocks; however, for multiday forecasts, there is no independence prediction because consecutive  $p_t$  observations are subject to at least one common random factor. For example, the two-day return over Monday and Tuesday and the two-day return over Tuesday and Wednesday are both affected by the Tuesday daily return. This means that they have a common random factor and are therefore not independent. We will ignore multistep-ahead models in the rest of our discussion, but the reader should keep in mind that we cannot assume independence for multi-step-ahead models or regard independence tests applied to such models as tests of model adequacy.


# Testing Uniformity

Returning to the one-step-ahead case, we can now test our model by applying conventional uniformity tests. One of the best known of these is the Kolmogorov-Smirnov (KS) test. The KS test statistic  $D$  is then the maximum distance between the predicted cumulative density  $F(x)$ , which is a 45-degree line, and the empirical cumulative density  $\hat{F}(x)$ , evaluated over each data point  $X_{t}$ :

$$
D = \max  _ {t} | F (X _ {t}) - \hat {F} (X _ {t}) | \tag {10}
$$

The test value of the KS statistic is then compared to the relevant critical value and the null is accepted or rejected accordingly. This test is easy to implement because the test statistic is straightforward to calculate and its critical values are easily obtained using Monte Carlo simulation. However, the KS test tends to be more sensitive to the distributional differences near the center of the distribution, and is less sensitive at the tails. This is obviously a drawback when back-testing VaR models, where we are usually much more interested in the tail than in the central mass of a distribution.

A way around this latter problem is to replace the KS test with a Kuiper test. The Kuiper test statistic  $D^{*}$  is the sum of the maximum amount by which each distribution exceeds the other:

$$
\begin{array}{l} D ^ {*} = \max  _ {t} \left| F \left(X _ {t}\right) - \hat {F} \left(X _ {t}\right) \right| \\ + \max  _ {t} \left| \hat {F} \left(X _ {t}\right) - F \left(X _ {t}\right) \right| \tag {11} \\ \end{array}
$$

The Kuiper test can be implemented in much the same way as the KS test: Its test statistic is straightforward to calculate and its critical values can be obtained by Monte Carlo simulation. The Kuiper test has the advantage over the KS test that it is more sensitive to deviations in the tail regions. It is also believed to be more robust to transformations in the data, and to be good at detecting cyclical and other features in the data. However, there is also evidence that it is very data intensive and needs large datasets to get reliable results.


We can also test uniformity by applying a textbook  $\chi^2$  test to binned (or classified) data). We divide the data into  $k$  classes and then compute the test statistic:

$$
\sum_ {i = 1} ^ {k} \frac {\left(O _ {i} - E _ {i}\right) ^ {2}}{E _ {i}} \tag {12}
$$ where  $O_{i}$  is the observed frequency of data in bin  $i$ , and  $E_{i}$  is the expected frequency of data in bin  $i$ . Under the null hypothesis, this test statistic is distributed as  $\chi^2 (k - c)$ , where  $c$  is the number of estimated parameters in the VaR model. The main disadvantage of the  $\chi^2$  test is that results are dependent on the way in which the data are binned and binning is (largely) arbitrary. In using it, we should be careful to check the sensitivity of results to alternative ways of binning the data.


# Applying the Berkowitz Transformation and Testing for Standard Normality

It is often more convenient to put the  $p_t$  through a second (or Berkowitz) transformation to make them standard normal under the null of model adequacy; that is, we work with the transformed variable:

$$ z _ {t} = \Phi^ {- 1} \left(p _ {t}\right) \tag {13}
$$ where  $\Phi(.)$  is the standard normal distribution function (see Berkowitz, 2001). This second transformation is helpful because testing for standard normality is more convenient than testing for standard uniformity, and because a normal variable is more convenient when dealing with temporal dependence. Under the null,  $z_{t}$  will be distributed as IID standard normal [denoted by IID  $N(0,1)$ ].


Testing model adequacy now boils down to testing whether  $z_{t}$  is distributed as IID  $N(0,1)$ . There are two distinct tasks here:

1. We need to test whether  $z_{t}$  is  $N(0,1)$ , taking as given that  $z_{t}$  is IID, and there are various tests we might apply. If  $z_{t}$  is standard normal, then it should have a zero mean, a variance of 1, a zero skew, and a kurtosis of 3. Assuming IID, we can test the mean prediction using a  $z$ -test or  $t$ -test, we can test the variance prediction using a variance ratio test, and we can test the skewness and kurtosis predictions using a Jarque-Bera test, which can also be regarded as a test of normality itself. All these tests are conventional textbook tests and are easy to apply.

2. We need to test whether  $z_{t}$  is IID, and there are many tests of the IID prediction. These include runs and binary regression tests, which we have already discussed above. We can also estimate the autocorrelation structure of our  $z_{t}$  observations or fit an autoregressive moving average (ARMA) process to them. All the parameters in an autocorrelation function or an ARMA process should be insignificant, and we can test for their significance using standard tests such as a Box-Pierce  $Q$  test. Another possibility, if we have enough data, is to test independence using a BDS test (Brock et al., 1987): a BDS test is very powerful, but also data-intensive.

Since the hypothesis of model adequacy predicts both  $N(0,1)$  and IID, it is important to note that the model must "pass" both types of test if it is to "pass" overall.

# Tests Applied to Truncated Distributions

There are also situations where we are only interested in part of the P/L distribution: For example, we might be interested only in the distribution of losses in excess of VaR. If we are working to a confidence level  $\alpha$ , we can take our earlier  $p_t$  series and delete all nontail obser vations from it. We then end up with a series that is IID uniformly distributed over the interval  $[0,1 - \alpha]$ , and this implies that  $p_t / (1 - \alpha)$  is IID uniformly distributed over the interval [0,1]. We can test this prediction using one of the uniformity tests discussed earlier. If we wish to, we can apply the Berkowitz transformation to  $p_t / (1 - \alpha)$  to obtain the series  $z_t = \Phi^{-1}(p_t / (1 - \alpha))$ , which is distributed as IID  $N(0,1)$  under the null. We can then apply the tests just discussed.


# USING BACK-TESTS FOR DIAGNOSTIC PURPOSES

We can also modify many of these back-test procedures to help diagnose problems with our VaR model. Model diagnosis is a key ingredient to successful model building, and requires the modeler to be on the lookout for evidence of possible problems. So, to use an earlier example, if we have 60 exceedances out of a sample of 1,000 and we are operating to a VaR at the  $95\%$  confidence level, then we know that this is associated with a prob-value of 0.0867. Were we carrying out a formal back-test of model adequacy at a conventional significance level such as  $5\%$ , we would dismiss this result as statistically insignificant because the significance level gives the model the benefit of the doubt. However, for diagnostic purposes we do not wish to give the model the benefit of the doubt: Instead, we are looking for evidence "against" the model, even if that evidence is statistically "weak." In these circumstances, a result like this would lead us to suspect whether the model has a tendency to underestimate the VaR. A wise risk manager would then start to ask whether other evidence could be found that would confirm or refute this suspicion. And, to put the same point a little differently, the last thing a risk manager should do in the face of such evidence is to wait and do nothing till the evidence has become overwhelming: The risk manager should act in a timely manner on the basis of any reasonable evidence available.

Independence tests can also be useful diagnostic tools. If we apply an independence test and the test result gives us some (not necessarily strong) reason to suspect that the model does not satisfy a valid independence prediction, then we can interpret this evidence as suggesting that there might be some dynamic misspecification in our model: Even if the broad coverage is about right, there might still be something wrong with the updating of our VaR forecasts from one day to the next. So, for example, if we have a parametric VaR model, then we might suspect that a key parameter in the model was not being updated efficiently, and the obvious suspects would be volatility or correlation parameters. Again, the evidence might be statistically "weak," but even weak evidence can be useful in pointing to areas of weakness in the model.

Another useful diagnostic is provided by empirical moments of the Berkowitz-transformed series (see equation (13) above), which we saw earlier are predicted to be standard normal under the null of model adequacy. Some very useful diagnostic information can then be obtained by estimating their sample moments and considering any departures from their predicted values:

- If the sample mean is different from zero, we might suspect whether the model's forecasts are biased in one direction or the other.
- If sample variance is less than 1, we might suspect that the model's predicted dispersion is too low, in which case the model might overestimate risk; and if the sample variance is greater than 1, we might suspect that the predicted dispersion is too high and the model underestimates risk.
- If the sample skew is positive or negative, we might suspect that the forecasts are skewed in one direction or the other.
- If the sample kurtosis is less than 3 or (as is more likely in risk management contexts) bigger than 3, we might ask ourselves if the model is overestimating or underestimating its tails.

In each of these cases, we should also check the strength of the evidence and we can do so by applying the relevant tests and checking out their prob-values: The lower the prob-value, the stronger the evidence against the model. However, since we are especially concerned in risk management with the possibility that the model might underestimate risks, then a sample variance that considerably exceeds 1 or a sample kurtosis that considerably exceeds 3 is potentially important evidence that might warrant further scrutiny.

# RANKING ALTERNATIVE MODELS

It is often the case that we are interested in how different models compare to each other. We can compare models using forecast evaluation methods that give each model a score in terms of some loss function; we then use the loss scores to rank the models—the lower the loss, the better the model. These approaches are not statistical tests of model adequacy and this means that they do not suffer from the low power of tests such as frequency tests: This makes them attractive for back-testing with the datasets typically available in real-world applications. In addition, they also allow us to tailor the loss function to take account of particular concerns: For example, we might be more concerned about higher losses than lower losses, and might therefore wish to give higher losses a greater weight in our loss function.

The ranking process has four key ingredients for each model:

1. A set of  $n$  paired observations—paired observations of losses or profits for each period and their associated VaR forecasts.
2. A loss function that gives each observation a score  $C_t$  depending on how the observed loss or profit compares to the VaR forecasted for that period.
3. A benchmark, which gives us an idea of the score we could expect from a "good" model.

4. A score function, which takes as its inputs our loss-function and benchmark values.

We need to specify the loss function, and a number of different loss functions have been proposed. Perhaps the most straightforward is the binary loss function proposed by Lopez (1998, p. 121), which gives exceedance observations a value of 1 and other observations a value of  $0$ .  $C_t$  is then as follows:

$$
C _ {t} = \left\{ \begin{array}{l l} 1 & \text {i f} \quad L _ {t} > V a R _ {t} \\ 0 & \end{array} \right. \tag {14}
$$

This loss function is intended for the user who is (exclusively) concerned with the frequency of exceedances. The natural benchmark for this loss function is  $p$ , the exceedance probability or expected value of  $E(C_t)$ . If we take our benchmark to be the expected value of  $C_t$  under the null hypothesis that the model is "good," then Lopez (1998) suggests that a good choice of score function is the following quadratic probability score (QPS) function:

$$
\mathrm {Q P S} = \frac {2}{n} \sum_ {t = 1} ^ {n} \left(C _ {t} - p\right) ^ {2} \tag {15}
$$

The QPS takes a value in the range [0,2], and the closer the QPS-value to zero, the better the model. We can therefore use the QPS (or some similar score function) to rank our models, with the better models having the lower scores. In addition, the QPS criterion has the attractive property that it (usually) encourages truth-telling by VaR modelers: If VaR modelers wish to minimize their QPS score, they will (usually) report their VaRs "truthfully." This is a useful property in situations where the back-tester and the VaR modeler are different, and where the back-tester might be concerned about the VaR modeler reporting false VaR forecasts to alter the results of the back-test.

A drawback of this loss function is that it ignores the magnitude of tail losses. If we wish to remedy this defect, Lopez suggests a second, size-adjusted, loss function:


$$
C _ {t} = \left\{ \begin{array}{l l} 1 + \left(L _ {t} - V a R _ {t}\right) ^ {2} & \text {i f} \quad L _ {t} > V a R \\ 0 & \end{array} \right. \tag {16}
$$

This loss function allows for the sizes of tail losses in a way that (15) does not: A model that generates higher tail losses would generate higher values of (16) than one that generates lower tail losses, other things being equal. However, with this loss function, there is no longer a straightforward condition for the benchmark, so we need to estimate the benchmark by some other means (e.g., Monte Carlo simulation). The size-adjusted loss function (17) also has the drawback that it loses some of its intuition, because squared monetary returns have no ready monetary interpretation.

A way around this last problem is suggested by Blanco and Ihle (1998), who suggest the following loss function:

$$
C _ {t} = \left\{ \begin{array}{l l} \left(L _ {t} - V a R _ {t}\right) / V a R _ {t} & \text {i f} \quad L _ {t} > V a R _ {t} \\ 0 & \end{array} \right. \tag {17}
$$

This loss function gives each tail-loss observation a weight equal to the tail loss divided by the VaR. This has a nice intuition and ensures that higher tail losses get awarded higher  $C_t$  values without the impaired intuition introduced by squaring the tail loss. The benchmark for this forecast evaluation procedure is also easy to derive: The benchmark is equal to the difference between the Expected Shortfall (ES) and the VaR, divided by the VaR. However, the Blanco-Ihle loss function also has a problem of its own: Because (17) has the VaR as its denominator, it is not defined if the VaR is zero, and can give awkward answers if VaR gets "close" to zero or becomes negative. We should therefore only use it if we can be confident of the VaR being sufficiently large and positive.

We therefore seek a size-based loss function that avoids the squared term in the second Lopez loss function, but also avoids denominators that might be zero-valued. A promising candidate is the tail loss itself:


$$
C _ {t} = \left\{ \begin{array}{l l} L _ {t} & \text {i f} \quad L _ {t} > V a R \\ 0 & \end{array} \right. \tag {18}
$$

The expected value of the tail loss is of course the ES, so we can choose the ES as our benchmark and use a quadratic score function such as:

$$
Q S = \frac {2}{n} \sum_ {t = 1} ^ {n} \left(C _ {t} - E S _ {t}\right) ^ {2} \tag {19}
$$

This approach penalizes deviations of tail losses from their expected value, which makes intuitive sense. Moreover, because it is quadratic, it gives very high tail losses much greater weight than more common tail losses, and thereby comes down hard on large losses.

# KEY POINTS

- In general, back-testing is the quantitative evaluation of a model. When back-testing is applied to a risk or probability density forecasting model, it involves a comparison of the model's density forecasts against subsequently realized outcomes of the random variable whose density is forecast.
- The main purposes of back-testing market risk models are to test model adequacy, to diagnose potential model problems, and to compare or rank alternative models. A good risk model should fare well by all three criteria: It should pass its statistical tests, should not generate any worrying diagnostics, and should rank well in comparison to alternative models.
- Because the typical market risk model is a model that forecasts the value-at-risk of a portfolio over one or more confidence levels for a specified horizon, back-testing of market risk models involves some comparison of VaR forecasts against subsequently realized values of profit or loss.
- Formal tests of market risk model adequacy can be applied to the frequency and inde pendence of exceedance observations, but can also be applied to forecasts of VaR at multiple confidence levels.


- Comparable approaches can be used for model diagnostic purposes, where the main concern is not to test model adequacy in a formal way, as such, but instead to gather evidence of possible model misspecification.
- Simple loss-scoring approaches can be used to rank the forecast performance of alternative models.

# REFERENCES

Berkowitz, J. (2001). Testing density forecasts, with applications to risk management. Journal of Business and Economic Statistics 19, 4: 465-474.
Blanco, C., and Ihle, G. (1999). How good is your VaR? Using back-testing to assess system performance. *Financial Engineering News* 11, August: 1-2.
Campbell, S. (2007). A review of back-testing and back-testing procedures. Journal of Risk 9, 2: 1-17.
Christoffersen, P. F. (1998). Evaluating interval forecasts. International Economic Review 39, 4: 841-862.
Crnkovic, C., and Drachman, J. (1996). Quality control. Risk 9, 9: 139-143.
Diebold, F. X., Gunther, T. A., and Tay, A. S. (1998). Evaluating density forecasts with applications to financial risk management. International Economic Review 39, 4: 863-883.
Dowd, K. (2004). A modified Berkowitz back-test. Risk 17, 4: 86.
Dowd, K. (2005). Measuring Market Risk, 2nd edition. Chichester: John Wiley & Sons.
Engle, R. F., and Manganelli, S. (2004). CAViaR: Conditional autoregressive value-at-risk by regression quantiles. Journal of Business and Economic Statistics 22, 4: 367-381.
Hendricks, D. (1996). Evaluation of value-at-risk models using historical data. Federal Reserve Bank of New York Economic Policy Review 2 (April): 39-70.
Kupiec, P. (1995). Techniques for verifying the accuracy of risk management models. Journal of Derivatives 4, 3: 73-84.
Lopez, J. A. (1998). Regulatory evaluation of value-at-risk models. *Federal Reserve Bank of New York Economic Policy Review* 4, 3: 119-124.
