
---
title: "Stochastic Processes in Continuous Time"
parent_directory: "III. Markets and Institutions/Encyclopedia of Financial Models/Volume III/Stochastic Processes and Tools"
formatted: 2025-12-21 11:30:00 AM
formatter_model: kimi-k2-turbo
cli_tool: claude-code
primary_tags:
  - stochastic processes
  - continuous time processes
  - brownian motion
  - levy processes
  - tempered stable processes
secondary_tags:
  - poisson processes
  - jump diffusion
  - gamma processes
  - inverse gaussian processes
  - variance gamma processes
  - alpha stable processes
  - time changed processes
  - mathematical finance
  - probability theory
cssclasses: academia
---

# Stochastic Processes in Continuous Time

## Authors

SVETLOZAR T. RACHEV, PhD, Dr Sci  
Frey Family Foundation Chair-Professor, Department of Applied Mathematics and Statistics, Stony Brook University, and Chief Scientist, FinAnalytica

YOUNG SHIN KIM, PhD  
Research Assistant Professor, School of Economics and Business Engineering, University of Karlsruhe and KIT

MICHELE LEONARDO BIANCHI, PhD  
Research Analyst, Specialized Intermediaries Supervision Department, Bank of Italy

FRANK J. FABOZZI, PhD, CFA, CPA  
Professor of Finance, EDHEC Business School

## Abstract

The dynamic of a financial asset's returns and prices can be expressed using a deterministic process if there is no uncertainty about its future behavior, or with a stochastic process in the more likely case when the value is uncertain. Stochastic processes in continuous time are the most used tool to explain the dynamics of a financial asset's returns and prices. They are the building blocks with which to construct financial models for portfolio optimization, derivatives pricing, and risk management. Continuous-time processes allow for more elegant theoretical modeling compared to discrete-time models and many results proven in probability theory can be applied to obtain a simple evaluation method.

In 1900, the father of modern option pricing theory, Louis Bachelier, proposed using Brownian motion for modeling stock market prices. There are several reasons why Brownian motion is a popular process. First, Brownian motion is the milestone of the theory of stochastic processes.

However, more realistic general processes that are better suited for financial modeling such as Lévy, additive, or self-similar processes have been developed only since the mid 1990s (see Samorodnitsky and Taqqu (1994), Sato (1999), and Embrechts and Maejima (2002)). Most of the practical problems in mathematical finance can be solved by taking into consideration these new processes. For example, the concept of stochastic integral with respect to Brownian motion was introduced in 1933 and only in the 1990s has the general theory of stochastic integration with respect to semi-martingale appeared. From a practical point of view, the second reason for the popularity of Brownian motion is that the normal distribution allows one to solve real-world pricing problems such as option prices as estimations and simulations in a few seconds, and most of the problems have a closed-form solution which can be easily used. See Øksendal (2003) or Karatzas and Shreve (1991) for a complete theoretical treatment of financial applications of continuous-time stochastic processes driven by Brownian motion.


The two basic classes of continuous-time stochastic processes are Brownian motion and the Poisson process. The name of the former is due to the botanist Robert Brown who in 1827 described the movement of pollen suspended in water. The theory of Brownian motion was founded by the work of Norbert Wiener who was the first to prove its existence and, as a result, Brownian motion is sometimes also referred to as a Wiener process. The Poisson process generated by the Poisson distribution is the building block of pure jump processes. Both processes are fundamentally different with respect to their path properties and they belong to the larger class of Lévy processes (for more details about Lévy processes see Sato [1999]). Schoutens (2003), Cont and Tankov (2004), and Rachev et al. (2011) provide details of Lévy processes with applications to option pricing.

Infinitely divisible distributions, including  $\alpha$ -stable and tempered stable distributions, can be considered to define continuous-time stochastic processes. In order to model the behavior of a financial asset's returns and prices, one can consider (1) a Brownian motion, (2) a process defined as the sum of a Brownian motion and a Poisson process, or (3) a pure jump Lévy process.

Before we continue with the discussion and the construction of processes, we will briefly define terms that will be used in this entry.

- A stochastic process  $X = (X_{t})_{t\geq 0}$  is a family of  $\mathbb{R}$ -valued random variables  $X_{t}$  with parameter  $t\geq 0$ , defined on the sample space  $\Omega$ . For every outcome  $\omega \in \Omega$ , the function  $t\mapsto X_t(\omega)$  is called a sample path of the process  $X$ .
- Let  $X$  be a stochastic process. Given  $0 < t_1 < t_2 < \ldots < t_n$ , if the random variables  $X_{t_1} - X_0, X_{t_2} - X_{t_1}, \dots, X_{t_n} - X_{t_{n-1}}$  are independent, we say that  $X$  has independent increments. Moreover, for  $t \geq 0$ , if the distribution of  $X_{t+h} - X_t$  does not depend on  $t \geq 0$ , we say that  $X$  has stationary increments. Loosely speaking, one could say that the distribution of the future changes does not depend on past realizations.
- A process  $X$  is said to be non-decreasing, if  $Y_{t} \geq 0$  almost surely (a.s.) for  $t \geq 0$ , and  $Y_{t} \geq Y_{s}$  a.s. for  $0 \leq s \leq t$ . Usually, a non-decreasing process is called a subordinator. A process  $X$  is said to be non-increasing if  $Y_{t} \leq 0$  a.s. for  $t \geq 0$ , and  $Y_{t} \leq Y_{s}$  a.s. for  $0 \leq s \leq t$ .
- We say that a process  $X$  has finite (infinite) variation if its sample paths are of finite (infinite) variation, that is, the variation

$$
\begin{array}{l} V (X (\omega))_{t} = \lim_{n \rightarrow \infty} \sum_{k = 1}^{n} | X_{t k / n} (\omega) - X_{t (k - 1) / n} (\omega) |, \\ \forall t > 0 \\ \end{array}
$$ is finite (infinite) for almost every  $\omega \in \Omega$ .

- The characteristic function of the stochastic process  $X = (X_{t})_{t\geq 0}$  on  $\mathbb{R}$  is defined as the function  $\phi :\mathbb{R}\to \mathbb{C}$


$$
\phi_{X_{t}} (u) = E \left[ e^{i u X_{t}} \right]
$$

## POISSON PROCESS

Consider a process  $N = (N_{t})_{t\geq 0}$  derived by a Poisson distribution with parameter  $\lambda$  as follows:

1.  $N_0 = 0$
2.  $N$  has independent increments and stationary increments.
3. For any real numbers  $t \geq 0$  and  $h \geq 0$ , the variable  $(N_{t + h} - N_t)$  is a Poisson distributed random variable with parameter  $\lambda h$ , that is,

$$
\mathbb {P} \left(N_{t + h} - N_{t} = n\right) = e^{- \lambda h} \frac{\left(\lambda h\right)^{n}}{n !}, \quad n = 0, 1, 2, \dots
$$

The process  $N$  is referred to as the Poisson process with intensity  $\lambda$ .

If  $(\tau_{j})_{j\in \mathbb{N}}$  are independent exponential random variables with parameter  $\lambda$  and the random variable  $N_{t}$  is given by

$$
N_{t} = \inf  \left\{n \geq 1: \sum_{j = 1}^{n} \tau_{j} > t \right\}
$$ then it can be proven that the process  $(N_{t})_{t\geq 0}$  is the Poisson process with intensity  $\lambda$


The Poisson process is a fundamental example of a stochastic process with discontinuous trajectories, and a building block for constructing more complex jump processes.

### Compounded Poisson Process

The process  $X = (X_{t})_{t\geq 0}$  is referred to as a compounded Poisson process if  $X$  is defined by

$$
X_{t} = \sum_{k = 1}^{N_{t}} Y_{k}
$$ where


-  $Y_1, Y_2, \dots$  are independent and identically distributed (IID) random variables, and  $f$  is the probability density function of  $Y_1$ .
-  $(N_{t})_{t\geq 0}$  is a Poisson process with intensity  $\lambda$
-  $N_{t}$  and  $Y_{k}$  are independent for all  $t \geq 0$  and  $k = 1,2,\dots$ .

The characteristic function of  $X_{t}$  is equal to

$$
\phi_{\mathrm{X}_{t}} (u) = \exp \left(\lambda t \int_{- \infty}^{\infty} \left(e^{i u x} - 1\right) f (x) d x\right)
$$

Moreover, if  $f$  is given by the probability density function of the normal distribution, then  $X$  is referred to as a jump diffusion process.

## PURE JUMP PROCESS

Consider a process  $X^{x} = (X_{t}^{x})_{t\geq 0}$  for a given real number  $x$  such that

$$
X_{t}^{x} = x N_{t}^{\lambda (x)}
$$ where  $(N_{t}^{\lambda (x)})_{t\geq 0}$  is the Poisson process with intensity  $\lambda (x)$ . The number  $x$  represents the jump size and the intensity  $\lambda (x)$  is the expected number of jumps with size  $x$  in the unit time.


Let  $S = \{x_{j} \in \mathbb{R} : x_{j} \neq 0, j = 1, 2, \dots\}$  be a discrete subset of jump sizes,  $\lambda(x_{j}) > 0$  for all  $x_{j} \in S$ , and  $Y = (Y_{t})_{t \geq 0}$  be a process defined by

$$
Y_{t} = \gamma t + \sum_{j = 1}^{\infty} X_{t}^{x_{j}}
$$

If  $S$  consists of positive real numbers and  $\gamma > 0$ , then the process  $Y$  is non-decreasing. Conversely, if  $S$  consists of negative real numbers and  $\gamma < 0$ ,  $Y$  is non-increasing.

Since the characteristic function of  $X_{t}^{x}$  is equal to

$$
\phi_{X_{i}^{s}} (u) = \exp (\lambda (x) t \left(e^{- i u x} - 1\right))
$$ the characteristic function of  $Y_{t}$  is obtained by


$$
\phi_{Y_{t}} = \exp \left(i \gamma u t + t \sum_{j = 1}^{\infty} \lambda (x_{j}) \left(e^{i u x_{j}} - 1\right)\right)
$$

For the process  $Y$ , the function  $\nu$  defined by  $\nu(A) = \sum_{x_j \in A} \lambda(x_j)$  represents the expected number of jumps with size  $x \in A$  in the unit time interval, where  $A$  is a subset of  $S$ . For example, the expected number of jumps whose sizes are in  $\{x_1, x_2, \dots, x_n\}$  is equal to  $\nu(\{x_1, x_2, \dots, x_n\}) = \sum_{j=1}^{n} \lambda(x_j)$ .

Now, we extend the set of jump size  $S$  to the real number set  $\mathbb{R}$ . Then the expected number of jumps is defined by a map  $\nu$  from a subset of  $\mathbb{R}$  to a positive number. The map  $\nu$  is a jump measure, that is, the expected number of jumps whose sizes are in a real interval  $[a,b]$  is represented by  $\nu ([a,b])$ . Using  $\nu$ , we can obtain an extended process  $Y$  such that the characteristic function of  $Y_{t}$  is given by


$$
\phi_{Y_{t}} = \exp \left(i \gamma u t + t \int_{- \infty}^{\infty} (e^{i u x} - 1) \nu (d x)\right) \qquad (1)
$$ where  $\gamma \in \mathbb{R}$ . Jump sizes of process  $Y$  can be defined continuously. In this case, the measure  $\nu$  is referred to as a Lévy measure, that is, a Borel measure on  $\mathbb{R}$  satisfying  $\nu(0) = 0$  and


$$
\int_{- \infty}^{\infty} \min  \{1, x^{2} \} v (d x) <   \infty
$$

The class of jump processes satisfying (1) cannot contain infinite variation processes. To include infinite variation processes in the class of jump processes we will be using, we need a more general definition. Consider a process  $Z = (Z_{t})_{t\geq 0}$  such that the characteristic function of  $Z_{t}$  is given by

$$
\phi_{Z_{t}} = \exp \left(i \gamma u t + t \int_{- \infty}^{\infty} \left(e^{i u x} - 1 - i u x 1_{| x | \leq 1}\right) v (d x)\right) \tag {2}
$$

The process  $Z$  is referred to as the pure jump process. If

$$
\int_{- 1}^{1} | x | \nu (d x) = \infty
$$ then the characteristic function (1) is not defined, but the function (2) is well defined. The details can be found in Sato (1999) and Cont and Tankov (2004). The path behavior of the pure jump process is determined by the Lévy measure  $\nu$  and real number  $\gamma$ .


-  $\gamma > 0$  and  $\nu(A) = 0$  for all  $A \subset (-\infty, 0)$ , then  $Z$  is non-decreasing.
-  $\gamma < 0$  and  $\nu(A) = 0$  for all  $A \subset (0, \infty)$ , then  $Z$  is non-increasing.
- If  $\nu(\mathbb{R}) < \infty$  (i.e., the expected number of jumps on the unit time is finite), then we say that  $Z$  has a finite activity.
- If  $\nu(\mathbb{R}) = \infty$  (i.e., the expected number of jumps on the unit time is infinite), then we say that  $Z$  has an infinite activity.

- If  $\int_{-1}^{1}|x|\nu(dx) < \infty$ , the process  $Z$  has finite variation.
- If  $\int_{-1}^{1}|x|\nu(dx) = \infty$ , the process  $Z$  has infinite variation.

The building block of the pure jump process  $Z$  is the Poisson process. Hence,  $Z$  has the following properties:

$Z_{0} = 0$
-  $Z$  has independent and stationary increments; that is, the random variable  $(Z_{t} - Z_{s})$  is independent of the random variable  $(Z_{v} - Z_{u})$  for all real numbers  $s, t, u$ , and  $v$  with  $0 \leq s < t < u < v$ .
-  $Z_{s + t} - Z_s \stackrel{\mathrm{d}}{=} Z_t$  for  $s \geq 0$  and  $t > 0$ . Moreover, we have

$$
\log \phi_{z_{t}} (u) = t \log \phi_{z_{1}} (u) \tag {3}
$$ where  $\phi_{z_t}(u)$  is the characteristic function of  $Z_{t}$  for  $t > 0$


If  $t = 1$ , then we obtain the purely non-Gaussian infinitely divisible random variable. In fact, there is a one-to-one correspondence between a purely non-Gaussian infinitely divisible random variable and a pure jump process.

### Gamma Process

Consider the gamma distribution with parameter  $(c,\lambda)$ . Since the gamma distribution is a purely non-Gaussian infinitely divisible distribution, we can define a pure jump process  $G = (G_{t})_{t\geq 0}$  such that  $G_{1}\sim \mathrm{Gamma}(c,\lambda)$ . By equation (3), the characteristic function  $\phi_{G_t}$  of  $G_{t}$  is given by

$$
\phi_{G_{t}} = \left(\frac{\lambda}{\lambda - i u}\right)^{c t} \tag {4}
$$

In this case, the process  $G$  is referred to as the gamma process with parameter  $(\lambda, c)$ . The sample path of the gamma process is non-decreasing, since the gamma distribution is supported only on the positive real line. When we take  $c = 1$  of the gamma process, the process is referred to as an exponential process.

### Inverse Gaussian Process

Consider the inverse Gaussian distribution with parameter  $(c,\lambda)$ . Since the inverse Gaussian distribution is also a purely non-Gaussian infinitely divisible distribution, we can define a pure jump process  $X = (X_{t})_{t\geq 0}$  such that  $X_{1}\sim IG(c,\lambda)$ . By equation (3), the characteristic function  $\phi_{X_t}$  of  $X_{t}$  is given by

$$
\phi_{X t} = \exp \left(- c t \left(\sqrt{\lambda^{2} - 2 i u} - \lambda\right)\right) \tag {5}
$$

In this case, the process  $X$  is referred to as the inverse Gaussian (IG) process with parameter  $(c, \lambda)$ . The sample path of the gamma process is nondecreasing, since the inverse Gaussian distribution is supported only on the positive real line.

### Variance Gamma Process

The variance gamma process is an infinitely divisible distribution. Thus we can define pure jump processes  $X = (X_{t})_{t\geq 0}$  such that  $X_{1}\sim VG(C, \lambda_{+},\lambda_{-})$ . By equation (3), the characteristic function  $\phi_{X_t}$  of  $X_{t}$  is given by

$$
\phi_{X_{t}} = \left(\frac{\lambda_{+} \lambda_{-}}{(\lambda_{+} - i u) (\lambda_{-} + i u)}\right)^{C t} \tag {6}
$$

In this case, the process  $X$  is referred to as the variance gamma (VG) process with parameter  $(C,\lambda_{+},\lambda_{-})$ .

### $\alpha$-Stable Process

The pure jump process  $X = (X_{t})_{t\geq 0}$  is referred to as the  $\alpha$  -stable process with parameters  $(\alpha ,\sigma ,\beta ,\mu)$  if  $X_{1}$  is an  $\alpha$  -stable random variable, that is,  $X_{1}\sim S_{\alpha}(\sigma ,\beta ,\mu)$  . By equation (3), the characteristic function  $\phi_{X_t}$  of  $X_{t}$  is given by

$$
\phi_{X_{i}} (u) = \left\{ \begin{array}{l} \big (\exp (i \mu u t - t | \sigma u |^{\alpha} \\ \qquad \times \Big (1 - i \beta (\text{si gn} u) \tan \frac{\pi \alpha}{2} \Big) \big),   \alpha \neq 1 \\ \big (\exp (i \mu u t - t \sigma | u | \\ \qquad \times \Big (1 + i \beta \frac{2}{\pi} (\text{si gn} u) \ln | u | \Big) \big),   \alpha = 1 \end{array} \right.
$$

Recall the Lévy measure of the  $\alpha$ -stable process can be written as

$$ v (d x) = \left(\frac{C_{+}}{x^{1 + \alpha}} 1_{x > 0} + \frac{C_{-}}{| x |^{1 + \alpha}} 1_{x <   0}\right) d x
$$ where  $C_+$  and  $C_-$  are positive constants. Then we can prove that


$$
\nu (\mathbb {R}) = \int_{- \infty}^{\infty} \nu (d x) = \infty
$$ and hence the  $\alpha$ -stable process is an infinite activity process. On the other hand, since we have


$$
\int_{- 1}^{1} | x | v (d x) = \left\{ \begin{array}{c c} \frac{C_{+} + C_{-}}{1 - \alpha}, & \alpha <   1 \\ \infty , & \alpha \geq 1 \end{array} \right.
$$ we conclude that the  $\alpha$ -stable process has finite variation if  $\alpha < 1$  and the infinite variation if  $\alpha \geq 1$ .


### Tempered Stable Process

The pure jump process  $X = (X_{t})_{t\geq 0}$  is referred to as the tempered stable process if  $X_{1}$  is the tempered stable random variable.

- The process  $X$  is referred to as the classical tempered stable (CTS) process with parameters  $(\alpha, C, \lambda_{+}, \lambda_{-}, m)$  if  $X_{1} \sim \mathrm{CTS}(\alpha, C, \lambda_{+}, \lambda_{-}, m)$ . The process  $X$  is referred to as the standard CTS process with parameters  $(\alpha, \lambda_{+}, \lambda_{-})$  if  $X_{1} \sim \mathrm{stdCTS}(\alpha, \lambda_{+}, \lambda_{-})$ .
- The process  $X$  is referred to as the generalized tempered stable (GTS) process with parameters  $(\alpha_{+},\alpha_{-},C_{+},C_{-},\lambda_{+},\lambda_{-},m)$  if  $X_{1}\sim \mathrm{GTS}(\alpha_{+},\alpha_{-}, C_{+},C_{-},\lambda_{+},\lambda_{-},m)$ . The process  $X$  is referred to as the standard GTS process with parameters  $(\alpha_{+},\alpha_{-},\lambda_{+},\lambda_{-},p)$  if  $X_{1}\sim \mathrm{stdGTS}(\alpha_{+},\alpha_{-},\lambda_{+},\lambda_{-},p)$ .
- The process  $X$  is referred to as the modified tempered stable (MTS) process with parameters  $(\alpha, C, \lambda_{+}, \lambda_{-}, m)$  if  $X_{1} \sim MTS(\alpha, C, \lambda_{+}, \lambda_{-}, m)$ . The process  $X$  is referred to as the standard MTS process with parameters  $(\alpha, \lambda_{+}, \lambda_{-})$  if  $X_{1} \sim \mathrm{stdMTS}(\alpha, \lambda_{+}, \lambda_{-})$ .
- The process  $X$  is referred to as the normal tempered stable (NTS) process with parameters  $(\alpha, C, \lambda, \beta, m)$  if  $X_1 \sim NTS(\alpha, C, \lambda, \beta, m)$ . The process  $X$  is referred to as the standard

NTS process with parameters  $(\alpha, \lambda, \beta)$  if  $X_1 \sim \mathrm{stdNTS}(\alpha, \lambda, \beta)$ .

Moreover, the process  $X$  is referred to as the normal inverse Gaussian (NIG) process with parameters  $(c,\lambda ,\beta ,m)$  if  $X_{1}\sim NIG(c,\lambda ,\beta ,m)$ . The process  $X$  is referred to as the standard NIG process with parameters  $(\lambda ,\beta)$  if  $X_{1}\sim$  stdNIG(λ,β).
- The process  $X$  is referred to as the Kim-Rachev tempered stable (KRTS) process with parameters  $(\alpha, k_{+}, k_{-}, r_{+}, r_{-}, p_{+}, p_{-}, m)$  if  $X_{1} \sim KRTS(\alpha, k_{+}, k_{-}, r_{+}, r_{-}, p_{+}, p_{-}, m)$ . The process  $X$  is referred to as the standard KRTS process with parameters  $(\alpha, r_{+}, r_{-}, p_{+}, p_{-})$  if  $X_{1} \sim \mathrm{stdKRTS}(\alpha, r_{+}, r_{-}, p_{+}, p_{-})$ .
- The process  $X$  is referred to as the rapidly decreasing tempered stable (RDTS) process with parameters  $(\alpha, C, \lambda_{+}, \lambda_{-}, m)$  if  $X_{1} \sim \mathrm{RDTS}(\alpha, C, \lambda_{+}, \lambda_{-}, m)$ .

$\lambda_{+}, \lambda_{-}, m)$ . The process  $X$  is referred to as the standard RDTs process with parameters  $(\alpha, \lambda_{+}, \lambda_{-})$  if  $X_{1} \sim \mathrm{stdRDTS}(\alpha, \lambda_{+}, \lambda_{-})$ .

The characteristic function  $\phi_{X_t}$  of  $X_{t}$  is obtained by equation (3). For example, if  $X$  is the CTS process with parameters  $(\alpha, C, \lambda_{+}, \lambda_{-}, m)$ , then

$$
\begin{array}{l} \phi_{X_{t}} (u) = \exp (t \log (\phi_{C T S} (u; \alpha , C, \lambda_{+}, \lambda_{-}, m))) \\ = \exp (i u m t - i u t C \Gamma (1 - \alpha) \left(\lambda_{+}^{\alpha - 1} - \lambda_{-}^{\alpha - 1}\right) \\ + t C \Gamma (- \alpha) \left(\left(\lambda_{+} - i u\right)^{\alpha} - \lambda_{+}^{\alpha} \right. \\ + \left(\lambda_{-} + i u\right)^{\alpha} - \left. \lambda_{-}^{\alpha}\right)) \\ \end{array}
$$

Characteristic exponents of tempered stable processes are presented in Table 1.

Let  $\nu(dx)$  be the Lévy measure of the tempered stable process. Then we can prove that  $\nu(\mathbb{R}) = \infty$ ,  $\int_{-1}^{1}|x|\nu(dx) < \infty$  if  $\alpha < 1$ , and  $\int_{-1}^{1}|x|\nu(dx) = \infty$  if  $\alpha \geq 1$ . Consequently, the

Table 1 Characteristic Exponents of Tempered Stable Processes

<table><tr><td>Process</td><td>ψxt(u) = log φxt(u)</td></tr><tr><td>CTS</td><td>iumt - iutCΓ(1 - α)(λ+α-1 - λ-α-1) +tCΓ(-α)((λ+ - iu)α - λ+α + (λ- + iu)α - λ-)</td></tr><tr><td>GTS</td><td>iumt - iutΓ(1 - α)(C+λ+α-1 - C-λ-α-1) +tC+Γ(-α+)((λ+ - iu)α+ - λ+) + tC-Γ(-α-)((λ- + iu)α- - λ-))</td></tr><tr><td>MTS</td><td>iumt + tC(GR(u; α, λ+)+GR(u; α, λ-)) + iutC(GI(u; α, λ+)-GI(u; α, λ-)) where GR(x; α, λ) = 2-α+3/2√πΓ(-α/2) ((λ2 + x2)α/2 - λα) and GI(x; α, λ) = 2-α+1/2Γ(1 - α/2) λα-1[2F1(1, 1 - α/2; 3/2; -x2/λ2) - 1]</td></tr><tr><td>NTS</td><td>iumt - iut2-α-1/2C√πΓ(1 - α/2)β(λ2 - β2)α/2-1 +t2-α+1/2C√πΓ(-α/2) ((λ2 - (β + iu)2)α/2 - (λ2 - β2)α/2)</td></tr><tr><td>NIG</td><td>iumt - iutcβ/√λ2 - tc(√λ2 - (β + iu)2 - √λ2 - β2)</td></tr><tr><td>KRTS</td><td>iumt - iutΓ(1 - α)(k+ r+ p+1 - k-r- p+1) +tk+H(iu; α, r+, p+) + tk-H(-iu; α, r-, p-) where H(x; α, r, p) = Γ(-α)/p(2F1(p, -α; 1 + p; rx) - 1)</td></tr><tr><td>RDTS</td><td>iumt + tC(Giu; α, λ+) + G(-iu; α, λ-)) where G(x; α, λ) = 2-α/2-1λαΓ(-α/2)(M(-α/2, 1/2; x2/2λ2) - 1) +2-α/2-1/2λα-1xΓ(1 - α/2)(M(1 - α/2, 3/2; x2/2λ2) - 1)</td></tr></table> tempered stable process has infinite activity, and has finite variation if  $\alpha < 1$  and infinite variation if  $\alpha \geq 1$ , as explained in Carr et al. (2002), Kim (2005), and Kim et al. (2008 and 2010).


## Brownian Motion

In this section, we will discuss Brownian motion by means of an example. We begin with a short summary of the most important and defining properties of a standard Brownian motion  $W = (W_{t})_{t\geq 0}$

1.  $W_{0} = 0$
2. W has independent increments and stationary increments.
3. For any real numbers  $t\geq 0$  and  $h\geq 0$  , the variable  $(W_{t + h} - W_t)$  is a normally distributed random variable with mean zero and variance  $h$
4. The paths of  $\mathrm{W} = (\mathrm{W}_t)_{t\geq 0}$  are continuous.

Every process fulfilling the above four properties is referred to as the standard Brownian motion. From the second and third conditions it can be deduced that Brownian motion  $W_{t}$  at time  $t$  (which equals the increment from time 0 to time  $t$ ) is normally distributed with mean zero and variance  $t$ .

The paths of Brownian motion are highly irregular and nowhere differentiable. In order to draw a true path, one would have to calculate the value of the process for every real number, which is clearly not feasible. Due to its characteristic path property, it is impossible to draw a real path of Brownian motion. The process can only be evaluated for a discrete set of points. Figure 1 illustrates possible paths of Brownian motion. Strictly speaking, the plotted paths are only discrete approximations to the true paths.

From the above definition of the process, it may not be clear how one can envision a Brownian motion or how one could construct it. Therefore, we will present a constructive method demonstrating how one can generate a Brownian motion as the limit of very simple processes. We restrict the presentation to the unit interval (i.e., we assume  $0 \leq t \leq 1$ ) but the generalization to the abstract case should be obvious. The procedure is iterative, which means that on the  $k$ th step of the iteration we define a process  $(X_{t}^{(k)})_{0 \leq t \leq 1}$ , which will serve as an approximation for a standard Brownian motion.


Let random variables  $I_1, I_2, I_3, \dots$  be IID with

$$
I_{j} = \left\{ \begin{array}{l l} 1 & \text{wi th pr ob ab il it y} p = 0. 5 \\ - 1 & \text{wi th pr ob ab il it y} 1 - p = 0. 5 \end{array} \right.,
$$

$$ j = 1, 2, \dots
$$

Define  $X_{t}^{(k)} = \frac{1}{\sqrt{k}}\sum_{j = 1}^{n}I_{j}$  where  $t = n / k$  and  $n = 0,1,\dots ,k$ . If the value  $t$  is on the interval  $\left(\frac{n}{k},\frac{n + 1}{k}\right)$ , then we take a value obtained by a linear interpolation as

$$
X_{t}^{(k)} = (k t - n) X_{n / k}^{(k)} + (k t - n - 1) X_{(n + 1) / k}^{(k)}
$$

By doing so, we get a stochastic process with continuous paths.

Let's start with  $k = 1$ . Then we have

$$
X_{0}^{(1)} = 0,
$$

$$
X_{1}^{(1)} = \left\{ \begin{array}{l l} 1 & \text{wi th pr ob ab il it y} p = 0. 5 \\ - 1 & \text{wi th pr ob ab il it y} 1 - p = 0. 5 \end{array} \right.
$$

At any time  $t$  the random variable  $X_{1}^{(1)}$  can take only two possible values, namely  $-t$  and  $t$ . At any time, the process has zero mean and the variance at time  $t = 1$  equals

$$
V \left(X_{1}^{(1)}\right) = 1^{2} \cdot 0. 5 + (- 1)^{2} \cdot 0. 5 = 1
$$

That is not so bad for the first step, but obviously the distribution of  $X_{1}^{(1)}$  is far from being normal.

What we do in the next step,  $k = 2$ , is allow for two different values until time  $t = \frac{1}{2}$  and three different values for  $\frac{1}{2} \leq t \leq 1$ . We do so by defining:

$$
X_{0}^{(2)} = 0,
$$

$$
X_{0. 5}^{(2)} = \left\{ \begin{array}{l l} \frac{1}{\sqrt{2}} & \text{wi th pr ob ab il it y} p = 0. 5 \\ - \frac{1}{\sqrt{2}} & \text{wi th pr ob ab il it y} 1 - p = 0. 5 \end{array} \right.
$$

$$
X_{1}^{(2)} = \left\{ \begin{array}{l l} \sqrt{2} & \text{wi th pr ob ab il it y} p^{2} = 0. 2 5 \\ 0 & \text{wi th pr ob ab il it y} p (1 - p) = 0. 5 \\ - \sqrt{2} & \text{wi th pr ob ab il it y} (1 - p)^{2} = 0. 2 5 \end{array} \right.
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-29/f56b2146-2a3e-454e-b6f0-d79cc2db7528/c1fade8c04774be16ce62fe0dea19bc325bb10b4b63bc9311067d2046c0f6049.jpg)
Figure 1 Possible Paths of a Standard Brownian Motion (Every Path Consists of 10,000 Equally Spaced Observations)

The process  $X_{t}^{(2)}$  now has four possible paths. The mean of  $X_{t}^{(2)}$  is zero and the variance of  $X_{t}^{(2)}$  equals

$$
V \left(X_{0. 5}^{(2)}\right) = \left(\frac{1}{\sqrt{2}}\right)^{2} \cdot 0. 5 + \left(- \frac{1}{\sqrt{2}}\right)^{2} \cdot 0. 5 = 0. 5
$$

$$
V \left(X_{1}^{(2)}\right) = \sqrt{2^{2}} \cdot 0. 2 5 + (- \sqrt{2})^{2} \cdot 0. 2 5 = 1
$$ but still the distribution of  $X_{t}^{(2)}$  is far from being normal.


By iterating the stated procedure, the probability of  $X_{t}^{(k)}$  is given by

$$
\mathbf {P} \left(X_{t}^{(k)} = \frac{n - 2 m}{\sqrt{k}}\right) = \left( \begin{array}{c} n \\ m \end{array} \right) \left(\frac{1}{2}\right)^{n}
$$ if  $m\in \{0,1,2,\dots ,n\}$ $t = n / k$ $n\in \{0,1,2,\dots ,k\}$  The mean and variance can be obtained as follows:


$$
E \left[ X_{t}^{(k)} \right] = \frac{1}{\sqrt{k}} \sum_{j = 1}^{n} E [ I_{j} ] = 0
$$

$$
V \left(X_{t}^{(k)}\right) = \frac{1}{k} \sum_{j = 1}^{n} E \left[ I_{j}^{2} \right] = \frac{n}{k}
$$ where  $t = n / k, n = 1,2,\dots ,k$ . Since  $X_{n / k}^{(k)}$  is defined by the sum of IID random variables, it has


- Independent increments:  $X_{n_1 / k}^{(k)}$  and  $X_{n_2 / k}^{(k)} - X_{n_1 / k}^{(k)}$  are independent, for all  $n_1, n_2 \in \{0, 1, \dots, k\}$  with  $n_1 < n_2$ .
- Stationary increments:  $X_{n_2 / k}^{(k)} - X_{n_1 / k}^{(k)} \stackrel{\mathrm{d}}{=} X_{(n_2 - n_1) / k}^{(k)}$  for all  $n_1, n_2 \in \{0, 1, \dots, k\}$  with  $n_1 < n_2$ .

Moreover, the distribution of  $X_{t}^{(k)}$  will approach the normal distribution due to the central limit theorem. Consequently, we have found all the defining properties of a Brownian motion in this simple approximating process, that is, the process  $(X_{t}^{(k)})_{0\leq t\leq 1}$  converges in distribution to the standard Brownian motion  $(W_{t})_{0\leq t\leq 1}$ .

In the context of financial applications, there are two main variants of the standard Brownian motion which have to be mentioned: the arithmetic and the geometric Brownian motion. Both are obtained as a function of the standard Brownian motion.

### Arithmetic Brownian Motion

Given a Brownian motion  $(W_{t})_{t\geq 0}$  and two real constants  $\mu$  and  $\sigma$ , the arithmetic Brownian motion  $(X_{t})_{t\geq 0}$  is obtained as:

$$
X_{t} = \mu t + \sigma W_{t}
$$

The process  $(X_{t})_{t\geq 0}$  consists of the sum of a purely deterministic linear trend function  $\mu t$  and a rescaled Brownian motion  $\sigma W_{t}$ . The latter has the property that at time  $t$ ,  $\sigma W_{t}$  is normally distributed with mean 0 and variance  $\sigma^2 t$ . The paths will therefore randomly jitter around the deterministic trend with a variance proportional to the point in time  $t$  under consideration. The arithmetic Brownian motion is a simple but popular model for financial asset returns.

### Geometric Brownian Motion

Given a Brownian motion  $(W_{t})_{t\geq 0}$ , two real constants  $\mu$  and  $\sigma$ , and a starting value  $S_0 > 0$ , the geometric Brownian motion  $(S_{t})_{t\geq 0}$  is obtained as:

$$
S_{t} = S_{0} e^{\mu t + \sigma W_{t}}
$$

The process  $(S_{t})_{t\geq 0}$  is just the exponential of an arithmetic Brownian motion multiplied by a factor. Therefore  $\log (S_t / S_0)$  is normally distributed and

$$
E [ S_{t} / S_{0} ] = e^{\mu t + \frac{1}{2} \sigma^{2} t}
$$

## Time-Changed Brownian Motion

If a pure jump process process  $T = (T_{t})_{t\geq 0}$  is nondecreasing, that is,  $T_{t}\geq 0$  a.s. for  $t > 0$ , and  $T_{t}\geq T_{s}$  a.s. for  $s\leq t$ , then the process  $T$  is referred to as the subordinator or intrinsic time process. Intuitively, it can be thought of as the cumulative trading volume process for a financial asset which measures the cumulative volume of all the transitions up to physical time  $t$  (Rachev and Mittnik, 2000).

The Poisson, gamma, and inverse Gaussian processes are non-decreasing, and hence they are subordinators. Moreover, for the case where

$0 < \alpha < 1$ , the support of the  $\alpha$ -stable distribution  $S_{\alpha}(\alpha, 1, 0)$  is the positive real line. Hence, the  $\alpha$ -stable process with parameters  $(\frac{\alpha}{2}, \sigma, 1, 0)$  and  $0 < \alpha < 2$  is a subordinator and referred to as  $\alpha$ -stable subordinator. If we can consider an additional assumption that  $E[T_t] = t$ , this would mean that the expected intrinsic time is the same as physical time.

If we take an arithmetic Brownian motion and change the physical time to a subordinator, then we obtain the time-changed Brownian motion. That is, take an arithmetic Brownian motion with drift  $\mu$  and volatility  $\sigma$  as follows:

$$
\mu t + \sigma W_{t}
$$ and consider a subordinator  $T = (T_{t})_{t\geq 0}$  independent to the standard Brownian motion  $(W_{t})_{t\geq 0}$ . Then substituting  $t = T_{t}$  in the arithmetic Brownian motion, we have a new process  $X = (X_{t})_{t\geq 0}$  with


$$
X_{t} = \mu T_{t} + \sigma W_{T_{t}}
$$ which is the time-changed Brownian motion.


If  $T_{t}$  is fixed, then the conditional probability of  $X_{t}$  with a fixed variable  $T_{t}$  follows a normal distribution, that is

$$
\begin{array}{l} \mathbf {P} \left(X_{t} <   y \mid T_{t}\right) = \mathbf {P} \left(\mu T_{t} + \sigma W_{T_{t}} <   y \mid T_{t}\right) \\ = \frac{1}{\sqrt{2 \pi \sigma^{2} T_{t}}} \int_{- \infty}^{y} e^{\frac{(x - \mu T_{t})^{2}}{2 \sigma^{2} T_{t}}} d x \\ \end{array}
$$

Using properties of the conditional probability and independence between  $W_{t}$  and  $T_{t}$ , the distribution function  $F_{X_{t}}$  and the probability density function  $f_{X_{t}}$  of  $X_{t}$  of  $X_{t}$  are obtained by

$$
\begin{array}{l} F_{X_{t}} (y) = \mathbf {P} \left(X_{t} <   y\right) \\ = \int_{- \infty}^{y} \int_{0}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^{2} s}} e^{- \frac{(x - \mu s)^{2}}{2 \sigma^{2} s}} f_{T_{t}} (s) d s d x \\ \end{array}
$$ and


$$ f_{X_{t}} (y) = \frac{d}{d y} F_{X_{t}} (y) = \int_{0}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^{2} s}} e^{- \frac{(y - \mu s)^{2}}{2 \sigma^{2} s}} f_{T_{t}} (s) d s
$$ respectively, where  $f_{T_t}$  is the probability density function of  $T_t$ . Moreover, we can derive the characteristic function  $\phi_{X_t}$  as follows:


$$
\phi_{\mathrm{X}_{t}} (u) = \phi_{T t} \left(\mu u + \frac{i u^{2} \sigma^{2}}{2}\right) \tag {7}
$$ where  $\phi_{T_t}$  is the characteristic function of  $T_{t}$ . Using the time-changed Brownian motion, we can define various processes.


The time-changed Brownian motion construction is well known from the theory of stochastic processes and is referred to as the Skorokhod embedding problem. Theoretically, every Lévy process can be defined as the time-changed Brownian motion. More in general, a process can be embedded in a Brownian motion if and only if it is a local semimartingale, as proved by Monroe (1978).

Although the representation via Brownian subordination is a nice property, we do not know a general constructive method to find the process  $T_{t}$  such that  $X_{t} = \mu T_{t} + \sigma W_{T_{t}}$ . This means that given a semimartingale  $X_{t}$ , the time process  $T_{t}$  is not always of known form. Thus, this approach can be applied only for some particular Lévy processes.

### Variance Gamma Process

By considering the gamma process as the subordinator of the Brownian motion, we obtain the VG process. That is, the VG process is defined by  $X = (X_{t})_{t\geq 0}$  with

$$
X_{t} = \mu G_{t} + \sigma W_{G_{t}}
$$ where  $G = (G_{t})_{t\geq 0}$  is the gamma process with parameter  $(c,\lambda)$ . In order to reduce the number of parameters, we consider the assumption  $E[G_{t}] = t$ . Since we have  $E[G_{t}] = \frac{ct}{\lambda}$ , the assumption is satisfied if  $c = \lambda$ . Then the characteristic function of  $X_{t}$  is equal to


$$
\phi_{X_{t}} (u) = \left(\frac{c}{c - i \mu u + \frac{u^{2} \sigma^{2}}{2}}\right)^{c t} = \left(\frac{\frac{2 c}{\sigma^{2}}}{\frac{2 c}{\sigma^{2}} - \frac{2 \mu}{\sigma^{2}} u i + u^{2}}\right)^{c t} \tag {8}
$$ by (7) and the characteristic function of  $G_{t}$  given in (4) with  $c = \lambda$ . Inserting into (8) the parametrization


$$
\lambda_{-} - \lambda_{+} = \frac{2 \mu}{\sigma^{2}}
$$

$$
\lambda_{+} \lambda_{-} = \frac{2 c}{\sigma^{2}}
$$

$$
C = c
$$ we obtain the form given by (6).


### Normal Inverse Gaussian Process

By considering the inverse Gaussian process as the subordinator of the Brownian motion, we obtain the NIG process.

Define a process  $X = (X_{t})_{t\geq 0}$  with

$$
X_{t} = \mu T_{t} + \sigma W_{T_{t}}
$$ where  $T = (T_{t})_{t\geq 0}$  is the inverse Gaussian process with parameter  $(c,\lambda)$ , satisfying  $E[T_{t}] = t$ . The condition  $E[T_{t}] = t$  holds if  $c = \lambda$ . Then the characteristic function of  $X_{t}$  is equal to


$$
\begin{array}{l} \phi_{X_{t}} (u) = \exp \left(- k t \left(\sqrt{k^{2} - 2 i \mu u + 2 \sigma^{2} u^{2}} - k\right)\right) \\ = \exp \left(- \sqrt{2} k \sigma t \left(\sqrt{\frac{k^{2}}{2 \sigma^{2}} - \frac{\mu}{\sigma^{2}} i u + u^{2}} \right. \right. \\ \left. - \sqrt{\frac{k^{2}}{2 \sigma^{2}}}\right) \tag {9} \\ \end{array}
$$ by (7) and the characteristic function of  $T_{t}$  given in (5) with  $k\coloneqq c = \lambda$  . Inserting into (9) the parametrization


$$
\lambda^{2} - \beta^{2} = \frac{k^{2}}{2 \sigma^{2}}
$$

$$
\beta = \frac{\mu}{2 \sigma^{2}}
$$

$$ c = \sqrt{2} k \sigma
$$ we obtain the NIG process with parameter  $(c, \lambda, \beta, \frac{c\beta}{\sqrt{\lambda^2 - \beta^2}})$ .


### Normal Tempered Stable Process

Assume Lévy measure  $\nu$  is equal to

$$
\nu (d x) = \frac{c e^{- \theta x}}{x^{\alpha / 2 + 1}} 1_{x > 0} d x \tag {10}
$$ where  $\alpha \in (0,2), c > 0$ , and  $\theta > 0$ , and consider the pure jump process  $T = (T_{t})_{t\geq 0}$  defined by  $\nu$  and  $\gamma$ , where


$$
\gamma = \int_{0}^{1} x v (d x)
$$

Since  $\nu(A) = 0$  for all  $A \subset (-\infty, 0)$  and  $\mu \geq 0$ , the process  $T$  is a nondecreasing process. Hence it is a subordinator and referred to as the tempered stable subordinator with parameters

$(\alpha, c, \theta)$ . Using equation (2), the characteristic function  $\phi_{T_t}$  of  $T_t$  is equal to

$$
\phi_{T_{t}} (u) = \exp \left(t c \int_{0}^{\infty} \left(e^{i u x} - 1\right) \frac{e^{- \theta x}}{x^{\alpha / 2 + 1}} d x\right)
$$

Solving the integration in the last equation, we can obtain the following formula,

$$
\phi_{T_{t}} (u) = \exp \left(t c \Gamma \left(- \frac{\alpha}{2}\right) \left(\left(\theta - i u\right)^{\frac{\alpha}{2}} - \theta^{\frac{\alpha}{2}}\right)\right) \tag {11}
$$

The mean of  $T_{t}$  is computed by the first cumulant, that is,

$$
E \left[ T_{t} \right] = \frac{1}{i} \frac{\partial}{\partial u} \log \phi_{T_{t}} (u) |_{u = 0} = t c \Gamma \left(1 - \frac{\alpha}{2}\right) \theta^{\frac{\alpha}{2} - 1}
$$

Hence, the condition  $E[T_{t}] = t$  holds if  $c = \left(\Gamma\left(1 - \frac{\alpha}{2}\right)\theta^{\frac{\alpha}{2} - 1}\right)^{-1}$ .

By considering the tempered stable subordinator as the subordinator of the Brownian motion, we obtain the NTS process. That is, define a process  $X = (X_{t})_{t\geq 0}$  with

$$
X_{t} = \mu T_{t} + \sigma W_{T_{t}}
$$ where  $T = (T_{t})_{t\geq 0}$  is the tempered stable subordinator with parameter  $\left(\alpha ,\left(\Gamma \left(1 - \frac{\alpha}{2}\right)\theta^{\frac{\alpha}{2} -1}\right)^{-1},\theta\right)$ . The characteristic function of  $X_{t}$  is equal to


$$
\begin{array}{l} \phi_{X_{t}} (u) \\ = \exp \left(\frac{t \Gamma \left(- \frac{\alpha}{2}\right)}{\Gamma \left(1 - \frac{\alpha}{2}\right) \theta^{\frac{\alpha}{2} - 1}} \left(\left(\theta - i \left(\mu u + \frac{i \sigma^{2} u^{2}}{2}\right)\right)^{\frac{\alpha}{2}} - \theta_{2}^{\alpha}\right)\right) \\ = \exp \left(\frac{- 2 t}{\alpha \theta^{\frac{\alpha}{2} - 1}} \left(\left(\theta - i \left(\mu u + \frac{i \sigma^{2} u^{2}}{2}\right)\right)^{\frac{\alpha}{2}} - \theta^{\frac{\alpha}{2}}\right)\right) \tag {12} \\ \end{array}
$$ by (7) and (11) with  $c = \left(\Gamma \left(1 - \frac{\alpha}{2}\right)\theta^{\frac{\alpha}{2} -1}\right)^{-1}$ . The last equation can be changed to the following expression:


$$
\begin{array}{l} \phi_{X_{t}} (u) = \exp \left( \right.\frac{t \Gamma \left(- \frac{\alpha}{2}\right)\left(\frac{\sigma^{2}}{2}\right)^{\frac{\alpha}{2}}}{\Gamma \left(1 - \frac{\alpha}{2}\right) \theta_{2}^{\alpha - 1}} \left(\left(\frac{2 \theta}{\sigma^{2}} + \left(\frac{\mu}{\sigma^{2}}\right)^{2} \right.\right. \\ \left.\left. - \left(\frac{\mu}{\sigma^{2}} + i u\right)^{2}\right)^{\frac{\alpha}{2}} - \left(\frac{2 \theta}{\sigma^{2}}\right)^{\frac{\alpha}{2}}\right)\left. \right) \tag {13} \\ \end{array}
$$

Inserting into (13) the parametrization

$$
\begin{array}{l} \lambda = \sqrt{\frac{2 \theta}{\sigma^{2}} + \left(\frac{\mu}{\sigma^{2}}\right)^{2}} \\ \beta = \frac{\mu}{\sigma^{2}} \\ C = \frac{\sqrt{2 \sigma^{\alpha}}}{\sqrt{\pi} \Gamma \left(1 - \frac{\alpha}{2}\right) \theta^{\frac{\alpha}{2} - 1}} \\ \end{array}
$$ we obtain the NTS process with parameter  $(\alpha, C, \lambda, \beta, m)$  where


$$ m = - 2^{- \frac{\alpha + 1}{2}} C \sqrt{\pi} \Gamma \left(\frac{\alpha}{2}\right) \beta (\lambda^{2} - \beta^{2})^{\frac{\alpha}{2} - 1}
$$

## LEVY PROCESS

A stochastic process  $X = (X_{t})_{t\geq 0}$  is called a Lévy process if the following five conditions are satisfied:

1.  $X_0 = 0$  a.s.
2.  $X$  has independent increments.
3. X has stationary increment.
4.  $X$  is stochastically continuous that is,  $\forall t\geq 0$  and  $a > 0$

$$
\lim_{s \rightarrow t} \mathbf {P} \left[ | X_{s} - X_{t} | > a \right] = 0
$$

5.  $X$  is right continuous and has left limits (cadlag).

The standard Brownian motion, arithmetic Brownian motions, and pure jump processes are all Lévy processes. Moreover, a Lévy process can be decomposed by a Brownian motion and a pure jump process  $(Z_{t})_{t\geq 0}$  independent to the Brownian motion, that is

$$
X_{t} = \sigma W_{t} + Z_{t}
$$

Hence we obtain the characteristic function of  $X_{t}$  as follows:

$$
\begin{array}{l} \phi x_{t} (u) = \phi_{\sigma W_{t}} (u) \phi_{Z_{t}} (u) \\ = \exp \left(- \frac{t}{2} \sigma^{2} u^{2}\right) \exp (i \gamma u t + t \int_{- \infty}^{\infty} (e^{i u x} - 1 \\ - i u x 1_{| x | \leq 1}) v (d x)) \\ = \exp \left(i \gamma u t - \frac{t}{2} \sigma^{2} u^{2} + t \int_{- \infty}^{\infty} (e^{i u x} - 1 \right. \\ - i u x 1_{| x | \leq 1}) v (d x)) \\ \end{array}
$$ where  $\phi_{\sigma W_t}(u)$  is the characteristic function of  $N(0,\sigma^2 t)$ , and  $\phi_{Z_t}(u)$  given by (2). Therefore,


if  $X = (X_{t})_{t\geq 0}$  is a Lévy process, then for any  $t\geq 0$ ,  $X_{t}$  is an infinitely divisible random variable. Conversely, if  $Y$  is an infinitely divisible random variable, then there exists uniquely a Lévy process  $(X_{t})_{t\geq 0}$  such that  $X_{1} = Y$ , as proved by Sato (1999, p. 38).

## KEY POINTS

- Continuous-time stochastic processes are the building block of financial modeling and they are usually used to explain the uncertain behavior of financial assets. Some results of probability theory can be usefully applied to financial derivatives pricing and risk management.
- Given any infinitely divisible random variable  $X_1$ , it is possible to define a stochastic process with independent and stationary increments such that for all  $t > s$ , the increment  $X_t - X_s$  has characteristic function  $\exp((t - s)\log \phi_{X_1}(u))$ . These processes are known as Lévy processes.
- Brownian motion and Poisson processes are Lévy processes. All Lévy processes can be constructed by changing the deterministic time  $t$  of the Brownian motion  $W_{t}$  with a stochastic time  $T_{t}$ . This construction is called Brownian subordination and the increasing process  $T_{t}$  is a subordinator.
- There are two main variants of the standard Brownian motion used in financial applications: the arithmetic and the geometric Brownian motion.
- The Poisson process is a fundamental example of a stochastic process with discontinuous trajectories, and a building block for constructing more complex jump processes.
- Pure jump processes include also the gamma process, the inverse Gaussian process, the variance gamma process, the  $\alpha$ -stable process, and the tempered stable process.

## REFERENCES

Carr, P., Geman, H., Madan, D., and Yor, M. (2002). The fine structure of asset returns: An empirical investigation. Journal of Business 75, 2: 305-332.
Cont, R., and Tankov, P. (2004). Financial Modelling with Jump Processes. Boca Raton: CRC Press.
Embrechts, P., and Maejima, M. (2002). Selfsimilar Processes. New Jersey: Princeton University Press.
Karatzas, I., and Shreve, S. (1991). Brownian Motion and Stochastic Calculus. New York: Springer.
Kim, Y. S., Rachev, S. T., Bianchi, M. L., and Fabozzi, F. J. (2008). A new tempered stable distribution and its application to finance. In G. Bol, S. T. Rachev, and R. Wuerth (Eds.), Risk Assessment: Decisions in Banking and Finance. Heidelberg: Springer Verlag, 77-109.
Kim, Y. S., Rachev, S. T., Bianchi, M. L., and Fabozzi, F. (2010). Tempered stable and tempered infinitely divisible GARCH models. Journal of Banking and Finance 34, 9: 2096-2109.
Kim, Y. S. (2005). The Modified Tempered Stable Processes with Application to Finance. Ph.D thesis, Sogang University.
Monroe, I. (1978). Processes that can be embedded in Brownian motion. The Annals of Probability 6, 1: 42-56.
Øksendal, B. (2003). Stochastic Differential Equations: An Introduction with Applications, 5th ed. New York: Springer.
Rachev, S. T., Kim, Y. S., Bianchi, M. L., and Fabozzi, F. J. (2011). Financial Models with Lévy Processes and Volatility Clustering. Hoboken, NJ: Wiley.
Rachev, S. T., and Mittnik, S. (2000). Stable Parelian Models in Finance. Chichester: John Wiley & Sons.
Samorodnitsky, G., and Taqqu, M. (1994). Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance. New York: CRC Press.
Sato, K. (1999). Lévy Processes and Infinitely Disvisible Distributions. Cambridge: Cambridge University Press.
Schoutens, W. (2003). Lévy Processes in Finance: Pricing Financial Derivatives. Hoboken, NJ: Wiley.
