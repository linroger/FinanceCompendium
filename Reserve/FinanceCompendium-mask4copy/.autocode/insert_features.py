#!/usr/bin/env python3
"""Insert features into the database."""

import sqlite3

DB_PATH = "/Users/rogerlin/Reserve/FinanceCompendium-mask4copy/.autocode/progress.db"

features = [
    ("infrastructure", "SQLite database created with full schema (files, chunks, concepts, mentions, proposals, applied_links, qa_samples, run_statistics, rejected_concepts tables with all indexes)", "python3 -c \"import sqlite3; conn=sqlite3.connect('vault_linking.db'); cur=conn.cursor(); cur.execute('SELECT name FROM sqlite_master WHERE type=\\'table\\''); tables=[r[0] for r in cur.fetchall()]; required=['files','chunks','concepts','concept_mentions','link_proposals','applied_links','qa_samples','run_statistics','rejected_concepts']; assert all(t in tables for t in required), f'Missing: {[t for t in required if t not in tables]}'; print('Schema OK')\""),
    ("phase1", "Vault discovery finds all markdown files in folders I-X, validates masking completeness (no unmasked LaTeX, tables, code fences, YAML frontmatter)", "python3 -c \"import os, re; vault_path=os.getcwd(); target_folders=[f'{vault_path}/I. Foundations',f'{vault_path}/II. Economics',f'{vault_path}/III. Markets and Institutions',f'{vault_path}/IV. Fixed Income',f'{vault_path}/V. Equities and Alternatives',f'{vault_path}/VI. Derivatives',f'{vault_path}/VII. Corporate Finance',f'{vault_path}/VIII. Portfolio Management',f'{vault_path}/IX. Risk Management',f'{vault_path}/X. Quantitative Trading']; files=[]; [files.extend([os.path.join(d,f) for f in fs if f.endswith('.md')]) for d in target_folders if os.path.exists(d) for r,d,fs in [os.walk(d)]]; assert len(files)>0; print(f'Found {len(files)} markdown files')\""),
    ("phase2", "Semantic chunking splits documents into 1500-4500 word chunks with cohesion assessment (high/medium/low)", "python3 -c \"import math; text='word '*4000; chunks=[]; target=3000; min_w=1500; max_w=4500; n_chunks=math.ceil(len(text.split())/target); [chunks.append(' '.join(text.split()[i*target:(i+1)*target])) for i in range(n_chunks)]; [assert min_w <= len(c.split()) <= max_w for c in chunks]; print(f'Chunked into {len(chunks)} chunks')\""),
    ("phase2", "4-pass concept extraction pipeline (Structural Mining, Mechanism/Causality Mining, Terminology Variation, Domain-Specific Patterns) extracts candidate concepts", "python3 -c \"import re; text='Liquidity risk drives fire sales. Systemic risk amplification channels transmit through interbank networks.'; trigger_verbs=['drives','leads to','amplifies','transmits','propagates']; candidates=[]; [candidates.extend(re.findall(r'\\\\b(\\\\w+(?: \\\\w+){{1,5}}?)\\\\b', line)) for line in text.split('.') if any(v in line for v in trigger_verbs)]; assert len(candidates)>=3; print(f'Extracted {len(candidates)} candidates')\""),
    ("quality", "Pentatonic Filter scores concepts across 5 dimensions (Syntactic Validity, Conceptuality, Specificity, Salience, Linkability) with minimum 7/10 threshold", "python3 -c \"def score_concept(concept): syntactic=2 if len(concept.split())>=3 and len(concept.split())<=6 else 0; conceptuality=2 if '2008' not in concept and 'Smith' not in concept else 0; specificity=2 if concept not in ['financial markets','risk management','monetary policy'] else 1; salience=2 if len(concept.split())>=3 else 1; linkability=2 if 'constraints' in concept or 'mechanisms' in concept else 1; return (syntactic+conceptuality+specificity+salience+linkability)/2.0; result=score_concept('intertemporal budget constraints'); assert result>=7.0; print(f'Pentatonic: {result}/10')\""),
    ("quality", "CRS (Composite Relevance Score) calculates 5-dimension scores for mentions with formula: CRS = (D×2.5) + (C×2.0) + (U×2.0) + (A×2.0) + (Cx×1.5)", "python3 -c \"def calculate_crs(depth, centrality, uniqueness, authority, context): weights={'depth':2.5,'centrality':2.0,'uniqueness':2.0,'authority':2.0,'context':1.5}; scores={'depth':depth,'centrality':centrality,'uniqueness':uniqueness,'authority':authority,'context':context}; return round(sum(weights[d]*scores[d] for d in weights), 1); crs=calculate_crs(2,2,1,1,2); assert crs==8.0; print(f'CRS: {crs}/10')\""),
    ("phase3", "Vault-wide concept deduplication identifies and marks concepts appearing in >50 documents as too_broad, generates 2-5 aliases per concept", "python3 -c \"def mark_too_broad(concepts, threshold=50): return {c['name']:{'too_broad':c['doc_freq']>threshold,'aliases':c['aliases']} for c in concepts}; concepts=[{'name':'intertemporal budget constraints','doc_freq':5,'aliases':['IBC']},{'name':'risk management','doc_freq':75,'aliases':[]},{'name':'liquidity risk','doc_freq':42,'aliases':['funding liquidity']}]; results=mark_too_broad(concepts); assert results['risk management']['too_broad']==True; assert results['intertemporal budget constraints']['too_broad']==False; print('Deduplication OK')\""),
    ("phase4", "Mention selection selects top 5 mentions per concept with per-document cap (1 per document max), applies frequency penalty for 20-50 doc concepts", "python3 -c \"def select_top_mentions(mentions, max_per_concept=5, penalty_start=20): per_doc={}; [per_doc.setdefault(m['file'], []).append(m) for m in mentions]; filtered=[max(per_doc[f], key=lambda x: x['crs']) for f in per_doc]; [m.__setitem__('adjusted', m['crs']*(1-0.01*(m['vault_freq']-penalty_start)) if m['vault_freq']>=penalty_start else m['crs']) for m in filtered]; filtered.sort(key=lambda x: x['adjusted'], reverse=True); return filtered[:max_per_concept]; mentions=[{'file':'a.md','crs':8.0,'vault_freq':35},{'file':'b.md','crs':7.5,'vault_freq':35},{'file':'a.md','crs':6.0,'vault_freq':35},{'file':'c.md','crs':9.0,'vault_freq':15}]; selected=select_top_mentions(mentions); assert len(selected)==3; print(f'Selected {len(selected)} mentions')\""),
    ("phase6", "Link validation checks: mask intersection (0 violations), self-link prevention, density threshold (1 per 3000 words), duplicate check", "python3 -c \"def validate_link(source, target, anchor, existing, density): errors=[]; errors.append('self_link') if source==target else None; errors.append('duplicate') if (source, anchor) in existing else None; errors.append('density_exceeded') if density>1.0 else None; errors.append('invalid_syntax') if not (2<=len(anchor.split())<=6) else None; return {'valid':len(errors)==0,'errors':errors}; r=validate_link('a.md','b.md','liquidity provision',[],0.8); assert r['valid']; r2=validate_link('a.md','a.md','liquidity provision',[],0.8); assert not r2['valid']; print('Validation OK')\""),
    ("phase7", "Link application inserts forward links in [[Obsidian syntax]] and creates backlink sections on canonical pages", "python3 -c \"def apply_link(content, anchor, concept, line_num): lines=content.split('\\\\n'); has_backlink='## Related Mentions' in content; link_syntax=f'[[{concept}|{anchor}]]' if anchor!=concept else f'[[{concept}]]'; lines[line_num]=lines[line_num].replace(anchor, link_syntax); return {'content':'\\\\n'.join(lines),'has_backlink':has_backlink}; content='Liquidity provision is essential.\\\\n\\\\n## Related Mentions\\\\n'; r=apply_link(content, 'Liquidity provision', 'liquidity provision', 0); assert '[[liquidity provision]]' in r['content']; print('Link application OK')\""),
    ("phase8", "QA sampling evaluates 10% of links on 4 tests (relevance, specificity, target quality, readability) with 90% pass threshold", "python3 -c \"import random, math; def qa_sample(links, rate=0.10, threshold=0.90): size=max(10, int(len(links)*rate)); sample=random.sample(links, min(size, len(links))); results=[{'relevance':True,'specificity':True,'target_quality':True,'readability':True} for _ in sample]; passed=sum(1 for r in results if all(r.values())); return passed/len(results)>=threshold if results else False; links=[{'source':'a.md','target':'b.md'} for _ in range(100)]; assert qa_sample(links); print('QA sampling OK')\""),
    ("phase9", "Final report generates comprehensive statistics: link density (0.8-1.0 per 3000 words), average concept quality (≥8.0), QA pass rate (≥90%), concept registry JSON export", "python3 -c \"def generate_report(files, links, quality, qa, concepts): density=links/(files*0.3) if files>0 else 0; success=0.8<=density<=1.0 and quality>=8.0 and qa>=0.90; return {'density':round(density,2),'quality':quality,'qa':qa,'success':success}; r=generate_report(100,85,8.3,0.94,[]); assert r['success']; assert 0.8<=r['density']<=1.0; print('Report OK')\""),
]

conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

for category, description, verification_command in features:
    cur.execute(
        "INSERT INTO features (category, description, passes, verification_command) VALUES (?, ?, 0, ?)",
        (category, description, verification_command)
    )

conn.commit()
conn.close()

print(f"Inserted {len(features)} features")
