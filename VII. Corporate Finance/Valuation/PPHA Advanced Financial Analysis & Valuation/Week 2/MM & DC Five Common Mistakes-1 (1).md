---
parent_directory:
title: Methods to Improve Decisions
tags:
aliases:
parent_folder: Week 2
subfolder:
key_concepts:
cssclasses: academia
linter-yaml-title-alias: Methods to Improve Decisions
---

# Methods to Improve Decisions

Five Common Mistakes and How to Address Them

February 24, 2014

Authors

Michael J. Mauboussin

michael.mauboussin@credit-suisse.com

Dan Callahan, CFA

daniel.callahan@credit-suisse.com

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-30/0e6352d9-d213-4ef2-bce1-b1e26f1a46bf/c164823226894029ee0c41d834665e94851664fe4e303821430bd4d3d9abdf33.jpg)

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-30/0e6352d9-d213-4ef2-bce1-b1e26f1a46bf/2d9629a11f35d101d0c6b370906d7d48b929e7c2b430fc90d896de49251e4318.jpg)

"You gain more by not being stupid than you do by being smart. Smart gets neutralized by other smart people. Stupid does not."

Phil Birnbaum

This report covers five common mistakes that investment firms make and offers practical guidance on how to manage each of them.

# Mistake

1. Relying too much on the "inside view."  
2. Failure to consider a sufficient range of alternatives.  
3. Underestimating or underappreciating an opposing point of view.  
4. Not imposing self-accountability.  
5. Creating an environment that is not conducive to good decisions.

# Solution

Integrate the "outside view."

Conduct a premortem.

Create a team to challenge your mind-set.

Maintain a decision-making journal.

Be mindful of your surroundings and work to improve them.

# Introduction

In his wonderful book, *The Checklist Manifesto*, Atul Gawande, a surgeon at Brigham and Women's Hospital in Boston, explains the value of using a checklist to improve outcomes in a wide range of fields including medicine, aviation, construction, and investing. The power of a checklist is not that it improves your skill, but rather that it makes sure you apply your skill consistently.

Gawande interviews an investor who uses a checklist to improve his investment process. This investor believes he can reliably beat the market, encouraged by the experience of those in other fields who have seen remarkable improvements in results by adhering to a checklist. He notes, "… they improve their outcomes with no increase in skill. That's what we're doing when we use a checklist."²

One of the most common questions we hear from investors is: "How do I improve my decision-making process?" Most investors have a process—some are more formalized than others—and poor outcomes are frequently the result of deviating from that process either consciously or unconsciously. A checklist is valuable precisely because it compels you to hew to your process.

In his well-known essay, "The Loser's Game," Charles Ellis describes research by Simon Ramo on the game of tennis. Ramo, who is now 100 years old and appears to be going strong, has a PhD in physics from the California Institute of Technology and is the "R" of the old TRW Inc. His study of tennis, published in a book called Extraordinary Tennis for the Ordinary Player, revealed that the sport is really two games with one name. In pro tennis, the players win 80 percent of the points through superior skill. In ordinary tennis, the players lose 80 percent of the points by making mistakes.

Ramo's prescription for winning in ordinary tennis, the variety most of us play, is to simply make fewer errors than your opponent does. Instead of trying for a brilliant shot, just return the ball methodically and let your opponent err. He writes, "Ordinary tennis consists in large part of a wide assortment of errors that can be gloriously and exclusively claimed by the person who committed them."4 Ellis makes a similar point about other competitive endeavors such as warfare and golf.5

Markets are different from sports in that you are competing not against another individual or team but rather against the collective wisdom, or madness, of the crowd. But the main lesson remains: It's often easier to succeed by making fewer mistakes than it is by being more brilliant.

This report covers five common mistakes and offers concrete and actionable ways to manage them. You can think of these mistakes as the common reasons that investors veer from their investment process or cases where the processes themselves are incomplete.

# Mistake 1: Relying too much on the "inside view." Solution: Integrate the "outside view."

The results of an experiment by Roger Buehler, a professor of psychology, will come as no surprise to you if you have kids in school. Buehler asked college students to tell him the date by which they were 99 percent confident they would have completed a school assignment. For example, if he asked on a Monday, the students might respond that they were nearly certain to be done with their academic project by Friday.[6]

When Buehler checked on the predictions, he found that only 45 percent of the students had completed the tasks on the designated date. The students had confidence in their estimates that vastly surpassed their

accomplishments. This tendency to be overly optimistic happens in many settings, from judging how long it will take to remodel your kitchen to estimating the launch date for a new product to forecasting the cost of an infrastructure project.<sup>7</sup>

There is a natural way of thinking about plans and predictions, which psychologists call the "inside view." We gather lots of information, consider the specifics of the situation, and combine the two to create a scenario for the future. For instance, a student might consider how busy she is in the upcoming days, assess the difficulty of the assignment, and then figure that it's almost a sure thing that she'll have it done by Friday.

But there's another way to think about plans and predictions that doesn't come naturally to us but is more robust. Psychologists call this the "outside view." The outside view considers the problem as an instance of a larger reference class. Basically, the outside view imposes a fundamental question: "What happened when others were in this position before?"

Research shows that the inside view often yields predictions that are too optimistic, revealing a form of overconfidence. The outside view generally tempers that overconfidence and provides a much stronger foundation for thinking about how the future might unfold.

As a case in point, scientists asked venture capitalists (VCs) to describe a transaction they were working on, including an estimate of the expected rate of return. The average expected return was about 30 percent. The researchers then asked the VCs to consider two other deals that they deemed comparable. The rate of return for those deals was 20 percent. After having exposed the VCs to the outside view, albeit a small sliver of the total number of deals, 80 percent of the VCs revised down the expected rate of return for the focal deal.[9]

The contrast between the inside and outside view is a good way to frame the debate about intuition and statistics. Intuition works very well in a narrow number of domains, but people tend to rely on their intuition in cases where the statistics yield much better insight. When given a choice, start with statistics and then allow your intuition a say. Daniel Kahneman, a psychologist who won the Nobel Prize in economics, calls this "disciplined intuition." If you start with intuition and then turn to statistics, you will likely select the data that makes your case.

Psychologists have blamed overconfidence for lots of bad outcomes, including wars, strikes, and stock market bubbles. But according to one theory there are three varieties of overconfidence that derive from distinct mental mechanisms. The variety of overconfidence that is relevant here is "overprecision," defined as "an excessive certainty regarding the accuracy of one's beliefs."10 The outside view foils overprecision by compelling the decision maker to consider a range of outcomes consistent with the problem.

Simulations can also enhance the input from the outside view. Say the value of a particular company is sensitive to the price of a commodity. A simulator, loaded with a sensible distribution of prices for that commodity, can then generate a range of possible values for the company. The exercise is useful precisely because the simulator has no intuition about how the values should come out.

Daniel Kahneman and his collaborator for much of his pioneering work, Amos Tversky, offer a number of steps to institute the outside view:<sup>11</sup>

1. Choose an appropriate reference class. The goal is to find a reference class that is large enough to be statistically useful but sufficiently narrow to be applicable to the decision you face. In the world of investing and corporate performance, there is a rich amount of reference class data. Take patterns in return on invested capital (ROIC) as an example. If you know a company's current ROIC, you can

examine the trajectory of results for companies in a similar position. $^{12}$  Another illustration is mergers and acquisitions (M&A) for companies. There are a lot of data on M&A, and what leads to value creation (cash deals at modest premiums with substantial synergies) and value destruction (stock deals at large premiums with modest synergies).

2. Assess the distribution of outcomes. Not all outcomes follow a normal, bell-shaped distribution. For example, of the roughly 2,900 initial public offerings (IPOs) in technology since 1980, a small fraction of the companies have created the vast preponderance of the value. So while this is a relevant reference class, the outcomes are heavily skewed. This renders the conventional notion of average or standard deviation meaningless. Other distributions are not as wild and can provide very robust guidelines for forecasting.  
3. Make a prediction. With data from the reference class and knowledge of the distribution, make an estimate. At this juncture you should be ready to consider a range of probabilities and outcomes. Let's say you want to make a point estimate for the total shareholder return for the S&P 500 Index in 2014, something dozens of strategists actually do. Your forecast would appeal to the reference class, which is the past results for S&P 500 returns, and would examine the shape of the distribution of those returns. You could also consider valuation, which might lead to a forecast of returns that are below or above the historical average.  
4. Assess the reliability of your prediction and adjust as appropriate. This last step is a crucial one, as it takes into account how much you should regress your estimate toward the average. Statisticians have a term they call "reliability," which measures the correlation of the same metric over different periods of time. In cases where correlation is low, indicating low reliability, it is appropriate to regress your estimate to the mean substantially. Since the correlation of the returns for the S&P 500 is close to zero from year to year, any forecast of the S&P 500 for a single year should be close to the historical average.

Finding an appropriate reference class and integrating it into a forecast is not always easy, and researchers have given considerable attention to how to do it most effectively. $^{13}$  But you are more likely to make a mistake by ignoring the outside view than you are by misusing it.

In his latest book, *Left Brain, Right Stuff*, Phil Rosenzweig, a professor at IMD, makes a point worth considering. For some decisions, you can have no influence on the outcome. If you flip a coin and call heads, the coin will land on one side or the other without regard for your prediction. In the cases where the decision maker can influence the outcome, Rosenzweig argues that optimism may lead to better performance.[14]

Executives and investors generally think by analogy, searching their mental databases for prior cases that appear similar and, hence, instructive. This is the basis for the case study approach. Humans are very good at placing items in categories, and categories facilitate inductive reasoning. For instance, the statement, "Rabbits have Property X; therefore, Squirrels have Property X," is a stronger argument than, "Rabbits have Property X; therefore, Goldfish have Property X," because it is more natural to place rabbits and squirrels in the same category than it is to place rabbits and goldfish together.[15]

Yet reliance on case studies can be perilous for at least two reasons. First, research shows that when we look for similarities in two items that we compare, we see similarities. But when we look for differences, we see differences.

In the 1970s, Amos Tversky asked subjects which pair of countries they deemed more similar, West Germany and East Germany or Nepal and Ceylon (which changed its name to Sri Lanka in 1972). Two-thirds of the

subjects selected West Germany and East Germany. Tversky then asked subjects which pair of countries they deemed more different. Logic suggests an answer that is the complement of the first response, hence two-thirds finding Nepal and Ceylon more different. But that's not what Tversky found. Seventy percent of the subjects rated West Germany and East Germany more different than the other pair. What you are looking for dictates what you see.[16]

Second, our natural tendency to rely on categorization to reason inductively leads to a focus on attributes as opposed to circumstances. Attributes are features that allow for categorization. For instance, animals with wings and feathers can fly. Circumstances capture causal mechanisms. Since the physics of lift causes flight, animals or objects that can create lift will fly, including most birds and airplanes, and those that can't create lift won't fly. To learn from history, you need to understand causality.[17]

# Mistake 2: Failure to consider a sufficient range of alternatives.

# Solution: Conduct a premortem.

Most people are familiar with a postmortem, where doctors try to learn from the mistakes that led to a patient's death. This technique seeks to learn from the past. The "premortem," popularized by a psychologist named Gary Klein, is another technique that sharpens our decision making. Rather than using the past as a guide for the present, the premortem goes from the future to the present. This is a technique called "prospective hindsight."[18]

Here's the idea. Before you actually make a decision, launch yourself into the future, say one year from now, and pretend that you made the decision. Now assume the decision turned out poorly, and you must document the reasons for the failure. Prospective hindsight places you ahead in time and grants you the knowledge of the bad result. You can then think carefully about what went awry. This process taps into our natural facility to explain events for known outcomes.

Jay Russo and Paul Schoemaker, leading researchers on decision making, provide an example of how this process works.[19] Answer the following question:

How likely is it that a woman will be elected the leader of your country in the first election after the next one? Think about all the reasons why this might happen. For specificity, provide a numerical probability.

Now consider another version of the question, which uses prospective hindsight:

Imagine that the first election after the next one has occurred and a woman has been elected the leader of your country. Think about all the reasons why this might have happened. Then provide a numerical probability of this actually occurring.

Russo and Schoemaker find that the second version of the question generates a greater number of paths to the event, as well as a higher probability, than the first one. For instance, in one version of an experiment, they found that the subjects who used prospective hindsight generated 25 percent more reasons than those who did not use the technique. In addition, the reasons were more specific and more closely aligned with the scenario the subjects considered.

Gary Klein recommends a six-step process to do a premortem:20

1. Prepare. Participating team members should be relaxed with paper and pen in hand, and should be familiar with the decision that the group is contemplating.  
2. Imagine a fiasco. Klein recommends considering a worst case scenario. He suggests that you should consider the outcome embarrassing and devastating to the point where people on the team are unwilling to speak to one another. He then suggests that while this crystal ball is good enough to see the failure, it is too shoddy to make out the causes.  
3. Generate reasons for the failure. He asks each team member to spend three minutes writing down all the reasons behind the failure. The team members should do this step independently and silently.  
4. Consolidate the lists. After everyone is done, the facilitator goes around the room and asks each team member for one item from his or her list and records it on a white board. The process is not complete until every item on everyone's list is captured on the board. This should represent a comprehensive list of the concerns and issues.  
5. Revisit the plan. The team is now in a position to revisit the main concerns regarding the prospective decision. If there isn't sufficient time, the team can arrange another meeting to address ways to tackle the other problems.  
6. Periodically review the list. Klein suggests periodically taking out the list in order to keep the specter of failure fresh in the minds of the team members.

Klein didn't develop the premortem to help investors, but the utility should be clear. A premortem tempers overconfidence, reduces the risk of groupthink, and "unleashes the imagination of knowledgeable individuals in a much-needed direction."21

Some have criticized the premortem as being too negative or pessimistic. The central value in the process is correcting the tendency to be overconfident. So, indeed, it may be useful to run through this exercise using positive scenarios if your assumption is based on negative scenarios. The value of the task is that it draws on the way our minds work to counterbalance the potentially excessive confidence in our view.

# Mistake 3: Underestimating or underappreciating an opposing point of view. Solution: Create a red team to challenge your mind-set.

It is common for investment firms to position their portfolios to reflect a particular point of view or theme. For example, the leaders of the firm may be bullish or bearish on the overall market, risk-seeking or risk-averse to reflect a "risk-on" or "risk-off" environment, or exposed to a theme such as "interest rates are rising" or "interest rates are falling." These beliefs, called "mind-sets," need to be explicit. You can define a mind-set as "a series of expectations through which a human sees the world."[22]

Mind-sets can be good, of course, when they get everyone on the same page. But mind-sets are a problem if the world changes. As Richards Heuer, who worked in the intelligence office of the Central Intelligence Agency (CIA), wrote, "The disadvantage of a mind-set is that it can color and control our perception to the extent that an experienced specialist may be among the last to see what is really happening when events take

a new and unexpected turn. When faced with a major paradigm shift, analysts who know the most about a subject have the most to unlearn.[23]

Red-teaming is a technique to offset the rigidity of mind-sets. The idea is an old one that comes from military strategy. A red team attacks and a blue team defends. In this case, the blue team would be assigned to defend the mind-set that underpins the firm's portfolio. The red team would be a small number of people within the analytical team who would be charged with contesting the mind-set. Red-teaming allows for an explicit challenge to the mind-set within the firm, and at a minimum forces the team members to seriously consider an alternative point of view.

There are some helpful guidelines in setting up a red-team, blue-team exercise. The first is to structure the debate using "linchpin analysis," which requires multiple steps:[24]

1. Identify the main uncertain factors or key drivers (variables) that will determine an outcome.  
2. Pinpoint working assumptions (linchpin premises) about how the key drivers will operate.  
3. Advance convincing evidence and reasoning to support the linchpin premises.  
4. Address any indicators or signposts that would render the linchpin premises unreliable.  
5. Ask what dramatic events or triggers could reverse the expected outcomes.

A senior member of the investment team should assign three to four analysts to be on the red team. The people in this group should be diverse and credible and should not be the main advocates for the mind-set. Once the red team has had time to prepare the challenge based on linchpin analysis, the investment team can sit together and the red team can present the case against the status quo. As befitting the intellectual tradition of the exercise, the goal of the red team is to figure out a way to "defeat" the blue team.

One of the essential ground rules in a red-team, blue-team exercise is to explicitly separate facts from opinions. A fact is a piece of information that is presumed to have objective reality. As a consequence, it can be disproved. An opinion is a belief that is stronger than an impression but less strong than positive knowledge. An opinion can be difficult to disprove. Presenters from both sides must be overt about what are facts and what are opinions. Facts, which themselves can change over time, should rule the day.[25]

If you want to practice discriminating between facts and opinions, pick up a report or memo written by a research analyst and highlight the facts in one color and the opinions in another color. You might be surprised at the relative contributions of each. Decision making in the face of uncertainty is a great challenge, and opinion is likely to play a role. But when opinion overshadows fact, it is time to update beliefs.

There is substantial evidence showing that once we reach a mind-set, we are not inclined to change our view. This is even in cases where arriving at the mind-set was intellectually difficult. In theory, our beliefs are supposed to be tentative and subject to change upon the arrival of new information. Bayes's Theorem provides the mathematical way to do this.[26]

The primary barrier to updating beliefs is what psychologists call confirmation bias. This bias says that we are more likely to seek information that confirms our belief than information that disconfirms it. It also says that when we face ambiguous information, we naturally interpret it in a way that is favorable to our belief. Confirmation bias is relevant for military leaders, executives, and investors, among others.[27]

Whereas the outside view or premortem help anticipate scenarios for the future by overcoming overconfidence, red-teaming seeks to bend a mind-set that has become too rigid by revealing alternative analyses. Michael Handle, formerly a professor at the U.S. Naval War College, wrote: "Clearly, the majority of failures to anticipate strategic surprise can be correlated with conceptual rigidity and a high incidence of perceptual continuity."[28] This is true, too, in the worlds of business and investing.

# Mistake 4: Not imposing self-accountability.

# Solution: Maintain a decision-making journal.

One of the challenges to learning in markets or business is that the environment constantly changes. You can anticipate outcomes only with some probability. As a result, sometimes good decisions turn out poorly and bad decisions turn out well. Since our minds are biased to assume that the outcome reflects the level of skill, keeping track of the quality of our decisions is difficult.[29] Most investment professionals and businesspeople don't keep track of how good their decisions were. They keep track of how things turned out as the result of their decisions.

Over the long haul, of course, good decisions provide a much higher chance of desirable outcomes. But in the short run the link between decisions and results can be very loose. The primary way to focus attention on the decision-making process is to keep a journal that documents your thinking. This is how you impose accountability on yourself.

Here's what you do. Go out and get a notebook. When you are making a consequential decision in your portfolio, business, or life, write down what you expect to happen, why you expect it to happen, and attach probabilities to your views. If you are so inclined, also jot down how you feel physically and emotionally. Make sure you note the date and time.

This practice is valuable because it mitigates some common cognitive traps. The first of these is hindsight bias, the sense that you knew what was going to happen, before the event occurred, with a greater probability than you actually did. Creeping determinism is a related trap. This is the name for the sense that what happened was inevitable. In both cases, your mind draws out the facts around an event that occurs and weaves a narrative to explain the result. You do this unconsciously and effortlessly. Knowledge of the outcome and the facts behind it bleed into your memory, and you start to believe that you knew more than you did.

Your decision-making journal stops this process in its tracks. Because your prior views are stated clearly, you can't reconstruct your beliefs in a faulty fashion. You also create an opportunity to learn about your own tendencies. You can keep score.

When maintaining your journal, it is essential to describe your views using specific probabilities. For instance, saying "there's a 70 percent chance of rain tomorrow" is a lot better than saying "there's a good chance it will rain tomorrow." There are at least two reasons to express your expectations with numerical probabilities.

The first reason is that language can be very ambiguous. Sherman Kent, the initial director of the CIA Office of National Estimates, asked 23 military officers to assign specific probabilities to various statements. For example, some statements, such as "almost certainly," consistently drew a probability of 80 percent or higher. Likewise, "chances are slight" generally meant a probability of 10 percent or less. But some statements had a wide range of assigned probabilities. "Probable" evoked a range from 25 to 90 percent. A statement that is so vague to its recipients is next to useless.[30]

Specific probabilities in your journal also allow you to keep score. This takes even more discipline, but can provide essential feedback. The Brier score is a classic way to measure the accuracy of probabilistic forecasts. $^{31}$  In its simplest form, a Brier score is the square of the error, where everything is expressed in percentages. For example, if you predict that it will rain tomorrow with 100 percent probability and it does, then the Brier score is zero  $(1.00 - 1)^{2}$ . A zero Brier score is a perfect forecast. If you predict rain tomorrow with 100 percent probability and it doesn't rain, then the Brier score is  $1.00(1.00 - 0)^{2}$ . A Brier score of one is the worst possible score.

We can consider a slightly more complicated case. Say you predict rain with an 80 probability and it rains. Your Brier score is  $0.04(0.80 - 1)^2$ . If you predict rain with an 80 percent probability and it doesn't rain, then your Brier score is  $0.64(0.80 - 0)^2$ . A Brier score is like golf in that the lower the number the better your ability.

In forecasting, there are two key measures of accuracy. The first is calibration, which captures how well your subjective probabilities match the objective probabilities over time. To illustrate, if it rains 70 percent of the days when you predict a 70 percent chance of rain, you are well calibrated. This gauges whether you have the appropriate humility.

The second measure is discrimination, which asks whether over the long haul you assign higher probabilities to things that actually occur. Lots of forecasts of 100 percent probability before rainy days and zero percent probability before sunny days would demonstrate good discrimination. This measure captures justified decisiveness.[32]

Calibration and discrimination tend to be positively correlated, but they can diverge. Consider the simple prediction that it rains 50 percent of the days in London. This is close to the actual percentage of days that it rains, and hence would be well calibrated over time.

But for planning picnics, you need discrimination. High discrimination would be a series of accurate predictions of rain where roughly half were zero percent probability and the other half 100 percent probability. In this case, both calibration and discrimination would be high, and the predictions would be very useful.

Besides their work on heuristics and biases, Kahneman and Tversky are also known for prospect theory. This theory describes how the choices people make depart from normative economic theory when the decisions are in probabilistic settings and involve risk. For example, most people are loss averse, which means that they suffer from a loss roughly 2.0 - 2.5 times as much as they enjoy a comparable gain.[33]

After publishing on prospect theory, Kahneman and Tversky sought to quantify the psychological weights, called "decision weights," which people placed on different types of financial propositions. Exhibit 1 shows the results. Were subjective and objective probabilities to line up in a way consistent with theory, all decisions would land on the line at a 45 degree angle.

Exhibit 1: Decision Weights versus Objective Probabilities for Financial Propositions  
![](https://cdn-mineru.openxlab.org.cn/result/2025-11-30/0e6352d9-d213-4ef2-bce1-b1e26f1a46bf/7a21c1ea81fb38e79d0f2986a6fae0d39d3de40a5eca424a5275eeafb4a21b3d.jpg)  
Source: Daniel Kahneman, Thinking, Fast and Slow (New York: Farrar, Straus and Giroux, 2011), 315.

In general, subjects offer accurate weights at the far extremes but have trouble in between. For instance, they tend to overweight low-probability events. Imagine you have a 1 percent chance of winning $1 million, and you'll know the outcome tomorrow. You have some hope, but it is slim. Subjects place a decision weight of 5.5 percent on a 1 percent objective probability. Kahneman calls this the "possibility effect."

Subjects also tend to underweight high probability events. Now let's say you have a 1 percent chance of not winning the $1 million. Subjects assign a decision weight of 91.2 percent on a 99 percent objective probability. Anxiety over the possibility of losing is more salient than the hope of winning. Kahneman calls this the "certainty effect." These patterns in decision weights can be highly relevant in setting probabilities, especially near the extremes.[34]

A journal that chronicles your decisions allows you to get honest feedback about your thoughts and provides valuable material to help sharpen your forecasts. Our best advice is to be disciplined in maintaining your journal, to document your views in probabilities, and to periodically review your predictions and score yourself. Note the essential distinction between calibration and discrimination.

# Mistake 5: Creating an environment that is not conducive to good decisions.

# Solution: Be mindful of your surroundings and work to improve them.

One idea that is well established in social psychology is the fundamental attribution error, or correspondence bias. This idea says that when we observe the behavior of others, we attribute that behavior to the individual's disposition and not to the situation. As important, there is substantial evidence that shows that the situation exerts a very powerful influence on the decisions that people make.

The Stanford Prison Experiment, conducted by a psychologist named Philip Zimbardo in the summer of 1971, is one of the most chilling experimental demonstrations of this point. Zimbardo converted part of the building housing the psychology department at Stanford University into a prison. He then found 24 physically and

mentally healthy male subjects to participate in the study. With the toss of a coin, he randomly assigned half of them to be "prisoners" and the other half "guards." Zimbardo assumed the role of "superintendent."35

With the help of the Palo Alto police, the prisoners were arrested and subsequently de-humanized and deindividualized. The guards were free, within limits, to do whatever they deemed necessary to maintain law and order. Only a few days into the planned two-week duration, Zimbardo had to call off the experiment. The prisoners had become depressed and showed signs of extreme stress, and the guards had become sadistic.

Zimbardo provides a day-by-day account of the events in his book, *The Lucifer Effect*.36 While his experiment is an extreme example of this phenomenon, Zimbardo notes that the same conditions were in place for other cases of bad behavior, including the abuse of prisoners at Abu Ghraib, Iraq, in 2003 and 2004. In the appendix, we summarize some of Zimbardo's recommendations for resisting the sway of bad social influences.

Most organizations don't find themselves in situations as extreme as the Stanford Prison Experiment, but the mistake of creating an environment that is less than ideal for quality decision making is prevalent nonetheless. One of the essential lessons from the fundamental attribution bias is that social context plays a major role in shaping decisions, and we tend to underestimate that role.

The first step in creating an environment favorable to good decisions is to audit the congruence between your stated process and your actual behavior. There are lots of processes that may lead to attractive portfolio results, from certain strategies that rely on rapid trading to low turnover of highly concentrated portfolios. But many firms stray from the essence of their process as the result of external pressure.

For example, some investment firms that claim to have a long-term orientation focus disproportionately on the short term following a spell of poor results. Others claim to use a fundamental approach yet use charts to time trades. It is essential to align what you say you do with what you actually do. This is where checklists can act as guardrails to keep the organization consistent and true to its principles.

Leaders of investment organizations must be particularly attuned to the environment they create. The role of stress is a good example. Some stress is good, of course, as it activates the body and mind and encourages focus. But too much stress is bad and causes the quality of decisions to deteriorate rapidly.[37]

Robert Sapolsky, a professor of biological sciences at Stanford University, is one of the world's foremost researchers on stress. $^{38}$  He suggests that for most of the animal world, stressors are generally physical: You're a zebra who becomes a lion's target for lunch. In those cases, the stress response kicks in and it's fight-or-flight. But once the emergency has passed, the body returns to its normal state. The stress response is extreme but short-lived.

Humans face physical stressors from time to time as well. But most of our stressors are psychological, including dealing with relationships, the big speech next week, and deadlines on the job. What is essential is that our bodies don't distinguish between physical and psychological stressors. We have the same reactions. Chronic psychological stress puts your body in a constant state of emergency.

To focus on the present, the stress response turns on short-term systems and turns off long-term systems. These include the digestive, immune, and reproductive systems. There's no use allocating resources to digest lunch or fend off disease if you are not long for this world. As a result, symptoms of chronic stress include ulcers, a higher likelihood of getting sick, and reproductive problems.

Here's the essential link back to investment management: Stress creates a focus on the short-term and makes long-term thinking next to impossible. This makes enormous sense from an evolutionary point of view. After all, the stress response evolved to help you elude danger. But it can be devastating to an organization that seeks to make investments that take years to pay off.

Most are familiar with the formula for reducing stress, which is easier said than done. Items include eating well, sleeping sufficiently, exercising frequently, and maintaining social connections (family, friends, and religious gatherings). Good leaders of investment organizations have an even keel. They don't get too excited when results are good or too despondent when results are challenging.

If you lead an investment team, you may want to evaluate the environment you have created across a few dimensions. Ask these questions:

1. Does the analytical team have access to, and avail themselves of, base rate data so as to properly use the outside view?  
2. As an organization, are we open to new ideas that may challenge our mind-sets? Do we need to do a red-term exercise to confront our beliefs?  
3. Are we always explicit about distinguishing between facts and opinions? Are we properly weighting the two?  
4. Are we structured so that we can keep track of the quality of our decisions—our process—as well as our outcomes? Are we communicating using probabilities instead of statements? How good are we at providing feedback?  
5. Do we have the correct amount of stress in our organization? Have we had episodes where we've veered toward too much stress, hence affecting our decisions?

# Summary

No investment organization is perfect, and almost all seek to improve. There are a couple of paths to improvement. One is to get smarter and the other is to be less stupid. This report covered five common mistakes that investment firms make and offered practical guidance on how to cope with each of them. Each solution relies not on getting smarter but rather on fending off poor practices that you can fix.

# Endnotes

<sup>1</sup> Phil Birnbaum, "Eliminating Stupidity Is Easier than Creating Brilliance," *Sabermetric Research Blog*, June 13, 2013. See <http://blog.philbirnbaum.com/2013/06/eliminating-stupidity-is-easier-than.html>.  
2 Atul Gawande, The Checklist Manifesto: How to Get Things Right (New York: Metropolitan Books, 2009), 168.  
<sup>3</sup> Charles D. Ellis, "The Loser's Game," *Financial Analysts Journal*, Vol. 31, No. 4, July-August 1975, 19-26.  
$^{4}$  Simon Ramo, Extraordinary Tennis for the Ordinary Player (New York: Crown Publishers, 1973), 22.  
Ellis quotes Admiral Samuel Eliot Morison, who said, "In warfare, mistakes are inevitable. Military decisions are based on estimates of the enemy's strengths and intentions that are usually faulty, and on intelligence that is never complete and often misleading. Other things being equal, the side that makes the fewest strategic errors wins the war." Tommy Armour, a professional golfer, wrote, "The best way to win is by making fewer bad shots."  
$^{6}$  Roger Buehler, Dale Griffin, and Michael Ross, "Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions," in Thomas Gilovich, Dale Griffin, and Daniel Kahneman, eds., Heuristics and Biases: The Psychology of Intuitive Judgment (Cambridge, UK: Cambridge University Press, 2002), 250-270.  
7 Bent Flyvbjerg, "Truth and Lies about Megaprojects," Speech at Delft University of Technology, September 26, 2007.  
Daniel Kahneman, Thinking, Fast and Slow (New York: Farrar, Straus and Giroux, 2011), 245-254. Also, Dan Lovallo and Daniel Kahneman, "Delusions of Success," Harvard Business Review, July 2003, 56-63.  
<sup>9</sup> Dan Lovallo, Carmina Clarke, and Colin Camerer, "Robust Analogizing and the Outside View: Two Empirical Tests of Case-Based Decision Making," *Strategic Management Journal*, Vol. 33, No. 5 May 2012, 496-512.  
<sup>10</sup> Don Moore and Paul J. Healy, "The Trouble with Overconfidence," Psychological Review, Vol. 115, No. 2, April 2008, 502-517.  
This section is based on Michael J. Mauboussin, Think Twice: Harnessing the Power of Counterintuition (Boston, MA: Harvard Business Review Press, 2011), 13-16.  
$^{12}$  Michael J. Mauboussin, Dan Callahan, Bryant Matthews, and David A. Holland, "How to Model Reversion to the Mean: Determining How Fast, and to What Mean, Results Revert," Credit Suisse Global Financial Strategies, September 17, 2013, 11.  
13 Lovallo, Clarke, and Camerer.  
$^{14}$  Phil Rosenzweig, Left Brain, Right Stuff: How Leaders Make Winning Decisions (New York: PublicAffairs, 2014), 112-118.  
<sup>15</sup> Evan Heit, "Features of Similarity and Category-Based Induction," Proceedings of the Interdisciplinary Workshop on Similarity and Categorization, 1997, 115-121.  
<sup>16</sup> Amos Tversky, "Features of Similarity," Psychological Review, Vol. 84, 1977, 327-352. Reprinted in Eldar Shafir, ed. Preference, Belief, and Similarity: Selected Writings, Amos Tversky (Cambridge, MA: MIT Press, 2004).  
<sup>17</sup> Paul R. Carlile and Clayton M. Christensen, "The Cycles of Theory Building in Management Research," Harvard Business School Working Paper Series, No. 05-057, 2005. Also, see the chapter, "History, The Fickle Teacher," in Duncan J. Watts, Everything Is Obvious*: *Once You Know the Answer* (New York: Crown Business, 2011), 108-134.  
<sup>18</sup> Deborah J. Mitchell, J. Edward Russo, and Nancy Pennington, "Back to the Future: Temporal Perspective in the Explanation of Events," Journal of Behavioral Decision Making, Vol. 2, No. 1, January/March 1989, 25-38.  
<sup>19</sup> J. Edward Russo and Paul J. H. Schoemaker, Winning Decisions: Getting it Right the First Time (New York: Currency, 2002), 111-112. There is also a clear discussion of this concept in Chip Heath and Dan Heath, Decisive: How to Make Better Choices in Life and Work (New York: Crown Business, 2013), 201-203.

20 Gary Klein, Intuition at Work: Why Developing Your Gut Instincts Will Make You Better at What You Do (New York: Currency, 2003), 88-91. Also, Gary Klein, "Performing a Project Premortem," Harvard Business Review, September 2007, 18-19.  
21 Kahneman, 265.  
22 Roger Z. George, "Fixing the Problem of Analytical Mind-Sets: Alternative Analysis," International Journal of Intelligence and Counterintelligence, Vol. 17, No. 3, 2004, 385-404.  
23 Richards J. Heuer, Jr., Psychology of Intelligence Analysis (Washington, D.C: Center for the Study of Intelligence, 1999), 5.  
24 George, 391.  
[25] Sam Arbesman argues that facts have a half-life, which differs by field. Awareness of how rapidly facts can change is very useful. See Samuel Arbesman, The Half-life of Facts: Why Everything We Know Has an Expiration Date (New York: Current, 2012).  
26 James V Stone, Bayes' Rule: A Tutorial Introduction to Bayesian Analysis (Sebel Press, 2013).  
27 Chetan Dave and Katherine W. Wolfe, "On Confirmation Bias and Deviations From Bayesian Updating," Working Paper, March 21, 2003.  
28 Michael I. Handle, War, Strategy, and Intelligence (New York: Frank Cass and Company, 1989), 270.  
29 Michael J. Mauboussin and Dan Callahan, "Outcome Bias and the Interpreter: How Our Minds Confuse Skill and Luck," Credit Suisse Global Financial Strategies, October 15, 2013.  
30 Heuer, 155.  
31 Glenn W. Brier, "Verification of Forecasts Expressed in Terms of Probability," Monthly Weather Review, Vol. 78, No. 1, January 1950, 1-3.  
$^{32}$  Philip E. Tetlock, *Expert Political Judgment: How Good Is It? How Can We Know?* (Princeton, NJ: Princeton University Press, 2005) 51-54.  
33 Daniel Kahneman and Amos Tversky, "Prospect Theory: An Analysis of Decision under Risk," Econometrica, Vol. 47, No. 2, March 1979, 263-291.  
34 Kahneman, 314-316.  
35 See http://www.prisonexp.org/.  
36 Philip Zimbardo, The Lucifer Effect: Understanding How Good People Turn Evil (New York: Random House, 2007).  
37 John Coates, The Hour Between Dog and Wolf: Risk Taking, Gut Feelings, and the Biology of Boom and Bust (New York: The Penguin Press, 2012).  
<sup>38</sup> Robert Sapolsky, *Why Zebras Don't Get Ulcers: A Guide to Stress*, Stress-Related Disease, and Coping (New York: W.H. Freeman & Co., 1994).  
39 http://www.lucifereffect.com/guide_tenstep.htm.

# Appendix

# Zimbardo's Ten-Step Program to Build Resistance and Resilience<sup>39</sup>

Philip Zimbardo provides ten steps to help resist undesirable social influence. The steps also promote resilience and civic virtue.

1. Admit your mistakes, say you're sorry, and if appropriate ask for forgiveness. Failure to admit mistakes can compound problems.  
2. Be mindful by being in the moment and heed situational clues. Pay attention to the things you do automatically and ask whether they make sense.  
3. Take responsibility for your actions. Diffusion of responsibility can allow unwelcome social influence to take root.  
4. Maintain your individuality by making sure no one places you into a category. Anonymity can conceal wrongdoing and undermines human connection.  
5. Respect just authority and rebel against unjust authority. Sort the real leaders who deserve respect from the false leaders who claim authority without substance.  
6. Balance group acceptance and independence. We are social animals and enjoy the comfort of the group. But sometimes conformity is bad for the social good. Never sacrifice personal beliefs or standards to conform to the group.  
7. Pay attention to how others frame ideas and be "frame vigilant." Be aware of how words or the presentation of concepts shape your decisions. As Zimbardo writes, he "who makes the frame becomes the artist, or the con artist."  
8. Consider the past and the future when dealing with the present. Adopting the outside view can help you with incorporating the past, and premortems and analysis of costs and benefits can help with the future.  
9. Try not to sacrifice personal freedom for the illusion of security. The sacrifices tend to be real in the present but the security is illusory in the future.  
10. Oppose unjust systems when you identify them as such. These include cults, gangs, families, and even corporations.

# General disclaimer / Important information

This document was produced by and the opinions expressed are those of Credit Suisse as of the date of writing and are subject to change. It has been prepared solely for information purposes and for the use of the recipient. It does not constitute an offer or an invitation by or on behalf of Credit Suisse to any person to buy or sell any security. Nothing in this material constitutes investment, legal, accounting or tax advice, or a representation that any investment or strategy is suitable or appropriate to your individual circumstances, or otherwise constitutes a personal recommendation to you. The price and value of investments mentioned and any income that might accrue may fluctuate and may fall or rise. Any reference to past performance is not a guide to the future.

The information and analysis contained in this publication have been compiled or arrived at from sources believed to be reliable but Credit Suisse does not make any representation as to their accuracy or completeness and does not accept liability for any loss arising from the use thereof. A Credit Suisse Group company may have acted upon the information and analysis contained in this publication before being made available to clients of Credit Suisse. Investments in emerging markets are speculative and considerably more volatile than investments in established markets. Some of the main risks are political risks, economic risks, credit risks, currency risks and market risks. Investments in foreign currencies are subject to exchange rate fluctuations. Before entering into any transaction, you should consider the suitability of the transaction to your particular circumstances and independently review (with your professional advisers as necessary) the specific financial risks as well as legal, regulatory, credit, tax and accounting consequences. This document is issued and distributed in the United States by Credit Suisse Securities (USA) LLC, a U.S. registered broker-dealer; in Canada by Credit Suisse Securities (Canada), Inc.; and in Brazil by Banco de Investimentos Credit Suisse (Brasil) S.A.

This document is distributed in Switzerland by Credit Suisse AG, a Swiss bank. Credit Suisse is authorized and regulated by the Swiss Financial Market Supervisory Authority (FINMA). This document is issued and distributed in Europe (except Switzerland) by Credit Suisse (UK) Limited and Credit Suisse Securities (Europe) Limited, London. Credit Suisse Securities (Europe) Limited, London and Credit Suisse (UK) Limited, authorised by the Prudential Regulation Authority (PRA) and regulated by the Financial Conduct Authority (FCA) and PRA, are associated but independent legal and regulated entities within Credit Suisse. The protections made available by the UK's Financial Services Authority for private customers do not apply to investments or services provided by a person outside the UK, nor will the Financial Services Compensation Scheme be available if the issuer of the investment fails to meet its obligations. This document is distributed in Guernsey by Credit Suisse (Guernsey) Limited, an independent legal entity registered in Guernsey under 15197, with its registered address at Helvetia Court, Les Echelons, South Esplanade, St Peter Port, Guernsey. Credit Suisse (Guernsey) Limited is wholly owned by Credit Suisse and is regulated by the Guernsey Financial Services Commission. Copies of the latest audited accounts are available on request. This document is distributed in Jersey by Credit Suisse (Guernsey) Limited, Jersey Branch, which is regulated by the Jersey Financial Services Commission. The business address of Credit Suisse (Guernsey) Limited, Jersey Branch, in Jersey is: TradeWind House, 22 Esplanade, St Helier, Jersey JE2 3QA. This document has been issued in Asia-Pacific by whichever of the following is the appropriately authorised entity of the relevant jurisdiction: in Hong Kong by Credit Suisse (Hong Kong) Limited, a corporation licensed with the Hong Kong Securities and Futures Commission or Credit Suisse Hong Kong branch, an Authorized Institution regulated by the Hong Kong Monetary Authority and a Registered Institution regulated by the Securities and Futures Ordinance (Chapter 571 of the Laws of Hong Kong); in Japan by Credit Suisse Securities (Japan) Limited; elsewhere in Asia/Pacific by whichever of the following is the appropriately authorized entity in the relevant jurisdiction: Credit Suisse Equities (Australia) Limited, Credit Suisse Securities (Thailand) Limited, Credit Suisse Securities (Malaysia) Sdn Bhd, Credit Suisse AG, Singapore Branch, and elsewhere in the world by the relevant authorized affiliate of the above.

This document may not be reproduced either in whole, or in part, without the written permission of the authors and CREDIT SUISSE.

© 2014 CREDIT SUISSE GROUP AG and/or its affiliates. All rights reserved